hello welcom to the cours in text retriev and search engin .
i'm cheng xiang zhai .
i have a nicknam cheng .
i'm a professor of the depart of comput scienc at the univers of illinoi at urbana champaign .
thi first lectur is a basic introduct to the cours .
a brief introduct to what we we'll cover in the cours .
we're go to first talk about the data mine special sinc thi cours is part of that special .
and then we'll cover motiv object of the cours .
thi will be follow by pre requisit and cours format and refer book .
and then final we'll talk about the cours schedul , which ha number of topic to be cover in the rest of thi cours .
so the data mine special offer by the univers of illinoi at urbana champaign is realli to address the need for data mine techniqu to handl a lot of big data , to turn the big data into knowledg .
there ar five lectur base cours , as you see on the slide .
plu on capston , project cours in the end .
i'm teach two of them which is thi cours , text retriev and search engin and thi on .
so the two cours that i cover here ar all about the text data .
in contrast , the other cours ar cover more gener techniqu that can be appli to all kind of data .
so patent discoveri taught by the professor jowi han and cluster analysi again taught by him about the gener data mine techniqu to handl structur .
the end and structur text data .
and data mine , data visual cover by professor jung hart is about the gener visual techniqu .
again applic to all kind of data .
so the motiv for thi cours .
in fact also for the other cours that i'm teach is that we have a lot of text data .
and the data is everywher , is grow rapidli , so you must have been experienc thi growth .
just think about how much text data you're deal with everi dai .
i list some data type here , for exampl , on the internet we see a lot of web page , new articl etcetera .
and then we have block articl , email , scientif literatur , tweet , as well speak , mayb a lot of tweet ar be written , and a lot of email ar , ar be sent .
so , the amount of text data is beyond our capac to understand them .
also , the amount of data make it possibl to actual analyz the data to discov interest knowledg and that's what we meant by , har big text data .
text data is veri special .
in contrast to the data captur by machin such as sensor , text data is produc by human .
and thei also ar meant to be consum by human .
and thi ha some interest consequ .
becaus it is produc by human , it tend to have a lot of us knowledg about peopl's' prefer , peopl's' opinion about everyth .
and that make it possibl to mine text data to discov those latent prefac of peopl , which could be veri us to build an intellig system to help peopl .
you can think about scientif literatur or so and it's a wai to encod our knowledg about the world .
so it's veri high qualiti content , yet we have difficulti digest all the content .
now as a result of the fact that text is consum by we human , we also need intellig softwar tool to help peopl digest the content , or otherwis we'd miss a lot of us content .
thi slide show that the human realli plai import role in test data mine .
we have to consid human in the loop , and we have to consid the fact that the text is gener by human .
so , here ar some exampl of us text inform system .
thi is by no mean a complet list of all applic .
i categor them into differ categori .
but you can probabl imagin other kind of applic .
so let's take a look at some of them .
search for exampl , we all know search engin is special .
web search engin , ipad , all of you ar us googl , or bing , or anoth web search engin all the time .
and we also have live research assist .
and in fact , wherev you have a lot of text data , you would have a search engin .
so for exampl , you might have a search box on your laptop .
all right , to search content on your comput .
so that's on kind of applic system , but we also have filter system or recommend system .
those system can push inform to user .
thei can recommend us inform to user .
so again , us filter , spam filter .
literatur the movi recommend .
now not of them ar necessari recommend the inform to you .
for exampl email filter , spam email filter , thi is actual to filter out the spam from your inbox , all right .
but in natur these ar similar system in that thei have to make a binari decis regard whether to retain a particular document or discard it .
anoth kind of system ar categor system .
so for exampl , in handl email , you might prefer automat , sorter that would automat sort incom email into a proper folder that you creat .
or we might want to categor product review into posit or neg .
new agenc might be interest in categor new articl into all kind of subject categori .
those ar all categor system .
final there ar also system that might do more analysi .
and oh , you can sai mine text data .
and these can be text mine system or inform extract system , and thei can be us to analyz text data in more detail to discov potenti us knowledg .
for exampl compani might be interest in discov major complaint from their custom base on the email messag that the , thei have receiv from the custom .
all right , so have a system to support that would realli help improv their product and the custom relat .
also in busi , intellig compani ar often interest in analyz product review to understand the rel strength of their own product in comparison with competitor .
and , and so these ar all exampl of these test mine system .
we have a lot of data in particular literatur data .
so , there's also great opportun of us comput system to analyz the data to automat read literatur , and to gain knowledg , and to help biologist make discoveri .
and you can imagin mani other .
so the point is that with so much text data , we can build veri us system to help peopl in mani differ wai .
now how do we build thi system ?
well these actual ar the main technolog that we'll be talk about in thi cours and the other cours that i'm teach for thi special .
the main techniqu for build these system and also for har the text data ar text retriev and text data mine .
so i us thi pictur to show the relat between these two some of the differ techniqu .
we start with big text data , right ?
but for ani applic , we don't necessarili need to us all the data .
often we onli need the small subset of the most relev data , and that's shown here .
so text retriev is to convert big , raw text data into that small subset of most relev data that ar most us for a particular applic .
and thi is usual done by search engin .
and so thi will be cover in thi cours .
after we have got a small amount of relev data , we also need to further analyz the data to help peopl digest the data , or to turn the data into action knowledg .
and thi step is call text mine , where we us a number of techniqu to mine the data to get us knowledg or pair .
and the knowledg can then be us in mani differ applic .
and thi part , text mine , will be cover in the other cours that i'm teach call text mine and analyt .
the emphasi of thi cours is on basic concept and practic techniqu in text retriev .
more specif we will cover how search engin work .
how to implement a search engin .
how to evalu a search engin , so that you know on search engin is better than anoth or on method is better than anoth .
how to improv and optim a search engin system .
and how to build a recommend system .
we also hope to provid a hand on experi on multipl aspect .
on is to creat a test collect for evalu search engin .
thi is veri import for know which techniqu actual work well .
and whether your search engin system is realli good for your applic .
the other aspect is to experi with search engin algorithm .
in practic , you will have to face choic of differ algorithm .
so it's import to know how to compar them and to figur out how thei work or mayb potenti , how to improv them .
and final , we'll provid a platform for you to do search engin competit .
where you can compar your differ idea to see which idea work better on some data set .
the prerequisit for thi cours ar minimum .
basic we hope you have some basic concept of comput scienc , for exampl data structur .
and we hope you will be comfort with program , especi in c .
becaus that's the languag that we'll us for some of the program assign .
the format is lectur plu quizz , as often happen in mooc .
and we also will provid a program assign for those of you that have the resourc to do that .
we don't realli have ani requir read for thi cours .
that just mean if you follow all the lectur video carefulli , and you're suppos to know all the basic concept and the basic techniqu .
but it's alwai us to read more , so here we provid a list of some us refer book .
and thi in time order , and that also includ a book that and i ar co author now , and we make some draft chapter avail on thi websit .
and we can find more read and refer book on thi websit .
final , and thi is the cours schedul .
that's just the top of the map for the rest of the cours , and it show the topic that we will cover in the remain lectur .
thi pictur also show basic flow of inform in a text inform system .
so start from the big text data , the first step is to do some natur languag content analysi , becaus text data is in the form of natur languag text .
so we need to understand the text to some extent in order to do someth us for the user .
so thi is the first topic that we will cover .
and then on top of that as you can see there ar two box here .
those ar two type of system that can be us to help peopl get access to the most relev data .
or in other word , those ar the two kind of system that will convert big text data into small relev text data .
search engin ar help user to search or to queri the data to get the most relev document out .
recommend system ar to recommend inform to user , to push inform to user .
so those ar two , complementari wa of get user connect to the most relev data at the right time .
so thi part is call text access , and thi will be the next topic .
and after we cover that we ar go to cover a number of topic , all about the search engin .
now the text access topic is a brief topic , a brief coverag of the two kind of system .
in the remain topic , we'll cover search engin in much more detail .
that includ text retriev problem , text retriev method , how to evalu these method , implement of the system , and web search applic .
and after these , we're go to go cover the recommend system .
so thi is what you expect in the rest of thi cours .
thank .
thi lectur is about natur languag content analysi .
as you see from thi pictur , thi is realli the first step to process ani text data .
text data ar in natur languag .
so , comput have to understand natur languag to some extent in order to make us of the data , so that's the topic of thi lectur .
we're go to cover three thing .
first , what is natur languag process , which is a main techniqu for process natur languag to obtain understand ?
the second is the state of the art in nlp , which stand for natur languag process .
final , we're go to cover the relat between natur languag process and text retriev .
first , what is nlp ?
well , the best wai to explain it is to think about , if you see a text in a foreign languag that you can't understand .
now , what you have to do in order to understand that text ?
thi is basic what comput ar face .
right ?
so , look at the simpl sentenc like , a dog is chase a boi on the playground .
we don't have ani problem understand thi sentenc , but imagin what the comput would have to do in order to understand it .
for in gener , it would have to do the follow .
first , it would have to know dog is a noun , chase's a verb , et cetera .
so , thi is a code lexil analysi or part of speech tag .
and , we need to pick out the , the syntax categori of those word .
so , that's a first step .
after that , we're go to figur out the structur of the sentenc .
so for exampl , here it show that a and dog would go togeth to form a noun phrase .
and , we won't have dog and is to go first , right .
and , there ar some structur that ar not just right .
but , thi structur show what we might get if we look at the sentenc and try to interpret the sentenc .
some word would go togeth first , and then thei will go togeth with other word .
so here , we show we have noun phrase as intermedi compon and then verb phrase .
final , we have a sentenc .
and , you get thi structur , we need to do someth call a syntact analysi , or pars .
and , we mai have a parser , a comput program that would automat creat thi structur .
at thi point , you would know the structur of thi sentenc , but still you don't know the mean of the sentenc .
so , we have to go further through semant analysi .
in our mind , we usual can map such a sentenc to what we alreadi know in our knowledg base .
and for exampl , you might imagin a dog that look like that , there's a boi and there's some activ here .
but for comput , will have to us symbol to denot that .
all right .
so , we would us the symbol d1 to denot a dog .
and , b1 to denot a boi , and then p1 to denot the playground , playground .
now , there is also a chase activ that's happen here , so we have the relat chase here , that connect all these symbol .
so , thi is how a comput would obtain some understand of thi sentenc .
now from thi represent , we could also further infer some other thing , and we might inde , natur think of someth els when we read text .
and , thi is call infer .
so for exampl , if you believ that if someon's be chase and thi person might be scare .
all right .
with thi rule , you can see comput could also infer that thi boi mai be scare .
so , thi is some extra knowledg that you would infer base on some understand of the text .
you can even go further to understand the , why the person said thi sentenc .
so , thi ha to do with the us of languag .
all right .
thi is call pragmat analysi .
in order to understand the speech actor of a sentenc , all right , we sai someth to basic achiev some goal .
there's some purpos there and thi ha to do with the us of languag .
in thi case , the person who said the sentenc might be remind anoth person to bring back the dog .
that could be on possibl intent .
to reach thi level of understand , we would requir all these step .
and , a comput would have to go through all these step in order to complet understand thi sentenc .
yet , we human have no troubl with understand that .
we instantli , will get everyth , and there is a reason for that .
that's becaus we have a larg knowledg base in our brain , and we us common sens knowledg to help interpret the sentenc .
comput , unfortun , ar hard to obtain such understand .
thei don't have such a knowledg base .
thei ar still incap of do reason and uncertainti .
so , that make natur languag process difficult for comput .
but , the fundament reason why the natur languag process is difficult for comput is simpl becaus natur languag ha not been design for comput .
thei , thei , natur languag ar design for us to commun .
there ar other languag design for comput .
for exampl , program languag .
those ar harder for us , right .
so , natur languag is design to make our commun effici .
as a result , we omit a lot of common sens knowledg becaus we assum everyon know about that .
we also keep a lot of ambigu becaus we assum the receiv , or the hearer could know how to discern an ambigu word , base on the knowledg or the context .
there's no need to invent a differ word for differ mean .
we could overload the same word with differ mean without the problem .
becaus of these reason , thi make everi step in natur languag of process difficult for comput .
ambigu's the main difficulti , and common sens reason is often requir , that's also hard .
so , let me give you some exampl of challeng here .
conced the word level ambigu .
the same word can have differ syntact categori .
for exampl , design can be a noun or a verb .
the word root mai have multipl mean .
so , squar root in math sens , or the root of a plant .
you might be abl to think of other mean .
there ar also syntact ambigu .
for exampl , the main topic of thi lectur , natur languag process , can actual be interpret in two wai , in term of the structur .
think for a moment and see if you can figur that out .
we usual think of thi as process of natur languag , but you could also think of thi as you sai , languag process is natur .
right .
so , thi is exampl of syntat ambigu .
where we have differ structur that can be appli to the same sequenc of word .
anoth exampl of ambigu sentenc is the follow , a man saw a boi with a telescop .
now , in thi case , the question is , who had the telescop ?
all right , thi is call a preposit phrase attach ambigu , or pp attach ambigu .
now , we gener don't have a problem with these ambigu becaus we have a lot of background knowledg to help us disintegr the ambigu .
anoth exampl of difficulti is anaphora resolut .
so , think about the sentenc like john persuad bill to bui a tv for himself .
the question here is , doe himself refer to john or bill ?
so again , thi is someth that you have to us some background or the context to figur out .
final , presupposit is anoth problem .
consid the sentenc , he ha quit smoke .
now thi obvious impli he smoke befor .
so , imagin a comput want to understand all the subtl differ and mean .
thei would have to us a lot of knowledg to figur that out .
it also would have to maintain a larg knowl , knowledg base of odd mean of word and how thei ar connect to our common sens knowledg of the word .
so thi is why it's veri difficult .
so as a result we ar still not perfect .
in fact , far from perfect in understand natur languag us comput .
so thi slide sort of give a simplifi view of state of the art technolog .
we can do part of speech tag pretti well .
so , i show minu <num> accuraci here .
now thi number is obvious base on a certain data set , so don't take thi liter .
all right , thi just show that we could do it pretti well .
but it's still not perfect .
in term of pars , we can do partial pars pretti well .
that mean we can get noun phrase structur or verb phrase structur , or some segment of the sentenc understood correctli in term of the structur .
and , in some evalu result we have seen about <num> accuraci in term of partial pars of sentenc .
again , i have to sai , these number ar rel to the data set .
in some other data set , the number might be lower .
most of exist work ha been evalu us new data set .
and so , a lot of these number ar more or less bias toward new data .
think about social media data .
the accuraci like is lower .
in term of semant analysi , we ar far from be abl to do a complet understand of a sentenc .
but we have some techniqu that would allow us to do partial understand of the sentenc .
so , i could mention some of them .
for exampl , we have techniqu that can allow us to extract the entiti and relat mention in text or articl .
for exampl , recogn the mention of peopl , locat , organ , et cetera in text .
right ?
so thi is call entiti extract .
we mai be abl to recogn the relat .
for exampl , thi person visit that per , that place .
or , thi person met that person , or thi compani acquir anoth compani .
such relat can be extract by us the current and natur languag process techniqu .
thei ar not perfect , but thei can do well for some entiti .
some entiti ar harder than other .
we can also do word sentenc disintegr to some extent .
we have to figur out whether thi word in thi sentenc would have certain mean , and in anoth context , the comput could figur out that it ha a differ mean .
again , it's not perfect but you can do someth in that direct .
we can also do sentiment analysi mean to figur out whether sentenc is posit or neg .
thi is a special us for , for review analysi for exampl .
so these exampl of semant analysi .
and thei help us to obtain partial understand of the sentenc .
right ?
it's not give us a complet understand as i show befor for the sentenc , but it will still help us gain understand of the content and these can be us .
in term of infer , we ar not yet there , probabl becaus of the gener difficulti of infer and uncertainti .
thi is a gener challeng in artifici intellig .
that's probabl also becaus we don't have complet semant reimplement for natur languag text .
so thi is hard .
yet in some domain , perhap in limit domain when you have a lot of restrict on the world of user , you mai be to mai be abl to perform infer to some extent , but in gener we cannot realli do that reliabl .
speech act analysi is also far from be done , and we can onli do that analysi for veri special case .
so , thi roughli give you some idea about the state of the art .
and let me also talk a littl bit about what we can't do .
and , and so we can't even do <num> part of speech tag .
thi look like a simpl task , but think about the exampl here , the two us of off mai have differ syntact categori if you try to make a fine grain distinct .
it's not that easi to figur out such differ .
it's also hard to do gener complet the pars .
and , again thi same sentenc that you saw befor is exampl .
thi , thi ambigu can be veri hard to disambigu .
and you can imagin exampl where you have to us a lot of knowledg i , in the context of the sentenc or from the background in order to figur out the , who actual had the telescop .
so is , i , although sentenc look veri simpl , it actual is pretti hard .
and in case when the sentenc is veri long , imagin it ha four or five preposit phrase , then there ar even more possibl to figur out .
it's also harder to precis deep semant analysi .
so here's exampl .
in thi sentenc , john own a restaur , how do we defin own exactli ?
the word , own , you know , is someth that we can understand but it's veri hard to precis describ the mean of own for comput .
so as a result we have robust and gener natur languag process techniqu that can process a lot of text data in a shallow wai , mean we onli do superfici analysi .
for exampl , part of s , of speech tag , or partial pars , or recogn sentiment .
and those ar not deep understand becaus we're not realli understand the exact mean of the sentenc .
on the other hand , the deep understand techniqu tend not to scale up well , mean that thei would fail on some unrestrict text .
and if you don't restrict the text domain or the us of word , then these techniqu tend not to work well .
thei mai work well , base on machin learn techniqu on the data that ar similar to the train data that the program ha been train on .
but thei gener wouldn't work well on the data that ar veri differ from the train data .
so thi pretti much summar the state of the art of natur languag process .
of cours , within such a short amount of time , we can't realli give you a , a complet view of ani of it , which is a big field , and either expect that to have , to see multipl cours on natur languag process topic itself .
but , becaus of it's relev to the topic that we talk about it's us for you to know the background in case you haven't been expos to that .
so , what doe that mean for text retriev ?
well , in text retriev we ar deal with all kind of text .
it's veri hard to restrict the text to a certain domain .
and we also ar often deal with a lot of text data , so that mean .
the nlp techniqu must be gener , robust , and effici and that just impli todai we can onli us fairli shallow nlp techniqu for text retriev .
in fact , most search engin todai us someth call a bag of word represent .
now thi is probabl the simplest represent you can probabl think of .
that is to turn text data into simpli a bag of word .
mean we will keep the individu word but we'll ignor all the order of word .
and we'll keep duplic occurr of word .
so thi is call a bag of word represent .
when you repres the text in thi wai , you ignor a lot about the inform , and that just make it harder to understand the exact mean of a sentenc becaus we've lost the order .
but yet , thi represent tend to actual work pretti well for most search task .
and thi is partli becaus the search task is not all that difficult .
if you see match of some of the queri word in a text document , chanc ar that that document is about the topic , although there ar except , right ?
so in comparison some other task , for exampl machin translat , would requir you to understand the languag accur , otherwis the translat would be wrong .
so in comparison , search task ar solv rel easi such a represent is often suffici .
and that's also the represent that the major search engin todai , like googl or bing ar us .
of cours i put in in parenthes but not all .
of cours there ar mani queri that ar not answer well by the current search engin , and thei do requir a represent that would go beyond bag of word represent .
that would requir more natur languag process , to be done .
there is anoth reason why we have not us the sophist nlp techniqu in modern search engin , and that's becaus some retriev techniqu actual natur solv the problem of nlp .
so , on exampl , is word sens disambigu .
think about a word like java .
it could mean coffe or it could mean program languag .
if you look at the word alon it would be ambigu .
but when the user us the water in the queri , usual there ar other word .
for exampl i'm look for usag of java applet .
when i have applet there that impli java mean program languag .
and that context can help us natur prefer document where java is refer to program languag , becaus those document would probabl match applet as well .
if java occur in the document in a wai that mean coffe , then you would never match applet , or with veri small probabl .
right .
so thi is a case when some retriev techniqu natur achiev the goal of word sens disambigu .
anoth exampl is some techniqu call feedback which we will talk about later in some of the lectur .
thi tech , techniqu would allow us to add addit word to the queri .
and those addit word could be relat to the queri word .
and these word can help match document where the origin queri word have not occur .
so thi achiev , to some extent , semant match of term .
so those techniqu also help us bypass some of the difficulti in natur languag process .
howev , in the long run , we still need deeper natur languag process techniqu in order to improv the accuraci of the current search engin .
and it's particularli need for complex search task , or for question answer .
googl ha recent launch a knowledg graph .
and thi is on step toward that goal , becaus knowledg graph would contain entiti and their relat .
and thi goe beyond the simpl bag of word represent .
and such techniqu should help us improv the search engin util significantli , although thi is a still open topic for research and explor .
in sum , in thi lectur we'll talk about what is nlp and we've talk about the state of the art techniqu , what we can do , what we cannot do .
and final , we also explain why bag of word represent remain the domin represent us in modern search engin even though deeper nlp would be need for futur search engin .
if you want to know more you can take a look at some addit read .
i onli cite on here .
and that's a good start point though .
thank .
in thi lectur , we're go to talk about text access .
in the previous lectur , we talk about natur languag content analysi .
we explain that the state of the art natur languag process techniqu ar still not good enough to process a lot of unrestrict text data in a robust manner .
as a result , bag of word represent remain veri popular in applic like search engin .
in thi lectur we're go to talk about some high level strategi to help user get access to the text data .
thi is also import step to convert raw , big text data into small relev data that ar actual need in a specif applic .
so the main question we address here is , how can a text inform system help user get access to the relev text data ?
we're go to cover two complementari strategi , push vs pull .
and then we're go to talk about two wai to implement the pull mode , queri vs brows .
so first , push vs pull .
these ar two differ wai to connect user with the right inform at the right time .
the differ is which take the initi , which parti it take in the initi .
in the pull mode , the user would take the initi to start the inform access process .
and in thi case , a user typic would us a search engin to fulfil the goal .
for exampl , the user mai type in a queri , and then brows result to find the relev inform .
so thi is usual appropri for satisfi a user's ad hoc inform need .
an ad hoc inform need is a temporari inform need .
for exampl , you want to bui a product so you suddenli have a need to read review about relat product .
but after you have collect inform , you have purchas your product , you gener no longer need such inform .
so it's a temporari inform need .
in such a case , it's veri hard for a system to predict your need , and it's more appropri for the user to take the initi .
and that's why search engin ar veri us todai , becaus mani peopl have mani ad hoc inform need all the time .
so as we ar speak googl probabl is process mani queri from thi , and those ar all , or mostli ad hoc inform need .
so thi is a pull mode .
in contrast , in the push mode the system will take the initi to push the inform to the user or to recommend the inform to the user .
so in thi case , thi is usual support by a recommend system .
now thi would be appropri if the user ha a stabl inform need .
for exampl , you mai have a research interest in some topic , and that interest tend to stai for a while , so it's rel stabl .
your hobbi is anoth exampl of a stabl inform need .
in such a case , the system can interact with you and can learn your interest , and then can monitor the inform stream .
if it is , the system hasn't seen ani relev item to your interest , the system could then take the initi to recommend inform to you .
so for exampl , a new filter or new recommend system could monitor the new stream and identifi interest in new to you , and simpli push the new articl to you .
thi mode of inform access mai be also appropri when the system ha a good knowledg about the user's need .
and thi happen in the search context .
so for exampl , when you search for inform on the web a search engin might infer you might be also interest in some relat inform .
and thei would recommend the inform to you .
so that should remind you for exampl , advertis place on a search page .
so thi is about the , the two high level strategi or two mode of text access .
now let's look at the pull mode in more detail .
in the pull mode , we can further thi in usual two wai to help user , queri vs brows .
in queri , a user would just enter a queri , typic a keyword queri , and the search engin system would return relev document to user .
and thi work well when the user know what exactli kei , ar the keyword to be us .
so if you know exactli what you're look for you tend to know the right keyword , and then queri would work veri well .
and we do that all the time .
but we also know that sometim it doesn't work so well , when you don't know the right keyword to us in the queri or you want to brows inform in some topic area .
in thi case brows would be more us .
so in thi case in the case of brows the user would simpli navig into the relev inform by follow the path that's support by the structur on the document .
so the system would maintain some kind of structur , and then the user could follow these structur to navig .
so thi strategi work well when the user want to explor inform space or the user doesn't know what ar the keyword to us in the queri .
or simpli becaus the user , find it inconveni to type in the queri .
so even if a user know what queri to type in , if the user is us a cell phone to search for inform , then it's still hard to enter the queri .
in such a case again , brows tend to be more conveni .
the relationship between brows and the queri is best understood by make an analog to sightse .
imagin if you ar tour a citi .
now if you know the exact address of a attract , then take a taxi there is perhap the fastest wai , you can go directli to the site .
but if you don't know the exact address , you mai need to walk around , or you can take a taxi to a nearbi place , and then walk around .
it turn out that we do exactli the same in the inform space .
if you know exactli what you ar look for , then you can us the right keyword in your queri to find the inform directli .
that's usual the fastest wai to do find inform .
but what if you don't know the exact keyword to us ?
well , your queri probabl won't work so well , and you will land on some relat page , and then you need to also walk around in the inform space .
mean by follow the link or by brows , you can then final get into the relev page .
if you want to learn about a topic again , you you will like do a lot of brows .
so just like you ar look around in some area and you want to see some interest attract in a relat in the same region .
so thi analog also tell us that todai we have veri good support for queri , but we don't realli have good support for brows .
and thi is becaus in order to brows effect , we need a a map to guid us , just like you need a map of chicago to tour the citi of chicago .
you need a topic map to tour the inform space .
so how to construct such a topic map is in fact a veri interest research question that like will bring us more interest brows experi on the web or in other applic .
so to summar thi lectur , we have talk about two high level strategi for text access , push and pull .
push tend to be support by a recommend system and pull tend to be support by a search engin .
of cours in the sophist intent in the inform system , we should combin the two .
in the pull mode we have further distinguish queri and brows .
again , we gener want to combin the two wai to help user so that you can support both queri and brows .
if you want to know more about the relationship between pull and push , you can read thi articl .
thi give a excel discuss of the relationship between inform filter and inform retriev .
here inform filter is similar to inform recommend , or the push mode of inform access .
thi lectur is about the text retriev problem .
thi pictur show our overal plan for lectur .
in the last lectur , we talk about the high level strategi for text access .
we talk about push versu pull .
search engin ar the main tool for support the pull mode .
start from thi lectur , we're go to talk about the how search engin work in detail .
so first , it's about the text retriev problem .
we're go to talk about the three thing in thi lectur .
first , we'll defin text retriev .
second , we're go to make a comparison between text retriev and the relat task , databas retriev .
final , we're go to talk about the document select versu document rank as two strategi for respond to a user's queri .
so what is text retriev ?
it should be a task that's familiar to most of us becaus we're us web search engin all the time .
so text retriev is basic a task where the system would respond to a user's queri with relev lock in , basic through support queri as on wai to implement the poor mold of inform access .
so the scenario's the follow .
you have a collect of text document .
these document could be all the web page on the web .
or all the literatur articl in the digit librari or mayb all the text file in your comput .
a user will typic give a queri to the system to express the inform need .
and then the system would return relev document to user .
relev document refer to those document that ar us to the user who type in the queri .
now thi task is a often call inform retriev .
but liter , inform retriev would broadli includ the retriev of other non textual inform as well .
for exampl , audio , video , et cetera .
it's worth note that text retriev is at the core of inform retriev in the sens that other media such as video can be retriev by exploit the companion text data .
so for exampl , can the imag search engin actual match a user's queri with the companion text data of the imag ?
thi problem is also call the , the search problem , and the technolog is often call search technolog in industri .
if you ever take on cours in databas , it'll be us to paus the lectur at thi point and think about the differ between text retriev and databas retriev .
now these two task ar similar in mani wai .
but there ar some import differ .
so , spend a moment to think about the differ between the two .
think about the data and inform manag by a search engin versu those that ar man , manag by a databas system .
think about the differ between the queri that you typic specifi for a databas system versu the queri that type in by user on the search engin .
and then final think about the answer .
what's the differ between the two ?
okai , so if we think probabl the inform out there ar manag by the two system .
we will see that in text retriev , the data is unstructur , it's free text .
but in databas , thei ar structur data , where there is a clear defin schema to tell you thi column is the name of peopl and that column is ag , et cetera .
in unstructur text , it's not obviou what ar the name of peopl mention in the text .
becaus of thi differ , we can also see that text inform tend to be more ambigu .
and we'll talk about that in the natur languag process lectur .
wherea in databas , the data tend to have well defin semant .
there is also import differ in the queri , and thi is partli due to the differ in the inform , or data .
so text queri tend to be ambigu , wherea in their research , the queri ar particularli well defin .
think about the sql queri , that would clear the specifi what record to be return .
so it ha veri well defin semant .
queue all queri or natur end queri tend to be incomplet .
also in that it doesn't realli , fulli specifi what document should be retriev .
wherea , in the databas search , the sql queri can be regard as a comput specif for what should be return .
and becaus of these differ , the answer would be also differ .
in the case of text retriev , we're look for relev document .
in the databas search , we ar retriev record or match record with the sql queri , more precis .
now in the case of text retriev , what should be the right answer to a queri is not veri well specifi , as we just discuss .
so it's unclear what should be the right answer to a queri .
and thi ha veri import consequ , and that is text retriev is an empir defin problem .
and so thi a problem becaus if it's empir defin , then we cannot mathemat prove on method is better than anoth method .
that also mean we must reli on emper evalu more than user to know which method work better .
and that's why we have on lectur , actual more than on lectur to cover the issu of evalu .
becaus thi is a veri import topic for search engin .
without know how to evalu an algorithm appropri , there's no wai to tell whether we have got the better algorithm or whether on system is better than anoth .
so now let's look at the problem in a formal wai .
so thi slide show a formal formul of the text retriev problem .
first , we have our vocabulari set which is just a set of word in a languag .
now here , we're consid just on languag , but in realiti on the web there might be multipl natur languag .
we have text that ar in all kind of languag .
but here for simplic , we just assum there is on kind of languag .
as the techniqu us for retriev data from multipl languag , .
ar more or less similar to the techniqu us for retriev document in on languag .
although there is import differ , the principl and method ar veri similar .
next we have the queri , which is a sequenc of word .
and so here you can see the queri is defin as a sequenc of word .
each q sub i is a word in the vocabulari .
a document is defin in the same wai .
so it's also a sequenc of word .
and here , d sub ij is also a word in the vocabulari .
now typic , the document ar much longer than queri .
but there ar also case where the document mai be veri short .
so you can think about the , what might be a exampl of that case .
i hope you can think of , of twitter search , all right ?
tweet ar veri short .
but in gener , document ar longer then the queri .
now , then we have a collect of document .
and thi collect can be veri larg .
so think about the web .
it could , could be veri larg .
and then the goal of text retriev is you'll find the set of relev document , which we denot by r of q , becaus it depend on the queri .
and thi is , in gener , a subset of all the document in the collect .
unfortun , thi set of random document is gener unknown , and usual depend in the sens that for the same queri type in by differ user , the expect relev document mai be differ .
the queri given to us by the user is onli a hint on which document should be in thi set .
and inde , the user is gener unabl to specifi what exactli should be in the set , especi in the case of a web search where the collect is so larg .
the user doesn't have complet knowledg about the whole collect .
so , the best a search system can do is to comput an approxim of thi relev document set .
so we denot it by r prime of q .
so , formal , we can see the task is to comput thi r prime of q , an approxim of the relev document .
so how can we do that ?
now , imagin if you ar now ask to write a program to do thi .
what would you do ?
now think for a moment .
right , so these ar your input .
with the queri , the document and then you will have comput the answer to thi queri , which is set of document that would be us to the user .
so how would you solv the problem ?
in gener there ar two strategi that we can us .
all right , the first strategi is to do document select .
and that is , we're go to have a binari classif function , or binari classifi .
that's a function that will take a document and queri as input , and then give a zero or on as output , to indic whether thi document is relev to the queri , or not .
so in thi case , you can see the document .
the , the relev document set is defin as follow .
it basic , all the document that have a valu of on by thi function .
and so in thi case , you can see the system must have decid if a document is relev or not .
basic , that ha to sai whether it's on or zero .
and thi is call absolut relev .
basic , it need to know exactli whether it's go to be us to the user .
altern , there's anoth strategi call document rank .
now in thi case , the system is not go to make a call whether a document is relev or not .
rather , the system's go to us a real valu function , f , here that would simpli give us a valu .
that would indic which document is more like relev .
so it's not go to make a call whether thi document is relev or not , but rather it would sai which document is more like relev .
so thi function then can be us to rank the document .
and then we're go to let the user decid where to stop when the user look at the document .
so we have a threshold , theta , here to determin what document should be in thi approxim set .
and we're go to assum that all the document that ar rank abov thi threshold ar in the set .
becaus in effect , these ar the document that we deliv to the user .
and theta is a cutoff determin by the user .
so here we've got some collabor from the user in some sens becaus we don't realli make a cutoff , and the user kind of help the system make a cutoff .
so in thi case , the system onli need to decid if on document is more like relev than anoth .
and that is , it onli need for determin rel relev as oppos to absolut relev .
now you can probabl alreadi sens that relev , rel relev would be easier to determin the absolut relev .
becaus in the first case , we have to sai exactli whether a document is relev or not , right ?
and it turn out that rank is inde gener prefer to document select .
so let's look thi these two strategi in more detail .
so thi pictur show how it work .
so on the left side , we see these document .
and we us the pluse to indic the relev document .
so we can see the true relev document here consist thi set of true relev document consist of these pluse , these document .
and with the document select function , we can do , basic classifi them into two group , relev document and non relev on .
of cours , the classifi will not be perfect , so it will make mistak .
so here we can see in the approxim of the relev document we have got some non relev document .
and similarli , there's a relev document that that's misclassifi as non relev .
in the case of document rank , we can see the system seem like simpli rank all the document in the descend order of the score .
and we're go to let the user stop wherev the user want to stop .
so if a user want to examin more document , then the user will go down the list to examin more and stop at the lower posit .
but if the user onli want to read a few random document , the user might stop at the top posit .
so in thi case , the user stop at d4 , so the effect , we have deliv these four document to our user .
so as i said , rank is gener prefer .
and on of the reason is becaus the classifi , in the case of document select , is unlik accur .
why ?
becaus the onli clue is usual the queri .
but the queri mai not be accur , in the sens that it could be overli constrain .
for exampl , you might expect the relev document to talk about all these topic you , by us specif vocabulari , and as a result , you might match no random document , becaus in the collect , no other have discuss the topic us these vocabulari .
all right .
so in thi case , we'll see there is thi problem of no relev document to return in the case of overli constrain queri .
on the other hand , if the queri is under constrain , for exampl , if the queri doe not have suffici discrimin word you'll find in relev document , you mai actual end up have .
over deliveri .
and thi is when you thought these word might be suffici to help you find the relev document , but it turn out that thei're not suffici .
and there ar mani distract document us similar word .
and so thi is the case of over deliveri .
unfortun , it's veri hard to find the right posit between these two extrem .
why ?
becaus , when the user look for the inform in gener , the user doe not have a good knowledg about the the inform to be found .
and in that case , the user doe not have a good knowledg about what vocabulari will be us in those random document .
so it's veri hard for a user to pre specifi the right level of of constraint .
even if the class file is accur , we also still want to rank these relev document becaus thei ar gener not equal relev .
relev is often a matter of degre .
so we must priorit these document for user to exam .
and thi , note that thi priorit is veri import , becaus a user cannot digest all the content at onc .
the user gener would have to look at each document sequenti .
and therefor , it would make sens to feed user with the most relev document , and that's what rank is do .
so for these reason rank is gener prefer .
now , thi prefer also ha a theoret justif , and thi is given by the probabl rank principl .
in the end of thi lectur there is a refer for thi .
thi princip sai , return a rank list of document in descend order of probabl , that a document is relev to the queri , is the optim strategi under the follow two assumpt .
first , the util of a document to a user is independ of the util of ani other document .
second , a user would be assum to brows the result sequenti .
now it's easi to understand why these two assumpt ar need , in order to justifi for the rank , strategi .
becaus , if the document ar independ , then we can evalu the util of each document that's separ .
and thi would allow us to comput a score for each document independ .
and then we're go to rank these document base on those score .
the second assumpt is to sai that the user would inde follow the rank list .
if the user is not go to follow the rank list , is not go to examin the document sequenti , then obvious the order would not be optim .
so under these two assumpt , we can theoret justifi the rank strategi is in fact the best that you could do .
now i've put on question here .
do these <num> assumpt hold ?
now i suggest you to paus the lectur for a moment to think about these .
now can you think of some exampl that would suggest these assumpt aren't necessarili true ?
now if you think for a moment you mai realiz none of the assumpt is actual true .
for exampl in the case of independ assumpt , we might have ident document that have similar content or exactli the same content .
if you look at each of them alon , each is relev .
but if the user ha alreadi seen on of them , we assum it's gener not veri us for the user to see anoth similar or duplic on .
so clearli the util of a document is depend on other document that the user ha seen .
in some other case , you might see a scenario where on document that mai not be us to the user , but when three particular document ar put togeth , thei provid answer to the user's question .
so thi is collect relev .
and that also suggest that the valu of the document might depend on other document .
sequenti brows gener would make sens if you have a rank list there .
but even if you have a run list , there is evid show that user don't alwai just go strictli sequenti through the entir list .
thei sometim would look at the bottom for exampl , or skip some .
and if you think about the more complic interfac that would possibl us like , two dimension interfac where you can put addit inform on the screen , then sequenti brows is a veri restrict assumpt .
so the point here is that , none of these assumpt is realli true , but nevertheless , the probabl rank principl establish some solid foundat for rank as a primari task for search engin .
and thi ha actual been the basi for a lot of research work in inform retriev .
and mani algorithm have been design base on thi assumpt .
despit that the assumpt aren't necessarili true .
and we can , address thi problem by do post process of a rank list .
for exampl , to remov redund .
so to summar thi lectur , the main point that you can take awai ar the follow .
first , text retriev is an empir defin problem .
and that mean which algorithm is better must be judg by the user .
second , document rank is gener prefer and thi is , will help user priorit examin of search result .
and thi is also to bypass the difficulti in determin absolut relev , becaus we can get some help from user in determin where to make the cut off .
it's more flexibl .
so thi further suggest that the main technic challeng in design the search engin is with design effect rank function .
in other word , we need to defin what is the valu of thi function f on the queri and document pair .
now how to design such a function is a main topic in the follow lectur .
there ar two suggest addit read .
the first is the classic paper on probabl rank principl .
the second , is a must read for anyon do research inform retriev .
it's classic ir book , which ha excel coverag of the main research result in earli dai , up to the time when the book wa written .
chapter six of thi book ha an in depth discuss of the probabl of the rank princip , and the probabilist retriev model , in gener .
thi lectur is a overview of text retriev method .
in the previou lectur we introduc you to the problem of text retriev .
we explain that the main problem is to design a rank function to rank document for a queri .
in thi lectur , we will give a overview of differ wai of design thi rank function .
so the problem is the follow .
we have a queri that ha a sequenc of word , and a document that , that's also a sequenc of word , and we hope to defin the function f that can comput a score base on the queri and document .
so the main challeng you here is with design a good rank function that can rank all the relev document , on top of all the non relev on .
now clearli thi mean our function must be abl to measur the likelihood that a document d is relev to a queri q .
that also mean we have to have some wai to defin relev .
in particular in order to implement the program to do that we have to have a comput definit of relev , and we achiev thi goal by design a retriev model , which give us a formal of relev .
now , over mani decad , research have design mani differ kind of retriev model , and thei fall into differ categori .
first , on fair mani of the model ar base on the similar idea .
basic , we assum that if a document is more similar to the queri than anoth document is , then we would sai the first document is more relev than the second on .
so in thi case , the rank function is defin as the similar between the queri and the document .
on well known exampl in thi case is vector space model , which we will cover more in detail later in the lectur .
the second kind of model ar call probabilist model .
in thi famili of model , we follow a veri differ strategi .
while we assum that queri and document ar all observ from random variabl , and we assum there is a binari random variabl call r here , to indic whether a document is relev to a queri .
we then defin the score of document with respect to a queri as is a probabl that thi random variabl r is equal to <num> , given a particular document and queri .
there ar differ case of such a gener idea .
on is classic probabilist model , anoth is languag model , yet anoth is diverg from random model .
in a later lectur , we will talk more about the , on case , which is languag model .
the third kind of model of thi is probabilist infer .
so here the idea is to associ uncertainti to infer rule .
and we can then quantifi the probabl that we can show that the queri follow from the document .
final , there is also a famili of model that ar us axiomat think .
here the idea is to defin a set of constraint that we hope a good retriev function to satisfi .
so in thi case the problem is you seek a good rank function that can satisfi all the desir constraint .
interestingli , although these differ model ar base on differ think , in the end the retriev function tend to be veri similar .
and these function tend to also involv similar variabl .
so now let's take a look at the , the common form of a state of that retriev model and examin some of the common idea us in all these model .
first , these model ar all base on the assumpt of us bag of word for repres text .
and we explain thi in the natur languag process lectur .
bag of word represent remain the main represent us in all the search engin .
so , with thi assumpt , the score of a queri like a presidenti campaign new , with respect to a document d here , would be base on score comput at , base on each individu word .
and that mean the score would depend on the score of each word , such as presidenti , campaign , and new .
here we can see there ar three differ compon , each correspond to how well the document match each of the queri word .
insid of these function , we see a number of heurist view .
so for exampl , on factor that affect the function g here is how mani time doe the word presidenti occur in the document ?
thi is call a term frequenc or tf .
we might also denot as c of presidenti and d .
in gener if the word occur more frequent in the document then the valu of thi function would be larger .
anoth factor is how long is the document , and thi is so to us the document length for score .
in gener , if a term occur in a long document that mani time , it's not as signific as if it occur the same number of time in a short document .
becaus in the long document ani term is expect to occur more frequent .
final , there is thi factor call a document frequenc , and that is we also want to look at how often presidenti occur in the entir collect .
and we call thi document frequenc , or df , of presidenti .
and in some other model we might also us a probabl to character thi inform .
so here , i show the probabl of presidenti in the collect .
so all these ar try to character the popular of the term in the collect .
in gener , match a rare term in the collect is contribut more to the overal score then match a common term .
so thi captur some of the main idea us in pretti much all the state of the art retriev model .
so now , a natur question is which model work the best ?
now , it turn out that mani model work equal well , so here i list the four major model that ar gener regard as a state of the art retriev model .
pivot length normal , bm25 , queri likelihood , pl2 .
when optim these model tend to perform similarli and thi wa , discuss in detail in thi refer at the end of thi lectur .
among all these , bm25 is probabl the most popular .
it's most like that thi ha been us in virtual all the search engin , and you will also often see thi method discuss in research paper .
and we'll talk more about thi method later in some other lectur .
so , to summar , the main point made in thi lectur ar , first the design of a good rank function pre requir a comput definit of relev , and we achiev thi goal by design a proper retriev model .
second , mani model ar equal effect but we don't have a singl winner here .
research ar still activ work on thi problem , try to find a truli optim retriev model .
final , the state of the art rank function tend to reli on the follow idea .
first , bag of word represent .
second , tf and the document frequenc of word .
such inform is us when rank function to determin the overal contribut of match a word , and document length .
these ar often combin in interest wai and we'll discuss how exactli thei ar combin to rank document in the lectur later .
there ar two suggest addit read if you have time .
the first is a paper where you can find a detail discuss and comparison of multipl state of the art model .
the second , is a book with a chapter that give a broad review of differ retriev model .
thi lectur is about the vector space retriev model .
we're go to give an introduct to it basic idea .
in the last lectur we talk about the differ wai of design a retriev model which would give us a differ the rank function .
in thi lectur , we're go to talk about the , the specif wai of design the ramp function call a vector space mutual model .
and we're go to give a brief introduct to the basic idea .
vector space model is a special case of similar base model as we discuss befor .
which mean , we assum relev is roughli similar between a document and a queri .
now whether thi assumpt is true , is actual a question .
but in order to solv our search problem we have to convert the vagu notion of relev into a more precis definit that can be implement with the program languag .
so in thi process we have to make a number of assumpt .
thi is the first assumpt that we make here .
basic we assum that if a document is more similar to a queri than anoth document , then the first document would be assum to be more relev than the second on .
and thi is the basi for rank document in thi approach .
again , it's question whether thi is realli the best definit for relev .
as we will see later there ar other wai to model relev .
the first idea of vector space retriev model is actual veri easi to understand .
imagin a high dimension space , where each dimens correspond to a term .
so , here , i show a three dimension space with three word , program , librari , and presidenti .
so each term , here , defin on dimens .
now we can consid vector in thi three dimension space .
and we're go to assum all our document and the queri will be place in thi vector space .
so , for exampl , on document that might be repres at by thi vector , d1 .
now thi mean thi document probabl cover librari and presidenti .
but it doesn't realli talk about program .
all right , what doe thi mean in term of present of document ?
that just mean , we're go to look at our document from the perspect of thi vector .
we're go to ignor everyth els .
basic what we see here is onli the vector of the document .
of cours the document ha other inform .
for exampl , the order of word ar simpli ignor and that's becaus we're assum that the word .
so with thi represent you have alreadi seen , d1 , seem to suggest a topic in either presidenti librari .
now thi is differ from anoth document .
which might be repres as a differ vector , d2 here .
now in thi case , the document that cover program and librari , but it doesn't talk about presidenti .
so what doe thi remind you ?
well , you can probabl guess , the topic is like about program languag and the librari is softwar librari , librari .
so thi show that by us thi vector space represent , we can actual captur the differ between topic of document .
now you can also imagin there ar other vector .
for exampl , d3 is point in that direct , that might be about presidenti program .
and in fact we're go to place all the document in thi vector space .
and thei will be point to all kind of direct .
and similarli , we're go to place our queri also in thi space , as anoth vector .
and then we're go to measur the similar between the queri vector and everi document vector .
so , in thi case for exampl , we can easili see d2 seem to be the closest of , to thi queri factor and therefor d2 will be rank abov other .
so thi wa a , basic the main idea of the , the vector space model .
so to be more pri , precis , be more precis .
vector space model is a framework .
in thi framework , we make the follow assumpt .
first , we repres a document and queri by a term vector .
so here a term can be ani basic concept .
for exampl , a word or a phrase , or even enneagram of charact .
those ar a sequenc of charact insid a word .
each term is assum to defin on dimens .
therefor n term .
in our vocabulari , we defin n dimension space .
a queri vector would consist of a number of element correspond to the weight of differ term .
each document vector is also similar .
it ha a number of element and each valu of each element is indic that weight of the correspond term .
here you can see , we have seen there ar n dimens .
therefor , there ar n element , each correspond to the weight on the particular term .
so the relev in thi case would be assum to be the similar between the two vector , therefor our rang in function is also defin as the similar between the queri vector and document vector .
now , if i ask you to write the program to the internet thi approach in the search engin .
you would realiz that thi wa far from clear , right ?
we haven't seen a lot of thing in detail therefor it's imposs to actual write the program to implement thi .
that's why i said thi is a framework .
and thi ha to be refin in order to actual suggest a particular function , that you can implement on the comput .
so , what doe thi framework not serv ?
well , it actual hasn't set mani thing that would be requir in order to implement thi function .
first , it did not sai how we should defin or select the basic concept exactli .
we clearli assum the concept ar orthogon , otherwis there will be redund .
for exampl , if two synonym ar somehow distinguish as two differ concept .
then thei would be defin in two differ dimens .
and then that would clearli caus a redund here .
or overemphas of match thi concept .
becaus it would be as if you match the two dimens when you actual match on semant concept .
secondli , it did not sai how we exactli should place document and queri in thi space .
basic i show you some exampl of queri and document vector .
but where exactli should the vector for a particular document point to ?
so thi is equival to how to defin the term weight .
how do you comput us element valu in those vector ?
thi is a veri import question becaus term weight in the queri vector indic the import of term .
so depend on how you assign the weight , you might prefer some term to be match over other .
similarli , term weight in the document is also veri meaning .
it indic how well the term character the document .
if you got it wrong , then you clearli don't repres thi document accur .
final , how we defin the similar measur is also not clear .
so these question must be address befor we can have an oper function that we can actual implement us a program languag .
so how do we solv these problem is the main topic of the next lectur .
in thi lectur , we're go to talk about how to instanti a vector space model , so that we can get a veri specif rank function .
so thi is the , to continu the discuss of the vector space model .
which is on particular approach to design rank function .
and we ar go to talk about how we us the gener framework of the the vector space model .
as a guidanc to instanti the framework to deriv a specif rank function .
and we're go to cover the simplest instanti of the framework .
so as we discuss in the previou lectur .
the vector space model is realli a framework .
it isn't , didn't sai .
as we discuss in the previou lectur , vector space model is realli a framework .
it doesn't , sai mani thing .
so for exampl here it show that it did not sai how we should defin the dimens .
it also did not sai how we place a document vector in thi space .
it did not sai how we place a queri vector in thi vector space .
and final , it did not sai how we should match a similar between the queri vector and the document vector .
so , you can imagin , in order to implement thi model .
we have to see specif , how we ar comput these vector .
what is exactli xi and what is exactli yi ?
thi will determin where we place the document vector .
where we place a queri vector .
and of cours , we also need to sai exactli what will be the similar function .
so if we can provid a definit of the concept that would defin the dimens and these xi's , or yi's .
and then , the wait of term for queri and document .
then we will be abl to place document vector and queri vector in thi well defin space .
and then , if we also specifi similar function , then we'll have well defin rank function .
so let's see how we can do that .
and think about the the simplicit instanti .
actual , i would suggest you to paus the lectur at thi point spend a coupl of minut to think about .
suppos you ar ask to implement thi idea .
you've come up with the idea of vector space model .
but you still haven't figur out how to comput thi vector exactli , how to defin thi similar function .
what would you do ?
so think for a coupl of minut and then , proce .
so let's think about some simplest wai of instanti thi vector space model .
first , how do we defin a dimens .
well the obviou choic is we us each word in our vocabulari to defin a dimens .
and a whole issu that there ar n word in our vocabulari , therefor there ar n dimens .
each word defin on dimens .
and thi is basic the bag of word instanti .
now let's look at how we place vector in thi space .
again here , the simplest of strategi is to us a bit vector to repres both a queri and a document .
and that mean each element xi and yi would be take a valu of either zero or on .
when it's on , it mean the correspond word is present in the document or in the queri .
when it's zero , it's go to mean that it's absent .
so you can imagin if the user type in a few word in your queri .
then the queri vector , we onli have a few on , mani , mani zero .
the document vector in gener we have more on of cours , but we also have mani zero .
so it seem the vocabulari is gener veri larg .
mani word don't realli occur in a document .
mani word will onli occasion occur in the document .
a lot of word will be absent in a particular document .
so , now we have place the document and the queri in the vector space .
let's look at how we match up the similar .
so , a commonli us similar measur here is dot product .
the dot product of two vector is simpli defin as the sum of the product of the correspond element of the two vector .
so here we see that it's the product of x1 and the y1 .
so here .
and then , x2 multipli by y2 .
and then final xn multipli by yn .
and then we take a sum here .
so that's the dot product .
now we can repres thi in a more gener wai , us a sum here .
so thi onli on of the mani differ wai of match the similar .
so now we see that we have defin the , the dimens .
we have defin the , the vector .
and we have also defin the similar function .
so now we final have the simplest vector space model .
which is base on the bit vector represent , dot product similar , and bag of word instanti .
and the formula look like thi .
so thi is our formula .
and that's actual a particular retriev function , a rank function all right ?
now , we can final implement thi function us a program languag and then rank document for queri .
now at thi point you should again paus the lectur to think about how we can interpret thi score .
so we have gone through the process of model the retriev problem us a vector space model .
and then , we make assumpt .
about how we place vector in the vector space and how we defin the similar .
so in the end we've got a specif retriev function shown here .
now the next step is to think about what of thi individu function actual make sens ?
i , can we expect thi function to actual perform well ?
where we us it to ramp it up , for us in queri .
so , it's worth think about , what is thi valu that we ar calcul ?
so in the end , we've got a number , but what doe thi number mean ?
is it meaning ?
so spend a coupl minut to think about that .
and of cours , the gener question here is do you believ thi is a good rank function ?
would it actual work well ?
so again , think about how to interpret thi valu .
is it actual meaning ?
doe it mean someth ?
so relat to how well that document match the queri .
so in order to assess whether thi simplest vector space model actual work well , let's look at the exampl .
so here i show some sampl document and a simpl queri .
the queri is new about the presidenti campaign .
and we have five document here .
thei cover differ , term in the queri .
and if you look at the , these document for a moment .
you mai realiz that some document ar probabl relev in some case or probabl not relev .
now if i ask you to rank these document , how would you rank them ?
thi is basic our ideal rank .
right .
when human can examin the document and then try to rank them .
now , so think for a moment and take a look at thi slide .
and perhap by paus the lectur .
so i think most of you would agre that d4 , and d3 , ar probabl better than other .
becaus thei realli cover the queri well .
thei match new , presidenti , and campaign .
so , it look like that these two document ar probabl better than the other .
thei should be rank on top .
and the other three , d1 , d2 , and d5 , ar realli non relav .
so we can also sai d4 and d3 ar relev document , and d1 , d2 , and d5 ar non relev .
so , now let see if our vector space model could do the same or could do someth closer .
so let's first think about how we actual us thi model to score document .
right here i show two document , d1 and d3 , and we have the queri also here .
in the vector space model , of cours we want to first comput the vector for these document and the queri .
now i issu with the vocabulari here as well , so these ar the n dimens that we'll be think about .
so what do you think is the vector represent for the queri ?
note that we ar assum that we onli us zero and on to indic whether a term is absent or present in the queri or in the document .
so these ar zero , on bit vector .
so what do you think is the queri vector ?
well the queri ha four word here .
so for these four word , there would be a on and for the rest , there will be zero .
now what about the document ?
it's the same .
so d1 ha two row , new and about .
so there ar two on here and the rest ar zero .
similarli , so now that we have the two vector , let's comput the similar .
and we're go to us dot product .
so you can see when we us dot product we just , multipli the correspond element .
right .
so these two would be , form a , be form a product .
and these two will gener anoth product .
and these two would gener yet anoth product .
and so on and so forth .
now you can , you need to see if we do that .
we actual don't have to care about these zero becaus if whenev we have a zero , the product will be zero .
so , when we take a sum over all these pair , then the zero entri will be gone .
as long as you have on zero , then the product would be zero .
so in the fact , we're just count how mani pair of on and on , right ?
in thi case , we have seen two .
so the result will be two .
so , what doe that mean ?
well that mean , thi number or the valu of thi score function .
is simpli the count of how mani uniqu queri term ar match in the document .
becaus if a document , if a term is match in the document , then there will be two on .
if it's not , then there will be zero on the document side .
similarli , if the document ha a term , .
but the term not in the queri there will be zero in the queri vector .
so those don't count .
so as a result thi score function basic mesh how mani uniqu queri term ar match in a document .
thi is how we interpret thi score .
now we can also take a look at the d3 .
in thi case , you can see the result is three .
becaus d3 match the three distinct queri word , new , presidenti , campaign .
wherea d1 onli match two .
now in thi case , it seem reason to rank d3 on top of d1 .
and thi simplest vector space model inde doe that .
so that look pretti good .
howev , if we examin thi model in detail , we like will find some problem .
so here i'm go to show all the score for these five document .
and you can even verifi thei ar correct .
becaus we're basic count the number of uniqu queri term match in each document .
now note that thi method actual make sens .
right ?
it basic mean if a document match more uniqu queri term , then the document will be assum to be more relev .
and that seem to make sens .
the onli problem is here , we can note set there ar three document , d2 , d3 , and d4 .
and thei ti with a three , as a score .
so that's a problem , becaus if you look at them carefulli it seem that d4 should be right abov d3 .
becaus d3 onli mention the presidenti onc .
but d4 mention it much more time .
in case of d3 , presidenti could be extend mention .
but d4 is clearli abov presidenti campaign .
anoth problem is that d2 and d3 also have the same soul .
but , if you look at the , the three word that ar match .
in the case of d2 , it match the new , about , and the campaign .
but in the case of d3 , it match the new , presidenti , and campaign .
so intuit , d3 is better .
becaus match presidenti is more import though than match about .
even though about and the presidenti ar both in the queri .
so intuit , we would like d3 to be rank abov d2 .
but thi model , doesn't do that .
so that mean thi is still not good enough , we have to solv these problem .
to summar , in thi lectur we talk about how to instanti a vector space model .
we mai need to do three thing .
on is to defin the dimens .
the second is to decid how to place document as vector in the vector space .
and to also place a queri in the vector space as a vector .
and third is to defin the similar between two vector , particularli the queri vector and the document vector .
we also talk about a veri simpl wai to instanti the vector space model .
inde , that's probabl the simplest vector space model that we can deriv .
in thi case , we us each word to defin a dimens .
we us a zero on bit vector to repres a document or a queri .
in thi case , we basic onli care about word presenc or absenc .
we ignor the frequenc .
and we us the dot product as the similar function .
and with such a , a , in situat .
and we show that the score function is basic to score a document base on the number of distinct queri word match in the document .
we also show that such a singl vector space model still doesn't work well , and we need to improv it .
and thi is the topic that we're go to cover in the next lectur .
in thi lectur , we're go to talk about how to improv the instant chang of the vector space model .
thi is the continu discuss of the vector space model .
we're go to focu on how to improv the instant chang of thi model .
in a previou lectur , you have seen that with simpl situat of the vector space model , we can come up with a simpl score function that would give us , basic , a count of how mani uniqu queri term ar match the document .
we also have seen that thi function ha a problem as shown on thi slide .
in particular , if you look at these three document , thei will all get the same score becaus thei match the three uniqu queri word .
but intuit we would like , d4 to be rank abov d3 .
and d2 is realli non relev .
so the problem here is that thi function couldn't captur the follow characterist .
first , we would like to give more gratitud to d4 becaus it match the presidenti more time than d3 .
second , intuit match presidenti should be more import than match about , becaus about is a veri common word that occur everywher .
it doesn't realli carri that much content .
so , in thi lectur , let's see how we can improv the model to solv these two problem .
it's worth think at thi point about why do we have these four problem .
if we look back at the assumpt we have made while substanti the vector space model , we will realiz that the problem is realli come from some of the assumpt .
in particular , it ha to do with how we place the vector in the vector space .
so then , natur , in order to fix these problem , we have to revisit those assumpt .
perhap , you will have to us differ wai to instanti the vector space model .
in particular , we have to place the vector in a differ wai .
so , let's see how can we prove thi ?
well , our natur thought is in order to consid multipl time of a term in a document .
we should consid the term frequenc instead of just the absenc or presenc .
in order to consid the differ between a document where a queri term occur multipl time and the on where the queri term occur just onc .
we have to conced a term frequenc , the count of a term be in the document .
in the simplest model , we onli model the presenc and absenc of a term .
we ignor the actual number of time that a term occur in a document .
so let's add thi back .
so we're go to do then repres a document by a vector with term frequenc as element .
so , that is to sai , now , the element of both the queri vector and the document vector will not be zero onc , but instead there will be the count of a word in the queri or the document .
so thi would bring addit inform about the document .
so thi can be seen as a more accur represent of our document .
so , now let's see what the formula would look like if we chang thi represent .
so as you see on thi slide , we still us that product , and , so the formula look veri similar in the form .
in fact , it look ident , but insid of the sum of co xi and yi ar now differ .
thei're now the count of word i in the queri and the document .
now at thi point , i also suggest you to paus the lectur for moment and just we'll think about how we have interpret the score of thi new function .
it's do someth veri similar to what the simplest vsm is do .
but becaus of the chang of the vector , now the new score ha a differ interpret .
can you see the differ ?
and it ha to do with the consider of multipl occurr of the same time in the document .
more importantli , we''ll try to know whether thi would fix the problem of the simplest vector space model .
so , let's look at the thi exampl again .
so suppos , we chang the vector to term frequenc vector .
now , let's look at these three document again .
the queri vector is the same becaus all these word occur exactli onc in the queri .
so the vector is still <num> <num> vector .
and in fact , d2 is also essenti in repres the same wai becaus none of these word ha been repeat mani time .
as a result , the score is also the same , still three .
the same issu for d3 and we still have a <num> .
but d4 would be differ , becaus now , presidenti occur twice here .
so the end in the four presidenti in the would be <num> instead of <num> .
as a result , now the score for d4 is higher .
it's a four now .
so thi mean , by us term frequenc , we can now rank d4 abov d2 and d3 as we hope to .
so thi solv the problem with default .
but , we can also see that d2 and d3 ar still featur in the same wai .
thei still have ident score , so it did not fix the problem here .
so , how can we fix thi problem ?
we would like , to give more credit for match presidenti than match about .
but how can we solv the problem in a gener wai ?
is there ani wai to determin which word should be treat more importantli and which word can be , basic ignor .
about is such a word .
and which it doe not realli carri that much content , we can essenti ignor that .
we sometim call such a word , a stock word .
those ar gener veri frequent and thei occur everywher , match it , doesn't realli mean anyth .
but comput how can we captur that ?
so again , i encourag you to think a littl bit about thi .
can you come up with ani statist approach to somehow distinguish presidenti from about .
if you think about it for a moment , you realiz that , on differ is that a word like abov occur everywher .
so if you count the current of the water in the whole collect that we would see that about as much higher for thi than presidenti , which it tend to occur onli in some document .
so thi idea suggest that we could somehow us the global statist of term or some other format to try to down weight the element for about in the vector represent of d2 .
at the same time , we hope to somehow increas the weight of presidenti in the vector of d3 .
if we can do that , then , we can expect that d2 will get the overal score to be less than three , while d3 will get the score about three .
then , we'll be abl to rank d3 on top of d2 .
so how can we do thi systemat ?
again , we can reli on some step that peopl count .
and in thi case , the particular idea is call the invers document frequenc .
we have seen document frequenc .
as on signal us in , the mode retriev function .
we discuss thi in a previou lectur .
so here's the specif wai of us it .
document frequenc is the count of document that contain a particular term .
here , we sai invers document frequenc becaus we actual want to reword a word that doesn't occur in mani document .
and so , the wai to incorpor thi into our vector is then to modifi the frequenc count by multipli it by the idea of the correspond word as shown here .
if we didn't do that , then we can penal common word which gener have a low idea of , and reward real word , which we're have a higher idf .
so most specif idf can be defin as the logarithm of m plu on divid by k , where m is the total number of document in the collect , k is df or document frequenc .
the total number of document contain the word w .
now , if you plot thi function by vari k , then you will see the curv that look like thi .
in gener , you can see it would give a higher valu for a low df word , a rare word .
you can also see the maximum valu of thi function is log of m plu <num> .
will be interest for you to think about what's minimum valu for thi function ?
thi could be interest exercis .
now , the specif function mai not be as import as the heurist to simpli penal popular term .
but it turn out thi particular function form ha also work veri well .
now , whether there is a better form of function here , is the open research question .
but , it's also clear that if we us a linear kernal like what's shown here with thi line , then , it mai not be as reason as the standard idf .
in particular , you can see the differ in the standard idf , and we , somehow have a point here .
after thi point , we're go to sai these term ar essenti not veri us .
thei can be essenti ignor .
and thi make sens when the term occur so frequent , and let's sai a term occur in more than <num> of the document .
then the term is unlik veri import and it's , it's basic , a common term .
it's not veri import to match thi word , so with the standard idf , you can see it's , basic , assum that thei all have lower weight , there's no differ .
but if you look at the linear kernel , at thi point there is , there's some differ .
so intuit , we want to focu more on the discrimin of low df word , rather than these common word .
well , of cours , which on work better , still ha to be valid by us the empir relat data set .
and we have to us user to judg which result of that .
so now let's see how thi can solv problem two .
so now , let's look at the two document again .
now without idf weight , befor , we just have vector , but with idf weight we now can adjust the df weight by multipli the , with the idf valu .
for exampl here , you can see is the adjust in particular for about , there is an adjust by us the idf valu of about which is smaller than the idf valu of presidenti .
so if you look at these , the idf will distinguish these two word .
as a result , adjust here would be larger , would make thi weight larger .
so if we score with these new vector , and what would happen is that the , of cours , thei share the same weight for new and the campaign , but the margin of about and presidenti with thi group mai .
so now as a result of idf weight , we will have d3 to be rank abov d2 .
becaus it match rail word , where as d2 match common word .
so thi show that the idea of weight can solv problem two .
so , how effect is thi model in gener when we us tf idf weight ?
well , let's look at all these document that we have seen befor .
these ar the new score of the new document .
but how effect is thi new weight method and new score function , all right ?
so now let's see overal how effect is thi new rank function with tf idf weight ?
here , we show all the five document that we have seen befor , and these ar their score .
now , we can see the score for the first four document here seem to be quit reason .
thei ar as we expect .
howev , we also see a new problem .
becaus now d5 , here , which did not have a veri high score with our simplest vector space model .
now , after it ha a veri high score .
in fact , it ha the highest score here .
so , thi creat a new problem .
thi actual a common phenomenon in design materi function .
basic , when you try to fix on problem , you tend to introduc other problem .
and that's why it's veri tricki how to design effect rank function .
and what's what's the best rank function is the open research question .
research ar still work on that .
but in the next few lectur , we're go to also talk about some addit idea to further improv thi model and try to fix thi problem .
so to summar thi lectur , we've talk about how to improv thi vector space model .
and we've got to improv the of the vector space model base on tf idf weight .
so the improv , most of it , is on the placement of the vector .
where we give higher weight to a term that occur mani time in the document , but infrequ in the whole collect .
and we have seen that thi improv model inde work better than the simplest vector space model , but it also still ha some problem .
in the next lectur , we're go to look at the how to address these addit problem .
in thi lectur , we continu the discuss of vector space model .
in particular , we ar go to talk about the tf transform .
in the previou lectur , we have deriv a tf idf weight formula us the vector space model .
and we have shown that thi model actual work pretti well for these exampl as shown on thi slide except for d5 , which ha receiv a veri high score .
inde , it ha receiv the highest score among all these document .
but thi document is intuit non relev , so thi is not desir .
in thi lectur , we're go to talk about how would you us tf transform to solv thi problem .
befor we discuss the detail , let's take a look at the formula for thi symbol here for idf weight rank function and see why thi document ha receiv such a high score .
so thi is the formula , and if you look at the formula carefulli , then you will see it involv a sum over all the match queri term .
and insid the sum , each match queri sum ha a particular weight .
and thi weight is tf idf weight .
so it ha an idf compon where we see <num> variabl .
on is the total number of document in the collect , and that is m .
the other is the document frequenc .
thi is the number of document that contain thi word w .
the other variabl in , involv the formula , includ the count of the queri term .
w in the queri , and the count of the word in the document .
if you look at thi document again , now it's not hard to realiz that the reason why it ha receiv a high score is becaus it ha a veri high count of campaign .
so the count of campaign in thi document is a four , which is much higher than the other document , and ha contribut to the high score of thi document .
so intriguingli , in order to lower the score for thi document , we need to somehow restrict the contribut of , the match of thi term in the document .
and if you think about the match of term in the document carefulli you actual would realiz we probabl shouldn't reward multipl occurr so gener .
and by that i mean the first occurr of a term sai a lot about the , the match of thi term , becaus it goe from zero count to a count of on , and that increas mean a lot .
onc we see a word in the document , it's veri like that the document is talk about thi word .
if we see an extra occurr on top of the first occurr , that is to go from on to two , then we also can sai that well , the second occurr kind of confirm that it's not a accident mention of the word .
now , we ar more sure that thi document is talk about thi word .
but imagin we have seen , let's sai , <num> time of the word in the document .
then , ad on extra occurr is not go to test more about evid becaus we ar alreadi sure that thi document is about thi word .
so if you're think thi wai it seem that we should restrict the contribut of a high account of term .
and that is the idea of tf transform .
so thi transform function is go to turn the raw count of word into a term frequenc weight , for the word in the document .
so here i show in x axi , that raw count , and in y axi i show the term frequenc weight .
so , in the previou rank function we actual have increasingli , us some kind of transform .
so for exampl in the zero on bit vector retent we actual us the suchier transform function as shown here .
basic if the count is zero then it ha zero weight .
otherwis it would have a weight of on .
it's flat .
now what about us term count as a tf weight .
well that's a linear function , right ?
so it ha just exactli the same weight as the count .
now we have just seen that thi is not desir .
so what we want is someth like thi .
so for exampl with a logarithm function , we can have a sub linear transform that look like thi .
and thi will control the influenc of realli high weight becaus it's go to lower it infer , yet it will retain the infer of small count .
or we might want to even bend the curv more by appli logarithm twice .
now peopl have tri all these method and thei ar inde work better than the linear form of the transform , but so far what work the best seem to be thi special transform call a bm25 transform .
bm stand for best match .
now in thi transform , you can see there's a paramet k here .
and thi k control the upper bound of thi function .
it's easi to see thi function ha a upper bound becaus if you look at the x divid by x plu k where k is not an activ number , then the numer will never be abl to exce the denomin , right ?
so , it's upper bound by k plu <num> .
now , thi is also differ between thi transform function and the logarithm transform .
which it doesn't have upperbound .
now furthermor , on interest properti of thi function is that as we vari k , we can actual simul differ transform function , includ the two extrem that ar shown here .
that is a zero on bit transform , and the unit transform .
so for exampl , if we set k to zero , now you can see the function valu would be on .
so we precis , recov the zero on bit transform .
if you set k to a veri larg number , on the other hand , other hand , it's go to look more like the linear transform function .
so in thi sens , thi transform is veri flexibl , it allow us to control the shape of the transform .
it also ha a nice properti of the upper bound .
and thi upper bound is us to control the infer of a particular term .
and so that we can prevent a , a spammer from just increas the count of <num> term to spam all queri that might match thi term .
in other word thi upper bound might also ensur that all term will be count when we aggreg the , the weight , to comput a score .
as i said , thi transform function ha work well , so far .
so to summaris thi lectur , the main point is that we need to do some sub linear of tf transform .
and thi is need to captur the intuit of diminish return from high term count .
it's also to avoid a domin by on singl term over all other .
thi bm25 transform , transform that we talk about is veri interest .
it's so far on of the best perform tf transform format formula .
it ha upper bound , and it's also robust and effect .
now , if we're plug in thi function into our tf idf weight vector space model then we would end up have the follow rank function , which ha a bm25 tf compon .
now thi is alreadi veri close to a state of the art rank function call a bm25 .
and we will discuss how we can further improv thi formula in the next lectur .
thi lectur is about document length normal in the vector space model .
in thi lectur we ar go to continu the discuss of the vector space model in particular we ar go to discuss .
the issu of document length normal .
so far in the lectur about the vector space model , we have us the variou signal from the document to assess the match of the document though with a preorder .
in particular we have consid the term frequenc , the count of a term in a document .
we have also consid a , it's global statist such as idf in word document frequenc .
but we have not consid a document length .
so , here i show two exampl document .
d4 is much shorter with onli <num> word .
d6 on the other hand ha <num> , <num> word .
if you look at the match of these queri word we see that in d6 there ar more match of the queri word but on might reason that d6 mai have match these queri word .
in a scatter manner .
so mayb the topic of d6 is not realli about the topic of the queri .
so the discuss of a campaign at the begin of the document mai have noth to do with the mention of presidenti at the end .
in gener , if you think about the long document , thei would have a higher chanc to match ani queri .
in fact , if you gener a , a long document that randomli sampl , sampl word from the distribut of word , then eventu you probabl will match ani queri .
so in thi sens we should penal no document becaus thei just natur have better chanc to match ani queri .
and thi is our idea of document answer .
we also need to be care in avoid to overpen small document .
on the on hand , we want to penal a long document .
but on the other hand , we also don't want to over penal them .
and the reason is becaus a document that mai be long becaus of differ reason .
in on case the document mai be more long becaus it us more word .
so for exampl think about the articl of a research paper .
it would us more word than the correspond abstract .
so thi is the case where we probabl should penal the match of a long document such as , full paper .
when we compar the match of word in such long document with match of the word in the short abstract .
then long paper gener have a higher chanc of match queri word .
therefor we should penal them .
howev , there is anoth case when the document is long and that is when the document simpli ha more content .
now consid anoth case of a long document , where we simpli concaten a lot of abstract of differ paper .
in such a case , obvious , we don't want to penal such a long document .
inde , we probabl don't want to penal such a document becaus it's long .
so that's why we need to be care .
about us the right degre of penal .
a method that ha been work well base on recent research is call , pivot length normal .
and in thi case the idea is to us .
the averag document length as a p word , as a refer point .
that mean we will assum that for the averag length document , the score is about right .
so , the normal would be <num> .
but if a document is longer than the averag document length then there will be some penal .
where as if it's shorter than there's even some reward .
so thi is an illustr that us thi slide .
on the axi , s axi you can see the length of document .
on the y axi we show the normal , in the case pivot length normal formula for the normal is is seem to be interpol of on and the normal the document length , control by a paramet b here .
so , you can see here , when we first divid the length of the document by the averag document length .
thi not onli give us some sens about the , how thi document is compar with the averag document length , but also give us a benefit of not worri about the unit of length , we can measur the length by word or by charact .
anywai thi normal ha an interest properti .
first we see that if we set the paramet b to <num> then the valu would be <num> , so there's no pair , length normal at all .
so b in thi sens control the length normal , where as if we set d to a non zero valu , then the normal will look like thi , right .
so the valu would be higher for document that ar longer than the averag document length .
where as the valu of the normal will be short will be smaller for shorter document .
so in thi sens we see there's a penal for long document .
and there's a reward for short document .
the degre of penal is conjur by b .
becaus if we set b to a larger valu then the normal .
what look like thi .
there's even more penal for long document and more reward for the short document .
by adjust b which vari from zero to on we can control the degre of length normal .
so if we're pluck thi length normal factor into the vector space model rank function that we have alreadi examin .
then we will end up head with formula , and these ar in fact the state of the ar vector space model .
formula .
so , let's talk an that , let's take a look at the each of them .
the first on's call a pivot length normal vector space model .
and , a refer in the end ha detail about the deriv of thi model .
and , here , we see that it's basic the tfidf weight model that we have discuss .
the idf compon should be veri familiar now to you .
there is also a queri term frequenc compon , here .
and , and then in the middl there is .
and normal the tf .
and in thi case , we see we us the doubl algorithm , as we discuss befor , and thi is to achiev a sublinear transform .
but we also put document length normal in the bottom , all right so thi would caus penalti for a long document , becaus the larger the denomin is , the denomin is then the smaller the shift weight is .
and thi is of cours control by the paramet b here .
and you can see again , b is set to <num> , and there , there is no length normal .
okai .
so thi is on of the two most effect .
not thi base model of formula .
the next on call a bm25 , or okapi , is , also similar .
in that , it also ha a i , df compon here , and a queri df compon here .
but in the middl , the normal's a littl bit differ .
as we expand there is thi or copi here for transform here .
and that doe , sublinear transform with an upper bound .
in thi case we have put the length normal factor here .
we ar adjust k , but it achiev a similar factor becaus we put a normal in the denomin .
therefor again , if a document is longer , then the term weight will be smaller .
so , you can see , after we have gone through all the instanc that we talk about , and we have , in the end , reach the , basic the state of the art mutual function .
so , so far we have talk about mainli how to place the document matter in the matter space .
and thi ha plai an import role in uh , determin the factor of the function .
but there ar also other dimens where we did not realli examin detail .
for exampl can we further improv the instanti of the dimens of the vector space model .
now we've just assum that the back of word .
so each dimens is a word .
but obvious we can see there ar mani other choic .
for exampl , stem word , those ar the word that have been transform into the same rule form .
so that comput and comput will all becom the same and thei can be match .
we need to stop water remov .
thi is remov on veri common word that don't carri ani content .
like the or of , we us the phrase to defin that we can even us late in the semantica , an answer sort of find in the sum cluster .
so word that repres a legend of concept as on .
we can also us smaller unit , like a charact in gram .
those ar sequenc of n charact for dimens .
howev , in practic peopl have found that the bag of word represent with the phrase is where the the most effect on .
and it's also effici so thi is still so far the most popular dimens instanti method and it's us in all the major search engin .
i should also mention that sometim we did to do languag specif and domain specif organ .
and thi is actual veri import as we might have variat of the term .
that might prevent us from match them with each other .
even though thei mean the same thing .
and some of them , which is like chines , the result of the .
segment text to obtain word boundari .
becaus it's just a sequenc of charact .
a word might , might correspond to on charact or two charact or even three charact .
so it's easier in english when we have a space to separ the word .
but in some other languag we mai need to do some natur languag process to figur out the , where ar the boundari for word .
there is also possibl to improv thi in narr function .
and so far we have us the about product , but on can imagin there ar other match .
for exampl we can match the cosin of the angl between two vector , or we can us euclidean distanc measur .
and these ar all possibl .
the dot product seem still the best and on of the reason is becaus it's veri gener .
in fact , it's suffici gener .
if you consid the possibl of do weight in differ wai .
so , for exampl , cosin measur can be regard as the dot product of two normal vector .
that mean we first normal each vector , and then we take the dot product .
that would be equival to the cosin measur .
i just mention that the bm25 .
seem to be on of the most effect formula .
but there ha been also further develop in , improv bm25 , although none of these work have chang the bm25 fundament .
so in on line of work , peopl have deriv bm25 f .
here f stand for field , and thi is a littl us bm25 for document with a structur .
for exampl you might consid titl field , the abstract , or bodi of the reasearch articl , or even anchor text on the web page .
those ar the text field that describ link to other page .
and these can all be combin with a appropri weight on differ field to help improv score for document .
us bm25 for such a document .
and the obviou choic is to appli bm25 for each field , and then combin the score .
basic , the ideal of bm25f , is to first combin the frequenc count of ton in all the field and then appli bm25 .
now thi ha advantag of avoid over count the first occurr of the term .
rememb in the sublinear transform of tf , the first recurr is veri import then , and contribut a larg weight .
and if we do that for all the field , then the same term might have gain a , a lot of advantag in everi field , but when we combin these word frequenc togeth .
we just do the transform on time , and that time then the extra occurr will not be count as fresh first occurr .
and thi method ha been work veri well for score structur document .
the other line of extens is call a bm25 plu and thi line , arrest have address the problem of over penal of long document by bm25 .
so to address thi problem , the fix is actual quit simpl .
we can simpli add a small constant to the tf normal formula .
but what's interest is that we can analyt prove that by do such a small modif , we will fix the problem of a , over penal of long document by the origin bm25 .
so the new formula call bm25 plu is empir and analyt shown to be better than bm25 .
so to summar all what we have said about the vector space model .
here ar the major takeawai point .
first , in such a model , we us the similar notion of relev , assum that the relev of a document with respect to a queri is basic proport to the similar between the queri and the document .
so , natur , that impli that the queri and document must be repres in the same wai , and in thi case , we repres them as vector in high dimension vector space .
where the dimens ar defin by word or concept or term in gener .
and we gener need to us a lot of heurist to design a rank function .
we us some exampl which show the need for sever heurist , includ tf wait and transform .
and idf weight , and document length normal .
these major heurist ar the most import heurist to ensur such a gener rank function to work well for all kind of task .
and final bm25 and pivot normal seem to be the most effect formula out of that space model .
now i have to sai that , i've put bm25 in the categori of vector space model .
but in fact the bm25 ha been deriv us model .
so the reason why i've put it in the vector space model is first the rank function actual ha a nice interpret in the vector space model .
we can easili see it look veri much like a vector space model with a special weight function .
the second reason is becaus the origin bm25 ha a somewhat differ from of idf .
and that form of idf actual doesn't realli work so well as the standard idf that you have seen here .
so as a effect origin function bm25 should probabl us a heurist modif of the idf to make that even more like a vector space model .
there ar some addit read .
the first is a paper about the pivot length normal .
it's an excel exampl of us empir data enhanc to suggest a need for length normal , and then further deriv a length normal formula .
the second is the origin paper when the wa propos .
the third paper ha a thorough discuss of and it extens , particularli bm 25f .
and final , the last paper ha a discuss of improv bm <num> to correct the overpen of long document .
thi lectur is about the implement of text retriev system .
in thi lectur , we will discuss how we can implement a text retriev method to build a search engin .
the main challeng is to manag a lot of text data and to enabl a queri to be answer veri quickli and to respond to mani queri .
thi is a typic text retriev system architectur .
we can see the document ar first process by a token to get token unit , for exampl word .
and then these word or token would be process by an index that would creat an index , which is a data structur for the search engin to us to quickli answer a queri .
and the queri will be go through a similar process step .
so , the token will be appris to queri as well so that the text can be process in the same wai .
the same unit will be match with each other .
and the queri's represent will then be given to the scorer .
which would us a index to quickli answer a user's queri by score the document and then rank them .
the result will be given to the user .
and then the user can look at the result and and provid some feedback that can be express judgement about which document ar good , which document ar bad , or implicit feedback such as pixel so the user doesn't have to ani , anyth extra .
the user will just look at the result and skip some and click on some result to view .
so these interact signal can be us by the system to improv the rank accuraci by assum that view document ar better than the skip on .
so , a search engin system then can be divid into three part .
the first part is the index , and the second part is the scorer , that respond to the user's queri .
and the third part is the feedback mechan .
now typic , the index is done in the offlin manner so you can pre process the correct data and to build the invert index which we will introduc in a moment .
and thi data structur can then be us by the onlin modul which is a scorer to process a user's queri dynam and quickli gener search result .
the feedback mechan can be done onlin or offlin depend on the method .
the implement of the index and the , the scorer is fairli standard , and thi is the main topic of thi lectur and the next few lectur .
the feedback mechan , on the other hand ha variat .
it depend on what method is us .
so that is usual done in a algorithm specif wai .
let's first talk about the token .
token is a normal lexic unit into the same form so that semant similar word can be match with each other .
now in the languag of english stem is often us and thi what map all the inflect form of word into the same root form .
so for exampl , comput comput and comput can all be match to the root form comput .
thi wai , all these differ form of comput can be match with each other .
normal thi is a good idea to increas the coverag of document that ar match with thi queri .
but it's also not alwai benefici becaus sometim the subtlest differ between comput and comput might still suggest the differ in the coverag of the content .
but in most case , stem seem to be benefici .
when we token the text in some other languag , for exampl chines , we might face some special challeng in segment the text to find the word boundari .
becaus it's not ob , obviou where the boundari is as there's no space separ them .
so , here , of cours , we have to us some languag specif natur languag process techniqu .
onc we do token , then we would index the text document , and that it will convert the document into some data structur that can enabl fast search .
the basic idea is to precomput as much as we can , basic .
so the most commonli us index is call a invert index .
and thi ha been us , to , in mani search engin to support basic search algorithm .
sometim other indic , for exampl a document index , might be need in order to support a , a feedback .
like i said , thi , thi kind of techniqu ar not realli standard in that thei vari a lot accord to the feedback method .
to understand why we ar us invert index .
it will be us for you to think about how you would respond to a singl term queri quickli .
so if you want to us more time to think about that , paus the video .
so think about how you can preprocess the text data so that you can quickli respond to a queri with just on word .
well , if you have thought about question , you might realiz that where the best is to simpli creat a list of document that match everi term in the vocabulari .
in thi wai , you can basic pre construct the answer .
so when you see a term , you can simpli just fetch the rank list of document for that term and return the list to the user .
so that's the fastest wai to respond to singl term queri .
now the idea of invert index is actual basic like that .
we can do , pre construct such a index .
that would allow us to quickli find the , all the document that match a particular term .
so let's take a look at thi exampl .
we have three document here , and these ar the document that you have seen in some previou lectur .
suppos we want to creat invert index for these document , then we will need to maintain a dictionari .
in the dictionari we'll have on entri for each term .
and we're go to store some basic statist about the term .
for exampl , the number of document that match the term or the total number of , fre , total frequenc of the term , which mean we would encount duplic occurr of the term .
and so , for exampl , new .
thi term occur in all the three document .
so the count of document is three .
and you might also realiz we need thi count of document or document frequenc for comput some statist to be us in the vector space model .
can you think of that ?
so , what wait heurist would need thi count ?
well , that's the idf , right , invers document frequenc .
so idf is a properti of the term , and we can comput it right here .
so with the document account here , it's easi to comput the idf either at thi time or when we build an index or .
at run time when we see a queri .
now in addit to these basic statist we also saw all the document that match new .
and these entri ar store in a file call a post .
so in thi case it match <num> document and we store inform about these <num> document here .
thi is the document id , document <num> , and the frequenc is <num> .
the tf is <num> for new .
in the second document it's also <num> , etc .
so from thi list that we can get all the document that match the term new .
and we can also know the frequenc of new in these document .
so , if the queri ha just on word , new , and we can easili look up in thi tabl to find the entri and go quickli to the post to fetch all the document that match new .
so , let's take a look at anoth term .
now thi time let's take a look at the word presidenti .
all right , thi word occur in onli <num> document , document <num> .
so , the document frequenc is <num> , but it occur twice in thi document .
and so the frequenc count is <num> , and the frequenc count is us for , in some other retriev method where we might us the frequenc to assess the popular of a , a term in the collect .
and similarli , we'll have a pointer to the post , right here .
and in thi case there is onli on entri here becaus the term occur in just on document .
and that's here .
the document id is <num> , and it occur twice .
so thi is the basic idea of invert index .
it's actual pretti simpl , right ?
with thi structur we can easili fetch all the document that match a term .
and thi will be the basi for store document for our queri .
now sometim we also want to store the posit of these term .
so , in mani of these case the term occur just onc in the document so there's onli on posit , for exampl in thi case .
but in thi case the term occur twice so it would store two posit .
now the posit inform is veri us for check whether the match of queri term is actual within a small window of , let's sai , five word , or ten word , or whether the match of , the two queri term , is in fact a phrase of two word .
thi can all be check quickli by us the posit inform .
so why is invert index good for faster search ?
well we just talk about the possibl of us the two end of a singl word queri .
and that's veri easi .
what about a multipl term queri ?
well , let's look at the , some special case of the boolean queri .
a boolean queri is basic a boolean express , like thi .
so i want the relev document to match both term a and term b .
all right , so that's on conjunct queri .
or , i want the relev document to match term a or term b .
that's a disjunct queri .
now how can we answer such a queri by us invert index ?
well if you think a , a bit about it , it would be obviou .
becaus we have simpli to fetch all the document that match term a and also fetch all the document that match term b .
and then just take the intersect to answer a queri like a and b .
or to take the union to answer the queri a or b .
so thi is all veri easi to answer .
it's go to be veri quick .
now what about the multi term keyword queri ?
we talk about the vector space model for exampl .
and we would match such a queri with a document and gener a score .
and the score is base on aggreg term weight .
so in thi case it's not a boolean queri , but the score can be actual done in a similar wai .
basic it's similar to disjunct boolean queri .
basic it's like a or b .
we take the union of all the , document that match at least on queri term , and then we would aggreg the term weight .
so thi is a , a , a basic idea of us invert index for score document in gener .
and we're go to talk about thi in more detail later .
but for now , let's just look at the question , why is invert index , a good idea ?
basic , why is it more effici than sequenti just scan document ?
right ?
thi is , the obviou approach .
you can just comput the score for each document , and then you can score them , sorri , you can then sort them .
thi is a , a straightforward method .
but thi is go to be veri slow .
imagin the web .
it ha a lot of document .
if you do thi , then it will take a long time to answer your queri .
so the question now is , why would the in , the invert index be much faster ?
well it ha to do with the word distribut in text .
so , here's some common phenomenon of word distribut in text .
there ar some languag in , independ pattern that seem to be stabl .
and these pattern ar basic character by the follow pattern .
a few word like the common word like the a , or we , occur veri , veri frequent in text .
so thei account for a larg percent of occurr of word .
but most word would occur just rare .
there ar mani word that occur just onc , let's sai , in a document , or onc in the collect .
and there ar mani such singl term .
it's also true that the most frequent word in on corpu mai actual be rare in anoth .
that mean , although the gener phenomenon is applic or is observ in mani case , the exact word that ar common mai vari from context to context .
so thi phenomena is character by what's call a zipf's law .
thi law sai that the rank of a word multipli by , the frequenc of the word is roughli constant .
so formal if we us f of w to denot the , frequenc , r of w to denot the rank of a word , then thi is the formula .
it basic sai the same thing , just mathemat term , where c is , basic a constant , right , so as , so .
and there is also paramet alpha that might , be adjust to better fit ani empir observ .
so if i plot the word frequenc in sort order , then you can see thi more easili .
the x axi is basic the word rank .
and thi is r of w .
and the y axi is the word frequenc , or f of w .
now , thi curv basic show that the product of the two is roughli the constant .
now , if you look these word , we can see .
thei can be separ into three group2s .
in the middl it's the immedi frequenc word .
these word tend to occur in quit a few document , right ?
but thei're not like those most frequent word .
and thei ar also not veri rare .
so thei tend to be often us in in , in queri .
and thei also tend to have high tfi diff weight in these intermedi frequenc word .
but if you look at the left part of the curv .
these ar the highest frequenc word .
thei occur veri frequent .
thei ar usual stopper word , the , we , of , et cetera .
those word ar veri , veri frequent .
thei ar , in fact , a too frequent to be discrimin .
and thei gener ar not veri us for , for retriev .
so , thei ar often remov , and thi is call a stop word remov .
so you can us pretti much just the count of word in the collect to kind of infer what word might be stop word .
those ar basic the highest frequenc word .
and thei also occupi a lot of space in the invert index .
you can imagin the post entri for such a word would be veri long .
and then therefor , if you can remov such word , you can save a lot of space in the invert index .
we also show the tail part , which is , ha a lot of rare word .
those word don't occur veri frequent , and there ar mani such word .
those word ar actual veri us for search , also , if a user happen to be interest in such a topic .
but becaus thei're rare it's often true that user ar , aren't the necessari interest in those word .
but retain them would allow us to match such a document accur , and thei gener have veri high idf .
so what kind of data structur should we us to to store invert index ?
well , it ha two part , right ?
if you recal we have a dictionari , and we also have post .
the dictionari ha modest size , although for the web , it still wouldn't be veri larg .
but compar with post , it's modest .
and we also need to have fast , random access to the entri becaus we want to look up the queri term veri quickli .
so , therefor , we prefer to keep such a dictionari in memori if it's possibl .
or , or , or if the connect is not veri larg , and thi is visibl .
but if the connect is veri larg , then it's in gener not possibl .
if the vocabulari size is veri larg , obvious we can't do that .
so , but in gener , that's our goal .
so the data structur that we often us for store dictionari would be direct access data structur , like a hash tabl or b tree if we can't store everyth in memori of the newest disk .
and but to try to build a structur that would allow it to quickli look up our entri .
right .
for post , thei're huge , you can see .
and in gener , we don't have to have direct access to a specif engin .
we gener would just look up a , a sequenc of document id and frequenc for all of the document that match a queri term .
so we would read those entri sequenti .
and therefor , becaus it's larg and we gener , have store post on disk , so thei have to stai on disk .
and thei would contain inform such as document id , term frequenc , or term posit , et cetera .
now becaus thei're veri larg , compress is often desir .
now thi is not onli to save disk space and thi is of cours , on benefit of compress .
it's not go to occupi that much space .
but it's also to help improv speed .
can you see why ?
well , we know that input and output will cost a lot of time in comparison with the time taken by cpu .
so cpu is much faster .
but io take time .
and so by compress the invert index , the post file will becom smaller .
and the entri that we have to read into memori to process a queri done , would would be smaller .
and then so we , we can reduc the amount of traffic and io .
and that can save a lot of time .
of cours , we have to then do more process of the data when we uncompress the , the data in the memori .
but as i said , cpu is fast , so overal , we can still save time .
so compress here is both to save disk space and to speed up the load of the invert index .
thi lectur is about the invert index construct .
in thi lectur , we will continu the discuss of system implement .
in particular , we're go to discuss how to construct the invert index .
the construct of the invert index is actual veri easi if the data set is veri small .
it's veri easi to construct a dictionari and then store the post in a file .
the problem's that when our data is not abl to fit to the memori , then we have to us some special method to deal with it .
and unfortun , in most retriev a petit , the data set would be larg and thei gener cannot be , load into the memori at onc .
and there ar mani approach to solv that problem , and sort base method , is quit common and work in four step as shown here .
first , we collect the the local termid , document id , and frequenc tupl .
basic , you overlook kind of term in a small set of document , and , and then , onc you collect those count , you can sort those count base on term so that you build a local , a partial invert index .
and these ar call , run .
and then , you write them into a temporari file on the disk .
and then , you merg in step three with do pair wise merg of these run , and here , you eventu merg all the run , we gener a singl invert index .
so thi is an illustr of thi method .
on the left , you see some document .
and on the right , we have , show a term lexicon and a document id lexicon .
and these lexicon's ar to map a stream base represent of document id or term into integ represent .
or , and , map back from , integ to the screen represent .
and the reason why we want , ar interest in us integ repres these id , is becaus , integ ar often easier to handl .
for exampl , integ can be us as index for arrai and thei ar also easi to compress .
so thi is a , on reason why we , tend to map these stream into integ so that so that we don't have to , carri these stream around .
so how doe thi approach work ?
well , it's veri simpl .
we're go to scan these document sequenti , and then paus the document and a count the frequenc of term .
and in thi , stage we gener sort the frequenc by document id becaus we process each document that sequenti .
so , we first encount all the term in , the first document .
therefor , the document id , ar all onc in thi stage .
and so , and , thi would be follow by document id <num> .
and , and thei're natur sort in thi order just becaus we process the data in thi order .
at some point , the , we will run out of memori and that would have to , to write them into the disk .
but befor we do that , we're go to a sort them , just , us whatev memori we have , we can sort them , and then , thi time , we're go to sort base on term id .
note that here , we're us , thi , the term id as a kei to sort .
so , all the entri that share the same term would be group togeth .
in thi case , we can see all the , all the id of document that match term on would be group togeth .
and we're go to write thi into the disk as a temporari file .
and that would , allow us to us the memori to process the next batch of document , and we're go to do that for all the document .
so we're go to write a lot of temporari file into the disk .
and then , the next stage is to do merg sort .
basic , we're go to , merg them and the sort them .
eventu , we will get a singl invert index where the , their entri ar sort base on term id .
and on the top , we can see these ar the order entri for the document that match term id <num> .
so thi is basic how we can do , the construct of invert index , even though that thei're or cannot be , or load into the memori .
now , we mention earlier that becaus the po , post ar veri larg , it's desir to compress them .
so let's now talk a littl bit about how we compress invert index .
well , the idea of compress , in gener , is you leverag skew distribut of valu .
and we gener have to us variabl length in code instead of the fix length in code as we' , us , by default a program languag like c .
and so , how can we leverag the skew distribut of valu to , compress these valu ?
well , in gener , we would us fewer bit to encod those frequent word at a cost of us , longer bit from the code than those , rare valu .
so in our case , let's think about how we can compress the tf , term frequenc .
if you can pictur what the invert index would look like and you'll see in post there ar a lot of , term frequenc .
those ar the frequenc of term , in all those document .
now , we , if you think about it , what kind of valu ar most frequent there ?
you probabl will , be abl to guess that the small number tend to occur far more frequent than larg number .
why ?
well , think of about the distribut of word , and thi is due to zipf's law and mani word occur just , rare .
so we see a lot of small number , therefor , we can us fewer bit for the small , but highli frequent integ , and at the cost of us more bit for larg integ .
thi is a trade off , of cours .
if the valu ar distribut uniformli and thi won't save us ani , space .
but becaus we tend to see mani small valu , thei're veri frequent .
we can save on averag even though sometim , when we see a larg number we have to us a lot of bit .
what about the document id that we also saw in post .
well , thei ar not , distribut in a skew wai , right ?
so , how can we deal with that ?
well , it turn out you can us a trick call the d gap , and that , that is to store the differ of these term id .
and we can , imagin if a term ha match mani document , then there will be a long list of document id .
so when we take the gap , and when we take differ between adjac document id , those gap will be small .
so we'll again see a lot of small number , wherea , if a term occur in onli a few document , then the gap would be larg .
the larger number will not be frequent , so thi creat some skew distribut that would allow us to , to compress these valu .
thi is also possibl becaus in order to uncov or uncompress these document id , we have to sequenti process the data becaus we store the differ .
and in order to recov the , the exact document id , we have to first recov the previou document id , and then , we can add the differ to the previou document id to restor the , the current document id .
now , thi wa possibl becaus we onli need to have sequenti access to those document id .
onc we look up a term we fetch all the document id that match the term , then we sequenti process them .
so it's veri natur that's why thi , trick actual work .
and there ar mani differ method for encod .
so binari code is a common us code in , in just ani program .
languag that we us basic a fix length in code .
unari code and gamma code , and delta code ar all possibl in thi and there ar mani other possibl in thi .
so let's look at some of them in more detail .
binari code is realli equal length in code .
and that's a properti for the randomli distribut valu .
the unari code is is a variabl and it's import in thi case , integ that is , i've miss on or we encod that as x minu <num> , <num> bit follow by <num> .
so for exampl , <num> would be encod as two 1s follow by a <num> , wherea <num> would be encod as four 1s follow by <num> , et cetera .
so now , now you can imagin how mani bit do we have to us for a larg number like <num> .
so , how mani bit do i have to us for exactli for a number like <num> ?
well , exactli , we have to us <num> bit , but so , it's the same number of bit as the valu of thi number .
so , thi is veri ineffici .
if you were like to see some larg number , imagin if you occasion see a number like <num> , you have to us <num> bit .
so , thi onli work where if you ar absolut sure that there would be no larg number .
mostli veri frequent , thei're often us veri small number .
now , how do you decod thi code ?
sinc these ar variabl length in code method , and you can't just count how mani bit and then just stop .
right ?
you can sai eight bit or <num> bit , then you , you will start anoth code .
there ar variabl length , so , you have to reli on some mechan .
in thi case for unari , you can see it's veri easi to see the boundari .
now you can easili see <num> would signal the end of encod .
so you just count how mani 1s you have seen , and then you hit the <num> .
you know you have finish on number , you start anoth number .
now which is to start at unari code is to aggress in reward small number .
and if you occasion can see a veri big number , it will be a disast .
so what about some other less aggress method ?
well , gamma code is on of them .
and in thi method , we can do , us unari code for a transform form of the valu .
so it's <num> plu the flow of log of x .
so the magnitud of thi valu is much lower than the origin , x .
so that's why we have four us urinari code for that so , and so we , first we have the urinari code for code thi log of s .
and thi will be follow by a uniform code or binari code , and thi is basic the same uniform code and binari code ar the same .
and we're go to us thi code to code the remain part of the valu of x .
and thi is basic , precis , x minu <num> , <num> to the flow of log of x .
so the unari code or basic code with a flow of log of x , well , i ad on there , and here .
but the remain part will , we us uniform code to actual code the differ between the x and and thi , <num> to the log of x .
and , and it's easi to to show that for thi thi valu , there's differ .
we onli need to us up to , thi mani bit and in flow of log of x bit .
and thi is easi to understand , if the differ is too larg then we would have a higher flow of log of x .
so , here ar some exampl .
for exampl , <num> is encod as <num> .
the first two digit ar the unari code .
right .
so , thi is for the valu <num> .
right .
<num> encod <num> in unari code .
and so , that mean log of x , the flow of log of x is <num> , becaus we will actual us unari code to encod <num> plu the flow of log of x .
sinc thi is <num> , then we know that the floor of log of x is actual <num> .
so but , <num> is still larger than <num> to the <num> , so the differ is <num> , and that <num> is encod here at the end .
so that's why we have <num> for <num> .
now , similarli <num> is encod as <num> follow by <num> .
and in thi case , the unari code encod <num> .
so , thi is the unari code for <num> and so the floor of log of x is <num> .
and that mean , we will comput the differ between <num> and the <num> to the <num> , and that's <num> , and so we now have again <num> at the end .
but thi time , we're go to us two bit becaus with thi level of flow of log of x , we could have more number , <num> , <num> , <num> .
thei would all share the same prefix here , <num> .
so , in order to differenti them , we have to us two bit , in the end to differenti them .
so you can imagin <num> would be , <num> here in the end instead of <num> , after <num> .
it's also true that the form of a gamma code is alwai , the first odd number of bit , and in the center , there wa a <num> .
that's the end of the unari code .
and befor that , or to , on the left side of thi <num> , there will be all 1s .
and on the right side of thi <num> , it's binari code or uniform code .
so how can you decod such a code ?
well , you again first do unari code , right ?
onc you hit <num> , you know you have got the unari code .
and thi also will tell you how mani bit you have to read further to decod the uniform code .
so thi is how you can decod a gamma code .
there is also delta code , but that's basic same as gamma code , except that you replac the unari prefix with the gamma code .
so that's even less conserv than gamma code , in term of avoid the small integ .
so that mean it's okai if you occasion see a larg number .
it's , it's , you know , it's okai with delta code .
it's also fine with gamma code .
it's realli a big loss for unari code , and thei ar all oper , of cours , at differ degre of favor short favor small integ .
and that also mean thei would appropri for sort distribut .
but none of them is perfect for all distribut .
and which method work , the best would have to depend on the actual distribut in your data set .
for invert index , compress , peopl have found that gamma code seem to work well .
so how to uncompress invert index ?
we just , talk about thi .
firstli , you decod those encod integ .
and we just , i think discuss how we decod unari code and gamma code .
so i won't repeat .
what about the document id that might be compress us d gap ?
well , we're go to do sequenti decod .
so suppos the encod idealist is x1 , x2 , x3 et cetera .
we first decod x1 to obtain the first document id , id1 .
then , we will decod x2 , which is actual the differ between the second id and the first on .
so we have to add the decod valu of x2 to id1 to recov the valu of the , the id at thi secondari posit , right .
so thi is where you can see the advantag of , convert document id into integ .
and that allow us to do thi kind of compress , and we just repeat until we decod all the document .
everi time we us the document id in the previou posit to help recov the document id in the next posit .
thi lectur is about how to do fast research by us invert index .
in thi lectur , we ar go to continu the discuss of the system implement .
in particular , we're go to talk about , to how to support a faster search by us invert index .
so , let's think about what a gener score function might look like .
now , of curs the vector space model is a special case of thi .
but we can imagin mani other retriev function of the same form .
so , the form of thi function is as follow .
we see thi score function of document d , and queri q is defin as first , a function of f a that's adjust in the function .
that what consid two factor that ar shown here at the end , f sub d of d , and f sub q of q .
these ar adjust factor of a document and queri , so thei're at the level of document , and queri .
so , and then insid of thi function we also see there's a anoth function call edg .
so , thi is the main part of the score function , and these as i just said of the score factor at the level of the whole document , and the queri .
for exampl , document and thi aggreg function would then combin all these .
now , insid thi h function , there ar function that would comput the weight of the contribut of a match queri term t i .
so , thi thi g , the function g give us the weight of a match queri term t i in document d .
and thi h function with that aggreg all these weight , so it were , for exampl , take a sum , but it of all the match queri in that term .
but it can also be a product , or could be anoth wai of aggreg them .
and then final , thi adjust function would then consid the document level , or queri level factor through further adjust score , for exampl , document len so , thi gener form would cover mani state of origin function .
let's look at how we can score such score document with such a function us invert index .
so here's the gener algorithm that work as follow .
first these these queri level and document level factor can be pre comput in the index term .
of cours , for the queri , we have to comput it as a queri term .
but for document , for exampl , document can be pre comput .
and then we maintain a score accumul for each document d to comput the h .
and h is aggreg function of all the match queri term .
so how do we do that ?
well , for each queri term , we go to do fetch invert list , from the invert index .
thi will give us all the document that match thi queri term , and that includ d1 , f1 , and so , d and fn .
so each pair is document id and the frequenc of the term in the document .
then for each entri d sub j and f sub j , a particular match of the term in thi particular document d sub j , we're go to comput the function g .
that would give us someth like a t of i , ef weight of thi term .
so , we're comput the weight contribut of match thi queri term in thi document .
and then we're go to updat the score accumul for thi document .
and thi would allow us to add thi to our accumul , that would increment comput function h .
so thi is basic a gener wai to allow sort of comput all function of thi form , by us invert index .
note that we don't have to attach ani document that that didn't match ani queri term , but thi is why it's fast .
we onli need to process the document that tap , that match at least on queri term .
in the end , then we're go to adjust the score to comput a , thi function f of a and then we can sort .
so let's take a look at the specif exampl .
in thi , case let's assum the score function's a veri simpl on .
it just take us sum of tf , the rule of tf , the count of , of term in the document .
now thi simpl equat with the help show the algorithm clearli .
it's veri easi to extend the , the comput to includ other weight like the transform of tf or document or idf weight .
so let's take a look at specif exampl with the queri's inform secur , and show some entri of the invert index on the right side .
inform occur befor document and the frequenc is also there , secur is code three document .
so , let's see how the algorithm work , all right ?
so , first we iter all the queri term , and we fetch the first queri then .
what is that ?
that's inform .
right ?
so , and imagin we have all these score accumul to score , score the , score the score for these document .
we can imagin there will be alloc , but then thei will onli be alloc as need .
so befor we do ani weight of term we don't even need a score accumul .
but conceptu we have these score accumul eventu alloc , right ?
so let's fetch the , the entri from the invert list for inform first , that's the first on .
so these score accumul obvious would be initi as zero .
so the first entri is d1 and <num> , <num> is occurr of inform in thi document .
sinc our score function assum that the score is just a sum of these raw count .
we just need to add a <num> to the score accumul to account for the increas of score , due to match thi term inform , a document d1 .
and now we go to the next entri .
that's d2 and <num> and then we'll add a <num> to the score accumul of d2 .
of cours , at thi point we will alloc the score accumul as need .
and so , at thi point , we have locat d1 and d2 , and the next on is d3 .
and we add <num> , or we locat anoth score come in the spot d3 and add <num> to it .
and final , the d4 get a <num> becaus the inform the term inform occur ti in five time in thi document .
okai , so thi complet the process of all the entri in the , invert index for inform .
it's process all the contribut of match inform in thi four document .
so now our arrow will go to the next queri term , that's secur .
so , we're go to factor all the invert index entri for secur .
so in thi case , there were three entri .
and we're go to go through each of them .
the first is d2 and <num> .
and that mean secur occur three time in d2 , and what do we do ?
well , we do exactli the same as what we did for inform .
so thi time we're go to do chang the score , accumul d2 see it's alreadi alloc .
and what we do is we'll add <num> to the exist valu which is a <num> , so we now get the <num> for d2 .
d2 sc , score is increas becaus of the match both inform and the secur .
go to the next step entri , that's d4 and <num> , so we've updat the score for d4 , and again we add <num> to d4 , so d4 goe from <num> to <num> .
final we process d5 and <num> .
sinc we have not yet equat a score accumul d4 to d5 , at thi point , we alloc on , <num> and we're go to add <num> to it .
so , those score on the last row ar the final score for these document .
if our score function is just a , a simpl sum of tf valu .
now what if we actual would like to , to do land normal .
well we can do the normal at thi point for each document .
so to summar thi , all right so you can see we first process the inform determin queri term inform , and we process all the entri in the invert index for thi term .
then we process the secur , all right , let's think about the what should be the order of process here when we consid queri term ?
it might make differ , especi if we don't want to keep to keep all the score accumul .
let's sai we onli want to keep the most promis score accumul .
what do you think it would be a good order to go through ?
would you go would you process a common term first or would you process a rare term first ?
the answer is we should go through we should process the rare term first .
a rare term will match fewer document and then the score confus will be higher , becaus the idf valu will be higher and , and then it allow us to attach the most diplomaci document first .
so it help prune some non promis on , if we don't need so mani document to be return to the user .
and so those ar heurist for further improv the accuraci .
here can also see how we can incorpor the idea of weight .
all right .
so thei can when we incorpor a on wai process each queri term .
when we fetch in word index we can fetch the document frequenc , and then we can comput the idf .
or mayb perhapsidf valu ha alreadi been pre comput when we index the document .
at that time we alreadi comput the idf valu that we can just fetch it .
so all these can be down at thi time .
so that will mean on will process all the entri for inform these these weight would be adjust by the same idf , which is idf for inform .
so thi is the basic idea of us invert index for faster search , and work well for all kind of formula that ar of the gener form and thi gener cov , the gener form cover actual most state of the art retriev function .
so there ar some trick to further improv the effici , some gener mac tech , techniqu includ cach .
thi is just a to store some result of popular queri's , so that next time when you see the same queri you simpli return the store result .
similarli , you can also score the miss of invert index in the memori for popular term .
and if the queri come popular you will assum it will fetch the invert index for the same term again .
so keep that in the memori would help .
and these ar gener techniqu for improv effici .
we can also onli keep the most promis accumul becaus a user gener doesn't want to examin so mani document .
we onli want to return high qualiti subset of document that like rank on the top , in , in for that purpos we can then prune the accumul .
we don't have to store all the accumul .
at some point we just keep the highest valu accumul .
anoth techniqu is to do parallel process , and that's need for realli process such a larg data set , like the web data set .
and to scale up to the web scale we need to special to have the special techniqu to do parallel process and to distribut the storag of file on multipl machin .
so here as a , here is a list of some text retriev toolkit .
it's , it's not a complet list .
you can find the more inform at thi url on the bottom .
here i list four here , lucen is on of the most popular toolkit that can support a lot of applic .
and it ha veri nice support for applic .
you can us it to build a search engin veri quickli , the downsid is that it's not that easi to extend it , and the algorithm increment there ar not the most advanc algorithm .
lemur or indri is anoth toolkit that that doe not have such a nice support applic as lucen .
but it ha mani advanc search algorithm .
and it's also easi to extend .
terrier is yet anoth toolkit that also ha good support for quotat capabl and some advanc algorithm .
so that's mayb in between lemur , or lucen or mayb rather combin the strand of both , so that's also us toolkit .
meta is the toolkit that we'll us for the program assign , and thi is a new toolkit that ha a combin of both text retriev algorithm and text mine algorithm .
and so , toolkit model ar implement , thei ar , there ar a number of text analysi algorithm , implement in the toolkit , as well as basic research algorithm .
so , to summar all the discuss about the system implement , here ar the major take awai point .
invert index is the primari data structur for support a search engin .
that's the kei to enabl faster respons to a user's queri .
and the basic idea is process that , pre process the data as much as we can , and we want to do compress when appropri .
so that we can save disk space and can speed up io and process of the invert index in gener .
we'll talk about how we will construct the invert index when the data can fit into the memori .
and then we talk about faster search us invert index , basic to exploit the invert index to accumul score for document match a queri term .
and we exploit zipf's law avoid touch mani document that don't match ani queri term .
and thi algorithm can , can support a wide rang of rank algorithm .
so these basic techniqu have mm , have great potenti for further scan output us distribut to withstand parallel process and the cach .
here ar two addit read that you can take a look at if you have time , and ar interest in learn more about thi .
the first on is a classic textbook on the scare the effici of invert index and the compress techniqu , and how to in gener , build a effici search engin in term of the space overhead and speed .
the second on is a newer textbook that ha a nice discuss of implement and evalu search engin .
thi lectur is about evalu of text retriev system .
in the previou lectur , we have talk about a number of text retriev method .
differ kind of rank function .
but how do we know which on work the best ?
in order to answer thi question , we have to compar them , and that mean we'll have to evalu these retriev method .
so thi is the main topic of thi lectur .
first , let's think about why do we have to do evalu ?
i alreadi gave on reason .
and that is , we have to us evalu to figur out which retriev method work better .
now thi is veri import for advanc our knowledg .
otherwis we wouldn't know whether a new idea work better than old idea .
in the begin of thi cours we talk about the , the problem of text retriev we compar it with databas retriev .
there , we mention that text retriev is imper to find the problem .
so , evalu must reli on user , which system work better , that would have to be judg by our user .
so thi becom veri challeng problem .
becaus how can we get user involv in , in matter , and how can we draw a fair comparison of differ method .
so just go back to the reason for evalu .
i list two reason here .
the second reason is basic what i just said but there is also anoth reason , which is to assess the actual util of a test region system .
now imagin you're build your own applic .
would be interest in know how well your search engin work for your user .
so in thi case measur must reflect the util to the actual user in the the real applic .
and typic , thi ha been done by us user studi and us the real search engin .
in the second case or for the second reason , the measur actual all need to be correl with the util to actual user .
thu thei don't have to accur reflect the , the exact util to user .
so the measur onli need to be good enough to tell which method work better .
and thi is usual done through test collect .
and thi is the main idea that we'll be talk about in thi cours .
thi ha been veri import for compar differ algorithm and for improv search engin system in gener .
so next we will talk about what to measur .
there ar mani aspect of a search engin we can measur , we can evalu and here i list the three major aspect .
on is effect or accuraci , how accur ar the search result ?
in thi case we're measur a system's capabl of rank relev document on top of non relev on .
the second is effici .
how quickli can a user get the result ?
how much comput resourc ar need to answer a queri ?
so in thi case we need to measur the space and time overhead of the system .
the third aspect is usabl .
basic the question is how us is the system for real user task ?
here , obvious , interfac and mani other thing ar also import and we typic would have to do user studi .
now , in thi cours , we're go to talk more , mostli about the effect and accuraci measur becaus , the effici and usabl dimens ar , not realli uniqu to search engin , and so , thei ar , need for evalu ani other softwar system .
and there is also good coverag of such materi in other cours .
but how to evalu a search engin is quit , you know accuraci is someth you need to text retriev , and we're go to talk a lot about thi .
the main idea that peopl have propos befor us a attitud , evalu a text retriev algorithm , is call the cranfield evalu methodolog .
thi on actual wa develop long time ago , develop in the 1960s .
it's a methodolog for laboratori test of system compon , it's actual a methodolog that ha been veri us , not just for search engin evalu .
but also for evalu virtual all kind of empir task .
and , for exampl in process or in other field where the problem is empir defin we typic would need to us to us such a methodolog .
and todai wa the big data challeng with the us of machin learn everi where .
we gener , thi methodolog ha been veri popular , but it wa first develop for search engin applic in the 1960s .
so the basic idea of thi approach is it'll build a reusabl test collect and defin measur .
onc such a test collect is build it can be us again and again to test the differ algorithm .
and we're go to defin measur that would allow you to quantifi perform of a system or an , an algorithm .
so how exactli would thi work ?
well , we're go to do , have assembl collect of document and thi is just similar to real document collect in your search applic .
we can also have a sampl set of queri or topic .
thi is to simul the user's queri .
then we'll have to have relev judgment .
these ar judgment of which document should be return for which queri .
ideal , thei have to made by user who formul the queri becaus those ar the peopl that know exactli what document would be us for .
and then final we have to have measur to quantifi how well a system's result match the ideal rank list .
that would be construct and base on user' relev judgement .
so thi methodolog is veri us for start retriev algorithm becaus the test can actual , can be reus mani time .
and it will also provid a fair comparison for all the method .
we have the same criteria , same data set to us and to compar differ algorithm .
thi allow us to compar a new algorithm with an old algorithm , that wa the method of mani year ago .
by us the same standard .
so thi is the illustr of how thi work , so as i said , we need a queri that ar shown here .
we have q1 , q2 , et cetera .
we also need a document , and that's call the document collect , and on the right side , you see we need relev judgment .
these ar basic the binari judgment of document with respect to a queri .
so , for exampl d1 is judg as be relev to q1 , d2 is judg as be relev as well .
and d3 is judg as non relev in the two , q1 , et cetera .
these would be creat by user .
onc we have these , and we basic have a test , correct , and then , if you have two system , you want to , compar them .
then you can just run each system on these queri and document and each system will then return result .
let's sai if the queri is q1 and then we would have the result here , here i show r sub a as result from system a .
so , thi is rememb we talk about task of comput approxim of the , relev document setter .
so a is , the system a's approxim here , and also b is system b's approxim of relev document .
now let's take a look at these result .
so which is better ?
now imagin for a user which on would you like ?
all right let take a look at both result .
and there ar some differ and there ar some document that ar return to both system .
but if you look at the result you will feel that well , mayb an a is better in the sens that we don't have mani number in document .
and among the three document return the two of them ar relev , so that's good , it's precis .
on the other hand can also sai mayb b is better becaus we've got more relev document , we've got three instead of two .
so which on is better and how do we quantifi thi ?
well obvious , thi question highli depend on a user's task .
and , it depend on user as well .
you might be abl to imagin , for some user mai be system made is better .
if the user is not interest in get all the relev document , right , in thi case thi is the user doesn't have to read .
user would see most relev document .
on the other hand on on count , imagin user might need to have as mani relev document as possibl , for exampl , take a literatur survei .
you might be in the second categori , and then you might find that system b's better .
so in either case , we'll have to also defin measur that would quantifi them .
and we might need to defin multipl measur becaus user have differ perspect of look at result .
thi lectur is about the , the basic measur for evalu of text origin system .
in thi lectur , we're go to discuss how we design basic measur to quantit , compar two origin system .
thi is a slide that you have seen earlier in the lectur , where we talk about the grand evalu methodolog .
we can have a test collect that consist of queri , document and relev judgement .
we can then run two system on these da , data set to , quantit evalu your perform .
and we rais to the question about , which settl result is better is system a better or system b better ?
so let's now talk about how to actual quantifi their perform .
suppos we have a total of , of <num> random document in the current folder for thi queri .
now , the relev judgement shown on the right , did not includ all the ten obvious .
and we have onli seen three render document there but we can imagin there ar other random document in judg for thi queri .
so now , intuit we thought that system a is better becaus it did not have much nois .
and in particular we have seen , amount of three result , two of them ar relev but in system b we have five result and onli three of them ar relev .
so intuit , it look like system a is more accur .
and thi can be captur by a match order precis .
where we simpli comput to what extent all the retriev result ar relev .
if you have <num> precis that would mean all the retriev document ar relev .
so , in thi case the system a ha a precis of two out of three .
system b as three over five .
and thi show that system a is better by precis .
but we also talk about system b might be prefer by some other user hold like to retriev as mani relev document as possibl .
so , in that case we have to compar the number of relev document that retriev .
and there is an other measur call a recal .
thi measur the complet of coverag of relev document in your retriev result .
so , we just assum that there ar ten relev document in the collect .
and here we've got two of them in system a , so the recal is two out of ten .
where as system b ha got a three , so it's a three out of ten .
now , we can see by recal system b is better and these two measur turn out to be the veri basic measur for evalu search engin .
and thei ar veri import becaus thei ar also wide us in mani other test variat problem .
for exampl , if you look at the applic of machin learn you tend to see precis recal number be report for all kind of task .
okai , so now , let's defin these two measur more precis and these measur ar to evalu a set of retriev document .
so that mean we ar consid that approxim of a set of relev document .
we can distinguish it four case , depend on the situat of a document .
a document that can be retriev or not retriev , right ?
becaus we're talk about the set of result .
the document can be also relev or not relev , depend on whether the user think thi is a us document .
so , we can now have count of document in each of the four categori .
we can have a to repres the number of document that ar retriev and relev , b for document that ar not retriev but relev , etc .
now , with thi tabl , then we have defin precis .
as the , ratio of , the relev retriev document a to the total number of retriev document .
so thi is just you know , a divid by the sum of a and c .
the sum of thi column .
signal recal is defin by divid a by the sum of a and b .
so that's , again , to divid a by the sum of the rule , instead of the column .
all right , so we go to see precis and recal is all focus on look at the a , that's the number of retriev relev document , but we're go to us differ denomin .
okai , so what would be an ideal result ?
well , you can abl to see in ideal case we have precis and recal , all to be <num> that mean we have got <num> of all the random document in our result .
and all the result that we return ar relev .
there's no singl not relev document return .
the realiti howev , high recal tend to be associ with low precis and you can imagin why that is the case .
as you go down the distant to try to get as mani relev action as possibl .
you tend to in time a lot of non relev document , so the precis goe down .
look at thi set , can also be defin by a cutoff in a rank list .
that's why , although these two measur ar defin for a set of retriev document , thei ar actual veri us for evalu a rank list .
thei ar the fundament measur in tension retriev and mani other task .
we often ar interest in to the precis up to ten document for web search .
thi mean we look at the , how mani document among the top result ar actual relev .
now , thi is a veri meaning measur , becaus it tell us how mani relev document a user can expect to see .
on the first page of search result , where thei typic show ten result .
so , precis and recal ar , the basic measur and we need to us them to further evalu a search engin but thei ar the build block realli .
we just to sai that there tend to be a trade off between precis and recal .
so , natur it would be interest to combin them and here's on measur that's often us , call f measur .
and it's harmon mean of precis and recal , it's defin on thi slide .
so you can see it first comput , invers of r and p here and then it would be interpret to by us a co , coeffici .
depend on the paramet beta and after some transform we can easili see it would be of thi form .
and in mani case it's just a combin of precis and recal .
and , and beta is a paramet that's often set to on .
it can control the emphasi on precis or recal .
when we set , beta to on we end up by have a special case of f measur , often call f1 .
thi is a popular measur , that is often us as a combin precis and recal .
and the formula look veri simpl it's just thi , here .
now it's easi to see that if you have , a larger precis or larger recal than f measur would be high .
but what's interest is that , the trade off between precis and recal , is captur in an interest wai in f1 .
so , in order to understand that , we , can first look at the natur question .
why not just the , combin them us a simpl arithmet mean as a here .
that would be like the most natur wai of combin them .
so , what do you think ?
if you want to think more , you can paus the media .
so why is thi not as good as f1 ?
or what's the problem with thi ?
now , if you think about the arithmet mean , you can see that thi is the sum of , of multipl term .
in thi case , thi is the sum of precis and recal .
in the case of the sum , the total valu tend to be domin by the larg valu .
that mean if you have a veri high p or a veri high r , then you realli don't care about the , whether the other vari is low .
so , the whole sum would be high .
now , thi is not the desir becaus on can easili have a perfect recal .
we can have a perfect recal is it ?
can you imagin how ?
it's probabl veri easi to imagin that we simpli retriev all the document in the collect , then we have a perfect recal and thi will give us <num> as the averag .
but search result ar clearli not veri us for user , even though the , the averag us thi formula would be rel high .
now , in contrast , you can see f1 will reward a case where precis and recal ar roughli but similar .
so , it would paralyz a case where you have extrem high matter for on of them .
so , thi mean f1 encod a differ trade off between that .
now thi exampl show actual , a veri import methodolog here .
when we try to solv a problem , you might natur think of on solut .
let's sai , in thi case , it's thi arithmet mean .
but it's import that not to settl on thi solut .
it's import to think whether you have other wai to combin them .
and onc you think about the multipl varianc .
it's import to analyz their differ and then think about which on make more sens .
in thi case , if you think more carefulli you will feel that if on problem make more sens .
then the simpl arithmet mean .
although in other case , there mai be , differ result .
but in thi case , the arithmet mean , seem not reason .
but if you don't pai attent to these subtl differ , you might just , take an easi wai to combin them and then go ahead with it .
and here later you'll find that , hm , the measur doesn't seem to work well .
right so , at thi methodolog is actual veri import in gener in solv problem and try to think about the best solut .
try to understand that the problem , veri well and then know why you need thi measur , and why you need to combin precis and recal .
and then us that to guid you in find a good wai to solv the problem .
to summar , we talk about precis , which address the question , ar the retriev result all relev ?
we'll also talk about the recal , which address the question , have all the relev document been retriev ?
these two ar the two basic measur in test retriev in variat .
thei ar ar us for , for mani other task as well .
we'll talk about f measur as a wai to combin precis and recal .
we also talk about the trade off between precis and recal .
and thi turn out to depend on the user search task and we'll discuss thi point more in the later lectur .
thi lectur is about , how we can evalu a rank list ?
in thi lectur , we will continu the discuss of evalu .
in particular , we ar go to look at , how we can evalu a rank list of result .
in the previou lectur , we talk about , precis recal .
these ar the two basic measur for , quantit measur the perform of a search result .
but , as we talk about , rank , befor , we frame that the text of retriev problem , as a rank problem .
so , we also need to evalu the , the qualiti of a rank list .
how can we us precis recal to evalu , a rank list ?
well , natur , we have to look after the precis recal at differ , cut off .
becaus in the end , the approxim of relev document , set , given by a rank list , is determin by where the user stop brows .
right ?
if we assum the user , secur brows , the list of result , the user would , stop at some point , and that point would determin the set .
and then , that's the most import , cut off , that we have to consid , when we comput the precis recal .
without know where exactli user would stop , then we have to consid , all the posit where the user could stop .
so , let's look at these posit .
look at thi slide , and then , let's look at the , what if the user stop at the , the first document ?
what's the precis recal at thi point ?
what do you think ?
well , it's easi to see , that thi document is so , the precis is on out of on .
we have , got on document , and that's relev .
what about the recal ?
well , note that , we're assum that , there ar ten relev document , for thi queri in the collect , so , it's on out of ten .
what if the user stop at the second posit ?
top two .
well , the precis is the same , <num> , two out of two .
and , the record is two out of ten .
what if the user stop at the third posit ?
well , thi is interest , becaus in thi case , we have not got ani , addit relev document , so , the record doe not chang .
but the precis is lower , becaus we've got number so , what's exactli the precis ?
well , it's two out of three , right ?
and , recal is the same , two out of ten .
so , when would see anoth point , where the recal would be differ ?
now , if you look down the list , well , it won't happen until , we have , see anoth relev document .
in thi case d5 , at that point , the , the recal is increas through three out of ten , and , the precis is three out of five .
so , you can see , if we keep do thi , we can also get to d8 .
and then , we will have a precis of four out of eight , becaus there ar eight document , and four of them ar relev .
and , the recal is a four out of ten .
now , when can we get , a recal of five out of ten ?
well , in thi list , we don't have it , so , we have to go down on the list .
we don't know , where it is ?
but , as conveni , we often assum that , the precis is zero , at all the , the oth , the precis ar zero at all the other level of recal , that ar beyond the search result .
so , of cours , thi is a pessimist assumpt , the actual posit would be higher , but we make , make thi assumpt , in order to , have an easi wai to , comput anoth measur call averag precis , that we will discuss later .
now , i should also sai , now , here you see , we make these assumpt that ar clearli not , accur .
but , thi is okai , for the purpos of compar to , text method .
and , thi is for the rel comparison , so , it's okai , if the actual measur , or actual , actual number deviat a littl bit , from the true number .
as long as the deviat , is not bias toward ani particular retriev method , we ar okai .
we can still , accur tell which method work better .
and , thi is import point , to keep in mind .
when you compar differ algorithm , the kei's to avoid ani bia toward each method .
and , as long as , you can avoid that .
it's okai , for you to do transform of these measur anywai , so , you can preserv the order .
okai , so , we'll just talk about , we can get a lot of precis recal number at differ posit .
so , now , you can imagin , we can plot a curv .
and , thi just show on the , x axi , we show the recal .
and , on the y axi , we show the precis .
so , the precis line wa mark as . <num> , . <num> , . <num> , and , <num> .
right ?
so , thi is , the differ , level of recal .
and , , the y axi also ha , differ amount , that's for precis .
so , we plot the , these , precis recal number , that we have got , as point on thi pictur .
now , we can further , and link these point to form a curv .
as you'll see , we assum all the other , precis as the high level recal , be zero .
and , that's why , thei ar down here , so , thei ar all zero .
and thi , the actual curv probabl will be someth like thi , but , as we just discuss , it , it doesn't matter that much , for compar two method .
becaus thi would be , underestim , for all the method .
okai , so , now that we , have thi precis recal curv , how can we compar rank to back list ?
all right , so , that mean , we have to compar two pr curv .
and here , we show , two case .
where system a is show red , system b is show blue , there's cross .
all right , so , which on is better ?
i hope you can see , where system a is clearli better .
why ?
becaus , for the same level of recal , see same level of recal here , and you can see , the precis point by system a is better , system b .
so , there's no question .
in here , you can imagin , what doe the code look like , for ideal search system ?
well , it ha to have perfect , precis at all the recal point , so , it ha to be thi line .
that would be the ideal system .
in gener , the higher the curv is , the better , right ?
the problem is that , we might see a case like thi .
thi actual happen often .
like , the two curv cross each other .
now , in thi case , which on is better ?
what do you think ?
now , thi is a real problem , that you actual , might have face .
suppos , you build a search engin , and you have a old algorithm , that's shown here in blue , or system b .
and , you have come up with a new idea .
and , you test it .
and , the result ar shown in red , curv a .
now , your question is , is your new method better than the old method ?
or more , practic , do you have to replac the algorithm that you're alreadi us , your , in your search engin , with anoth , new algorithm ?
so , should we us system , method a , to replac method b ?
thi is go to be a real decis , that you to have to make .
if you make the replac , the search engin would behav like system a here , wherea , if you don't do that , it will be like a system b .
so , what do you do ?
now , if you want to spend more time to think about thi , paus the video .
and , it's actual veri us to think about that .
as i said , it's a real decis that you have to make , if you ar build your own search engin , or if you're work , for a compani that , care about the search .
now , if you have thought about thi for a moment , you might realiz that , well , in thi case , it's hard to sai .
now , some user might like a system a , some user might like , like system b .
so , what's the differ here ?
well , the differ is just that , you know , in the , low level of recal , in thi region , system b is better .
there's a higher precis .
but in high recal region , system a is better .
now , so , that also mean , it depend on whether the user care about the high recal , or low recal , but high precis .
you can imagin , if someon is just go to check out , what's happen todai , and want to find out someth relev in the new .
well , which on is better ?
what do you think ?
in thi case , clearli , system b is better , becaus the user is unlik examin a lot of result .
the user doesn't care about high recal .
on the other hand , if you think about a case , where a user is do you ar , start a problem .
you want to find , whether your idea ha , ha been start befor .
in that case , you emphas high recal .
so , you want to see , as mani relev document as possibl .
therefor , you might , favor , system a .
so , that mean , which on is better ?
that actual depend on user , and more precis , user task .
so , thi mean , you mai not necessarili be abl to come up with on number , that would accur depict the perform .
you have to look at the overal pictur .
yet , as i said , when you have a practic decis to make , whether you replac our with anoth , then you mai have to actual come up with a singl number , to quantifi each , method .
or , when we compar mani differ method in research , ideal , we have on number to compar , them with , so , that we can easili make a lot of comparison .
so , for all these reason , it is desir to have on , singl number to match it up .
so , how do we do that ?
and , that , need a number to summar the rang .
so , here again it's the precis recal curv , right ?
and , on wai to summar thi whole rank , list , for thi whole curv , is look at the area underneath the curv .
right ?
so , thi is on wai to measur that .
there ar other wai to measur that , but , it just turn out that , , thi particular wai of match it ha been veri , popular , and ha been us , sinc a long time ago for text and , thi is , basic , in thi wai , and it's call the averag precis .
basic , we're go to take a , a look at the , everi differ , recal point .
and then , look out for the precis .
so , we know , you know , thi is on precis .
and , thi is anoth , with , differ recal .
now , thi , we don't count to thi on , becaus the recal level is the same , and we're go to , look at the , thi number , and that's precis at a differ recal level et cetera .
so , we have all these , you know , ad up .
these ar the precis at the differ point , correspond to retriev the first relev document , the second , and then , the third , that follow , et cetera .
now , we miss the mani relev document , so , in all of those case , we just , assum , that thei have zero precis .
and then , final , we take the averag .
so , we divid it by ten , and which is the total number of relev document in the collect .
note that here , we're not divid thi sum by four .
which is a number retriev relev document .
now , imagin , if i divid by four , what would happen ?
now , think about thi , for a moment .
it's a common mistak that peopl , sometim , overlook .
right , so , if we , we divid thi by four , it's actual not veri good .
in fact , that you ar favor a system , that would retriev veri few random document , as in that case , the denomin would be veri small .
so , thi would be , not a good match .
so , note that thi denomina , denomin is ten , the total number of relev document .
and , thi will basic , comput the area , and the need occur .
and , thi is the standard method , us for evalu a rank list .
note that , it actual combin recal and , precis .
but first , you know , we have precis number here , but secondli , we also consid recal , becaus if miss mani , there would be mani zero here .
all right , so , it combin precis and recal .
and furthermor , you can see thi measur is sensit to a small chang of a posit of a relev document .
let's sai , if i move thi relev document up a littl bit , now , it would increas thi mean , thi averag precis .
wherea , if i move ani relev document , down , let's sai , i move thi relev document down , then it would decreas , uh , the averag precis .
so , thi is a veri good , becaus it's a veri sensit to the rank of everi relev document .
it can tell , small differ between two rank list .
and , that is what we want , sometim on algorithm onli work slightli better than anoth .
and , we want to see thi differ .
in contrast , if we look at the precis at the ten document .
if we look at thi , thi whole set , well , what , what's the precis , what do you think ?
well , it's easi to see , that's a four out of ten , right ?
so , that precis is veri meaning , becaus it tell us , what user would see ?
so , that's pretti us , right ?
so , it's a meaning measur , from a user perspect .
but , if we us thi measur to compar system , it wouldn't be good , becaus it wouldn't be sensit to where these four relev document ar rank .
if i move them around the precis at ten , still , the same .
right .
so , thi is not a good measur for compar differ algorithm .
in contrast , the averag precis is a much better measur .
it can tell the differ of , differ , a differ in rank list in , subtl wai .
so averag precis is comput for just on .
on queri .
but we gener experi with mani differ queri and thi is to avoid the varianc across queri .
depend on the queri you us you might make differ conclus .
right , so it's better then us more queri .
if you us more queri then , you will also have to take the averag of the averag precis over all these queri .
so how can we do that ?
well , you can natur .
think of just do arithmet mean as we alwai tend to , to think in , in thi wai .
so , thi would give us what's call a mean averag posit , or map .
in thi case , we take arithmet mean of all the averag precis over sever queri or topic .
but as i just mention in anoth lectur , is thi good ?
we call that .
we talk about the differ wai of combin precis and recal .
and we conclud that the arithmet mean is not as good as the map measur .
but here it's the same .
we can also think about the altern wai of aggreg the number .
don't just automat assum that , though .
let's just also take the arithmet mean of the averag posit over these queri .
let's think about what's the best wai of aggreg them .
if you think about the differ wai , natur you will , probabl be abl to think about anoth wai , which is geometr mean .
and we call thi kind of averag a gmap .
thi is anoth wai .
so now , onc you think about the two differ wai .
of do the same thing .
the natur question to ask is , which on is better ?
so .
so , do you us map or gmap ?
again , that's import question .
imagin you ar again test a new algorithm in , by compar the wai your old algorithm made the search engin .
now you test multipl topic .
now you've got the averag precis for these topic .
now you ar think of look at the overal perform .
you have to take the averag .
but which , which strategi would you us ?
now first , you should also think about the question , well did it make a differ ?
can you think of scenario where us on of them would make a differ ?
that is thei would give differ rank of those method .
and that also mean depend on the wai you averag or detect the .
averag of these averag posit .
you will get differ conclus .
thi make the question becom even more import .
right ?
so , which on would you us ?
well again , if you look at the differ between these .
differ wai of aggreg the averag posit .
you'll realiz in arithmet mean , the sum is domin by larg valu .
so what doe larg valu here mean ?
it mean the queri is rel easi .
you can have a high pre , averag posit .
wherea gmap tend to be affect more by low valu .
and those ar the queri that don't have good perform .
the averag precis is low .
so if you think about the , improv the search engin for those difficult queri , then gmap would be prefer , right ?
on the other hand , if you just want to .
have improv a lot .
over all the kind of queri or particular popular queri that might be easi and you want to make the perfect and mayb map would be then prefer .
so again , the answer depend on your user , your user task and their pref , their prefer .
so the point that here is to think about the multipl wai to solv the same problem , and then compar them , and think carefulli about the differ .
and which on make more sens .
often , when on of them might make sens in on situat and anoth might make more sens in a differ situat .
so it's import to pick out under what situat on is prefer .
as a special case of the mean averag posit , we can also think about the case where there wa precis on rank in the document .
and thi happen often , for exampl , in what's call a known item search .
where you know a target page , let's sai you have to find amazon , homepag .
you have on relev document there , and you hope to find it .
that's call a known item search .
in that case , there's precis on relev document .
or in anoth applic , like a question and answer , mayb there's onli on answer .
ar there .
so if you rank the answer , then your goal is to rank that on particular answer on top , right ?
so in thi case , you can easili verifi the averag posit , will basic boil down to reciproc rank .
that is , <num> over r where r is the rank posit of that singl relev document .
so if that document is rank on the veri top or is <num> , and then it's <num> for reciproc rank .
if it's rank at the , the second , then it's <num> over <num> .
et cetera .
and then we can also take a , a averag of all these averag precis or reciproc rank over a set of topic , and that would give us someth call a mean reciproc rank .
it's a veri popular measur .
for no item search or , you know , an problem where you have just on relev item .
now again here , you can see thi r actual is meaning here .
and thi r is basic indic how much effort a user would have to make in order to find that relev document .
if it's rank on the top it's low effort that you have to make , or littl effort .
but if it's rank at <num> then you actual have to , read presum <num> document in order to find it .
so , in thi sens r is also a meaning measur and the reciproc rank will take the reciproc of r , instead of us r directli .
so my natur question here is why not simpli us r ?
i imagin if you were to design a ratio to , measur the perform of a random system , when there is onli on relev item .
you might have thought about us r directli as the measur .
after all , that measur the user's effort , right ?
but , think about if you take a averag of thi over a larg number of topic .
again it would make a differ .
right , for on singl topic , us r or us <num> over r wouldn't make ani differ .
it's the same .
larger r with correspond to a small <num> over r , right ?
but the differ would onli show when , show up when you have mani topic .
so again , think about the averag of mean reciproc rank versu averag of just r .
what's the differ ?
do you see ani differ ?
and would , would thi differ chang the oath of system .
in our conclus .
and thi , it turn out that , there is actual a big differ , and if you think about it , if you want to think about it and then , yourself , then paus the video .
basic , the differ is , if you take some of our directori , then .
again it will be domin by larg valu of r .
so what ar those valu ?
those ar basic larg valu that indic that lower rank result .
that mean the relev item rank veri low down on the list .
and the sum that's also the averag that would then be domin by .
where those relev document ar rank in , in , in , in the lower portion of the rank .
but from a user perspect we care more about the highli rank document .
so by take thi transform by us reciproc rank .
here we emphas more on the differ on the top .
you know , think about the differ between <num> and the <num> , it would make a big differ , in <num> over r , but think about the <num> , and <num> , and where and when won't make much differ if you us thi .
but if you us thi there will be a big differ in <num> and let's sai <num> , <num> , right .
so thi is not the desir .
on the other hand , a <num> and <num> won't make much differ .
so thi is yet anoth case where there mai be multipl choic of do the same thing and then you need to figur out which on make more sens .
so to summar , we show that the precis recal curv .
can character the overal accuraci of a rank list .
and we emphas that the actual util of a rank list depend on how mani top rank result a user would actual examin .
some user will examin more .
than other .
an averag person us a standard measur for compar two rank method .
it combin precis and recal and it's sensit to the rank of everi random document .
thi lectur is about how to evalu the text retriev system when we have multipl level of judgment .
in thi lectur we will continu the discuss of evalu .
we're go to look at how to evalu the text retriev system .
and we have multipl level of judgement .
so , so far we have talk about bind judgement , that mean a document is judg as be relev or not relev .
but earlier we will also talk about , relev as a matter of degre .
so we often can distinguish it veri higher relev option , those ar veri us option , from you know , lower rate relev option .
thei ar okai , thei ar us perhap .
and further from non relev document .
those ar not us .
right ?
so imagin you can have rate for these page .
then you would have much more level of rate .
for exampl , here i show an exampl of three level , three were relev .
sorri , three were veri relev .
two for margin relev and on for non relev .
now how do we evalu such a new system us these judgement of us of the map doesn't work , averag of precis doesn't work , precis and record doesn't work becaus thei reli on vinyl judgement .
so let's look at the sum top regular result when us these judgment .
right ?
imagin the user would be mostli care about the top ten result here .
right .
and we mark the the rate level or relev level for these document as shown here .
three , two , on , on , three , et cetera .
and we call these gain .
and the reason why we call it a gain , is becaus the measur that we ar infus is call , ntcg , normal discount of accumul gain .
so thi gain basic can mesh your , how much gain of random inform a user can obtain by look at each document , all right .
so look after the first document the user can gain three point .
look at the non relev document the user would onli gain on point .
right .
look at the multi level relev or margin relev document the user would get two point et cetera .
so thi gain usual match the util of a document from a user's perspect .
of cours if we assum the user stop at the ten document , and we're look at the cutoff at ten we can look after the total gain of the user .
and what's that , well that's simpli the sum of these and we call it the cumul gain .
so if we us a stop at the positua that's just a three .
if the user look at anoth document that's a <num> plu <num> .
if the user look at the more document .
then the cumul gain is more .
of cours , thi is at the cost of spend more time to examin the list .
so cumul gain give us some idea about how much total gain the user would have if the user examin all these document .
now , in ndcg , we also have anoth letter here , d , discount cumul gain .
so why do we want to do discount ?
well , if you look at thi cumul gain , there is on defici which is it did not consid the rank posit of these these document .
so , for exampl look at the , thi sum here and we onli know there is onli on highli relev document , on margin relev document , two non relev document .
we don't realli care where thei ar rank .
ideal , we want these two to be rank on the top .
which is the case here .
but how can we captur that intuit ?
well we have to sai , well thi <num> here is not as good as thi <num> on the top .
and that mean the contribut of , the game from differ posit , ha to be weight by their posit .
and thi is the idea of discount , basic .
so , we're go to sai , well , the first on , doesn't it need to be discount becaus the user can be assum that you alwai see thi document , but the second on , thi on will be discount a littl bit , becaus there's a small possibl that the user wouldn't notic it .
so , we divid thi gain by the weight , base on the posit .
so , log of two , two is the rank posit of thi document and , when we go to the third posit , we , discount even more becaus the number is log of three , and so on and so forth .
so when we take a such a sum then a lowli rank document would not contribut contribut that much as a highli rank document .
so that mean if you , for exampl , switch the posit of thi and let's sai thi posit and thi on , and then you would get more discount if you put for exampl , veri relev document here as oppos to two here .
imagin if you put the three here , then it would have to be discount .
so it's not as good as if you would put the three here .
so thi is the idea of discount .
okai , so n , now at thi point that we have got thi discount cumul gain for measur the util of thi rank list with multipl level of judgment .
so ar we happi with thi ?
well we can us thi rank system .
now we still need to do a littl bit more in order to make thi measur comfort across differ topic .
and thi is the last step .
and by the wai , here we just show that dcg at the ten .
all right .
so thi is the total sum of dcg over all these ten document .
so the last step is call n , normal .
and if we do that then we get normal dcg .
so how do we do that ?
well , the idea here is within the normal dcg by the ideal dcg at the same cutoff .
what is the ideal dcg ?
well thi is a dcg of ideal rank .
so imagin if we have nine document in the whole collect rate a three here and that mean in total we have nine document rate three .
then , our ideal rank the lister would have put all these nine document on the veri top .
so all these would have to be three and then thi would be follow by a two here , becaus that's the best we could do after we have run out of three .
but all these posit would be three .
right ?
so thi would be our ideal rank list .
and then we can comput the dcg for thi ideal rank list .
so thi would be given by thi formula you see here , and so thi idea dcg would be us as the normal dcg .
like so here , and thi idealdcg would be us as a normal .
so you can imagin now normal essenti is to compar the actual dcg with the best decis you can possibl get for thi topic .
now why do we want to do thi ?
well by do thi we'll map the dcg valu in to a rang of zero through on , so the best valu , or the highest valu for everi queri would be on .
that's when you're relev is in fact the idealist .
but otherwis in gener you will be lower than on .
now what if we don't do that ?
well , you can see thi transform or thi number , doesn't realli affect the rel comparison of system for just on topic , becaus thi ideal dcg is the same for all the system .
so the rank of system base on onli dcg would be exactli the same .
as if you rank them base on the normal decis .
the differ howev is when we have multipl topic becaus if we don't do normal , differ topic will have differ scale of dcg .
for a topic like thi on we have nine highli relev document .
the dcg can get realli high .
but imagin that in anoth case there ar onli two veri relev document .
in total in the whole collect .
then the highest dcg that ani system could achiev for such a topic would not be veri high .
so again we face the problem of differ scale of dcg valu and when we take an averag we don't want the averag to be domin by those high valu .
those ar again easi quir .
so by do the normal we have all , avoid the problem .
make all the purist contribut equal to the averag .
so thi is the idea of ndcg .
it's us for measur relev base on much more level relev judgment .
so more in the more gener wai , thi is basic a measur that can be appli through ani rank task with much more level of , of judgment .
and the scale of the judgment can be multipl can be more than binari , not onli more than binari , thei can be multipl level , like on's or five , or even more depend on your applic .
and the main idea of thi measur just to summar , is to measur the total util of the top k document .
so you alwai choos a cutoff , and then you measur the total util .
thi lectur is about some practic issu that you would have to address in evalu of text retriev system .
in thi lectur we will continu the discuss of evalu .
we will cover some practic issu that you will have to solv in actual evalu of text retriev system .
so , in order to creat a test collect , we have to creat a set of queri , a set of document and a set of relev judgment .
it turn out that each is actual challeng to creat .
so first , the document and queri must be repres .
thei must rep , repres the real queri and real document that the user handl .
and we also have to us mani queri and mani document in order to avoid bias conclus .
for the match of relev document with the queri , we also need to ensur that there exist a lot of relev document for each queri .
if a queri ha onli on that is a relev document in the collect then , you know , it's not veri inform to compar differ method us such a queri becaus there is not much room for us to see differ .
so , ideal there should be more relev document in the collect .
but yet the queri also should repres real queri that we care about .
in term of relev judgement , the challeng is to ensur complet judgement of all the document for all the queri , yet , minim human and fault .
becaus we have to us the human labor to label these document .
it's veri labor intens .
and as a result , it's imposs to actual label all of the document for all the queri , especi consid a joint , data set like the web .
so , thi is actual a major challeng .
it's a veri difficult challeng .
for measur , it's also challeng becaus what we want with measur is that with accuraci reflect the perceiv util of user .
we have to consid carefulli what the user care about and then design measur to measur that .
if we , your measur is not measur the right thing , then your conclus would , would be misl .
so it's veri import .
so we're go to talk about a coupl issu here .
on is the statist signific test , and thi also is the reason why we have to us a lot of queri , and the question here is how sure can you be that i observ the differ ?
it doesn't simpli result from the particular queri you choos .
so here ar some sampl result of averag precis for system a and system b in two differ experi .
and you can see in the bottom , we have mean averag posit , all right ?
so the mean , if you look at the mean averag posit the mean averag posit ar exactli the same in both experi .
all right , so you can see thi is <num> , thi is <num> for system b and again here it also <num> and <num> .
so thei ar ident .
yet if you look at the , these exact averag posit for differ queri , if you look at these number in detail , you will realiz that in on case you would feel that you can trust the conclus here given by the averag .
in anoth case , in the other case , you will feel that , well , i'm not sure .
so , why don't you take a look at all these number for a moment .
paus the video .
so , if you at the averag , the main averag posit , we can easili sai that , well , system b is better , right ?
so it's , after all , it's <num> and then thi is twice as much as <num> .
so that's a better perform .
but if you look at these two experi and look at the detail result , you will see that we'll be more confid to sai that in the case on .
in experi on .
in thi case becaus these number seem to be consist better than for system b .
where as in , experi two , we're not sure .
becaus , look at some result , like thi , after system a is better .
and thi is anoth case where system a is better .
but yet , if we look at on the averag , system b is better .
so what do you think ?
you know , how reliabl is our conclus if we onli look at the averag ?
now in thi case , intuit , we feel it's better than on , it's more reliabl .
but how can we quantit answer thi question ?
and thi is why we need to do statist signific test .
so the idea of a statist signific test is basic to assess the vari , varianc across these differ queri .
if there's a , a big varianc that mean that the result could fluctuat a lot accord to differ queri .
then we should believ that unless you have us a lot of queri the result might chang if we us anoth set of queri .
right ?
so , thi is then not so if you have seen high varianc then it's not veri reliabl .
so let's look at these result again in the second case .
so here we show two , differ wai to compar them .
on is a sign test .
and we'll , we'll just look at the sign .
if system b is better than system a , then we have a plu sign .
when system a is better we have a minu sign etc .
us thi case if you see thi , well , there ar seven case .
we actual have four case where system b is better .
but three case system a is better .
you know intuit , thi is almost like random result .
right , so if you just take a random sampl of to , to flip seven coin , and if you us plu to denot the head and then minu to denot the tail , and that could easili be the result of just randomli flip , these seven coin .
so , the fact that the , the averag is larger doesn't tell us anyth .
you know , we can't reliabl concur that .
and thi can be quantit in the measur by , a p valu .
and that basic , mean , the probabl that thi result is in fact from random fluctuat .
in thi case , probabl is on .
it mean it sure is a random fluctuat .
now in wilcoxon , test , it's a non parametr test , and we would be not onli look at the sign we'll be also look at the magnitud of the differ .
but , we , we , we can draw a similar conclus where you sai well it's veri like to be from random .
so to illustr thi let's think about such a distribut .
and thi is call a normal distribut .
we assum that the mean is zero here .
let's sai , well , we start with the assumpt that there's no differ between the two system .
but we assum that becaus of random fluctuat depend on the queri we might observ a differ , so the actual differ might be on the left side here or on the right side here , right ?
and , and thi curv kind of show the probabl that we would actual observ valu that ar deviat from zero here .
now , so if we , look at thi pictur then we see that if a differ is observ here , then the chanc is veri high that thi is in fact , a random observ , right .
we can defin region of you know , like observ becaus of random fluctuat .
and thi is <num> of all outcom .
and in thi interv then the observ valu mai still be from random fluctuat .
but if you observ a valu in thi region or a differ on thi side , then the differ is unlik from random fluctuat .
right , so there is a veri small probabl that you will observ such a differ just becaus of random fluctuat .
so in that case , we can then conclud the differ must be real .
so system b is inde better .
so , thi is the idea of the statist signific test .
the takeawai messag here is that you have us mani queri to avoid jump into a conclus as in thi case to sai system b is better .
there ar mani differ wai of do thi statist signific test .
so now , let's talk about the other problem of make judgement and as we said earlier , it's veri hard to judg all the document complet unless it is a small data set .
so the question is , if we can't afford judg all the document in the collect , which subset should we judg ?
and the solut here is pool .
and thi is a strategi that ha been us in mani case to solv thi problem .
so the idea of pull is the follow .
we would first choos a divers set of rank method , these ar type of retriev system .
and we hope these method can help us nomin like relev in the document .
so the goal is to pick out the relev document . .
it mean we ar to make judgement on relev document becaus those ar the most us document from the user perspect .
so , that wai we would have each to return top k document .
and the k can vari from system , right .
but the point is to ask them to suggest the most like relev document .
and then we simpli combin all these top k set to form a pool of document for human assessor to judg .
so , imagin you have mani system .
each will return k document , you know , take the top k document , and we form the unit .
now , of cours there ar mani document that ar duplic , becaus mani system might have retriev the same random document .
so there will be some duplic document .
and there ar , there ar also uniqu document that ar onli return by on system , so the idea of have divers set of result rank method is to ensur the pool is broad .
and can includ as mani possibl random document as possibl .
and then the user with the human assessor would make complet the judgement on thi data set , thi pool .
and the other unjudg document ar usual just a assum to be non relev .
now if the pool is larg enough , thi assumpt is okai .
but the , if the pool is not veri larg , thi actual ha to be reconsid , and we might us other strategi to deal with them and there ar inde other method to handl such case .
and such a strategi is gener okai for compar system that contribut to the pool .
that mean if you particip in contribut to the pool then it's unlik that it will penal your system becaus the top rank document have all been judg .
howev , thi is problemat for even evalu a new system that mai not have contribut to the pool .
in thi case , you know , a new system might be penal becaus it might have nomin some relev document that have not been judg .
so those document might be assum to be non relev .
and that , that's unfair .
so to summar the whole part of text retriev evalu , it's extrem import becaus the problem is an empir defin problem .
if we don't reli on user , there's no wai to tell whether on method work better .
if we have inappropri experi design , we might misguid our research or applic .
and we might just draw wrong conclus .
and we have seen thi in some of our discuss .
so , make sure to get it right for your research or applic .
the main methodolog is cranfield evalu methodolog and thi is near the main paradigm us in all kind of empir evalu task , not just a search engin variat .
map and ndcg ar the two main measur that should definit know about and thei ar appropri for compar rank algorithm .
you will see them often in research paper .
perceiv up to ten document is easier to interpret from user perspect .
so , that's also often us .
what's not cover is some other evalu strategi like a b test where the system would mix two of the result of two method randomli .
and then will show the mix of result to user .
of cours , the user don't see which result is from which method .
the user would judg those result or click on those those document in in a search engin applic .
in thi case , then , the search engin can keep track of the click document , and see if on method ha contribut more to the click document .
if the user tend to click on on the result from on method , then it's just that method mai , mai be better .
so thi is what leverag a real user of a search engin to do evalu .
it's call a b test , and it's a strategi that's often us by the modern search engin , the commerci search engin .
anoth wai to evalu ir or text retriev is user studi , and we haven't cover that .
i've put some refer here that you can look at if you want to know more about that .
so there ar three addit read here , these ar three mini book about evalu .
and thei ar all excel in cover a broad review of inform retriev and evalu .
and thi cover some of the thing that we discuss .
but thei also have a lot of other to offer .
thi lectur is about the probabilist retriev model .
in thi lectur , we're go to continu the discuss of text retriev method .
we're go to look at anoth kind of veri differ wai to design rank function , then the vector space model that we discuss befor .
in probabilist model we defin the rank function base on the probabl that thi document is random to thi queri .
in other word , we ar , we introduc a binari random variabl here .
thi is the variabl r here .
and we also assum that the queri and the document ar all observ from random variabl .
note that in the vector space model , we assum thei ar vector .
but here we assum we assum thei ar the data observ from random variabl .
and so the problem , model retriev becom to estim the probabl of relev .
in thi categori of model , there ar differ variant .
the classic probabilist model ha led to the bm25 retriev function , which we discuss in the vector space model , becaus it's form is actual similar to a vector space model .
in thi lectur , we're go to discuss anoth subclass in thi big class call a languag model approach to retriev .
in particular , we're go to discuss the queri likelihood retriev model , which is on of the most effect model in probabilist model .
there is also anoth line call a diverg from random model , which ha latitud the pl2 function .
it's also on of the most effect state of the art attribut function .
in queri likelihood , our assumpt is that thi probabl readi can be approxim by the probabl of queri given a document and readi .
so , intuit , thi probabl just captur the follow probabl .
and that is if a user like document d , how like would the user enter queri q in order to retriev document d .
so we'll assum that the user like d , becaus we have a relev valu here .
and the we ask the question about how like we will see thi particular queri from thi user ?
so thi is the basic idea .
now to understand thi idea , let's take a look at the gener idea or the basic idea of probabilist retriev model .
so here , i list some imagin relev statu valu or relev judgment of queri and document .
for exampl , in thi slide , it show that queri on is a queri that the user type in and d1 is a document the user ha seen and on mean the user think d1 is relev to to q1 .
so thi r here can be also approxim by the click littl data that the search engin can collect it by watch how you interact with the search result .
so , in thi case , let's sai , the user click on thi document , so there's a on here .
similarli , the user click on d2 also , so there's a on here .
in other word , d2 is assum to relev at two , q1 .
on the other hand , d3 is non relev , there's a zero here .
and d4 is non relev and then d <num> is again relev and so on and so forth .
and thi part of mayb , thei ar collect from a differ user .
right .
so thi user type in q1 and then found that d1 is actual not us , so d1 is actual non relev .
in contrast here we see it's relev and , or thi could be the same queri type by the same user at differ time , but d2 is also relev , et cetera .
and then here , we can see more data that about other queri .
now we can imagin , we have a lot of search data .
now we can ask the question , how can we then estim the probabl of relev ?
right .
so how can we comput thi probabl of relev ?
well , intuit , that just mean if we look at the , all the entri where we see thi particular d and thi particular q , how like will we see a on on the third column ?
basic , that just mean we can correct the count .
we can first count how mani time where we see q and d as a pair in thi tabl and then count how mani time we actual have also seen on in the third column and then we just comput the ratio .
so let's take a look at some specif exampl .
suppos we ar try to comput thi probabl for d1 , d2 and d3 for q1 .
what is the estim probabl ?
now think about that .
you can paus the video if need .
try to take a look at the tabl and try to give your estim of the probabl .
have you seen that if we ar interest in q1 and d1 , we've been look at the , these two pair and in both case or actual in on of the case , the user ha said that thi is on , thi is relev .
so r is equal to <num> in onli on of the two case .
in the other case , thi is zero .
so that's on out of two .
what about the d1 and the d2 ?
well , thei're ar here , you want d2 , d1 , d2 .
in both case , in thi case r is equal to <num> .
so , it's two out of two and so and so forth .
so you can see with thi approach , we captur it score these document for the queri .
right ?
we now have a score for d1 , d2 and d3 for thi queri .
we can simpli rank them base on these probabl and so that's the basic idea of probabilist retriev model .
and you can see , it make a lot of sens .
in thi case , it's go to rank d2 abov all the other document .
becaus in all the case , when you have seen q<num> and d2 , r is equal to <num> .
the user click on thi document .
so thi also show show that with a lot of click through data , a search engin can learn a lot from the data to improv the search engin .
thi is a simpl exampl that show that with even a small number of entri here , we can alreadi estim some probabl .
these probabl would give us some sens about which document might be more read or more us to a user for type thi queri .
now , of cours , the problem is that we don't observ all the queri and all of the document and all the relev valu .
right ?
there will be a lot of unseen document .
in gener , we can onli collect data from the document's that we have shown to the user .
there ar even more unseen queri , becaus you cannot predict what queri will be type in by user .
so , obvious , thi approach won't work if we appli it to unseen queri or unseen document .
nevertheless , thi show the basic idea of the probabilist retriev model and it make sens intuit .
so what do we do in such a case when we have a lot of unseen document and , and unseen queri ?
well , the solut that we have to approxim in some wai .
right .
so , in thi particular case call the queri likelihood retriev model , we just approxim thi by anoth condit probabl , p q d , r is equal to <num> .
so , in the condit part , we assum that the user like the document , becaus we have seen that the user click on thi document .
and thi part , show that we're interest in how like the user would actual enter thi queri .
how like we will see thi queri in the same row .
so note that here , we have made an interest assumpt here .
basic , we , we're go to assum that whether the user type in thi queri ha someth to do with whether user like the document .
in other word , we actual make the foreign assumpt and that is a user formula to queri base on an imaginari relev document .
well , if you just look at thi as a condit probabl , it's not obviou we ar make thi assumpt .
so what i realli meant is that to us thi new condit probabl to help us score then thi new condit of probabl .
we have to somehow be abl to estim thi condit probabl without reli on thi big tabl .
otherwis , it would be have similar problem as befor .
and by make thi assumpt , we have some wai to bypass thi big tabl and try to just mortar how to us a formula to the queri .
okai .
so thi is how you can simplifi the , the gener model so that we can give either specif function later .
so let's look at how thi model work for our exampl .
and basic , what we ar go to do in thi case is to ask the follow question .
which of these document is most like the imaginari relev document in the user's mind when the user formul thi queri ?
and so we ask thi question and we quantifi the probabl and thi probabl is a condit probabl of observ thi queri if a particular document is in fact the imaginari relev document in the user's mind .
here you can see we comput all these queri likelihood probabl , the likelihood of queri given each document .
onc we have these valu , we can then rank these document base on these valu .
so to summar , the gener idea of modern relev in the probabl risk model is to assum that we introduc a binari random variabl , r here .
and then let the score function be defin base on thi condit probabl .
we also talk about a proxim in thi by us the queri likelihood .
and in thi case , we have a rank function that's basic base on a probabl of a queri given the document .
and thi probabl should be interpret as the probabl that a user who like document d would pose queri q .
now the question , of cours is how do we comput thi addit probabl ?
at thi in gener ha to do with how to comput the probabl of text , becaus q is a text .
and thi ha to do with a model call a languag model .
and thi kind of model ar propos to model text .
so most specif , we will be veri interest in the follow condit probabl as i show you , you thi here .
if the user like thi document , how like the user would approv thi queri ?
and in the next lectur , we're go to give introduct to languag model , so that we can see how we can model text with a probabl risk model in gener .
thi lectur is about the feedback in the languag model approach .
in thi lectur we will continu the discuss of feedback in text retriev .
in particular we're go to talk about the feedback in languag model approach .
so we deriv the queri likelihood rank function by make variou assumpt .
as a basic retriev function , that formula , or those formula work well .
but if we think about the feedback inform , it's a littl bit awkward to us queri likelihood to perform feedback becaus a lot of time the feedback inform is addit inform about the queri .
but we assum the queri is gener by assembl word from a languag model in the queri likelihood method .
it's kind of unnatur to sampl , word that , form feedback document .
as a result , then research is propos , a wai to gener queri likelihood function .
it's call a kullback leibler diverg retriev model .
and thi model is actual , go to make the queri likelihood , our retriev function much closer to vector space model .
yet thi , form of the languag model can be , regard as a gener of queri likelihood in the sens that if it can cover queri likelihood as a special case .
and in thi case the feedback can be achiev through simpli queri model estim or updat .
thi is veri similar to rocchio which updat the queri vector .
so let's see what the , is the scale of diverg , which we will model .
so , on the top , what you see is queri likelihood retriev function , all right , thi on .
and then kl diverg or also call cross entropi retriev model is basic to gener the frequenc part , here , into a layer model .
so basic it's the differ , given by the probabilist model here to character what the user's look for versu the kind of queri word there .
and thi differ allow us to plot variou differ wai to estim thi .
so thi can be estim in mani differ wai includ us feedback inform .
now thi is call a kl diverg becaus thi can be interpret as measur the kl diverg of two distribut .
on is the queri model denot by thi distribut .
on is the talk , the languag model here .
and though is a languag model , of cours .
and we ar not go to talk about the detail of that , and you'll find the thing in refer .
it's also call cross entropi , becaus , in , in fact , we can ignor some term in the kl diverg function and we will end up have actual cross entropi , and that , both ar term in inform theori .
but , anywai for our purpos here you can just see the two formula look almost ident , except that here we have a probabl of a word given by a queri languag model .
thi , and here , the sum is over all the word that ar in the document , and also with the non zero probabl for the queri model .
so it's kind of , again , a gener of sum over all the match queri word .
now you can also , easi to see , we can recov the queri likelihood , which we will find here by as simpl as set thi queri model to the rel frequenc of a word in the queri , right ?
thi is veri to easi see onc you practic thi .
and to here , you can elimin thi queri len , that's a constant , and then you get exactli like that .
so you can see the equival .
and that's also why thi kl diverg model can be regard as a gener of queri likelihood becaus we can cover queri likelihood as a special case , but it would also allow it to do much more than that .
so thi is how we us the kl diverg model to then do feedback .
the pictur show that we first estim a document languag model , then we estim a queri languag model and we comput the kl diverg , thi is often denot by a d here .
but thi basic mean , thi wa exactli like in vector space model becaus we comput the vector for the document in the comput and not the vector for the queri , and then we comput the distanc .
onli that these vector ar of special form , thei have probabl distribut .
and then we get the result , and we can find some feedback document .
let's assum thei ar more select sorri , mostli posit document .
although we could also consid both kind of document .
so what we could do is , like in rocchio , we can comput anoth languag model call feedback languag model here .
again , thi is go to be anoth vector just like a comput centroid vector in rocchio .
and then thi model can be combin with the origin queri model us a linear interpol .
and thi would then give us an updat model , just like again in rocchio .
right , so here , we can see the paramet of our control amount of feedback if it's set to <num> , then it sai here there's no feedback .
after set to <num> , we've got full feedback , we can ignor the origin queri .
and thi is gener not desir , right .
so thi unless you ar absolut sure you have seen a lot of relev document and the queri term ar not import .
so of cours the main question here is how do you comput thi theta f ?
thi is the big question here .
and onc you can do that , the rest is easi .
so here we'll talk about on of the approach .
and there ar mani approach of cours .
thi approach is base on gener model and i'm go to show you how it work .
thi is a user gener mixtur model .
so thi pictur show that the we have thi model here , the feedback model that we want to estim .
and we the basi is the feedback option .
let's sai we ar observ the posit document .
these ar the collect document by user , or random document judg by user , or simpli top rank document that we assum to be random .
now imagin how we can comput a centroid for these document by us languag model .
on approach is simpli to assum these document ar gener from thi languag model as we did befor .
what we could do is do it , just normal the word frequenc here .
and then we , we'll get thi word distribut .
now the question is whether thi distribut is good for feedback .
well you can imagin well the top rank of the word would be what ?
what do you think ?
well those word would be common word , right ?
as well we see in , in the languag model , in the top right , the word ar actual common word like , the , et cetera .
so , it's not veri good for feedback , becaus we will be ad a lot of such word to our queri when we interpret , thi wa the origin queri model .
so , thi is not good , so we need to do someth , in particular , we ar try to get rid of those common word .
and we all , we have seen actual on wai to do that , by us background languag model in the case of learn the associ with of word , right .
the word that ar relat to the word comput .
we could do that , and that would be anoth wai to do thi .
but here , we're go to talk about anoth approach , which is a more principl approach .
in thi case , we're go to sai , well , you , you said that there ar common word here in thi , these document that should not belong to thi top model , right ?
so now , what we can do is to assum that , well , those word ar , gener , from background languag model , so thei will gener a , those word like the , for exampl .
and if we us maximum likelihood estim , note that if all the word here must be gener from thi model , then thi model is forc to assign high probabl to a word like the , becaus it occur so frequent here .
note that in order to reduc it probabl in thi model , we have to have anoth model , which is thi on to help explain the word , the , here .
and in thi case , it's not appropri to us the background languag model to achiev thi goal becaus thi model will assign high probabl to these common word .
so in thi approach then , we assum thi machin that which gener these word would work as follow .
we have a sourc control here .
imagin we flip a coin here to decid what distribut to us .
with the probabl of lambda the coin show up as head .
and then we're go to us the background languag model .
and we can do then sampl word from that model .
with probabl of <num> minu lambda now , we now decid to us a unknown topic model here that we will try to estim .
and we're go to then gener a word here .
if we make thi assumpt , and thi whole thing will be just on model , and we call thi a mixtur model , becaus there ar two distribut that ar mix here togeth .
and we actual don't know when each distribut is us .
right , so again think of thi whole thing as on model .
and we can still ask it for word , and it will still give us a word in a random method , right ?
and of cours which word will show up will depend on both thi distribut and that distribut .
in addit , it would also depend on thi lambda , becaus if you sai , lambda is veri high and it's go to alwai us the background distribut , you'll get differ word .
if you sai , well our lambda is veri small , we're go to us thi , all right ?
so all these ar paramet , in thi model .
and then , if you're think thi wai , basic we can do exactli the same as what we did befor , we're go to us maximum likelihood estim to adjust thi model to estim the paramet .
basic we're go to adjust , well , thi paramet so that we can best explain all the data .
the differ now is that we ar not ask thi model alon to explain thi .
but rather we're go to ask thi whole model , mixtur model , to explain the data becaus it ha got some help from the background model .
it doesn't have to assign high probabl toward like the , as a result .
it would then assign high probabl to other word that ar common here but not have high probabl here .
so those would be common here .
right ?
and if thei're common thei would have to have high probabl , accord to a maximum likelihood estim .
and if thei ar rare here , all right , so if thei ar rare here , then you don't get much help from thi background model .
as a result , thi topic model must assign high probabl .
so the higher probabl word accord to the topic model will be those that ar common here , but rare in the background .
okai , so , thi is basic a littl bit like a idea for weight here .
thi would allow us to achiev the effect of remov these top word that ar meaningless in the feedback .
so mathemat what we have is to comput the likelihood again , local likelihood of the feedback document .
and , and note that , we also have anoth paramet , lambda here .
but we assum that lambda denot nois in the feedback document .
so we ar go to , let's sai , set thi to a paramet , let's sai , sai <num> of the word ar nois , or <num> ar nois .
and thi can then be , assum it will be fix .
if we assum thi is fix , then we onli have these probabl as paramet just like in the simplest unigram languag model , we have n paramet .
n is the number of word and , then , the likelihood function will look like thi .
it's veri similar to the likelihood function , normal likelihood function we see befor except that insid the logarithm there's a sum in here .
and thi sum is becaus we can see the two distribut .
and which on us would depend on lambda and that's why we have thi form .
but mathemat thi is the function with theta as unknown variabl , right ?
so , thi is just a function .
all the other variabl ar known , except for thi gui .
so , we can then choos thi probabl distribut to maxim thi log likelihood .
the same idea as the maximum likelihood estim .
as a mathemat problem which is to , we just have to solv thi optim problem .
we said we would try all of the theta valu , and here we find on that give thi whole thing the maximum probabl .
so , it's a well defin math problem .
onc we have done that , we obtain thi theta f , that can be the interpret with the origin queri model to do feedback .
so here ar some exampl of the feedback model learn from a web document collect , and we do pseudo feedback .
we just us the top <num> document , and we us thi mixtur model .
so the queri is airport secur .
what we do is we first retriev ten document from the web databas .
and thi is of cours pseudo feedback , right ?
and then we're go to feed to that mixtur model , to thi ten document set .
and these ar the word learn us thi approach .
thi is the probabl of a word given by the feedback model in both case .
so , in both case , you can see the highest probabl of word includ veri random word to the queri .
so , airport secur for exampl , these queri word still show up as high probabl in each case natur becaus thei occur frequent in the top rank of document .
but we also see beverag , alcohol , bomb , terrorist , et cetera .
right , so these ar relev to thi topic , and thei , if combin with origin queri can help us match more accur , on document .
and also thei can help us bring up document that onli manag the , some of these other word .
and mayb for exampl just airport and then bomb for exampl .
these so , thi is how pseudo feedback work .
it show that thi model realli work and pick up mm , some relat word to the queri .
what's also interest is that if you look at the two tabl here , and you compar them , and you see in thi case , when lambda is set to a small valu , and we'll still see some common word here , and that mean .
when we don't us the background model often , rememb lambda can us the probabl of us the background model to gener to the text .
if we don't reli much on background model , we still have to us thi top model to account for the common word .
wherea if we set lambda to a veri high valu we would us the background model veri often to explain these word , then there is no burden on expand those common word in the feedback document by the top model .
so , as a result , the top of the model here is veri discrimin .
it contain all the relev word without common word .
so thi can be ad to the origin queri to achiev feedback .
so to summar in thi lectur we have talk about the feedback in languag model approach .
in gener , feedback is to learn from exampl .
these exampl can be assum exampl , can be pseudo exampl , like assum the , the top ten document ar assum to be random .
thei could be base on us fraction like feedback , base on quick sort or implicit feedback .
we talk about the three major feedback scenario , relev feedback , pseudo feedback , and implicit feedback .
we talk about how to us rocchio to do feedback in vector space model and how to us queri model estim for feedback in languag model .
and we briefli talk about the mixtur model and the basic idea and there ar mani other method .
for exampl the relev model is a veri effect model for estim queri model .
so , you can read more about the , these method in the refer that ar list at the end of thi lectur .
so there ar two addit read here .
the first on is a book that ha a systemat , review and discuss of languag model of more inform retriev .
and the second on is an import research paper that's about relev base languag model and it's a veri effect wai of comput queri model .
thi lectur is about a statist languag model .
in thi lectur , we're go , we're go to get an introduct to the probabilist model .
thi ha to do with how mani model have to go into these model .
so , it's readi to how we model theori base on a document .
we're go to talk about , what is a languag model and , then , we're go to talk about the simplest languag model call a unigram languag model .
which also happen to be the most us model for text retriev .
and final we'll discuss possibl us of an m model .
what is a languag model ?
well , it's just a probabl distribut over word sequenc .
so , here i show on .
thi model give the sequenc todai's wednesdai a probabl of <num> . <num> it give todai wednesdai is a veri veri small probabl , becaus it's algorithmat .
you can see the probabl given to these sentenc or sequenc of word can vari a lot depend on the model .
therefor , it's clearli context depend .
in ordinari convers , probabl todai is wednesdai is most popular among these sentenc .
but imagin in the context of discuss a privat math , mayb the higher valu posit would have a higher probabl .
thi mean it can be us to repres as a topic of a test .
the model can also be regard as a probabilist mechan for gener text , and thi is why it is often call a gener model .
so , what doe that mean ?
we can imag thi is a mechan that's visual here as a system that can gener a sequenc of word .
so we can ask for a sequenc and it's to sampl a sequenc from the devic if you want .
and it might gener , for exampl , todai is wednesdai , but it could have gener mani other sequenc .
so for exampl , there ar mani possibl , right ?
so thi , in thi sens , we can view our data as basic a sampl observ from such a gener model .
so why is such a model us ?
well , it's mainli becaus it can quantifi the uncertainti in natur languag .
where do uncertainti come from ?
well , on sourc is simpli the ambigu in natur languag that we discuss earlier in the lectur .
anoth sourc is becaus we don't have complet understand .
we lack all the knowledg to understand languag .
in that case there will be uncertainti as well .
so let me show some exampl of question that we can answer with an averag model that would have an interest applic in differ wai .
given that we see john and feel .
how like will we see happi as oppos to habit as the next word in a sequenc of word ?
obvious thi would be veri us speech recognit becaus happi and habit would have similar acoust sound .
acoust signal .
but if we look at the languag model we know that john feel happi would be far more like than john feel habit .
anoth exampl , given that we observ basebal three time and gain onc in the new articl how like is it about the sport ?
thi obvious is relat to text categor and inform .
also , given that a user is interest in sport new , how like would the user us basebal in a queri ?
now thi is clearli relat to the queri that we discuss in the previou lectur .
so now let's look at the simplest languag model .
call a lan , unigram languag model .
in such a case , we assum that we gener the text by gener each word independ .
so thi mean the probabl of a sequenc of word would be then the product of the probabl of each word .
now normal thei ar not independ , right ?
so if you have seen a word like languag .
now , we'll make it far more like to observ model than if you haven't seen languag .
so thi assumpt is not necessari sure but we'll make thi assumpt to simplifi the model .
so now , the model ha precis n paramet , where n is vocabulari size .
we have on probabl for each word , and all these probabl must sum to <num> .
so strictli speak , we actual have n minu <num> paramet .
as i said , text can be then be assum to be a sampl drawn from thi word distribut .
so for exampl , now we can ask the devic , or the model , to stochast gener the word for us instead of in sequenc .
so instead of give a whole sequenc like todai is wednesdai , it now give us just on word .
and we can get all kind of word .
and we can assembl these word in a sequenc .
so , that would still allow you to comput the probabl of todai is wed as the product of the three probabl .
as you can see even though we have not ask the model to gener the , the sequenc it actual allow us to comput the probabl for all the sequenc .
but thi model now onli need n paramet to character .
that mean if we specifi all the probabl for all the word then the model's behavior is complet specifi .
wherea if you , we don't make thi assumpt we would have to specifi .
find probabl for all kind of combin of word in sequenc .
so by make thi assumpt , it make it much easier to estim these paramet .
so let's see a specif exampl here .
here i show two unigram lambda model with some probabl and these ar high probabl word that ar shown on top .
the first on clearli suggest the topic of text mine becaus the high probabl word ar all relat to thi topic .
the second on is more relat to health .
now , we can then ask the question how like we'll observ a particular text from each of these three model .
now suppos with sampl word to form the document , let's sai we take the first distribut which ar the sampl word .
what word do you think it would be gener or mayb text ?
or mayb mine mayb anoth word ?
even food , which ha a veri small probabl , might still be abl to show up .
but in gener , high probabl word will like show up more often .
so we can imagin a gener text that look like text mine .
a factor with a small probabl , you might be abl to actual gener the actual text mine paper that would actual be meaning , although the probabl would be veri , veri small .
in the extrem case , you might imagin we might be abl to gener a , a text paper , text mine paper that would be accept by a major confer .
and in that case the probabl would be even smaller .
for instanc nonzero probabl , if we assum none of the word will have a nonzero probabl .
similarli from the second topic , we can imagin we can gener a food and nutrit paper .
that doesn't mean we cannot gener thi paper from text mine distribut .
we can , but the probabl would be veri , veri small , mayb smaller than even gener a paper that can be accept by a major confer on text mine .
so the point of here is that given a distribut , we can talk about the probabl of observ a certain kind of text .
some text would have higher probabl than other .
now , let's look at the problem in a differ wai .
supposedli , we now have avail a particular document .
in thi case , mayb the abstract or the text mine paper , and we see these word account here .
the total number of word is <num> .
now the question you ask here is a estim question .
we can ask the question , which model , which word distribut ha been us to , to gener thi text .
assum the text ha been gener by assembl word from the distribut .
so what would be your guess ?
what have to decid what probabl test , mine , et cetera would have .
so paus a view for a second and try to think about your best guess .
if you're like a lot of peopl you would have guess that well , my best guess is text ha a probabl of <num> out of <num> becaus i have seen text ten time and there ar a total of <num> word .
so we simpli notic , normal these count .
and that's in fact justifi .
and your intuit is consist with mathemat deriv .
and thi is call a maximum likelihood in thi estim , we'll assum that the paramet set , .
ar those that would give our observ the maximum probabl .
that mean if we chang these probabl , then the probabl of observ the particular text would be somewhat smaller .
so we can see thi ha a veri simpl formula .
basic , we just need to look at the count of a word in the document and then divid it by the total number of word in the document .
about the length .
normal the frequenc .
well a consequ of thi , is of cours , we're go to assign <num> probabl to unseen word .
if we have an observ word , there will be no incent to assign a non <num> probabl us thi approach .
why ?
becaus that would take awai probabl mass for thi observ word .
and that obvious wouldn't maxim the probabl of thi particular observ data .
but on can still question whether thi is our best estim .
well , the answer depend on what kind of model you want to find , right ?
thi is made if it's a best model base on thi particular layer .
but if you're interest in a model that can explain the content of the four paper of , for thi abstract , then you might have a second thought , right ?
so for on thing there should be other thing in the bodi of that articl .
so thei should not have , zero probabl , even though thei ar not observ the abstract .
we're go to cover thi later , in , discuss the queri model .
so , let's take a look at some possibl us of these languag model .
on us is simpli to us it to repres the topic .
so here it show some gener english background that text .
we can us thi text to estim a languag model .
and the model might look like thi .
right ?
so on the top we'll have those all common word , is we , is , and then we'll see some common word like these , and then some veri , veri real word in the bottom .
thi is the background imag model .
it repres the frequenc on word , in english in gener , right ?
thi is the background model .
now , let's look at anoth text .
mayb thi time , we'll look at comput scienc research paper .
so we have a correct of comput scienc research paper , we do again , we can just us the maximum where we simpli normal the frequenc .
now , in thi case , we look at the distribut , that look like thi .
on the top , it look similar , becaus these word occur everywher , thei ar veri common .
but as we go down we'll see word that ar more relat to comput scienc .
comput , or softwar , or text et cetera .
so , although here , we might also see these word , for exampl , comput .
but , we can imagin the probabl here is much smaller than the probabl here .
and we will see mani other word here that , that would be more common in gener in english .
so , you can see thi distribut character a topic of the correspond text .
we can look at the , even the smaller text .
so , in thi case let's look at the text mine paper .
now if we do the same we have anoth .
distribut again the can be expect to occur on the top .
soon we will see text , mine , associ , cluster , these word have rel high probabl in contrast in thi distribut ha rel small probabl .
so thi mean , again base on differ text data that we can have a differ model .
and model captur the topic .
so we call thi document an lm model and we call thi collect lm model .
and later , we'll see how thei're us in a retriev function .
but now , let's look at the , anoth us of thi model .
can we statist find what word ar semant relat to comput ?
now how do we find such word ?
well our first thought is well let's take a look at the text that match .
comput .
so we can take a look at all the document that contain the word comput .
let's build a languag model .
okai , see what word we see there .
well , not surprisingli , we see these common word on top as we alwai do .
so in thi case , thi languag model give us the .
condit probabl of see a word in the context of comput .
and these common word will natur have high probabl .
other word will see comput itself , and softwar will have rel high probabl .
but we , if we just us thi model we cannot .
i just sai all these word ar semant relat to comput .
so intuit what we'd like to get rid of these these common word .
how can we do that ?
it turn out that it's possibl to us languag model to do that .
now i suggest you think about that .
so how can we know what word ar veri common so that we want to kind of get rid of them .
what model will tell us that ?
well , mayb you can think about that .
so the background languag model precis tell us thi inform .
it tell us what word ar common in gener .
so if we us thi background model , we would know that these word ar common word in gener .
so it's not surpris to observ them in the context of comput .
wherea comput ha a veri small probabl in gener .
so it's veri surpris that we have seen comput in , with thi probabl .
and the same is true for softwar .
so then we can us these two model to somehow figur out .
the word that ar relat to comput .
for exampl we can simpli take the ratio of these two probabl and normal the top of the model by the probabl of the word in the background model .
so if we do that , we take the ratio , we'll see that then on the top , comput , is ramp , and then follow by softwar , program , all these word relat to comput .
becaus thei occur veri frequent in the context of comput , but not frequent in whole connect .
where as these common word will not have a high probabl .
in fact , thei have a ratio of about on down there .
becaus thei ar not realli relat to comput .
by take the same ball of text that contain the comput we don't realli see more occurr of that in gener .
so thi show that even with thi simpl lm model , we can do some limit analysi of semant .
so in thi lectur , we talk about , languag model , which is basic a probabl distribut over the text .
we talk about the simplist languag model call unigram languag model .
which is also just a word distribut .
we talk about the two us of a languag model .
on is to repres the , the topic in a document , in a class or in gener .
the other is discov word associ .
in the next lectur we're go to talk about the how languag model can be us to design a retriev function .
here ar two addit read .
the first is a textbook on statist and natur languag process .
the second is a articl that ha a survei of statist languag model with other pointer to research work .
thi lectur is about queri likelihood and probabilist retriev model .
in thi lectur , we continu the discuss of probabilist retriev model .
in particular , we're go to talk about the queri likelihood of the retriev function .
in the queri of likelihood retriev model our idea is a model .
how a like a user , who like a document would pose a particular queri .
so in thi case , you can imagin , if a user like thi particular document about the presidenti campaign new .
then we can assum , the user would us thi work as a basi to oppos a queri to try and retriev thi doc .
so you can imagin the user , could us a process that work as follow , where we assum that the queri is gener by sampl word from the document .
so for exampl , a user might pick a word like presidenti from thi document , and then us thi as a queri word .
and then the user would pick anoth word , like campaign and that would be the second queri word .
now thi , of cours , is assumpt that we have made about , how a user would post a queri .
whether a user actual follow thi process .
mayb a differ question .
but thi assumpt , ha allow us to formal character thi condit probabl .
and thi allow to also not reli on the big tabl that i show you earlier to us imper data to estim thi probabl .
and thi is why we can us thi idea to then further deriv retriev function that we can implement with the languag .
so , as you see , the assumpt that we've made here is , each queri word , is independ in thi sampl , and also , each word is basic obtain from the document .
so now let's see how thi work exactli .
well , sinc we ar comput a queri likelihood , then the probabl here is just the probabl of thi particular queri , which is a sequenc of word .
and we make the assumpt that each word is gener independ .
so , as a result , the probabl of the queri is just a product of the probabl of each queri word .
now , how do we comput the probabl of each queri word ?
well , base on the assumpt , that a word is pick from the document , that the user ha in mind .
now we know the probabl of each word is just the , the rel frequenc of the word in the document .
so , for exampl the probabl of presidenti given the document , would be just the count of presidenti in the document , divid by the total number of word in the document or document length .
so with thi these assumpt , we now have actual simpl formula for retriev , right ?
we can us thi to rank our document .
so doe thi model work ?
let's take a look , here ar some exampl document that you have seen befor .
suppos now the queri is presidenti campaign .
and we see the formula here on the top .
so how do we score these document ?
well it's veri simpl , right , we just count how mani time we have seen presidenti , how mani time we have seen campaign etc .
and see here <num> and we've seen presid jou tai , so that's two over the land of document the four .
multipli by <num> over land of document of <num> for the probabl of campaign and seem we can probabl for the other two document .
now if you'll look at thi , these number or these , thi , these formula for score all these document , it seem to make sens becaus , if we assum d3 and d4 have about the same length , then it look like we will rank d4 abov d3 and which is abov d2 , right ?
and as we would expect , look like it did captur the tf heurist .
and so thi seem to work well .
howev , if we try a differ queri like thi on , presidenti campaign updat , then we might see a problem .
but what problem ?
well , think about updat , now none of these document ha mention updat .
so accord to our assumpt that a user would pick a order from a document to gener a queri , then the probabl of obtain a word like updat would be what .
would be zero , right ?
so that caus a problem , becaus it would caus all these document to have zero probabl of gener thi queri .
now , while it's fine to have a zero probabl for d2 which is not relev .
it's not okai to have zero for d3 and d4 , becaus now we no longer can distinguish them .
what's wors , we can't even distinguish them from d <num> .
all right , so that's obvious not desir .
now when on ha such result , we should think about what ha caus thi problem .
so we have to examin what assumpt have been made , as we deriv thi rank function .
now if you examin those assumpt carefulli you would realiz .
what ha caus thi problem , right ?
so , take a moment to think about , what do you think is the reason why updat ha zero probabl , and how do we fix it ?
right ?
so , if you think about thi for the moment that you realiz that .
that's becaus we have made an assumpt that everi queri word must be drawn from the document in the user's mind .
so , in order to fix thi , we have to assum that , the user could have drawn a word , not necessarili from the document .
so let's see improv model .
an improv here is to sai that , well , instead of draw a word from the document , let's imagin that the user would actual draw a word from a document model and so i show a model here .
here we assum that thi document is gener , by us thi unigram imag model .
now , thi model , doesn't necessarili assign zero probabl for updat .
in fact we assum thi model doe not assign zero probabl for ani word .
now if we're think thi wai then the gener process is a littl bit differ .
now the user ha thi model in mind , instead of thi particular document .
although the model ha to be estim base on the document .
so the user can again gener the queri us a similar process .
thei mai pick a word , for exampl presidenti and anoth word campaign .
now the differ is that , thi time we can also pick a word like updat , even though updat it doesn't occur in the document to potenti gener a queri word like updat .
so that , a queri wa updat we want to have zero probabl .
so thi would fix our problem and it's also reason , becaus we're now think of what the user is look for in a more gener wai , that is uniqu languag model instead of a fix document .
so how do we comput thi queri , like if we make thi sum where it involv two step , right ?
the first is the comput's model , and we call it talk the languag model here .
for exampl , i have shown two possibl energi model here .
thi ha been base on two document .
and then given a queri and i get a mine algorithm .
the second step , is just to comput the likelihood of thi queri .
and by make independ assumpt , we could then have thi probabl as a product of the probabl of each queri word , all right ?
but we do thi for both document .
and then we're go to score these two document and then rank them .
so that's the basic idea of thi queri likelihood retriev function .
so more gener than thi rank function would look like the follow and here as , we assum that queri ha end word w1 through wn .
and then the score function , the rank function is the probabl that we observ thi queri , given that the user is think of thi document .
and thi assum to be product of probabl of all individu word and thi is base on the independ assumpt .
now we actual often score the , document for thi queri by us log of the queri likelihood , as shown on the sigma line .
now we do thi to avoid have a lot of small probabl .
m , multipli togeth .
and thi could caus underflow and we might lose precis by transform the valu as a logarithm function .
we maintain the order of these document , yet we can avoid the end of flow problem .
so if we take longer than transform of coars the product that would becom a sum , as you stake in the line here .
so it's a sum of all of the queri word , and insid the sum that is log of the probabl of thi word given by the document .
and then we can further rewrit the sum , into a differ form .
so in the first of the sum here , in thi sum , we have it over all the queri word n queri word .
and in thi sum , we have a sum of all the possibl word but we put a counter here of each word in the queri .
essenti we ar onli consid the word in the queri , becaus if a word is not in the queri , it can would be zero .
so we're still consid onli these end word .
but we're us a differ form as if we were go to a sum of all the word , in the vocabulari .
and of cours a word might occur multipl time in the queri .
that's wh , why we have a count here .
and then thi part is log of the probabl of the word given by the document mg model .
so you can see , in thi materi function , we actual know the count of the word in the queri .
so , the onli thing that we don't know is thi document languag model .
therefor , we can convert through the retriev problem into the problem of estim thi document languag model .
so that we can comput , the probabl of each queri we're given by thi document .
at differ estim method here , would lead to differ rank function .
and thi is just like a differ a wai to place a doc in the vector , in the vector space .
would lead it to a differ rank function in the vector space model .
here ar differ wai to estim thi stuff in the languag model , will lead you to a differ rank function for queri likelihood .
thi lectur is about smooth of languag model .
in thi lectur we're go to continu talk about the probabilist retriev model .
in particular , we're go to talk about smooth of languag model and the queri likelihood of it , which will method .
so you have seen thi slide from a previou lectur .
thi is the rank function base on the queri likelihood .
here we assum that the independ of gener each queri word and the formula would look like the follow .
where we take a sum over all of the queri word and insid is the sum there is a log of probabl of a word given by the document , or document languag model .
so the main task now is to estim thi document languag model .
as we said befor differ method for estim thi model would lead to differ retriev function .
so , in thi lectur we're go to look into thi in more detail .
so , how do i estim thi languag model ?
well , the obviou choic would be the maximum likelihood estim that we have seen befor .
and that is we're go to normal the word frequenc in the document .
and the estim probabl would look like thi .
thi is a step function here .
which mean all the word that have the same frequenc count will have an equal probabl .
thi is anoth frequenc in the count that ha a differ probabl .
note that for word that have not occur in the document here , thei all have zero probabl .
so we know thi is just like a model that we assum earlier in the lectur , where we assum the user with the sampl word from the document to formul the queri .
and there is no chanc of sampl ani word that is not in the document .
and we know that's not good .
so how would we improv thi ?
well , in order to assign a non zero probabl to word that have not been observ in the document , we would have to take awai some probabl to mass from the word that ar observ the document .
so for exampl here , we have to take awai some mass , becaus we need some extra problem in the mass for the unseen word .
otherwis , thei won't sum to <num> .
so all these probabl must be sum to <num> .
so to make thi transform , and to improv the maximum by assign nonzero probabl to word that ar not observ in the data .
we have to do smooth , and smooth ha to do with improv the estim by consid the possibl that , if the author had been written .
help , ask to write more word for the document .
the user , the author might have rethink other word .
if you think about thi factor then a smooth lm model would be a more accur represent of the actual topic .
imagin you have seen abstract of such articl .
let's sai thi document is abstract .
right .
if we assum and see word in thi abstract we have or , or probabl of <num> that would mean it's no chanc of sampl a word outsid the abstract that the formula to queri .
but imagin the user who is interest in the topic of thi abstract , the user might actual choos a word that is not in the abstractor to to us as queri .
so obvious if we had ask thi author to write more , the author would have written a full text of that articl .
so smooth of the languag model is attempt to to try to recov the model for the whole , whole articl .
and then of cours we don't have written knowledg about ani word ar not observ in the abstract there , so that's why smooth is actual a tricki problem .
so let's talk a littl more about how to smooth a lm word .
the kei question here is what probabl should be assign to those unseen word .
right .
and there ar mani differ wai of do that .
on idea here , that's veri us for retriev is let the probabl of an unseen word be proport to it probabl given by a refer languag model .
that mean if you don't observ the word in the data set , we're go to assum that it probabl is kind of govern by anoth refer languag model that we were construct .
it will tell us which unseen word we have like a higher probabl .
in the case of retriev a natur choic would be to take the collect languag model as a refer languag model .
that is to sai if you don't observ a word in the document we're go to assum that .
the probabl of thi word would be proport to the probabl of the word in the whole collect .
so , more formal , we'll be estim the probabl of a word get a document as follow .
if the word is seen in the document , then the probabl would be a discount the maximum estim p sub c here .
otherwis , if the word is not seen in the document , we'll then let probabl be proport to the probabl of the word in the collect , and here the coeffici of is to control the amount of probabl mass that we assign to unseen word .
obvious all these probabl must sum to <num> .
so , alpha sub d is constrain in some wai .
so , what if we plug in thi smooth formula into our queri likelihood rank function ?
thi is what we would get .
in thi formula , you can see , right , we have thi as a sum over all the queri word .
and note that we have written in the form of a sum over all the vocabulari .
you see here thi is a sum of all the word in the vocabulari , but note that we have a count of the word in the queri .
so , in effect we ar just take a sum of queri word , right .
thi is in now a common wai that we will us becaus of it conveni in some transform .
so , thi is as i said , thi is sum of all the queri word .
in our smooth method , we're assum the word that ar not observ in the document , that we have a somewhat differ form of probabl .
and then it's for thi form .
so we're go to then decompos thi sum into two part .
on sum is over all the queri word that ar match in the document .
that mean in thi sum , all the word have a non zero probabl , in the document , sorri .
it's , the non zero count of the word in the document .
thei all occur in the document .
and thei also have to , of cours , have a non zero count in the queri .
so , these ar the word that ar match .
these ar the queri word that ar match in the document .
on the other hand in thi sum we ar s , take the sum over all the word that ar note our queri wa not match in the document .
so thei occur in the queri due to thi term but thei don't occur in the document .
in thi case , these word have thi probabl becaus of our assumpt about the smooth .
but that here , these c word have a differ probabl .
now we can go further by rewrit the second sum as a differ of two other sum .
basic the first sum is actual the sum over all the queri word .
now we know that the origin sum is not over the queri word .
thi is over all the queri word that ar not match in the document .
so here we pretend that thei ar actual over all the queri word .
so , we take a sum over all the queri word .
obvious thi sum ha extra term that ar , thi sum ha extra term that ar not in thi sum .
becaus here we're take sum over all the queri word .
there it's not match in the document .
so in order to make them equal , we have to then subtract anoth sum here .
and thi is a sum over all the queri word that ar mention in the document .
and thi make sens becaus here we're consid all queri word .
and then we subtract the queri that wa match in the document .
that will give us the queri rule that not match in the document .
and thi is almost a revers process of the first step here .
and you might wonder why we want to do that .
well , that's becaus if we do thi then we'll have differ form of term insid these sum .
so , now we can see in the sum we have , all the word , match queri word , match in the document with thi kind of term .
here we have anoth sum over the same set of term .
match queri term in document .
but insid the sum it's differ .
but these two sum can clearli be merg .
so , if we do that we'll get anoth form of the formula that look like the follow at the bottom here .
and note that thi is a veri interest , becaus here we combin the , these two , that ar a sum of the queri word match in the document in the on sum here .
and the other sum , now is the compos to two part , and , and these two part look much simpler .
just becaus these ar the probabl of unseen word .
but thi formula is veri interest , becaus you can see the sum is now over all the match queri term .
and just like in the vector space model , we take a sum of term , that intersect of queri vector and the document vector .
so it all alreadi look a littl bit like the vector space model .
in fact there is even more sever here .
as we , we explain on thi slide .
so , i show you how we rewrit the into a form that look like a , the formula on thi slide .
after we make the assumpt about smooth the languag model base on the collect of the languag model .
now , if we look at the , thi rewrit , it actual would give us two benefit .
the first benefit is , it help us better understand that thi rank function .
in particular , we're go to show that from thi formula we can see smooth is the correct that we model will give us someth like a tf idf weight and length normal .
the second benefit is that it also allow us to comput the queri likelihood more effici .
in particular , we see that the main part of the formula is a sum over the match queri term .
so , thi is much better than if we take the sum over all the word .
after we smooth the document the languag model , we essenti have nonzero probabl for all the word .
so , thi new form of the formula is much easier to score , or to comput .
it's also interest to note that the last of term here is actual independ of the document .
sinc our goal is to rank the document for the same queri , we can ignor thi term for rank .
becaus it's go to be the same for all the document .
ignor it wouldn't effect the order of the document .
insid the sum , we also see that each match queri term would contribut a weight .
and thi weight actual , is veri interest becaus it look like tf idf weight .
first , we can alreadi see it ha a frequenc of the word in the queri , just like in the vector space model .
when we take adult product , we see the word frequenc in the queri to show up in such a sum .
and so natur , thi part will correspond to the vector element from the document vector .
and here , inde , we can see it actual encod a weight that ha similar factor to tf idf weight .
i let you examin it .
can you see it ?
can you see which part is captur tf , and which part is captur idf weight ?
so if you want , you can paus the video to think more about it .
so , have you notic that thi p sub seen is relat to the term frequenc in the sens that if a word occur veri frequent in the document , then the s probabl here will tend to be larger .
right ?
so , thi mean thi term is realli do someth like tf weight .
have you also notic that thi time in the denomin is actual achiev the factor of idf ?
why ?
becaus thi is the popular of the term in the collect , but it's in the denomin .
so , if the probabl in the collect is larger than the weight is actual smaller .
and , thi mean a popular term .
we actual have a smaller weight .
and , thi is precis what idf weight is do .
onli not , we now have a differ form of tf and idf .
rememb , idf ha a log , logarithm of document frequenc , but here we have someth differ .
but intuit , it achiev a similar effect .
interestingli , we also have someth relat to the length normal .
again , can you see which factor is relat to the length in thi formula .
well , i just sai that , that thi term is relat to idf weight .
thi , thi collect probabl .
but , it turn out thi term here is actual relat to a document length normal .
in particular , d might be relat to document n , length .
so , it , it encod how much probabl mass we want to give to unseen word .
how much smooth you ar allow to do .
intuit , if a document is long , then we need to do less smooth .
becaus we can assum that it is larg enough that , we have probabl observ all of the word that the author could have written .
but if the document is short , the unseen ar expect to be to be larg , and we need to do more smooth .
it's like that there ar word that have not been retain yet by the author .
so , thi term appear to paralyz long document tend to be longer than , larger than for long document .
but note that the also occur here .
and so , thi mai not actual be necessari , penal long document , and in fact is not so clear here .
but as we will see later , when we consid some specif smooth method , it turn out that thei do penal long document .
just like in tf idf weight and the document end formula in the vector space model .
so , that's a veri interest observ becaus it mean we don't even have to think about the specif wai of do smooth .
we just need to assum that if we smooth with thi languag model , then we would have a formula that look like a tf idf weight and document length normal .
what's also interest that we have a veri fix form of the rank function .
and see , we have not heurist put a logarithm here .
in fact , if you can think about , why we would have a logarithm here ?
if you look at the assumpt that we have made , it will be clear .
it's becaus we have us a logarithm of queri likelihood for score .
and , we turn the product into a sum of logarithm of probabl .
and , that's why we have thi logarithm .
note that if we onli want to heurist implement a tf weight and idf weight , we don't necessarili have to have a logarithm here .
imagin if we drop thi logarithm , we would still have tf and idf weight .
but , what's nice with probabilist model is that we ar automat given a logarithm function here .
and , that's basic , a fix reform of the formula that we did not realli have to huerist line .
and in thi case , if you try to drop thi logarithm the model probabl won't , won't work as well as if you keep the logarithm .
so , a nice properti of probabilist model is that by follow some assumpt and the probabl rule , we'll get a formula automat .
and , the formula would have a particular form , like in thi case .
and , if we huerist design the formula , we mai not necessarili end up have such a specif form .
so to summar , we talk about the need for smooth a document and model .
otherwis , it would give zero probabl for unseen word in the document .
and , that's not good for score a queri with such an unseen word .
it's also necessari , in gener , to improv the acc , accuraci of estim the model repres the topic of thi document .
the gener idea of smooth in retriev is to us the connect languag model to give us some clue about which unseen word would have a higher probabl .
that is the probabl of the unseen word is assum to be proport to it probabl in the collect .
with thi assumpt , we've shown that we can deriv a gener rank formula for queri likelihood .
that ha a fact of tf idf wait and document length normal .
we also see that through some rewrit , the score of such rank function , is primarili base on sum of weight on match queri term , just like in the vector space model .
but , the actual rank function is given us automat by the probabl rule and assumpt we have made .
unlik in the vector space model , where we have to heurist think about the form of the function .
howev , we still need to address the question , how exactli we should we should smooth a document imag model ?
how exactli we should us the refer languag model base on the connect to adjust the probabl of the maximum .
and , thi is the topic of the next to that .
thi lectur is about the specif smooth method for languag model us in probabilist retriev model .
in thi lectur we will continu the discuss of languag model for inform retriev , particularli the queri likelihood retriev method .
and we're go to talk about the specif smooth method us for such a retriev function .
so , thi is a slide from a previou lectur where we show that with queri likelihood rank and the smooth with the collect languag model .
we end up have a retriev function that look like the follow .
so , thi is the retriev function , base on these assumpt that we have discuss .
you can see it's a sum of all the match queri term here .
and insid the sum it's a count of term in the queri , and some weight for the term in the document .
we have tfi , tf weight here .
and then we have anoth constant here , in n .
so clearli , if we want to implement thi function us a program languag , we'll still need to figur out a few variabl .
in particular , we're go to need to know how to estim the , probabl of would exactli .
and how do we set alpha ?
so in order to answer these question , we have to think about thi veri specif smooth method , and that is the main topic of thi lectur .
we're go to talk about two smooth method .
the first is the simpl linear interpol , with a fix coeffici .
and thi is also call a jelinek and mercer smooth .
so the idea is actual veri simpl .
thi pictur show how we estim document languag model by us maximum method , that give us word count normal by the total number of word in the text .
the idea of us thi method is to maxim the probabl of the observ text .
as a result , if a word like network , is not observ in the text .
it's go to get zero probabl , as shown here .
so the idea of smooth , then , is to reli on collect averag model , where thi word is not go to have a zero probabl to help us decid what non zero probabl should be assign to such a word .
so , we can know that network as a non zero probabl here .
so , in thi approach what we do is , we do a linear interpol between the maximum likelihood or estim here and the collect languag model .
and thi control by the smooth paramet , lambda .
which is between <num> and <num> .
so thi is a smooth paramet .
the larger lambda is the two the more smooth we have , we will have .
so by mix them togeth , we achiev the goal of assign non zero probabl .
and these two ar word in our network .
so let's see how it work for some of the word here .
for exampl if we comput to the smallest probabl for text .
now , the next on right here is made give us <num> over <num> , and that's go to be here .
but the connect probabl is thi , so we just combin them togeth with thi simpl formula .
we can also see a , the word network .
which us to have zero probabl now is get a non zero probabl of thi valu .
and that's becaus the count is go to be zero for network here , but thi part is non zero and that's basic how thi method work .
if you think about thi and you can easili see now the alpha sub d in thi smooth method is basic lambda becaus that's , rememb , the coeffici in front of the probabl of the word given by the collect languag model here , right ?
okai , so thi is the first smooth method .
the second on is similar , but it ha a find end for manual interpret .
it's often call a durat of the ply or bayesian smooth .
so again here , we face the problem of zero probabl for like network .
again we'll us the collect languag model , but in thi case we're go to combin them in a somewhat differ wai .
the formula first can be seen as a interpol of the maximum and the collect languag model as befor .
as in the j m's onli and after the coeffici is not the lambda , a fix lambda , but a dynam coeffici in thi form , when mu is a paramet , it's a non , neg valu .
and you can see if we set mu to a constant , the effect is that a long document would actual get smaller coeffici here .
right ?
becaus a long document we have a longer length .
therefor , the coeffici is actual smaller .
and so a long document would have less smooth as we would expect .
so thi seem to make more sens than a fix coeffici smooth .
of cours , thi part would be of thi form , so that the two coeffici would sum to <num> .
now , thi is on wai to understand that thi is smooth .
basic , it mean that it's a dynam coeffici interpol .
there is anoth wai to understand thi formula .
which is even easier to rememb and that's thi side .
so it's easi to see we can rewrit thi modern method in thi form .
now , in thi form , we can easili see what chang we have made to the maximum estim , which would be thi part , right ?
so it normal the count by the top element .
so , in thi form , we can see what we did , is we add thi to the count of everi word .
so , what doe thi mean ?
well , thi is basic someth rel to the probabl of the word in the collect . .
and we multipli that by the paramet mu .
and when we combin thi with the count here , essenti we ar ad pseudo count to the observ text .
we pretend everi word , ha got thi mani pseudocount .
so the total count would be the sum of these pseudocount and the actual count of the word in the document .
as a result , in total , we would have ad thi minut pseudocount .
why ?
becaus if you take a sum of thi , thi on , move over all the word and we'll see the probabl of the word would sum to <num> , and that give us just mu .
so thi is the total number of pseudo counter that we ad .
and , and so these probabl would still sum to <num> .
so in thi case , we can easili see the method is essenti to add these as a pseudocount to thi data .
pretend we actual augment the data by includ by some pseudo data defin by the collect languag model .
as a result , we have more count .
it's the , the total count for , for word , a word that would be like thi .
and , as a result , even if a word ha zero count here .
and sai if we have zero come here and that it would still have none , zero count becaus of thi part , right ?
and so thi is how thi method work .
let's also take a look at thi specif exampl here .
all right , so for text again , we will have <num> as origin count .
that we actual observ but we also ad some pseudocount .
and so , the probabl of text would be of thi form .
natur the probabl of network would be just thi part .
and so , here you can also see what's alpha sub d here .
can you see it ?
if you want to think about you can paus the video .
have you notic that thi part is basic of a sub t ?
so we can see thi case of our sub t doe depend on the document , right ?
becaus thi len depend on the document wherea in the linear interpol .
the jame move method thi is the constant .
so let's plug in these model mass into the rank function to see what we will get , okai ?
thi is a gener smooth .
so a gener rank function for smooth with subtract and you have seen thi befor .
and now we have a veri specif smooth method , the jm smooth method .
so now let's see what what's a valu for offic of d here .
and what's the valu for p sub c here ?
right , so we mai need to decid thi in order to figur out the exact form of the rank function .
and we also need to figur out of cours alpha .
so let's see .
well thi ratio is basic thi , right , so , here , thi is the probabl of c board on the top , and thi is the probabl of unseen war or , in other word basic <num> time basic the alpha here , thi , so it's easi to see that .
thi can be then rewritten as thi .
veri simpl .
so we can plug thi into here .
and then here , what's the valu for alpha ?
what do you think ?
so it would be just lambda , right ?
and what would happen if we plug in thi valu here , if thi is lambda .
what can we sai about thi ?
doe it depend on the document ?
no , so it can be ignor .
right ?
so we'll end up have thi rank function shown here .
and in thi case you can easi to see , thi a precis a vector space model becaus thi part is a sum over all the match queri term , thi is an element of the queri map .
what do you think is a element of the document up there ?
well it's thi , right .
so that's our document left element .
and let's further examin what's insid of thi logarithm .
well on plu thi .
so it's go to be nonneg , thi log of thi , it's go to be at least <num> , right ?
and these , thi is a paramet , so lambda is paramet .
and let's look at thi .
now thi is a tf .
now we see veri clearli thi tf weight here .
and the larger the count is , the higher the weight will be .
we also see idf weight , which is given by thi .
and we see dock the lan's relationship here .
so all these heurist ar captur in thi formula .
what's interest that we kind of have got thi weight function automat by make variou assumpt .
wherea in the vector space model , we had to go through those heurist design in order to get thi .
and in thi case note that there's a specif form .
and when you see whether thi form actual make sens .
all right so what do you think is the denomin here , hm ?
thi is a math of document .
total number of word , multipli by the probabl of the word given by the collect , right ?
so thi actual can be interpret as expect account over word .
if we're go to draw , a word , from the connect that we model .
and , we're go to draw as mani as the number of word in the document .
if you do that , the expect account of a word , w , would be precis given by thi denomin .
so , thi ratio basic , is compar the actual count , here .
the actual count of the word in the document with expect count given by thi product if the word is in fact follow the distribut in the clutch thi .
and if thi counter is larger than the expect counter in thi part , thi ratio would be larger than on .
so that's actual a veri interest interpret , right ?
it's veri natur and intuit , it make a lot of sens .
and thi is on advantag of us thi kind of probabilist reason where we have made explicit assumpt .
and , we know precis why we have a logarithm here .
and , why we have these probabl here .
and , we also have a formula that intuit make a lot of sens and doe tf idf weight and document and some other .
let's look at the , the dirichlet prior smooth .
it's veri similar to the case of jm smooth .
in thi case , the smooth paramet is mu and that's differ from lambda that we saw befor .
but the format look veri similar .
the form of the function look veri similar .
so we still have linear oper here .
and when we comput thi ratio , on will find that is that the ratio is equal to thi .
and what's interest here is that we ar do anoth comparison here now .
we're compar the actual count .
which is the expect account of the world if we sampl meal world accord to the collect world probabl .
so note that it's interest we don't even see dock the len here and lighter in the jm model .
all right so thi of cours should be plug into thi part .
so you might wonder , so where is dock len .
interestingli the dock len is here in alpha sub d so thi would be plug into thi part .
as a result what we get is the follow function here and thi is again a sum over all the match queri word .
and we're against the queer , the queri , time frequenc here .
and you can interpret thi as the element of a document vector , but thi is no longer a singl dot product , right ?
becaus we have thi part , i know that n is the name of the queri , right ?
so that just mean if we score thi function , we have to take a sum over all the queri word , and then do some adjust of the score base on the document .
but it's still , it's still clear that it doe document len modul becaus thi len is in the denomin so a longer document will have a lower weight here .
and we can also see it ha tf here and now idf .
onli that thi time the form of the formula is differ from the previou on in jm on .
but intuit it still implement tfidf wait and document len rendit again , the form of the function is dictat by the probabilist reason and assumpt that we have made .
now there ar also disadvantag of thi approach .
and that is , there's no guarante that there's such a form of the formula will actual work well .
so if we look about at thi geo function , all those tf idf wait and document len rendit for exampl it's unclear whether we have sub linear transform .
unfortun we can see here there is a logarithm function here .
so we do have also the , so it's here right ?
so we do have the sublinear transform , but we do not intention do that .
that mean there's no guarante that we will end up in thi , in thi wai .
suppos we don't have logarithm , then there's no sub linear transform .
as we discuss befor , perhap the formula is not go to work so well .
so that's an exampl of the gap between a formal model like thi and the relev that we have to model , which is realli a subject motion that is ti to user .
so it doesn't mean we cannot fix thi .
for exampl , imagin if we did not have thi logarithm , right ?
so we can take a risk and we're go to add on , or we can even add doubl logarithm .
but then , it would mean that the function is no longer a proper risk model .
so the consequ of the modif is no longer as predict as what we have been do now .
so , that's also why , for exampl , pm45 remain veri competit and still , open channel how to us public risk model as thei arriv , better model than the pm25 .
in particular how do we us queri like how to deriv a model and that would work consist better than dm <num> .
current we still cannot do that .
still interest open question .
so to summar thi part , we've talk about the two smooth method .
jelinek mercer which is do the fix coeffici linear interpol .
dirichlet prior thi is what add a pseudo count to everi word and is do adapt interpol in that the coeffici would be larger for shorter document .
in most case we can see , by us these smooth method , we will be abl to reach a retriev function where the assumpt ar clearli articul .
so thei ar less heurist .
explain the result also show that these , retriev function .
also ar veri effect and thei ar compar to bm <num> or pm len adult .
so thi is a major advantag of probabl smaller where we don't have to do a lot of heurist design .
yet in the end that we natur implement tf idf weight and doc length normal .
each of these function also ha precis on smooth paramet .
in thi case of cours we still need to set thi smooth paramet .
there ar also method that can be us to estim these paramet .
so overal , thi show by us a probabilist model , we follow veri differ strategi then the vector space model .
yet , in the end , we end up uh , with some retriev function that look veri similar to the vector space model .
with some advantag in have assumpt clearli state .
and then , the form dictat by a probabilist model .
now , thi also conclud our discuss of the queri likelihood probabilist model .
and let's recal what assumpt we have made in order to deriv the function that we have seen in thi lectur .
well we basic have made four assumpt that i list here .
the first assumpt is that the relev can be model by the queri likelihood .
and the second assumpt with med is , ar queri word ar gener independ that allow us to decompos the probabl of the whole queri into a product of probabl of old word in the queri .
and then , the third assumpt that we have made is , if a word is not seen , the document or in the late , it probabl proport to it probabl in the collect .
that's a smooth with a collect ama model .
and final , we made on of these two assumpt about the smooth .
so we either us jm smooth or dirichlet prior smooth .
if we make these four assumpt then we have no choic but to take the form of the retriev function that we have seen earlier .
fortun the function ha a nice properti in that it implement tf idf weight and document machin and these function also work veri well .
so in that sens , these function ar less heurist compar with the vector space model .
and there ar mani extens of thi , thi basic model and you can find the discuss of them in the refer at the end of thi lectur .
thi lectur is about the feedback in text retriev .
so , in thi lectur , we're go to continu the discuss on text retriev method .
in particular , we're go to talk about feedback in text retriev .
thi is a diagram that show the retriev process .
we can see the user would type in a queri and then the queri would be sent to a retriev engin or search engin and the engin would return result .
these result would be shown to the user .
after the user ha seen these result , the user can actual make judgment .
so for exampl , the user ha sai , well , thi is good and thi document is not veri us .
thi is good again , et cetera .
now thi is call a relev judgment or relev feedback , becaus we've got some feedback inform from the user base on the judgment .
thi can be veri us to the system .
learn what exactli is interest to the user .
so the feedback modul would then take thi as input and also us the document collect to try to improv rank .
typic , it would involv updat the queri .
so the system can now rank the result more accur for the user .
so thi is call relev feedback .
the feedback is base on relev judgement made by the user .
now these judgement ar reliabl , but the user gener don't want to make extra effort , unless thei have to .
so the downsid's that involv some extra effort by the user .
there is anoth form of feedback call a pseudo relev feedback , or a blind feedback also call an automat feedback .
in thi case , you can see onc the user ha got without an effect , we don't have to involv user .
so you can see there's no user involv here .
and we simpli assum that the top rank document to be relev .
let's sai , we have assum the top ten is relev .
and then we will then us these assum document to learn and to improv the queri .
now you might wonder , you know , how could thi help if we simpli assum the top rank document would be random .
well you can imagin these top rank document ar actual similar to relev document , even if thei ar not relev , thei look like relev document .
so , it's possibl to learn some relat term to the queri from thi set .
in fact , you mai recal that we talk about us languag model to analyz word associ to learn relat word to the word comput .
right ?
and then what we did is first , us comput to retriev all the document that contain comput .
so , imagin now the queri here is a comput .
right ?
and then the result will be those document that contain comput .
and what we can do then is to take the top end result .
thei can match comput veri well and we're go to count the term in thi set and then we're go to then us the background languag model to choos the term that ar frequent the in thi set , but not frequent the in the whole collect .
so , if we make a contrast between these two , what we can find is that we'll learn some relat term too , the work comput as what i've seen befor .
and these relat word can then be ad to the origin queri to expand the queri .
and thi would help us free document that don't necessarili match comput , but match other word like program and softwar .
so thi is factor for improv the search doubt .
but of cours , pseudo relev feedback is complet unreli .
we have to arbitrarili set a cutoff .
so there is also someth in between call implicit feedback .
in thi case , what we do , we do involv user , but we don't have to ask user to make judgement .
instead , we ar go to observ how the user interact with the search result .
so , in thi case , we're go to look at the clickthrough .
so the user click on thi on and the , the user view thi on .
and the user skip thi on and the user view thi on again .
now thi also is a clue about whether a document is us to the user and we can even assum that we're go to us onli the snippet here in thi document .
the text that's actual seen by the user , instead of the actual document of thi entri in the link .
there that same web search mai be broken , but that , it doesn't matter .
if the user tri to fetch thi document that becaus of the displai text , we can assum thi displai text is probabl relev is interest to user , so we can learn from such inform .
and thi is call implicit feedback and we can again , us the inform to updat the queri .
thi is a veri import techniqu us in modern search engin .
you know , think about googl and bing and thei can collect a lot of user activ .
why thei ar serverless ?
right .
so thei would observ what document we click on , what document we skip .
and thi inform is veri valuabl and thei can us thi to encod the search engin .
so to summar , we would talk about the three kind of feedback here rather than feedback .
where the us exquisit judgement , it take some us effort , but the judgement that inform is reliabl .
we talk about the pseudo feedback , where we simpli assum top random document .
we get random , we don't have to involv the user .
therefor , we could do that actual befor we , we return the result to the user .
and the third is implicit feedback , where we us clickthrough .
where we don't , we involv user , but the user doesn't have to make explicit effort to make judgement .
thi lectur is about the feedback in the vector space model .
in thi lectur , we continu talk about the feedback and text retriev .
particularli we're go to talk about feedback in the vector space model .
as we have discuss befor in the case of feedback the task of a text retriev system is relearn from exampl to improv retriev accuraci .
we will have posit exampl , those ar the document that ar assum that will be random or judg with be random and all the document that ar view by user .
we also have neg exampl , those ar document known to be non relev .
thei can also be the document that ar escap by user .
the gener method in the vector space model for feedback is to modifi our queri vector .
now we want to place the queri vector in a better posit to make that accur and what doe that mean exactli ?
well , if you think about the queri vector that would mean you would have to do someth to vector element .
and in gener that would mean we might add new term .
we might adjust weight of old term or assign weight to new term .
and as a result in gener the queri will have more term so we often call thi queri expans .
the most effect method in the vector space model of feedback is call rocchio feedback which wa actual propos sever decad ago .
so , the idea is quit simpl we illustr thi idea by us a two dimension displai of all the document in the collect and also the queri vector .
so , now we can see the queri vector is here in the center and these ar all of the document .
so when we us a queri vector and us a similar function to find the most similar document .
we ar basic draw a circl here and then these document would be basic the top rank document .
and thi process of relev document , right ?
and these ar random document for exampl that's relev , etc .
and then these minus ar neg document like thi .
so our goal here is try to move thi queri vector to some posit to improv the retriev accuraci .
by look at thi diagram what do you think where should we move the queri vector so that we can improv the retriev accuraci .
intuit , where do you want to move the queri back to ?
if you want to think more you can paus the video .
now if you think about thi pictur you can realiz that in order to work well in thi case you want the queri vector to be as close to the posit vector as possibl .
that mean , ideal you want to place the queri vector somewher here or we want to move the queri vector closer to thi point .
now , so what exactli at thi point ?
well , if you want these relev document to be rank on the top you want thi to be in the center of all of these relev document , right ?
becaus then if you draw a circl around thi on you get all these relev document .
so that mean we can move the queri back toward the centroid of all the relev document vector .
and thi is basic the idea of rocchio , of cours you then can see that the centroid of neg document .
and on move awai from the neg document .
now geometr we're talk about a move vector closer to some other vector and awai from other vector .
algebra it just mean we have thi formula .
here you can see thi is origin queri vector and thi averag basic is the centroid vector of relev document .
when we take the averag over these vector then we're comput the centroid of these vector .
and similarli thi is the averag in that non relev document of vector so it's essenti of now random , document .
and we have these three paramet here , alpha , beta and gamma .
thei're control the amount of movement .
when we add these two vector togeth we're move the queri at the closer to the centroid , all right , so when we add them , togeth .
when we subtract thi part we kind of move the queri vector awai from that centroid so thi is the main idea of rocchio feedback .
and after we have done thi we will get a new queri vector which can us it to store document .
thi new new queri vector will then reflect the move of thi origin queri vector toward thi relev centroid vector and awai from the non relev centroid vector , okai ?
so let's take a look at exampl , right ?
thi is the exampl that we have seen earlier onli that i in the , the displai of the actual document i onli show the vector represent of these document .
we have five document here and we have true red in the document here , right ?
thei ar displai in red and these ar the term vector .
now , i just assum an idea of weight , a lot of time we have zero weight of cours .
these ar neg document , there ar two here , there is anoth on here .
now in thi rocchio method we first comput the centroid of each categori and so let's see .
look at the centroid of the posit document but we simpli just so it's veri easi to see .
we just add thi with thi on the correspond element and that's down here and take the averag .
and then we're go to add the correspond element and then just take the averag , right ?
so we do thi for all these .
in the end , what we have is thi on .
thi is the averag vector of these two so it's a centroid of these two , right ?
let's also look at the centroid of the nest document .
thi is basic the same we're go to take the averag of three element .
and these ar the correspond element in these three vector and so on and so forth .
so in the end , that we have thi on .
now , in the rocchio feedback method we're go to combin all these with origin queri vector , which is thi .
so now let's see how we combin them togeth .
well , that's basic thi , right ?
so we have a paramet outlier control the origin queri term weight that's <num> .
and now i've beta to control the infer of the posit centroid vector weight that's <num> that come from here , right ?
so thi goe here and we also have thi neg wait here .
conduit by a gamma here and thi weight ha come from of cours the nectiv centroid here .
and we do exactli the same for other term each is for on term .
and thi is our new vector .
and we're go to us thi new queri vector , thi on to run the document .
you can imagin what would happen , right ?
becaus of the movement that thi on or the match of these red document much better .
becaus we move thi vector closer to them and it's go to penal these black document , these non relev document .
so thi is precis what we want from feedback .
now of cours , if we appli thi method in practic we will see on potenti problem and that is the origin queri ha onli four time that ar not zero .
but after we do queri , imagin you can imagin we'll have mani term that would have a number of weight .
so the calcul would have to involv more term .
in practic , we often truncat thi vector and onli retain the term which is the highest weight .
so let's talk about how we us thi method in practic .
i just mention that we often truncat the vector consid onli a small number of word that have highest weight in the centroid vector .
thi is for effici concern .
i also sai that here that a neg exampl or non relev exampl tend not to be veri us especi compar with posit exampl .
now you can think about the , why .
on reason is becaus neg document tend to distract the queri in all direct so when you take the averag it doesn't realli tell you where exactli it should be move to .
wherea , posit document tend to be cluster togeth and thei respond to you to consist the direct .
so that also mean that sometimesw we don't have those neg exampl but note that in , in some case in difficult queri where most top random result ar neg .
neg feedback afterward is veri us .
anoth thing is to avoid over fit that mean we have to keep rel high weight on the origin queri term .
why ?
becaus the sampl that we see in feedback is a rel small sampl .
we don't want to overli trust the small sampl and the origin queri term ar still veri import .
those term ar type in by the user and the user ha decid that those term ar most import .
so in order to prevent the us from over fit or drift .
a type of drift prevent type of drift due to the bia toward the , the feedback exampl .
we gener would have to keep a pretti high weight on the origin term so it is safe to do that .
and thi is especi , true for pseudo awar feedback .
now thi method can be us for both relev feedback and pseudo relev feedback .
in the case of pseudo feedback , the paramet beta should be set to a , a smaller valu becaus the random exampl ar assum to be random there not as reliabl as your relev feedback , right ?
in the case of relev feedback , we obvious could us a larger valu .
so , those paramet still have to be set and .
and the ro , rocchio method is usual robust and effect .
it's , it's still a veri popular method for feedback .
thi lectur is about the web search .
in thi lectur we ar go to talk about on of the most import applic of text retriev , web search engin .
so let's first look at some gener challeng and opportun in web search .
now , mani inform retriev algorithm had been develop at the , befor the web wa born .
so , when the web wa born , it creat the best opportun to appli those algorithm to major applic problem that everyon would care about .
so natur , there had to be some further extens of the classic search algorithm to address some new challeng encount in web search .
so here ar some gener challeng .
firstli , thi is a scalabl challeng .
how we handl the size of the web , and ensur complet of coverag of all the inform .
how to serv mani user quickli , and by answer all their queri .
all right , so , that's on major challeng .
and befor the web wa born , the scale of search wa rel small .
the second problem is that there is low qualiti inform .
and there ar often spam .
the third challeng is dynam of the web .
the new page ar constantli creat and some page mai be updat , ev , veri quickli .
so it make it harder to , keep the index fresh .
so these ar some of the challeng that the , we have to solv in order to , build a high qualiti web search engin .
on the other hand , there ar also some interest opportun that we can leverag to improv search result .
there ar mani addit heurist .
for exampl you know us link that we can leverag to improv score .
now the error that we talk about such as the vector space model ar gener algorithm .
and thei can be appli to ani search applic , so that's , the advantag .
on the other hand , thei also don't take advantag of special characterist of page , or document , in the specif applic such as web search .
web page ar link with each other so obvious the link is someth that we can also leverag .
so becaus of these challeng and opportun there ar new techniqu that have been develop for web search , or due to the need of a web search .
on is parallel index and search , and thi is to address the issu of scalabl , in particular googl's imag of mapreduc is veri inferenti , and ha been veri help in that aspect .
second , there ar techniqu that ar develop for , address the problem of spam .
so , spam detect .
we'll have to prevent those , spam page from be rank high .
and there ar also techniqu to achiev robust rank .
and we're go to us a lot of signal to rank page so that it's not easi to spam the search engin with particular trick .
and the third line of techniqu is link analysi .
and these ar techniqu that can allow us to to improv search result by leverag extra inform .
and in gener in web search we're go to us multipl featur for rank .
not just link analysi but also exploit all kind of crawl like the layout of web page or anchor text that describ a link to anoth page .
so here's a pictur show the basic search engin technolog .
basic , thi is the web on the left and then user on the right side .
and we're go to help these , thi user get access to the web inform .
and the first compon is the crawler that with the crawl page and the second compon is index .
that will take these page creat an invert index .
the third compon that is a retriev , not with the us , but the index to answer user's queri , by talk to the user's browser .
and then , the search result would be , given to the user .
and , and then the browser will show those result and , to allow the user to interact with the web .
so we're go to talk about each of these compon .
first we're go to talk about the crawler also call a spider or a softwar robot that would do someth like a crawl page on the web .
to build a toi crawler is rel easi becaus you just need to start with a set of seed page and then fetch page from the web and pars these page new link .
and then add them to the prioriti of q and then just explor those addit link , right .
but to build a real crawler actual is tricki and there ar some complic issu that we have do deal with .
for exampl robust , what if the server doesn't respond .
what if there's a trap that gener dynam gener webpag that might , attract your crawler to keep crawl the same site and to fetch dynam gener page .
the result of thi issu of crawl and you don't want to overload on particular server with mani crawl request .
and you have to respect the , the robot exclus protocol .
you also need to handl differ type of file .
there ar imag , pdf file , all kind of format on the web .
and you have to also consid url extens .
so , sometim those ar cgi script , and , you know , intern refer , etc . , and sometim , you have javascript on the page that , thei also creat challeng .
and you ideal should also recogn the page becaus you don't have to duplic to the , those page .
and final , you mai be interest to discov hidden url .
those ar url that mai not be link , to ani page .
but if you truncat the url to , shorter pass , you might be abl to get some addit page .
so , what ar the major crawl strategi ?
in gener , breadth first , is most common , becaus it natur balanc , balanc server load .
you would not , keep probe a particular server also parallel crawl is veri natur , becaus thi task is veri easi to parallelis and there ar some variat of the crawl task .
on interest variat is call focus crawl .
in thi kind we're go to crawl just some page about a particular topic .
for exampl , all page about automobil .
and , and , thi is typic go to start with a queri , and then you can us the queri to get some result .
from the major search engin .
and then you can start it with those result and gradual crawl more .
so on challeng in crawl is to find the new page that peopl have creat , and peopl probabl ar creat new page all the time , and thi is veri challeng if the new page have not been actual link to ani old page .
if thei ar , then you can probabl refin them by recrawl the older page .
so these ar also some um , interest challeng that have to be solv .
and final we might face the scenario of increment crawl or repeat crawl .
right ?
so your first , let's sai if you want to be abl to web search engin .
and you were the first to crawl a lot of data from the web .
and then , but then onc you have collect all the data and in futur we just need to crawl the , the updat page .
you , you , in gener you don't have to re crawl everyth , right ?
or it's not necessari .
so , in thi case you , you go as you minim a resourc overhead by us minimum resourc to , to just still crawl updat page .
so thi is after a veri interest research question here .
and research question is that there aren't mani standard algorithm for do thi , thi task .
right ?
but in gener , you can imagin , you can learn from the past experi .
right .
so the two major factor that you have to consid ar first , will thi page be updat frequent ?
and do i have to crawl thi page again ?
if the page is a static page that hasn't been chang for month you probabl don't have to re crawl it everydai , right ?
becaus it's unlik that it will be chang frequent .
on the other hand if it's you know , sport score page that get updat veri frequent and you mai need to re crawl it mayb even multipl time , on the same dai .
the other factor to consid is , is thi page frequent access by user ?
if it , if it is , that mean it's a high util page , and then thu it's more import to ensur such a page to be fresh .
compar it with anoth page that ha never been fetch by ani user for a year .
than , even though that page ha been chang a lot , then , it's probabl not necessari to crawl that page or at least it's not as urgent as , to maintain the fresh of frequent access page by user .
so to summar , web search is on of the most import applic of text retriev .
and there ar some new challeng particularli scalabl , effici , qualiti inform .
there ar also new opportun particularli , rich link inform and layout , et cetera .
crawler is an essenti compon of web search applic .
and , in gener , we can classifi two scenario .
onc is initi crawl and here we want to have complet crawl of the web if you ar do a gener search engin or focu crawl if you want to just target it at a certain type of page .
and then there is anoth scenario that's increment updat of the crawl data or increment crawl .
in thi case you need to optim the resourc .
thi lectur is about recommend system .
so , so far we have talk about a lot of aspect of search engin .
and we have talk about the problem of search and the rank problem , differ method for rank , implement of search engin and how to evalu the search engin , et cetera .
thi is partli becaus we know that web search engin ar , by far , the most import applic of text retriev .
and thei ar the most us tool to help peopl convert big raw text data into a small set of relev document .
anoth reason why we spend so mani lectur on search engin is becaus mani techniqu us in search engin ar actual also veri us for recommend system , which is the topic of thi lectur .
and so overal the two system ar actual well connect , and there ar mani techniqu that ar share by them .
so thi is a slide that you have seen befor when we talk about the two differ mode of text access pull and push .
and , we mention that recommend system ar the main system to serv user in the push mode , where the system will take the initi to recommend the inform to user , or to push the relev inform to the user .
and thi often work well when the user ha a rel stabl inform need , when the system ha good knowledg about what a user want .
so a recommend system is sometim call a filter system .
and it's becaus recommend us item to peopl is like discard or filter out the useless articl .
so in thi sens , thei ar kind of similar .
and in all the case , the system must make a binari decis .
and usual , there is a dynam sourc of inform item , and you have some knowledg about the user's interest , and then the system would make a deliveri decis , whether thi item is interest to the user .
and then if it is interest then the system would recommend the articl to the user .
so the basic filter question here is realli , will thi user like , thi item ?
will u like item x ?
and there ar two wai to answer thi question if you think about it , right ?
on is look at what item u like , and then we can see if x is actual like those item .
the other is to look at who like x , and we can see if thi user look like a , on of those user , or like most of those user .
and these strategi can be combin .
if we follow the first strategi and look at item similar in the case of recommend text object , then we ar talk about a content base filter or content base recommend .
if we look at the second strategi then , thi will compar user .
and in thi case , we're exploit user similar , and the techniqu is often call a collabor filter .
so let's first look at the content base filter system .
thi is what a system would look like .
insid the system , there would be a binari classifi that would have some knowledg about the user's interest , and it's call the user interest profil .
it maintain the profil to keep track of the user's interest .
and then there is a util function to guid the user to make decis , and i'll explain the util of the function in a moment .
it help the system decid where to set the threshold .
and then the accept document will be those that have pass the threshold accord to the classifi .
there should be also an initi modul that would take a user's input , mayb from a user's , specifi keyword , or a chosen categori , et cetera .
and thi will be , to feed the system with a initi user profil .
there is also typic a learn modul that will learn from user' feedback over time .
now note that in thi case , typic user' inform need is stabl so the system would have a lot of opportun to observ the user , you know , if the user ha taken a recommend item as view that , and thi is a cu , a signal to indic that the recommend item mai be relev .
if the user discard it , no , it's not relev .
and so , such feedback can be a long term feedback and can last for a long time and the system can clock , collect a lot of inform about thi user's interest .
and thi can then be us to improv the classifi .
now what the criteria for evalu such a system ?
how do we know thi filter system actual perform well ?
now in thi case we cannot us the rank evalu measur , like a map , becaus we can't afford wait for a lot of document , and then rank the document to make a decis for the user .
and so , the system must make a decis , in real time , in gener to decid whether the item is abov the threshold or not .
so in other word , we're try to decid absolut relev .
so in thi case on common us strategi is to us a util function through a valid system .
so here i show a linear util function that's defin as , for exampl , <num> multipli by the number of good item that you deliv , minu <num> multipli by the number of bad item you delet , that you deliv .
so in other word , we , we could kind of just treat thi as almost a , in a gambl game .
if you delet , if you deliv on good item , let's sai you win <num> , you gain <num> .
but if you deliv a bad on , you would lose <num> .
and thi util function basic kind of measur , how much monei you would , get by do thi kind of game , right .
and so it's clear that if you want to maxim thi util function , your strategi should be to deliv as mani good articl as possibl , and minim the deliveri of bad articl .
that , that's obviou , right .
now on interest question here is , how should we set these coeffici ?
now i just show a <num> and a neg <num> , as the possibl coeffici , but on can ask the question , ar thei reason ?
so what do you think ?
do you think that's a reason choic ?
what about other choic ?
so for exampl , we can have <num> and minu <num> , or <num> minu <num> .
what's the differ ?
what do you think ?
how would thi util function affect the system's threshold of thi issu ?
right , you can think of these two extrem case , <num> minu <num> versu <num> minu <num> .
which on do we think it would encourag the system to over deliv ?
and which on would encourag the system to be conserv ?
yeah ?
if you think about it , you will see that when we get a big award for deliv a good document , you incur onli a small penalti for deliv a bad on .
intuit , you would be encourag to deliv more , right ?
and you can try to deliv more in hope of get a good on deliv , and then you'll get a big award .
right , so on the other hand , if you choos <num> minu <num> , you don't realli get such a big prize if you deliv a good document .
on the other hand , you will have a big loss if you deliv bad on .
you can imagin that the system would be veri reluct to deliv lot of document .
it ha to be absolut sure that it's a non relev on .
so thi util function ha to be design base on a specif applic .
the three basic problem in content base filter ar the follow .
first ha to make a filter decis .
so it ha to be a binari decis maker , a binari classifi .
given a text , a text document , and a profil descript of the user , it ha to sai ye or no , whether thi document should be deliv or not .
so that's a decis modul , and there should be a initi modul as you have seen earlier .
and thi is to get the system start .
and we have to initi the system base on onli veri limit text descript , or veri few exampl from the user .
and the third compon is a learn modul which ha , ha to be abl to learn from limit relev judgment becaus we can onli learn from the user about their prefer on the deliveri document .
if we don't deliv a document to the user , we'd never know we would never be abl to know whether the user like it or not , right .
and we can accumul a lot of document , we can learn from the entir histori .
now , all these model would have to be optim to maxim the util .
so how can we build a such a system ?
and there ar mani differ approach .
here we ar go to talk about how to extend a retriev system , a search engin for inform filter .
again , here's why we've spent a lot of time talk about the search engin .
becaus it's actual not veri hard to extend the search engin for inform filter .
so , here is the basic idea for extend a retriev system for inform filter .
first , we can reus a lot of retriev techniqu to do score .
all right , so we know how to score document against queri et cetera .
we can measur the similar between a profil text descript and a document .
and then we can us a score threshold for the filter decis .
we , we do retriev and then we kind of find the score of document , and then we appli a threshold to , to sai , to see whether a document is pass thi threshold or not .
and if it's pass the threshold , we ar go to sai it's relev and we ar go to deliv it to the user .
and anoth compon that we have to add is , of cours , to learn from the histori .
and here we can us the tradit feedback techniqu to learn to improv score .
and we know rocchio can be us for score improv , right ?
and , but we have to develop new approach to learn how to set the threshold .
and you know , we need to set it initi , and then we have to learn how to updat the threshold over time .
so here's what the system might look like if we just gener a vector space model for filter problem , right ?
so you can see the document vector could be fed into a score modul , which it alreadi exist in in a search engin that implement a vector space model .
and the profil will be treat as a queri essenti .
and then the profil vector can be match with the document vector , to gener the score .
and then thi score will be fed into a threshold modul that would sai ye or no .
and then the evalu would be base on the util for the filter result .
if it sai ye , and then the document will be sent to the user , and then the user could give some feedback .
and the feedback inform would have been us , would be us to both adjust to the threshold and adjust the vector represent .
so the vector learn is essenti the same as queri modif or feedback in the case of search .
the threshold learn is a no , new compon in that we need to talk a littl bit more about .
there ar some interest challeng in threshold .
would have known in the filter problem .
so here i show the , sort of the data that you can collect in , in the filter system .
so you can see the score and the statu of relev .
so the first on ha a score <num> , and it's relev .
the second on is not relev .
of cours , we have a lot of document for which we don't know the statu , becaus we will have to the user .
so as you can see here , we onli see the judgement of document deliv to the user .
so thi is not a random sampl .
so it's a censor data .
it's kind of bias , so that creat some difficulti for learn .
and secondli , there ar in gener veri littl label data and veri few relev data , so it's , it's also challeng for machin learn approach .
typic thei requir requir more train data .
and in the extrem case at the begin , we don't even have ani , label there as well .
the system still ha to make a decis , so that's a veri difficult problem at the begin .
final , the result of thi issu of explor versu exploit tradeoff .
now thi mean we also want to explor the document space a littl bit , and to , to see if the user might be interest in the document that we have not yet label .
so , in other word , we're go to explor the space of user interest by test whether the user might be interest in some other document that current ar not match the user's interest .
thi so well .
so how do we do that ?
well we could lower the threshold a littl bit and do we just deliv some near miss to the user to see what the user would respond so see how the user will , would respond to thi extra document .
and , and thi is a trade off , becaus on the on hand , you want to explor , but on the other hand , you don't want to realli explor too much , becaus then you would over deliv non relev inform .
so exploit mean you would , exploit what you learn about the user .
and let's sai you know the user is interest in thi particular topic , so you don't want to deviat that much .
and , but if you don't deviat at all , then you don't explor at all .
that's also not good .
you might miss opportun to learn anoth interest of the user .
so thi is a dilemma .
and that's also a difficult problem to solv .
now how do we solv these problem ?
in gener , i think why can't i us the empir util optim strategi ?
and thi strategi is basic to optim the threshold base on , histor data , just as you have seen on the previou slide , right ?
so you can just comput the util on the train data for each candid score threshold .
pretend that cut at thi point .
what if i cut out the threshold , what would happen ?
what's util ?
comput the util , right ?
we know the statu , what's it base on approxim of click through , right ?
so then we can just choos thi threshold that give the maximum util on the train data .
now but thi of cours doesn't account for explor that we just talk about .
and there is also the difficulti of bia .
train sampl , as we mention .
so in gener , we can onli get an upper bound or , for the true optim threshold becaus the , the al , the threshold might be actual lower than thi .
so it's possibl that the discard item might be actual interest to the user .
so how do we solv thi problem ?
well we gener as i said we can lower the threshold to explor a littl bit .
so here's on particular approach call the beta gamma threshold learn .
so the , the idea is foreign .
so , here i show a rank list of all the train document that we have seen so far .
and thei ar rank by their posit .
and on the y axi , we show the util .
of cours , thi function depend on how you specifi the coeffici in the util function .
but we can not imagin depend on the cut off posit we will have a util .
that mean suppos i cut at thi posit and that will be the util .
so we can for exampl i then find some cut off point .
the optim point theta optim is the point when we would achiev the maximum util if we had chosen thi threshold .
and there is also <num> threshold , <num> util threshold .
as you can see at thi cut off .
the util is <num> .
now , what doe that mean ?
that mean if i lower the threshold , and then get the , and now i'm i reach thi threshold , the util would be lower , but it's still posit .
still non elect , at least .
so it's not as high as the optim util , but it give us a a safe point to explor the threshold .
as i just explain , it's desir to explor the interest space .
so it's desir to lower the threshold base on your train data .
so that mean , in gener , we want to set the threshold somewher in thi rang .
it's the when user off fault to control the the deviat from the optim util point .
so you can see the formula of the threshold will be just the incorpor of the zero util threshold and the optim between the threshold .
now the question is how , how should we set r form , you know and when should we deviat more from the optim util point .
well thi can depend on multipl factor and the on wai to solv the problem is to encourag thi threshold mechan to explor up the <num> point , and that's a safe point , but we're not go to necessarili reach all the wai to the <num> point .
but rather we're go to us other paramet to further defin alpha .
and thi specif is as follow .
so there will be a beta paramet to control .
the deviat from the optim threshold .
and thi can be base on for exampl can be account for the over throughout the train data let's sai .
and so thi can be just the adjust factor .
but what's more interest is thi gamma paramet here , and you can see in thi formula gamma is control the the influenc of the number of exampl in train data set .
so you can see in thi formula as n which denot the number of train exampl .
becom bigger than it would actual encourag less explor .
in other word , when n is veri small , it will try to explor more .
and that just mean if we have seen few exampl , we're not sure whether we have exhaust the space of interest .
so but as we have seen mani exampl from the user , mani data point , then we feel that we probabl dont' have to explor more .
so thi give us a dynam of strategi for explor , right ?
the more exampl we have seen , the less explor we ar go to do .
so , the threshold will be closer to the optim threshold .
so , that's the basic idea of thi approach .
now , thi approach actual , ha been work well in some evalu studi .
and , particularli effect .
and , also can welcom arbitrari util with a appropri lower bound .
and explicitli address explor explor tradeoff .
and it kind of us a zero in thi threshold point as a , a safeguard .
for explor and exploit tradeoff .
we're not , never go to explor further than the zero util point .
so , if you take the analog of gambl , and you , you don't want to risk lose monei .
you know , so it's a safe strategi , a conserv strategi for explor .
and the problem is , of cours , thi approach is pure heurist .
and the zero util lower bound is also often too conserv .
and there ar , of cours , call ar more advanc than machin learn project that have been propos for solv these problem .
and thi is a veri activ research area .
so to summar there ar two strategi for recommend system or filter system .
on is content base , which is look at the item similar .
and the other is collabor filter , which is look at the user similar .
in thi lectur we have cover content base filter approach .
in the next lectur , we're go to talk about collabor filter .
the content base filter system we gener have to solv sever problem relat to filter decis and learn , etc .
and such a system can actual be base on a search engin system by ad a threshold mechan and ad adapt learn algorithm to allow the system to learn from long term feedback from the user .
thi lectur is about collabor filter .
in thi lectur , we're go to continu the discuss of recommend system .
in particular , we're go to look at the approach of collabor filter .
you have seen thi slide befor when we talk about the two strategi to answer the basic question will user u like item x .
in the previou lectur , we look at the item similar , that's content base filter .
in thi lectur , we're go to look at the user similar .
thi is a differ strategi call collabor filter .
so first of all , what is collabor filter ?
it is to make filter decis for individu user base on the judgement of other user and that is to sai , we will infer individu's interest or prefer from that , of other similar user .
so the gener idea is the follow .
given a user u , we ar go to first find the similar user , u1 through and then we're go to predict the us prefer base on the prefer of these similar user , u1 through .
now the user similar here can be judg base on their similar .
the prefer is on a common set of item .
now here you'll see that the exact content of item doesn't realli matter .
we're go to look at the onli , the relationship between the user and the item .
so thi mean thi approach is veri gener if it can be appli to ani item not just with text object .
so thi approach , it would work well under the follow assumpt .
first user with the same interest will have similar prefer .
second , the user with similar prefer probabl share the same interest .
so for exampl , if the interest of the user is in inform retriev , then we can infer the user probabl favor sigir paper .
and so those who ar interest in inform retriev research probabl all favor sigir paper , that's someth that we make .
and if thi assumpt is true , then it would help collabor filter to work well .
we can also assum that if we see peopl favor sigir paper , then we can infer the interest is probabl inform retriev .
so these simpl exampl , it seem what make sens .
and in mani case such as assumpt actual doe make sens .
so , anoth assumpt you have to make is that there ar a suffici larg number of user prefer avail to us .
so for exampl , if you see a lot of rate of user for movi and those indic their prefer in movi .
and if you have a lot of such data , then collabor filter can be veri effect .
if not , there will be a problem and that's often call a cold start problem .
that mean you don't have mani prefer avail , so the system could not fulli take advantag of collabor filter yet .
so let's look at the collabor filter problem in a more formal wai .
and so thi pictur show that we ar in gener consid a lot of user and show we're show m user here .
so , u1 through and we're also consid a number of object .
let's sai , n object denot as o1 through on and then we will assum that the user will be abl to judg those object and the user could for exampl , give rate to those item .
for exampl , those item could be movi , could be product and then the user would give rate on through five , let's sai .
so what you see here is that we have assum some rate avail for some combin .
so some user have watch movi , thei have rate those movi .
thei obvious won't be abl to watch all the movi and some user mai actual onli watch a few movi .
so thi is in gener a respons matrix , right ?
so mani item mani entri have unknown valu and what's interest here is we could potenti infer the valu of a element in thi matrix base on other valu and that's actual the central question in collabor filter .
and that is , we assum an unknown function here f , that would map a pair of user and object to a rate .
and we have observ there ar some valu of thi function and we want to infer the valu of thi function for other pair that we , that don't have valu avail here .
so thi is ve , veri similar to other machin learn problem , where we would know the valu of the function on some train there that and we hope to predict the the valu of thi function on some test there .
all right .
so thi is the function approxim .
and how can we pick out the function base on the observ rate ?
so thi is the , the setup .
now there ar mani approach to solv thi problem .
and in fact , thi is a veri activ research area .
a reason that there ar special confer dedic to the problem is a major confer devot to the problem .
and here what will do is talk about basic strategi , and that would be base on similar of user and then predict the rate of an object by a , a , activ user us the rate of similar user to thi activ user .
thi is call a memori base approach becaus it's a littl bit similar to store all the user inform .
and when we ar consid a particular user , we're go to try to kind of retriev the relev user , or the similar user through thi user case .
and then try to us that user's inform about those user to predict the prefer of thi user .
so here's the gener idea , and we us some notat here , so .
x sub i j denot the rate of object o j by user u i .
and n sub i is averag rate of all object by thi user .
so thi n i is need .
becaus we would like to normal the rate of object by thi user .
so how do you do normal ?
well , where do you adjust that ?
subtract the , the averag rate from all the rate .
now thi is the normal rate so that the rate from differ user will be compar .
becaus some user might be more gener and thei gener give more high rate .
but , some other might be more critic .
so , their rate can not be directli compar with each other or aggreg them togeth .
so , we need to do thi normal .
now , the predict of the rate .
on the item by anoth user or activ user , u sub a here can be base on the averag rate of similar user .
so the user u sub a is the user that we ar interest in recommend item to .
and we now ar interest in recommend thi o sub j .
so we're interest in know how like thi user will like thi object .
how do we know that ?
well the idea here is to look at the how whether similar user to thi user have like thi object .
so mathemat , thi is , as you sai , the predict the rate of thi user on thi app , object .
user a on object oj is basic combin of the normal rate of differ user .
and in fact , here , we're pick a sum of all the user .
but not all user contribut equal to the averag .
and thi is control by the weight .
so thi .
weight control the infer of a user on the predict .
and of cours , natur thi weight should be relat to the similar between ua and thi particular user , ui .
the more similar thei ar then the more contribut we would like user u i to make in predict the prefer of u a .
so the formula is extrem simpl .
you're go to see it's a sum of all the possibl user .
and insid the sum , we have their rate , well their normal rate as i just explain .
the rate need to be normal in order to be comfort with each other .
and then these rate ar rate by their similar .
so we can imagin a w of a and i is just a similar of user a user i .
now , what's k here ?
well , k is a simpler normal .
it's just it's just on over the sum of all the weight , over all the user .
and so thi mean , basic , if you consid the weight here togeth with k .
and we have coeffici or weight that would sum to on for all the user .
and it's just a normal strategi , so that you get thi predict rate in the same rang as the these rate that we us to make the predict .
right ?
so , thi is basic the main idea of memori base approach for collabor filter , okai ?
onc we make thi predict , we also would like to map back to the rate that the user .
the user would actual make .
and thi is to further add the , mean rate or averag rate of thi user u sub a to the predict valu .
thi would recov .
a meaning rate for thi user .
so if thi user is gener , then the averag would be somewhat high , and when we ad that , the rate will be adjust to a rel high rate .
now , when you recommend an item to a user , thi actual doesn't realli matter becaus you ar interest in basic the normal rate that's more meaning .
but when thei evalu these collabor filter approach is typic assum that actual rate of user on these object to be unknown .
and then you do the predict and then you compar the predict rate with their actual rate .
so thei , you do have access to the actual rate .
but then you pretend you don't know .
and then you compar real system predict with the actual rate .
in that case , obvious the system's predict would have to be adjust to match the actual result the user , and thi is not what's happen here , basic .
okai ?
so thi is the memori base approach .
now of cours if you look at the formula , if you want to write the program to implement it .
you still face the problem of determin what is thi w function , right ?
onc you know the w function , then the formula is veri easi to implement .
so inde there ar mani differ wai to comput thi function or thi weight , w .
and , specif approach gener differ in how thi is comput .
so , here ar some possibl .
and , you can imagin , there ar mani pro , other possibl .
on popular approach is we us the pearson correl coeffici .
thi would be a sum of a common rang of item , and the formula is a standard pearson correl coeffici formula , as shown here .
so , thi basic measur weather the two user tend to all give higher rate to similar item , or lower rate to similar item .
anoth measur is the cosin measur and thi is the retreat the rate vector as vector in the vector space , and then we're go to measur the the angel and comput the cosign of the angl of the two vector .
and thi measur ha been us in the vector space more for retriev as well .
so as you can imagin , there ar so mani differ wai of do that .
in all these case , note that the user similar is base on their prefer on item , and we did not actual us ani content inform of these item .
it didn't matter what these item ar .
thei can be movi , thei can be book , thei can be product , thei can be tax document .
we just didn't care about the content .
and so thi allow such approach to be appli to a wide rang of problem .
now in some newer approach of cours , we would like to us more inform about the user .
clearli , we know more about the user , not just a , these prefer on these item .
and so in a actual filter system , us collabor filter , we could also combin that with content base filter , we could us context inform .
and those ar all interest approach that peopl ar still studi .
there ar newer approach propos .
but thi approach ha been shown to work reason well and it's easi to implement .
and practic applic could be a start point to see if the strand here work well for your applic .
so there ar some obviou wai to also improv thi approach .
and mainli would like to improv the user similar measur .
and there ar some practic issu to deal with here as well .
so for exampl , there will be a lot of miss valu .
what do you do with them ?
well , you can set them to default valu or the averag rate of the user .
and that will be a simpl solut .
but there ar advantag to approach that can actual try to predict those miss valu and then us the predict valu to improv the similar .
so in fact , the memori databas approach , you can predict those with miss valu , right ?
so you can imagin , you have iter approach where you first do some preliminari predict and then you can us the predictor valu to further improv the similar function .
right so thi is here is a wai to solv the problem .
and the strategi of thi in the effect of the perform of clariti filter , just like in the other heurist , we improv the similar function .
anoth idea which is actual veri similar to the idea of idf that we have seen in text research , is call the invers user frequenc or iuf .
now here the idea is to look at the where the two user share similar rate .
if the item is a popular item that ha been aah , view by mani peopl and seemingli lead to peopl interest in thi item mai not be so interest .
but if it's a rare item and ha not been view by mani user .
but , these two user to thi item .
and thei give similar rate , and it sai more about their similar , right ?
so it's kind of to emphas more on similar on item that ar not view by mani user .
so to summar our discuss of recommend system in some sens the filter task of recommend is easi and in some other sens and the task is actual difficult .
so it easi becaus the user dexpect , though in thi case , the system take initi to push the inform to the user .
so the user doesn't realli make an effort .
so ani recommend is better than noth , right ?
so unless you recommend that all the you know , noisi item or useless document , if you can recommend that some us inform us gener , would appreci it , all right .
so that's in that sens , that's easi .
howev , filter is actual a much harder task .
becaus you have to make a binari decis , and you can't afford wait for a lot of item and then you will whether on item is better than other .
you have to make a decis when you see thi item .
let's think about new filter as well as you see the new .
and you have to decid whether the new would be interest to a user .
if you wait for a few dai , well , even if you can make accur recommend of the most relev new , onli two dai wouldn't be significantli decreas .
anoth reason why it's hard , it's becaus of data spars .
if you think of thi as a learn problem in collabor filter , for exampl , it's pure base on learn from the past rate .
so if you don't have mani rate , there's realli not much you can do , right ?
and mai i just mention thi problem .
thi is actual a veri seriou problem .
but of cours there ar strategi that have been propos to solv the problem .
and there ar , there ar differ strategi that we will us to allevi the problem .
we can us , for exampl , more user inform to assess their similar instead of us the prefer .
of these user on these item the immedi addit inform or better for about the user etcetera and , and we also talk about the two strategi for filter task .
on is content base where we look at item in clariti you know there's a clariti of filter where we look at the user similar .
and thei obvious can be combin .
in a practic system , you can imagin , thei gener would have to be combin .
so that will give us a hybrid strategi for filter .
a , and , we also could recal that we talk about push versu pull as two strategi for get access to the text data .
and recommend the system is it will help , user in the push mode .
and search engin ar , certain user in the pull mode .
of us the tool should be combin , and thei can be combin into have a system that can support user with multipl mode and format access .
so in the futur , we could anticip for such a system to be more usabl to a user .
and also thi is a activ research area so there ar a lot of new algorithm be , be propos over time .
in particular , those new algorithm tend to us a lot of context inform .
now the context here could be the context of the user , you know , it could also be context of document or item .
the item ar not isol .
thei ar connect in mani wai .
the user might form social network as well , so there's a rich context there that we can leverag in order to realli solv the problem well , and then that's a activ research area where also machin learn algorithm have been appli .
here ar some addit read in the handbook call recommend system .
and ha a collect of a lot of good articl that can give you an overview of a number of specif approach to recommend system .
thi lectur is a summari of thi cours .
thi map show the major topic we have cover in thi cours .
and here ar some kei high level take awai messag .
first we talk about natur languag content analysi .
here the main take awai messag is natur languag process is the foundat for textual retriev , but current nlp isn't robust enough .
so the back of word replenish is gener the main method us in modern search engin and it's often suffici for most of the search task .
but obvious , for more compass search task , then we need a deeper measur process techniqu .
and we then talk about a high level strategi for text access and we talk about push versu pull in plural .
we talk about a queri , which is brows .
now , in gener in futur search engin , we should integr all these techniqu to provid a multipl inform access and then we talk about a number of issu relat to search engin .
we talk about the search problem and we frame that as a rank problem and we talk about the a number of retriev method .
we start with an overview of the vector space model and probabilist model and then we talk about the vector space model in that .
we also later talk about leverag learn approach and that's probabilist model .
and here , the main take awai messag is that model retriev function tend to look similar and thei gener us variou heurist .
most import on ar tf idf wait document length normal and that tf is often transform through a sub linear transform function and then we talk about how to implement a retriev system .
and here the main techniqu that we talk about how to construct an invert index .
so that we can prepar the system to answer a queri quickli and we talk about how to , to fast research by us the invert index and we then talk about how to evalu the text retriev system mainli introduc the cranfield evalu methodolog .
thi wa a veri import the variou methodolog of that can be appli to mani task .
we talk about the major evalu measur .
so the most import measur for a search engin ar map mean averag precis and ndcg .
normal discount accumul gain and also precis and record the two basic measur .
and we then talk about feedback techniqu .
and we talk about the rock you in the vector space model and the mixtur model in the languag model approach .
feedback is veri import techniqu especi consid the opportun of learn from a lot of pixel on the web .
we then talk about the web search .
and here , we talk about the how to us parallel index to resolv the scalabl issu in index , we introduc a mapreduc and then we talk about the how to us inform interact pull search .
we talk about page random hit as the major algorithm to analyz link on the web .
we then talk about learn to rank .
thi is a us of machin learn to combin multipl featur for improv score .
not onli the effect can be improv us thi approach but we can also improv the robust of the rank function , so that it's not easi to spam a search engin with just a , a some featur to promot a page .
and final , we talk about the futur of web search .
we talk about some major interact that we might assum in the futur in improv the current gener of search engin .
and then final , we talk about the recommend system and these ar system to implement the push mode and we'll talk about the two approach .
on is content base , on is collabor filter and thei can be combin togeth .
now an obviou miss piec in thi pictur is the user , you can see .
so user interfac is also a import compon in ani search engin , even though the current search interfac is rel simpl .
there actual have been a lot of studi of user interfac relat to visual for exampl and thi is topic to that , you can learn more by read thi book .
it's a excel book about all kind of studi of search user interfac .
if you want to know more about the , the topic that we talk about , you can also read some addit read that ar list here .
in thi short cours , we ar onli manag to cover some basic topic in text retriev in search engin .
and these resourc provid addit inform about more advanc topic and thei give more thorough treatment of some of the topic that we talk about .
and a main sourc is synthesi digit librari where you can see a lot of short textbook or textbook or long tutori .
thei tend to provid us with a lot of inform to explain a topic and there ar multipl seri that ar relat to thi cours .
on is inform concept , retriev and servic .
anoth is human languag technolog and yet , anoth is artifici intellig and machin learn .
there ar also some major journal and confer list over here that tend to have a lot of research paper relat to the topic of thi cours .
and final for more inform about resourc includ read and tool kit , etc .
you can check out thi url .
so , if you have not taken the text mine cours in thi in thi data mine special seri , then natur , the next step is to take that call .
as thi pictur show to mine the text data , we gener need two kind of techniqu .
on is text retriev , which is cover in thi cours .
and these techniqu will help us convert raw big text data into small , relev text data , which ar actual need in the specif applic .
and human plai import role in mine ani text data , becaus text data is written for human to consum .
so , involv human in the process of data mine is veri import .
and in thi cours , we have cover variou strategi to help user get access to the most relev data .
these techniqu ar also essenti in ani text mine system to help provid provid and to help user interpret the inner pattern that the user would find through text data mine .
so , in gener , the user would have to go back to the origin data to better understand the pattern .
so the text mine cours or rather text mine and ana , analyt cours will be deal , deal with what to do onc the user ha found the inform .
so thi is a in thi pictur where we would convert the text data into action or knowledg .
and thi ha to do with help user to go further digest with a found inform or to find the pattern and to reveal knowledg buri in text and such knowledg can be us in applic system to help decis make or to help user finish a task .
so , if you have not taken that cours the natur step and the natur next step would be to take that cours .
thank you for take thi cours .
i hope you have found thi cours to be us to you and i look forward to interact with you at a futur activ .
thi lectur is about web index .
in thi lectur , we will continu talk about web search , and we're go to talk about how to creat a web scale index .
so onc we crawl the web we've got a lot of web page .
the next step is we us the index to creat the invert index .
in gener , we can us the standard inform retriev techniqu for creat the index , and that is what we talk about in the previou lectur .
but there ar new challeng that we have to solv for web scale index , and the two main challeng of scalabl and effici .
the index will be so larg that it cannot actual fit into ani singl machin or singl disk , so we have to store the data on multipl machin .
also , becaus the data is so larg , it's benefici to process the data in parallel so that we can produc the index quickli .
to address these challeng , googl ha made a number of innov .
on is the googl file system , that's a gener distribut file system that can help programm manag file store on a cluster of machin .
the second is mapreduc .
thi is a gener softwar framework for support parallel comput .
hadoop is the most well known open sourc implement of mapreduc , now us in mani applic .
so thi is the architectur of the googl file system .
it us a veri simpl central manag mechan to manag all the specif locat of file .
so it maintain the file namespac and look up tabl to know where exactli each file is store .
the applic client would then talk to thi gf master .
and that obtain specif locat of the file that thei want to process .
and onc the gf client obtain the specif inform about the file , then the applic client can talk to the specif server where the data actual sit directli .
so that you can avoid avoid involv other node in the network .
so when thi file system store the file on machin the system also would creat a fix size of chunk .
so the data file ar separ into mani chunk , each chunk is <num> megabyt , so it's pretti big .
and that's appropri for larg data process .
these chunk ar replic to ensur reliabl .
so thi is someth that the , the programm doesn't have to worri about , and it's all taken care of by thi file system .
so from the applic perspect , the programm would see thi as if it's a normal file .
the program doesn't have to know where exactli it's store , and can just invok high level oper to process the file .
and anoth featur is that the data transfer is directli between applic and chunk server , so it's , it's effici in thi sens .
on top of the googl file system , and googl also propos mapreduc as a gener framework for parallel program .
now , thi is veri us to support a task like build invert index .
and so thi framework is hide a lot of low level featur from the programm .
as a result , the programm can make minimum effort to creat a applic that can be run on a larg cluster in parallel .
so , some of the low level detail hidden in the framework , includ the specif natur commun , or load balanc , or where the task ar execut , all these detail ar hidden from the programm .
there is also a nice featur which is the built in fault toler .
if on server is broken , let's sai , so it's down , and then some task mai not be finish , then the mapreduc mechan would know that the task ha not been done .
so it would automat dispatch the task on other server that can do the job .
and therefor , again , the programm doesn't have to worri about that .
so here's how mapreduc work .
the input data will be separ into a number of kei , valu pair .
now , what exactli is in the valu will depend on the data .
and it's actual a fairli gener framework to allow you to just partit the data into differ part .
and each part can be then process in parallel .
each kei , valu pair will be then sent to a map function .
the programm will write the map function , of cours .
and then the map function will then process thi kei valu pair and gener the , a number of other kei valu pair .
of cours , the new kei is usual differ from the old kei that's given to the map as input .
and these kei valu pair ar the output of the map function .
and all the output of all the map function will be then collect .
and then thei will be further sort base on the kei .
and the result is that all the valu that ar associ with the same kei will be then group togeth .
so now we've got a pair of a kei and a set of valu that ar attach to thi kei .
so thi will then be sent to a reduc function .
now , of cours , each reduc function will handl a differ each a differ kei .
so we will send thi , these output valu to multipl reduc function , each handl a uniqu kei .
a reduc function would then process the input , which is a kei and a set of valu , to produc anoth set of kei valu as the output .
so these output valu would be then collect togeth to form the , the final output .
right , so thi is the , the gener framework of mapreduc .
now , the programm onli need to write the the map function and the reduc function .
everyth els is actual taken care of by the mapreduc framework .
so , you can see the programm realli onli need to do minimum work .
and with such a framework , the input data can be partit into multipl part .
each is process in parallel first by map , and then in the process after we reach the reduc stage , then much more reduc function can also further process the differ kei and their associ valu in parallel .
so it achiev some it achiev the purpos of parallel process of a larg dataset .
so let's take a look at a simpl exampl , and that's word count .
the input is is file contain word .
and the output that we want to gener is the number of occurr of each word , so it's the word count .
right , we know thi , thi kind of count would be us to , for exampl , assess the popular of a word in a larg collect .
and thi is us for achiev a factor of idf weight for search .
so how can we solv thi problem ?
well , on natur thought is that , well , thi task can be done in parallel by simpli count differ part of the file in parallel and then in the end , we just combin all the count .
and that's precis the idea of what we can do with mapreduc .
we can parallel line in thi input file .
so more specif , we can assum the input to each map function is a kei valu pair that repres the line number and the stream on that line .
so the first line , for exampl , ha a kei of on .
and the valu is hello world bye world , and just four word on that line .
so thi kei valu pair will be sent to a map function .
the map function would then just count the word in thi line .
and in thi case , of cours , there ar onli four word .
each word get a count of on .
and these ar the output that you see here on thi slide , from thi map function .
so , the map function is realli veri simpl .
if you look at the , what the pseudocod look like on the right side , you see , it simpli need to iter over all the word in thi line , and then just call a collect function , which mean it would then send the word and the counter to the collector .
the collector would then try to sort all these kei valu pair from differ map function .
right ?
so the function ar veri simpl .
and the programm specifi thi function as a wai to process each part of the data .
of cours , the second line will be handl by a differ map function , which will produc a similar output .
okai , now the output from the map function will be then sent to a collector .
and the collector will do the intern group or sort .
so at thi stage , you can see we have collect multipl pair .
each pair is a word and it count in the line .
so onc we see all these these pair , then we can sort them base on the kei , which is the word .
so we will collect all the count of a word , like bye , here , togeth .
and similarli , we do that for other word .
like hadoop , hello , etc .
so each word now is attach to a number of valu , a number of count .
and these count repres the occurr of thi word in differ line .
so now we have got a new pair of a kei and a set of valu , and thi pair will then be fed into a reduc function .
so the reduc function now will have to finish the job of count the total occurr of thi word .
now it ha alreadi got all these partial count , so all it need to do is simpli to add them up .
so the reduc function shown here is veri simpl as well .
you have a counter and then iter over all the word that you see in thi arrai , and then you just accumul these count , right .
and then final , you output the kei and and the total count , and that's precis what we want as the output of thi whole program .
so , you can see , thi is alreadi veri similar to build a invert index , and if you think about it , the output here is index by a word , and we have alreadi got a dictionari , basic .
we have got the count .
but what's miss is the document id and the specif frequenc count of word in those document .
so we can modifi thi slightli to actual build a invert index in parallel .
so here's on wai to do that .
so in thi case , we can assum the input to a map function is a pair of a kei which denot the document id and the valu denot the string for that document .
so it's all the word in that document .
and so the map function will do someth veri similar to what we have seen in the water compani exampl .
it simpli group all the count of thi word in thi document togeth .
and it will then gener a set of kei valu pair .
each kei is a word .
and the valu is the count of thi word in thi document plu the document id .
now , you can easili see why we need to add document id here .
of cours , later , in the invert index , we would like to keep thi inform , so the map function should keep track of it .
and thi can then be sent to the reduc function later .
now , similarli anoth document d2 can be process in the same wai .
so in the end , again , there is a sort mechan that would group them togeth .
and then we will have just a kei like java associ with all the document that match thi kei , or all the document where java occur , and their count , right , so the count of java in those document .
and thi will be collect togeth .
and thi will be , so fed into the reduc function .
so , now you can see , the reduc function ha alreadi got input that look like a invert index entri , right ?
so , it's just the word and all the document that contain the word and the frequenc of the word in those document .
so , all you need to do is simpli to concaten them into a continu chunk of data , and thi can be then retain into a file system .
so basic , the reduc function is go to do veri minim work .
and so , thi is pseudo code for invert index construct .
here we see two function , procedur map and procedur reduc .
and a programm would specifi these two function to program on top of mapreduc .
and you can see , basic , thei ar do what i just describ .
in the case of map , it's go to count the occurr of a word us an associ arrai , and will output all the count togeth with the document id here .
right ?
so thi , the reduc function , on the other hand simpli concaten all the input that it ha been given and then put them togeth as on singl entri for thi kei .
so thi is a veri simpl mapreduc function , yet it would allow us to construct an invert index at a veri larg scale , and data can be process by differ machin .
the program doesn't have to take care of the detail .
so thi is how we can do parallel index construct for web search .
so to summar , web scale index requir some new techniqu that go beyond the standard tradit index techniqu .
mainli , we have to store index on multipl machin , and thi is usual done by us a file system like googl file system , a distribut file system .
and secondli , it requir creat the index in parallel , becaus it's so larg , it take a long time to creat an index for all the document .
so if we can do it in parallel , it would be much faster , and thi is done by us the mapreduc framework .
note that the both the gf and mapreduc framework ar veri gener , so thei can also support mani other applic .
thi lectur is about link analysi for web search .
in thi lectur we're go to talk about web search , and particularli focus on how to do link analysi and us the result to improv search .
the main topic of thi lectur is to look at the rank algorithm for web search .
in the previou lectur , we talk about how to creat index .
now that we have got index , we want to see how we can improv rank of page on the web .
standard ir model can also be appli here , in fact thei ar import build block for support web search , but thei aren't suffici , mainli for the follow reason .
first , on the web we tend to have veri differ inform need .
for exampl , peopl might search for a web page or entri page , and thi is differ from the tradit librari search where peopl ar primarili interest in collect literatur inform .
so these kind of queri ar often call navig queri , the purpos is to navig into a particular target page .
so for such queri , we might benefit from us link inform .
secondli , document have addit inform .
and on the web , web page ar web format .
there ar a lot of other group , such as the layout , the titl , or link inform again .
so thi ha provid an opportun to us extra context inform of the document to improv score .
and final , inform qualiti vari a lot .
so that mean we have to consid mani factor to improv the rank algorithm .
thi would give us a more robust wai to rank the page make it the harder for ani spammer to just manipul the on signal to improv the rank of a page .
so as a result peopl have made a number of major extens to the rank algorithm .
on line is to exploit link to improv score and that's the main topic of thi lectur .
peopl have also propos algorithm to exploit larg scale implicit feedback inform in the form of clickthrough .
that's of cours in the categori of feedback techniqu , and machineri is often us there .
in gener , in web search the rank algorithm ar base on machineri algorithm to combin all kind of featur .
and mani of them ar base on the standard origin model such as bm25 that we talk about , or queri icod to score differ part of document or to , provid addit featur base on content match .
but link inform is also veri us so thei provid addit score signal .
so let's look at link in more detail on the web .
so thi is a snapshot of some part of the web , let's sai .
so we can see there ar mani link that link differ page togeth .
and in thi case you can also look at the , the center here .
there is a descript of a link that's point to the document on the right side .
now thi descript text is call anchor text .
if you think about thi text , it's actual quit us becaus it provid some extra descript of that page be point to .
so , for exampl , if someon want to bookmark amazon . com front page , the person might sai , the big onlin bookstor , and then with a link to amazon , right ?
so the descript here is actual veri similar to what the user would type in in the queri box when thei ar look for such a page .
that's why it's veri us for , for , rank page .
suppos someon type in a queri like onlin bookstor or big onlin bookstor , right .
the queri would match thi anchor text in the page here .
and then thi actual provid evid for match the page that's been point to , that is the amazon entri page .
so if you match the anchor text that describ the link to a page , actual that provid good evid for the relev of the page be point to .
so anchor text is veri us .
if you look at the bottom part of thi pictur , you can also see there ar some pattern of link , and these link might indic the util of a document .
so for exampl , on the right side you can see thi page ha receiv mani in , in link .
that mean mani other page ar point to thi page .
and thi show that thi page is quit us .
on the left side you can see , thi is a page that point to mani other page .
so , thi is a theater page that would allow you to actual see a lot of other page .
so we can call the first case author page and the second case a hub page .
thi mean the link inform can help in two wai .
on is to provid extra text for match .
the other is to provid some addit score for the web page to character how like a page is a hub , how like a page is a author .
so peopl then , of cours , propos idea to leverag thi , thi link inform .
googl's pagerank , which wa a main techniqu that thei us in earli dai , is a good exampl .
and that , that is the algorithm to captur page popular , basic to score author .
so the intuit here ar , link ar just like citat in the literatur .
think about on page point to anoth page .
thi is veri similar to on paper cite anoth paper .
so , of cours , then if a page is cite often , then we can assum thi page to be more us in gener , right ?
so that's a veri good intuit .
now , page rank is essenti to take advantag of thi intuit to implement the , with the principl approach .
intuit it's essenti do citat count or in link count .
it just improv thi simpl idea in , in two wai .
on is would consid indirect citat .
so that mean you don't just look at the how mani in link you have , you also look at the what ar those page that ar point to you .
if those page , themselv , have a lot of in link , well that mean a lot .
in some sens you will get some credit from that .
but if those page that ar point to you ar not ar be point to by other page , thei themselv don't have mani in link , then , well , you don't get that much credit .
so that's the idea of get indirect citat .
right , so you can also understand thi idea by look at , again , the research paper .
if you ar cite by , let's sai ten paper , and those ten paper ar , just workshop paper and that , or some paper that ar not veri influenti , right , so although you got ten in link , that's not as good as if you have , you're cite by ten paper that themselv have attract a lot of other citat .
so thi is a case where we would like to consid indirect link and pagerank doe that .
the other idea is , it's good to smooth the citat .
or , or , or assum that basic everi page is have a non zero pseudo citat count .
essenti , you ar try to imagin there ar mani virtual link that will link all the page togeth so that you , you actual get pseudo citat from everyon .
the , the reason why thei want to do that is thi would allow them to solv the problem elegantli with linear algebra techniqu .
so i think mayb the best wai to understand the page rank is through think of thi as through comput , the probabl of a random surfer , visit everi web page , right .
so let's take a look at thi in detail .
so in thi random surf model .
and ani page would assum random surfer would choos the next page to visit .
so thi is a small graph here .
that's , of cours an oversimplif of the complic it well .
but let's sai there ar four document here .
right , d1 , d2 , d3 and d4 .
and let's assum that a random surfer or random walker can be ani of these page .
and then the random surfer could decid to just randomli jump into ani page .
or follow a link and then visit the next page .
so if the random server is at d1 .
then , you know , with some probabl that random surfer will follow the link .
now there two outlink here .
on is point to thi d3 .
the other is point to d4 .
so the random surfer could pick ani of these two to reach e3 and d4 .
but it also assum that the random surfer might , get bore sometim .
so the random surfer would decid to ignor the actual link , and simpli randomli jump to ani page on the web .
so , if it doe that , eh , it would be abl to reach ani of the other page even though there is no link directli from to that page .
so thi is the assum the random of .
imagin a random server is realli do surf like thi , then we can ask the question .
how like on averag the server would actual reach a particular page d1 , or d2 , or d3 .
that's the averag probabl of visit a particular page .
and thi probabl is precis what page rank comput .
so the page rank score of the document is the averag probabl that the surfer visit a particular page .
now , intuit thi will basic kept you the link account .
why ?
becaus if a page ha a lot of in link then it would have a higher chanc of be visit , becaus there will be more opportun of have the surfer to follow a link to come to thi page .
and thi is why the random surf model actual captur the idea of count the in link .
note that is also consid the indirect in link .
why ?
becaus if the page that point to you have themselv a lot of in link , that would mean the random server would veri like reach on of them .
and therefor it increas the chanc of visit you .
so thi is a nice wai to captur both indirect and direct link .
so mathemat , how can we comput thi problem enough to see that we need to take a look at how thi problem in comput .
so first let's take a look at the transit match sphere .
and thi is just a matrix with valu indic how like a rand , the random surfer will go from on page to anoth .
so each rule stand for a start page .
for exampl , rule on would indic the probabl of go to ani other four page from e1 .
and here we see there ar onli non two non zero entri .
each is <num> over <num> , a half .
so thi is becaus if you look at the graph , d1 is point to d3 and d4 .
there's no link from d1 to d1 server or d2 , so we've got 0s for the first two column and <num> for d3 and d4 .
in gener , the m in thi matrix m sub i j is the probabl of go from d , i , to d , j .
and obvious for each rule , the valu should sum to on , becaus the surfer will have to go to precis on of these other page .
right ?
so thi is a transit matrix .
now how can we comput the probabl of a server visit a page ?
well if you look at the , the server model , then basic we can comput the probabl of reach a page as follow .
so , here on the left hand side , you see it's the probabl of visit page dj at time t plu <num> becaus it's the next time cont .
on the right hand side , you can see the question involv the probabl of , at page ei at time t .
so you can see the subsequ index t , here .
and that indic that's the probabl that the server wa at a document at time t .
so the equat basic captur the two possibl of reach at d j at time t plu <num> .
what ar these two possibl ?
well on is through random surf , and on is through follow a link as we just explain .
so the first part captur the probabl that the random server would reach thi page by follow a link .
and you can see , and the random surfer choos thi strategi wa probabl the as we assum .
and so there is a factor of on minu alpha here .
but the main part is realli sum over all the possibl page that the server could have been at time t , right ?
there were n page , so it's a sum over all the possibl n page .
insid the sum is the product of two probabl .
on is the probabl that the server wa at d i at time t .
that's p sub t of d i .
the other is the transit probabl from di to dj .
and so in order to reach thi dj page , the surfer must first be at di at time t .
and then also would have to follow the link to go from di to dj , so the probabl is the probabl of be at di at time t , not divid by the probabl of , go from that page to the top of the page dj here .
the second part is a similar sum .
the onli differ is that now the transit probabl is uniform , transit probabl .
<num> over n .
and thi part captur the probabl of reach thi page , through random jump .
right .
so , the form is exactli the same .
and in , in , thi also allow us to see why pagerank essenti assum smooth of the transit matrix .
if you think about thi <num> over n as come from anoth transit matrix that ha all the element be <num> over n , the uniform matrix .
then you can see veri clearli essenti we can merg the two part .
becaus thei ar of the same form , we can imagin there's a differ of metric that's a combin of thi m and that uniform matrix where everi element is <num> over n .
in thi sens , page on us thi idea of smooth and ensur that there's no <num> , entri in such a transit matrix .
of cours thi is , time depend , calcul of probabl .
now , we can imagin if we want to comput averag probabl , the averag probabl probabl would satisfi thi equat without consid the time index .
so let's drop the time index and just assum that thei would be equal .
now thi would give us n equat .
becaus for each page we have such a equat .
and if you look at the what variabl we have in these equat , there ar also precis n variabl , right ?
so thi basic mean we now have a system of n equat with n variabl , and these ar linear equat .
so basic , now the problem boil down to solv thi system of equat and here i also show that the equat in the metric form .
it's the vector p here equal a metric or the transport of the metric here .
and multipli it by the vector again .
now if you still rememb some knowledg that you learn from linear algebra and then you will realiz thi is precis the equat for item vector .
right ?
when metric by thi method you get the same valu as thi method .
and thi can solv by us an iter algorithm .
so is it , becaus she's here , on the ball , easili taken from the previou , slide .
so you see the , relationship between the , the page sourc of differ page .
and in thi iter approach or power approach we simpli start with , randomli the p .
and then we repeatedli just updat thi p by multipli .
the metric here by thi p vector .
so i also show a concret exampl here .
so you can see thi now , if we assum .
how far is point two .
then with the exampl that we show here on thi slide we have the origin transit metric here .
right ?
that encod , that encod the graph .
the actual link .
and we have thi smooth transit metric , uniform transit metric , repres random jump .
and we can combin them togeth with interpol to form anoth metric that would be like thi .
so essenti we can imagin now the look .
like thi can be captur by that .
there ar virtual link between all the page now .
so the page rank algorithm will just initi the p vector first , and then just comput the updat of thi p vector by us thi , metric multipl .
now if you rewrit thi metric multi , multipl in term of just a , an individu equat , you'll see thi .
and thi is a , basic , the updat formula for thi particular page is a , page rank score .
so you can also see , even if you want to comput the valu of thi updat score for d1 , you basic multipl thi rule .
right ?
by thi column , i will take the total product of the two , right ?
and that will give us the valu for thi valu .
so thi is how we updat the vector .
we start with some initi valu for these gui .
for , for thi , and then , we just revis the score which gener a new set of score .
and the updat formula is thi on .
so we just repeatedli appli thi , and here it converg .
and when the metric is like thi .
where there is no zero valu and it can be guarante to converg .
and at that point we will just , have the pagerank score for all the page .
now we typic set the initi valu just to <num> over n .
so interestingli , thi updat formula can be also interpret as propag score on the graph .
all right .
can you see why ?
well if you look at thi formula and then compar that with thi graph , and can you imagin how we might be abl to interpret thi as essenti propag score over the graph .
i hope you will see that inde we can imagin we have valu initi on each of these page .
all right , so we can have valu here that sai , that's on over four for each .
and then welcom to us these matrix to updat thi , the score .
and if you look at the equat here , thi on , basic we're go to combin the score of the page that possibl would lead to , reach thi page .
so we'll look at all the page that ar point to thi page .
and then combin their score and the propag score , the sum of the score to thi document d1 .
we look after the , the score that repres the probabl that the random server would be visit the other page befor it reach the d1 .
and then just do the propag to simul the probabl of reach thi , thi page d <num> .
so there ar two interpret here .
on is just the matrix multipl .
and we repeat that .
multipli the vector by thi metric .
the other is to just think of it as propag the score repeatedli on the web .
so in practic the composit of pagerank score is actual effici becaus the metric ar spars and there ar some wai to transform the equat so you avoid actual liter comput the valu of all of those element .
sometim you mai also normal the equat , and that will give you a somewhat differ form of the equat , but then the rank of page will not chang .
the result of thi potenti problem of zero out link problem .
in that case if the page doe not have ani outlook , then the probabl of these page will , will not sum to <num> .
basic , the probabl of reach the next page from thi page will not sum to <num> .
mainli becaus we have lost some probabl mass when we assum that there's some probabl that the server will try to follow link but then there's no link to follow , right ?
and on possibl solut is simpli to us page specif damp factor and that , that could easili fix thi .
basic that's to sai , how far do we want from zero for a page with no outlink .
in that case the server would just have to render them to anoth page instead of try to follow the link .
so there ar mani extens of page rank .
on extens is to do top specif page rank .
note that page rank doesn't realli us the queri format machin , right ?
so , so we can make page rank , appear specif , howev .
so , for exampl , in the topic specif page rank , we can simpli assum when the surfer , is bore .
the surfer is not go to randomli jump into ani page on the web .
instead , it's go to jump , to onli those page that ar to a queri .
for exampl , if the queri is about sport then we could assum that when it's do random jump , it's go to randomli jump to a sport page .
by do thi then we canbui a pagerank to topic align with sport .
and then if you know the current queri is about sport then you can us thi special pagerank score to rank the option .
that would be better than if you us a gener pagerank score .
pagerank is also gener algorithm that can be us in mani other .
locat for network analysi , particular for exampl for social network .
we can imagin if you comput their pagerank score for social network , where a link might indic friendship relat , you'll get some meaning score for peopl .
so we talk about a page rank as a wai to to captur the author .
now we also look at the , some other exampl where a hub might be interest .
so , there is anoth algorithm call the hit and that's go to do comput the score for us .
author hub .
intuit of , page that ar wide cite , good , sorri , there is , then , there is page that ar cite .
mani other page ar good hub , right ?
but there , i think that the .
most interest idea of thi algorithm hit is , it's go to us , a reinforc mechan to kind of help improv the score for hub and the author .
and here , so here's the idea , it will assum that good author ar cite by good hub .
that mean if you're cite by mani page with good hub score , then that increas your author score .
and similarli , good hub ar those that point to good author .
so if you get you point it to a lot of good author page , then your hub score would be increas .
so you then , you would have iter reinforc each other , becaus you can point it to some good hub .
sorri , you can point it to some good author .
to get a good hub score .
wherea those author score , would be also improv , becaus thei ar point to by a good hub .
and thi hub is also gener , it can have mani applic in graph and network analysi .
so just briefli , here's how it work .
we first also construct the matrix , but thi time we're go to construct the adjac matrix .
we're not go to normal the valu , so if there's a link there's a y .
if there's no link that's zero .
right again , it's the same graph and then , we're go to defin the top score of page as a sum of the author score of all the page that it appoint to .
so whether you ar hub that realli depend on whether you ar point to a lot of , good author page .
that's what it sai in the first equat .
your second equat , will defin the author score of a page as a sum of the hub score of all those page .
that thei point to , so whether you ar a good author would depend on whether those page that ar point to you ar good hub .
so you can see thi a form a iter reinforc mechan .
now these two equat can be also written .
in the matrix fo , format .
right , so what we get here is then the hub vector is equal to the product of the adjac matrix .
and the author vector .
and thi is basic the first equat .
right .
and similarli , the second equat can be return as the author vector is equal to the product of a transpos multipli by the hub vector .
and these ar just differ wai of express these equat .
but what's interest is that if you look at to the matrix form .
you can also plug in the author equat into the first on .
so if you do that , you can actual make it limit to the author vector complet , and you get the equat of onli hub score .
right , the hub score vector is equal to a multipli by a transpos .
multipli by the hub score vector again .
and similarli we can do a transform to have equat for just the author score .
so although we frame the problem as comput hub author , we can actual elimin the on of them to obtain equat just for on of them .
now the differ between thi and page is that , now the matrix is actual a multipl of the mer , adjac matrix and it transpos .
so thi is differ from page rank .
right ?
but mathemat then we would be comput the same problem .
so in ha , in hit , we're keep would initi the valu that state on for all these valu .
and then with the algorithm will appli these , these equat essenti and thi is equival if you multipli that .
by , by the matrix .
a and a transpos .
right .
and so the arrow of these ar exactli the same in the debat rank .
but here , becaus the adjac matrix is not normal , so what we have to do is to , what we have to do is after each iter we have to do normal .
and thi would allow us to control the groov of valu .
otherwis thei would , grew larger and larger .
and if we do that , and then we will basic get a , hit .
i wa in the comput , the hub score and also the score for all of the page .
and these score can then be us , in rang to start the pagerank score .
so to summar , in thi lectur we have seen that link inform is veri us .
in particular , the anchor text base is veri us .
to increas the the text represent of a page .
and we also talk about the pagerank and hit algorithm as two major link analysi algorithm .
both can gener score for .
what page that can be us for the , the rank function .
those that pagerank and the hit also veri gener algorithm , so thei have mani applic in analyz other graph or network .
thi lectur is about learn to rank .
in thi lectur , we're go to continu talk about web search .
in particular , we're go to talk about us machin run to combin definit featur to improv rank function .
so the question that we address in thi lectur is how we can combin mani featur to gener a , a singl rank function to optim search result .
in the previou lectur , we have talk about the , a number of wai to rank document .
we have talk about some retriev model , like a bm25 or clear light code .
thei can gener a content base score for match document with a queri .
and we also talk about the link base approach , like page rank that can give addit score to help us improv rank .
now the question now is how can we combin all these featur and potenti mani other featur to do rank ?
and thi will be veri us for rank web page not onli just to improv accuraci , but also to improv the robust of the rank function .
so that's it not easi for a spammer to just perturb a on or a few featur to promot a page .
so the gener idea of learn to rank is to us machin learn to combin these featur to optim the weight on differ featur to gener the optim rank function .
so we would assum that the given a queri document pair , q and d , we can defin a number of featur .
and these featur can vari from content base featur such as a score of the document it wa respect to the queri accord to a retriev function , such as bm25 or queri light or pivot command from a machin or pl2 , et cetera .
it can also be link base score like pagerank score .
it can be also applic of retriev model to the anchor text of the page .
right ?
those ar the type of descript of link that point to thi page .
so these can all be clue about whether thi document is relev or not .
we can even includ a , a featur such as whether the url ha a , becaus thi might be the indic of home page or entri page .
so , all of these featur can then be combin togeth to gener the rank function .
the question is of cours , how can we combin them ?
in thi approach , we simpli hypothes that the probabl that thi document is random to thi queri is a function of all these featur .
so we can hypothes thi that the probabl of relev is relat to these featur through a particular form of the function that ha some paramet .
these paramet can control the influenc of differ featur on the final relev .
thi is of cours , just a assumpt .
whether thi assumpt realli make sens is still a , a big question .
howev , you have to empir evalu the , the , the function .
but by hypothes that the relev is relat to those featur in the particular wai , we can then combin these futur to gener the potenti more power rank function , a more robust rank function .
natur , the next question is how do we estim loos paramet ?
you know , how do we know which featur should have high weight and which featur should have low weight ?
so thi is a task of train or learn .
all right .
so , in thi approach what we will do is us some train data .
those ar the data that have been judg by user .
so that we alreadi know the relev judgment .
we alreadi know which document should be rather high for which queri and thi inform can be base on real judgment by user or can , thi can also be approxim by just us click through inform .
where we can assum the click document ar better than the skip document or click document ar relev and the skip document ar not relev .
so , in gener , the fit such hypothes rang function to the train dai , mean that we will try to optim it retriev accuraci on the train data .
and we adjust these paramet to see how we can optim the perform of the function on the train data in term of some measur such as map or ndcg .
so the train data would look like a tabl of tupl .
h tupl it ha three element , the queri , the document and the judgment .
so , it look veri much like our relev judgment that we talk about in evalu of retriev system .
so now let's take a look at the specif , method that's base on regress .
now thi is on of the mani differ method in fact , it's the on of the simplest method .
and i choos thi to explain the idea becaus it's it's so simpl .
so in thi approach we simpli assum that the relev of a document with respect to the queri , is relat to a linear combin of all the featur .
here i us the xi to emot the featur .
so xi of q and d is a featur .
and we can have as mani featur as , we would like .
and we assum that these featur can be combin in a linear manner .
and each featur is control by a paramet here .
and thi beta is a paramet , that's a weight paramet .
a larger valu would mean the featur would have a higher weight and it would contribut more to the score function .
the specif form of the function actual also involv a transform of the probabl of relev .
so thi is the probabl of relev .
we know that the probabl of relev is within the rang from <num> to <num> .
and we could have just assum that the score function is relat to thi linear combin .
right , so we can do a , a linear regress but then the valu of thi linear combin could easili go beyond <num> .
so thi transform here would map ze , <num> to <num> rang through the whole rang of real valu .
you can , you can verifi it , it by yourself .
so thi allow us then to connect to the probabl of relev which is between <num> and <num> to a linear combin of arbitrari effici .
and if we rewrit thi into a probabl function , we will get the next on .
so on thi side on thi equat , we will have the probabl of relev .
and on the right hand side , we will have thi form .
now thi form is creat non activ .
and it still involv the linear combin of featur .
and it's also clear that is , if thi valu is , is .
of the linear combin in the equat abov .
if thi thi , thi valu here , if thi valu is larg then it will mean thi valu is small .
and therefor , thi probabl , thi whole probabl , would be larg .
and that's what we expect .
basic , it would be if thi combin give us a high valu , then the document's more like relev .
so thi is our hypothesi .
again , thi is not necessarili the best hypothesi .
that thi is a simpl wai to connect these featur with the probabl of relev .
so now we have thi thi combin function .
the next task is to see how we need to estim the paramet so that the function can truli be appli .
right .
without them know that thei have valu , it's , it's harder to appli thi function , okai .
so let's how we can estim , beta valu .
all right .
let's take a look , at a simpl exampl .
in thi exampl , we have three featur .
on is bm25 score of the document under the queri .
on is the page rank score of the document , which might or might not depend on the queri .
hm , we might have a top sensit page rank .
that would depend on the queri .
otherwis , the gener page rank doesn't realli depend on the queri .
and then we have bm25 score on the anchor task of the document .
these ar then the featur valu for a particular doc , document queri pair .
and in thi case the document is d1 .
and the , the judgment sai that it's relev .
here's anoth train instanc , and these featur valu .
but in thi case it's non relev , okai ?
thi is a overli simplifi case , where we just have two instanc .
but it , it's suffici to illustr the point .
so what we can do is we us the maximum likelihood estim to actual estim the paramet .
basic , we're go to do , predict the relev statu of the document , the , base on the featur valu .
that is given that we observ these featur valu here .
can we predict the relev ?
yeah .
and of cours , the predict will be us thi function that you see here .
and we hypothes thi that the probabl of relev is relat featur in thi wai .
so we're go to see for what valu of beta we can predict that the relev well .
what do we mean ?
well , what , what do we mean by predict the relev well ?
well we just mean .
in the first case for d1 , thi express here , right here , should give higher valu .
in fact , thei would hope thi to give a valu close to on .
why ?
becaus thi is a relev document .
on the other hand , in the second case for d2 we hope thi valu would be small .
right .
why ?
it's becaus it's a non relev document .
so now let's see how thi can be mathemat express .
and thi is similar to , express the probabl of a document .
onli that we ar not talk about the probabl of word but talk about the probabl of relev , <num> or <num> .
so what's the probabl of thi document ?
the relev if it ha these featur valu .
well thi is .
just thi express , right ?
we just need to pluck in the x , the xi .
so that's what we'll get .
it's exactli like , what we have seen that , onli that we replac these xi .
with now specif valu .
and so , for exampl , thi <num> goe to here and thi <num> . <num> goe to here .
and these ar differ featur valu and we'll combin them in thi particular wai .
the beta valu ar still unknown .
but thi give us the probabl that thi document is relev if we assum such a model .
okai , and we want to maxim thi probabl sinc thi is a random document .
what we do for the second document .
well , we want to comput to the probabl that the predict is , is n , non relev .
so , thi would mean , we have to comput a <num> minu , right thi express .
sinc thi express .
is actual the probabl of relev , so to comput the non relev from relev , we just do <num> minu the probabl of relev , okai ?
so thi whole express then .
just is our probabl of predict these two relev valu .
on is <num> .
here , on is a <num> .
and thi whole equat is our probabl .
of observ a <num> here and observ a <num> here .
of cours thi probabl depend on the beta valu , right ?
so then our goal is to adjust the beta valu to make thi whole thing reach it maximum .
make that as larg as possibl .
so that mean we ar go to comput thi .
the beta is just the , the paramet valu that would maxim thi for like holder express .
and what it mean is if look at the function is we're go to choos beta to make thi as larg as possibl .
and make thi also as larg as possibl which is equival to sai make thi the part as small as possibl .
and thi is precis what we want .
so onc we do the train , now we will know the beta valu .
so then thi function will be well defin onc their valu ar known .
both thi and thi will becom pretti less specifi .
so for ani new queri and new document we can simpli comput the featur for that pair and then we just us thi formula to gener a rank score .
and thi score function can be us in for rank document for a particular queri .
so that's the basic idea of , learn to rank .
there ar mani more advanc learn algorithm than the regress base reproach .
and thei gener account to theoret optim or retriev method .
like map or ndcg .
note that the optim object function that we have seen on the previou slide is not directli relat to retriev measur .
right ?
by maxim the predict of on or zero .
or we don't necessarili optim the rank of those document .
on can imagin that why , our predict mai not be too bad and let's sai both ar around <num> .
so it's kind of in the middl of zero and on for the two document , but the rank can be wrong .
so we might have the , a larger valu for .
d2 and then e1 .
so that won't be good from retriev perspect , even though by likelihood function , it's not bad .
in contrast , we might have anoth case where we predict valu .
or around <num> let's sai , and by the object function , the error will be larger , but if we can get the order of the two document correct , that's actual a better result .
so these new more advanc approach will try to correct that problem .
of cours then the challeng is that .
that the optim problem will be harder to solv .
and then research have propos mani solut to the problem .
and you can read more of the refer at the end .
know more about the these approach .
now these learn to random approach .
ar actual gener , so thei can also be appli to mani other rank problem , not just retriev problem .
so here i list some for exampl recommend system , comput adv , advertis , or summar , and there ar mani other that you can probabl encount in your applic .
to summar thi lectur , we have talk about , us machin learn to combin much more featur to incorpor a rank without .
actual the us of machin learn , in inform retriev ha start sinc mani decad ago .
so for exampl on the rocchio feedback approach that we talk about earlier wa a machin learn approach appli to to learn thi feedback , but the most reason us of machin learn ha been driven by some chang .
in the environ of applic of retriev system .
and first it's , mostli , driven by the avail of a lot of train data in the form of click rule .
such data weren't avail befor .
so the data can provid a lot of us knowledg about relev and machin learn method can be appli to leverag thi .
secondli it's also due by the need of combin them .
in the featur .
and thi is not onli just becaus there ar more featur avail on the web that can be natur re us with improv score .
it's also becaus by combin them , we can improv the robust of rank .
so thi is design for combat spam .
modern search engin all us some kind of machin learn techniqu to combin mani featur to optim rank and thi is a major featur of these current engin such as googl , bing .
the topic of learn to rank is still activ research .
topic in the commun , and so you can expect to see new result be develop , in the next , few year .
perhap .
here ar some addit read that can give you more inform about .
about , how learn to rank book and also some advanc method .
thi lectur is about the futur of web search .
in thi lectur , we're go to talk about some possibl futur trend of web search and intellig inform retriev system in gener .
in order to further improv the accuraci of a search engin , it's import that to consid special case of inform need .
so on particular trend could be to have more and more special than custom search engin , and thei can be call vertic search engin .
these vertic search engin can be expect to be more effect than the current gener search engin becaus thei could assum that user ar a special group of user that might have a common inform need , and then the search engin can be custom with thi ser , so , such user .
and becaus of the custom , it's also possibl to do person .
so the search can be person , becaus we have a better understand of the user .
becaus of the restrict with domain , we also have some advantag in handl the document , becaus we can have better understand of document .
for exampl , particular word mai not be ambigu in such a domain .
so we can bypass the problem of ambigu .
anoth trend we can expect to see , is the search engin will be abl to learn over time .
it's like a lifetim learn or lifelong learn , and thi is , of cours , veri attract becaus that mean the search engin will self improv itself .
as more peopl ar us it , the search engin will becom better and better , and thi is alreadi happen , becaus the search engin can learn from the of feedback .
more user us it , and the qualiti of the search engin allow for the popular queri that ar type in by mani user allow it to becom better , so thi is sort of anoth featur that we will see .
the third trend might be to the integr of bottl of inform access .
so search , navig , and recommend or filter might be combin to form a full fledg inform manag system .
and in the begin of thi cours , we talk about push versu pull .
these ar differ mode of inform access , but these mode can be combin .
and similarli , in the pull mode , queri and the brows could also be combin .
and in fact we're do that basic , todai , is the search end .
we ar queri , sometim brows , click on link .
sometim we've got some inform recommend .
although most of the case the inform recommend is becaus of advertis .
but in the futur , you can imagin seamlessli integr the system with multi mode for inform access , and that would be conveni for peopl .
anoth trend is that we might see system that try to go beyond the search to support the user task .
after all , the reason why peopl want to search is to solv a problem or to make a decis or perform a task .
for exampl consum might search for opinion about product in order to purchas a product , choos a good product by , so in thi case it would be benefici to support the whole workflow of purchas a product , or choos a product .
in thi era , after the common search engin alreadi provid a good support .
for exampl , you can sometim look at the review , and then if you want to bui it , you can just click on the button to go the shop site and directli get it done .
but it doe not provid a , a good task support for mani other task .
for exampl , for research , you might want to find the realm in the literatur or site of the literatur .
and then , there's no , not much support for finish a task such as write a paper .
so , in gener , i think , there ar mani opportun in the wait .
so in the follow few slide , i'll be talk a littl bit more about some specif idea or thought that hopefulli , can help you in imagin new applic possibl .
some of them might be alreadi relev to what you ar current work on .
in gener , we can think about ani intellig system , especi intellig inform system , as we specifi by these these three node .
and so if we connect these three into a triangl , then we'll abl to specifi an inform system .
and i call thi data user servic triangl .
so basic the three question you ask would be who ar you serv and what kind of data ar you ar manag and what kind of servic you provid .
right there , thi would help us basic specifi in your system .
and there ar mani differ wai to connect them depend on how you connect them , you will have a differ kind of system .
so let me give you some exampl .
on the top , you can see differ kind of user .
on the left side , you can see differ type of data or inform , and on the bottom , you can see differ servic function .
now imagin you can connect all these in differ wai .
so , for exampl , you can connect everyon with web page , and the support search and brows , what do you get ?
well , that's web search , right ?
what if we connect uiuc employe with organ document or enterpris document to support the search and brows , but that's enterpris search .
if you connect the scientist with literatur inform to provid all kind of servic , includ search , brows , or alert of new random document or mine analyz research trend , or provid the task with support or decis support .
for exampl , we might be , might be abl to provid a support for automat gener relat work section for a research paper , and thi would be closer to task support .
right ?
so then we can imagin thi would be a literatur assist .
if we connect the onlin shopper with blog articl or product review then we can help these peopl to improv shop experi .
so we can provid , for exampl data mine capabl to analyz the review , to compar product , compar sentiment of product and to provid task support or decis support to have them choos what product to bui .
or we can connect custom servic peopl with email from the custom , and , and we can imagin a system that can provid a analysi of these email to find that the major complaint of the custom .
we can imagin a system we could provid task support by automat gener a respons to a custom email .
mayb intellig attach also a promot messag if appropri , if thei detect that that's a posit messag , not a complaint , and then you might take thi opportun to attach some promot inform .
wherea if it's a complaint , then you might be abl to automat gener some gener respons first and tell the custom that he or she can expect a detail respons later , etc .
all of these ar try to help peopl to improv the product .
so thi show that the opportun ar realli a lot .
it's just onli restrict by our imagin .
so thi pictur show the trend of the technolog , and also , it character the , intellig inform system in three angl .
you can see in the center , there's a triangl that connect keyword queri to search a bag of word represent .
that mean the current search engin basic provid search support to user and mostli model user base on keyword queri and see the data through bag of word represent .
so it's a veri simpl approxim of the actual inform in the document .
but that's what the current system doe .
it connect these three node in such a simpl wai , or it onli provid a basic search function and doesn't realli understand the user , and it doesn't realli understand that much inform in the document .
now , i show some trend to push each node toward a more advanc function .
so think about the user node here , right ?
so we can go beyond the keyword queri , look at the user search histori , and then further model the user complet to understand the , the user's task environ , task need context or other inform .
okai , so thi is push for person and complet user model .
and thi is a major direct in research in , in order to build intellig inform system .
on the document side , we can also see , we can go beyond bag of word implement to have entiti relat represent .
thi mean we'll recogn peopl's name , their relat , locat , etc .
and thi is alreadi feasibl with todai's natur process techniqu .
and googl is the reason the initi on the knowledg graph .
if you haven't heard of it , it is a good step toward thi direct .
and onc we can get to that level without initi robust manner at larger scale , it can enabl the search engin to provid a much better servic .
in the futur we would like to have knowledg represent where we can add perhap infer rule , and then the search engin would becom more intellig .
so thi call for larg scale semant analysi , and perhap thi is more feasibl for vertic search engin .
it's easier to make progress in the particular domain .
now on the servic side , we see we need to go beyond the search of support inform access in gener .
so search is onli on wai to get access to inform as well recommend system and push and pull so differ wai to get access to random inform .
but go beyond access , we also need to help peopl digest the inform onc the inform is found , and thi step ha to do with analysi of inform or data mine .
we have to find pattern or convert the text inform into real knowledg that can be us in applic or action knowledg that can be us for decis make .
and furthermor the knowledg will be us to help a user to improv product in finish a task , for exampl , a decis make task .
right , so thi is a trend .
and , and , and so basic , in thi dimens , we anticip in the futur intellig inform system will provid intellig and interact task support .
now i should also emphas interact here , becaus it's import to optim the combin intellig of the user and the system .
so we , we can get some help from user in some natur wai .
and we don't have to assum the system ha to do everyth when the human , user , and the machin can collabor in an intellig wai , an effici wai , then the combin intellig will be high and in gener , we can minim the user's overal effort in solv problem .
so thi is the big pictur of futur intellig inform system , and thi hopefulli can provid us with some insight about how to make further innov on top of what we handl todai .
in thi lectur we give an overview of text mine and analyt .
first , let's defin the term text mine , and the term text analyt .
the titl of thi cours is call text mine and analyt .
but the two term text mine , and text analyt ar actual roughli the same .
so we ar not realli go to realli distinguish them , and we're go to us them interchang .
but the reason that we have chosen to us both term in the titl is becaus there is also some subtl differ , if you look at the two phrase liter .
mine emphas more on the process .
so it give us a error rate medic view of the problem .
analyt , on the other hand emphas more on the result , or have a problem in mind .
we ar go to look at text data to help us solv a problem .
but again as i said , we can treat these two term roughli the same .
and i think in the literatur you probabl will find the same .
so we're not go to realli distinguish that in the cours .
both text mine and text analyt mean that we want to turn text data into high qualiti inform , or action knowledg .
so in both case , we have the problem of deal with a lot of text data and we hope to .
turn these text data into someth more us to us than the raw text data .
and here we distinguish two differ result .
on is high qualiti inform , the other is action knowledg .
sometim the boundari between the two is not so clear .
but i also want to sai a littl bit about these two differ angl of the result of text field mine .
in the case of high qualiti inform , we refer to more concis inform about the topic .
which might be much easier for human to digest than the raw text data .
for exampl , you might face a lot of review of a product .
a more concis form of inform would be a veri concis summari of the major opinion about the featur of the product .
posit about , let's sai batteri life of a laptop .
now thi kind of result ar veri us to help peopl digest the text data .
and so thi is to minim a human effort in consum text data in some sens .
the other kind of output is actual more knowledg .
here we emphas the util of the inform or knowledg we discov from text data .
it's action knowledg for some decis problem , or some action to take .
for exampl , we might be abl to determin which product is more appeal to us , or a better choic for a shock decis .
now , such an outcom could be call action knowledg , becaus a consum can take the knowledg and make a decis , and act on it .
so , in thi case text mine suppli knowledg for optim decis make .
but again , the two ar not so clearli distinguish , so we don't necessarili have to make a distinct .
text mine is also relat to text retriev , which is a essenti compon in mani text mine system .
now , text retriev refer to find relev inform from a larg amount of text data .
so i've taught anoth separ book on text retriev and search engin .
where we discuss variou techniqu for text retriev .
if you have taken that book , and you will find some overlap .
and it will be us to know the background of text retriev of understand some of the topic in text mine .
but , if you have not taken that book , it's also fine becaus in thi book on text mine and analyt , we're go to repeat some of the kei concept that ar relev for text mine .
but thei're at the high level and thei also explain the relat between text retriev and text mine .
text retriev is veri us for text mine in two wai .
first , text retriev can be a preprocessor for text mine .
mean that it can help us turn big text data into a rel small amount of most relev text data .
which is often what's need for solv a particular problem .
and in thi sens , text retriev also help minim human effort .
text retriev is also need for knowledg proven .
and thi roughli correspond to the interpret of text mine as turn text data into action knowledg .
onc we find the pattern in text data , or action knowledg , we gener would have to verifi the knowledg .
by look at the origin text data .
so the user would have to have some text retriev support , go back to the origin text data to interpret the pattern or to better understand an analog or to verifi whether a pattern is realli reliabl .
so thi is a high level introduct to the concept of text mine , and the relationship between text mine and retriev .
next , let's talk about text data as a special kind of data .
now it's interest to view text data as data gener by human as subject sensor .
so , thi slide show an analog between text data and non text data .
and between human as subject sensor and physic sensor , such as a network sensor or a thermomet .
so in gener a sensor would monitor the real world in some wai .
it would sens some signal from the real world , and then would report the signal as data , in variou form .
for exampl , a thermomet would watch the temperatur of real world and then we report the temperatur be a particular format .
similarli , a geo sensor would sens the locat and then report .
the locat specif , for exampl , in the form of longitud valu and latitud valu .
a network send over the monitor network traffic , or activ in the network and ar report .
some digit format of data .
similarli we can think of human as subject sensor .
that will observ the real world and from some perspect .
and then human will express what thei have observ in the form of text data .
so , in thi sens , human is actual a subject sensor that would also sens what's happen in the world and then express what's observ in the form of data , in thi case , text data .
now , look at the text data in thi wai ha an advantag of be abl to integr all type of data togeth .
and that's inde need in most data mine problem .
so here we ar look at the gener problem of data mine .
and in gener we would be deal with a lot of data about our world that ar relat to a problem .
and in gener it will be deal with both non text data and text data .
and of cours the non text data ar usual produc by physic sens .
and those non text data can be also of differ format .
numer data , categor , or relat data , or multi media data like video or speech .
so , these non text data ar often veri import in some problem .
but text data is also veri import , mostli becaus thei contain a lot of symmetr content .
and thei often contain knowledg about the user , especi prefer and opinion of user .
so , but by treat text data as the data observ from human sensor , we can treat all thi data togeth in the same framework .
so the data mine problem is basic to turn such data , turn all the data in your action knowledg to that we can take advantag of it to chang the real world of cours for better .
so thi mean the data mine problem is basic take a lot of data as input and give action knowledg as output .
insid of the data mine modul , you can also see we have a number of differ kind of mine algorithm .
and thi is becaus , for differ kind of data , we gener need differ algorithm for mine the data .
for exampl , video data might requir comput vision to understand video content .
and that would facilit the more effect mine .
and we also have a lot of gener algorithm that ar applic to all kind of data and those algorithm , of cours , ar veri us .
although , for a particular kind of data , we gener want to also develop a special algorithm .
so thi cours will cover special algorithm that ar particularli us for mine text data .
thi lectur is about the syntagmat relat discoveri , and entropi .
in thi lectur , we're go to continu talk about word associ mine .
in particular , we're go to talk about how to discov syntagmat relat .
and we're go to start with the introduct of entropi , which is the basi for design some measur for discov such relat .
by definit , syntagmat relat hold between word that have correl co occurr .
that mean , when we see on word occur in context , we tend to see the occurr of the other word .
so , take a more specif exampl , here .
we can ask the question , whenev eat occur , what other word also tend to occur ?
look at the sentenc on the left , we see some word that might occur togeth with eat , like cat , dog , or fish is right .
but if i take them out and if you look at the right side where we onli show eat and some other word , the question then is .
can you predict what other word occur to the left or to the right ?
right so thi would forc us to think about what other word ar associ with eat .
if thei ar associ with eat , thei tend to occur in the context of eat .
more specif our predict problem is to take ani text segment which can be a sentenc , a paragraph , or a document .
and then ask i the question , is a particular word present or absent in thi segment ?
right here we ask about the word w .
is w present or absent in thi segment ?
now what's interest is that some word ar actual easier to predict than other word .
if you take a look at the three word shown here , meat , the , and unicorn , which on do you think is easier to predict ?
now if you think about it for a moment you might conclud that the is easier to predict becaus it tend to occur everywher .
so i can just sai , well that would be in the sentenc .
unicorn is also rel easi becaus unicorn is rare , is veri rare .
and i can bet that it doesn't occur in thi sentenc .
but meat is somewher in between in term of frequenc .
and it make it harder to predict becaus it's possibl that it occur in a sentenc or the segment , more accur .
but it mai also not occur in the sentenc , so now let's studi thi problem more formal .
so the problem can be formal defin as predict the valu of a binari random variabl .
here we denot it by x sub w , w denot a word , so thi random variabl is associ with precis on word .
when the valu of the variabl is <num> , it mean thi word is present .
when it's <num> , it mean the word is absent .
and natur , the probabl for <num> and <num> should sum to <num> , becaus a word is either present or absent in a segment .
there's no other choic .
so the intuit with thi concept earlier can be formal state as follow .
the more random thi random variabl is , the more difficult the predict will be .
now the question is how doe on quantit measur the random of a random variabl like x sub w ?
how in gener , can we quantifi the random of a variabl and that's why we need a measur call entropi and thi measur introduc in inform theori to measur the random of x .
there is also some connect with inform here but that is beyond the scope of thi cours .
so for our purpos we just treat entropi function as a function defin on a random variabl .
in thi case , it is a binari random variabl , although the definit can be easili gener for a random variabl with multipl valu .
now the function form look like thi , there's the sum of all the possibl valu for thi random variabl .
insid the sum for each valu we have a product of the probabl that the random variabl equal thi valu and log of thi probabl .
and note that there is also a neg sign there .
now entropi in gener is non neg .
and that can be mathemat prove .
so if we expand thi sum , we'll see that the equat look like the second on .
where i explicitli plug in the two valu , <num> and <num> .
and sometim when we have <num> log of <num> , we would gener defin that as <num> , becaus log of <num> is undefin .
so thi is the entropi function .
and thi function will give a differ valu for differ distribut of thi random variabl .
and it clearli depend on the probabl that the random variabl take valu of <num> or <num> .
if we plot thi function against the probabl that the random variabl is equal to <num> .
and then the function look like thi .
at the two end , that mean when the probabl of x equal <num> is veri small or veri larg , then the entropi function ha a low valu .
when it's <num> in the middl then it reach the maximum .
now if we plot the function against the probabl that x is take a valu of <num> and the function would show exactli the same curv here , and you can imagin why .
and so that's becaus the two probabl ar symmetr , and complet symmetr .
so an interest question you can think about in gener is for what kind of x doe entropi reach maximum or minimum .
and we can in particular think about some special case .
for exampl , in on case , we might have a random variabl that alwai take a valu of <num> .
the probabl is <num> .
or there's a random variabl that is equal like take a valu of on or zero .
so in thi case the probabl that x equal <num> is <num> .
now which on ha a higher entropi ?
it's easier to look at the problem by think of a simpl exampl us coin toss .
so when we think about random experi like toss a coin , it give us a random variabl , that can repres the result .
it can be head or tail .
so we can defin a random variabl x sub coin , so that it's <num> when the coin show up as head , it's <num> when the coin show up as tail .
so now we can comput the entropi of thi random variabl .
and thi entropi indic how difficult it is to predict the outcom of a coin toss .
so we can think about the two case .
on is a fair coin , it's complet fair .
the coin show up as head or tail equal like .
so the two probabl would be a half .
right ?
so both ar equal to on half .
anoth extrem case is complet bias coin , where the coin alwai show up as head .
so it's a complet bias coin .
now let's think about the entropi in the two case .
and if you plug in these valu you can see the entropi would be as follow .
for a fair coin we see the entropi reach it maximum , that's <num> .
for the complet bias coin , we see it's <num> .
and that intuit make a lot of sens .
becaus a fair coin is most difficult to predict .
wherea a complet bias coin is veri easi to predict .
we can alwai sai , well , it's a head .
becaus it is a head all the time .
so thei can be shown on the curv as follow .
so the fair coin correspond to the middl point where it's veri uncertain .
the complet bias coin correspond to the end point where we have a probabl of <num> and the entropi is <num> .
so , now let's see how we can us entropi for word predict .
let's think about our problem is to predict whether w is present or absent in thi segment .
again , think about the three word , particularli think about their entropi .
now we can assum high entropi word ar harder to predict .
and so we now have a quantit wai to tell us which word is harder to predict .
now if you look at the three word meat , the , unicorn , again , and we clearli would expect meat to have a higher entropi than the unicorn .
in fact if you look at the entropi of the , it's close to zero .
becaus it occur everywher .
so it's like a complet bias coin .
therefor the entropi is zero .
thi lectur is about the syntagmat relat discoveri and condit entropi .
in thi lectur , we're go to continu the discuss of word associ mine and analysi .
we're go to talk about the condit entropi , which is us for discov syntagmat relat .
earlier , we talk about us entropi to captur how easi it is to predict the presenc or absenc of a word .
now , we'll address a differ scenario where we assum that we know someth about the text segment .
so now the question is , suppos we know that eat occur in the segment .
how would that help us predict the presenc or absenc of water , like in meat ?
and in particular , we want to know whether the presenc of eat ha help us predict the presenc of meat .
and if we frame thi us entrophi , that would mean we ar interest in know whether know the presenc of eat could reduc uncertainti about the meat .
or , reduc the entrophi of the random variabl correspond to the presenc or absenc of meat .
we can also ask as a question , what if we know of the absent of eat ?
would that also help us predict the presenc or absenc of meat ?
these question can be address by us anoth concept call a condit entropi .
so to explain thi concept , let's first look at the scenario we had befor , when we know noth about the segment .
so we have these probabl indic whether a word like meat occur , or it doesn't occur in the segment .
and we have an entropi function that look like what you see on the slide .
now suppos we know eat is present , so now we know the valu of anoth random variabl that denot eat .
now , that would chang all these probabl to condit probabl .
where we look at the presenc or absenc of meat , given that we know eat occur in the context .
so as a result , if we replac these probabl with their correspond condit probabl in the entropi function , we'll get the condit entropi .
so thi equat now here would be the condit entropi .
condit on the presenc of eat .
so , you can see thi is essenti the same entropi function as you have seen befor , except that all the probabl now have a condit .
and thi then tell us the entropi of meat , after we have known eat occur in the segment .
and of cours , we can also defin thi condit entropi for the scenario where we don't see eat .
so if we know it did not occur in the segment , then thi entri condit of entropi would captur the instanc of meat in that condit .
so now , put differ scenario togeth , we have the complet definit of condit entropi as follow .
basic , we're go to consid both scenario of the valu of eat zero , on , and thi give us a probabl that eat is equal to zero or on .
basic , whether eat is present or absent .
and thi of cours , is the condit entropi of meat in that particular scenario .
so if you expand thi entropi , then you have the follow equat .
where you see the involv of those condit probabl .
now in gener , for ani discret random variabl x and y , we have the condit entropi is no larger than the entropi of the variabl x .
so basic , thi is upper bound for the condit entropi .
that mean by know more inform about the segment , we want to be abl to increas uncertainti .
we can onli reduc uncertainti .
and that intuit make sens becaus as we know more inform , it should alwai help us make the predict .
and cannot hurt the predict in ani case .
now , what's interest here is also to think about what's the minimum possibl valu of thi condit entropi ?
now , we know that the maximum valu is the entropi of x .
but what about the minimum , so what do you think ?
i hope you can reach the conclus that the minimum possibl valu , would be zero .
and it will be interest to think about under what situat will achiev thi .
so , let's see how we can us condit entropi to captur syntagmat relat .
now of cours , thi condit entropi give us directli on wai to measur the associ of two word .
becaus it tell us to what extent , we can predict the on word given that we know the presenc or absenc of anoth word .
now befor we look at the intuit of condit entropi in captur syntagmat relat , it's us to think of a veri special case , list here .
that is , the condit entropi of the word given itself .
so here , we list thi condit entropi in the middl .
so , it's here .
so , what is the valu of thi ?
now , thi mean we know where the meat occur in the sentenc .
and we hope to predict whether the meat occur in the sentenc .
and of cours , thi is <num> becaus there's no incid anymor .
onc we know whether the word occur in the segment , we'll alreadi know the answer of the predict .
so thi is zero .
and that's also when thi condit entropi reach the minimum .
so now , let's look at some other case .
so thi is a case of know the and try to predict the meat .
and thi is a case of know eat and try to predict the meat .
which on do you think is smaller ?
no doubt smaller entropi mean easier for predict .
which on do you think is higher ?
which on is not smaller ?
well , if you at the uncertainti , then in the first case , the doesn't realli tell us much about the meat .
so know the occurr of the doesn't realli help us reduc entropi that much .
so it stai fairli close to the origin entropi of meat .
wherea in the case of eat , eat is relat to meat .
so know presenc of eat or absenc of eat , would help us predict whether meat occur .
so it can help us reduc entropi of meat .
so we should expect the sigma term , name thi on , to have a smaller entropi .
and that mean there is a stronger associ between meat and eat .
so we now also know when thi w is the same as thi meat , then the condit entropi would reach it minimum , which is <num> .
and for what kind of word would either reach it maximum ?
well , that's when thi stuff is not realli relat to meat .
and like the for exampl , it would be veri close to the maximum , which is the entropi of meat itself .
so thi suggest that when you us condit entropi for mine syntagmat relat , the hour would look as follow .
for each word w1 , we're go to enumer the overal other word w2 .
and then , we can comput the condit entropi of w1 given w2 .
we thought all the candid wa in ascend order of the condit entropi becaus we're out of favor , a world that ha a small entropi .
mean that it help us predict the time of the word w1 .
and then , we're go to take the top ring of the candid word as word that have potenti syntagmat relat with w1 .
note that we need to us a threshold to find these word .
the stresser can be the number of top candid take , or absolut valu for the condit entropi .
now , thi would allow us to mine the most strongli correl word with a particular word , w1 here .
but , thi algorithm doe not help us mine the strongest that k syntagmat relat from an entir collect .
becaus in order to do that , we have to ensur that these condit entropi ar compar across differ word .
in thi case of discov the mathemat relat for a target word like w1 , we onli need to compar the condit entropi for w1 , given differ word .
and in thi case , thei ar compar .
all right .
so , the condit entropi of w1 , given w2 , and the condit entropi of w1 , given w3 ar compar .
thei all measur how hard it is to predict the w1 .
but , if we think about the two pair , where we share w2 in the same condit , and we try to predict the w1 and w3 .
then , the condit entropi ar actual not compar .
you can think of about thi question .
why ?
so why ar thei not comfort ?
well , that wa becaus thei have a differ outer bound .
right ?
so those outer bound ar precis the entropi of w1 and the entropi of w3 .
and thei have differ upper bound .
so we cannot realli compar them in thi wai .
so how do we address thi problem ?
well later , we'll discuss , we can us mutual inform to solv thi problem .
thi lectur is about the syntagmat relat discoveri and mutual inform .
in thi lectur we ar go to continu discuss syntagmat relat discoveri .
in particular , we ar go to talk about anoth the concept in the inform seri , we call it mutual inform and how it can be us to discov syntagmat relat .
befor we talk about the problem of condit entropi and that is the condit entropi comput differ pair of word .
it is not realli compar , so that make it harder with thi cover , strong synagmat relat global from corpu .
so now we ar go to introduc mutual inform , which is anoth concept in the inform seri that allow us to , sometim , normal the condit entropi to make it more compar across differ pair .
in particular , mutual inform in order to find i x y , match the entropi reduct of x obtain from know y .
more specif the question we ar interest in here is how much of an entropi of x can we obtain by know y .
so mathemat it can be defin as the differ between the origin entropi of x , and the condit of y of x given y .
and you might see , as you can see here it can also be defin as reduct of entropi of y becaus of know x .
now normal the two condit interfac h of x given y and the entropi of y given x ar not equal , but interestingli , the reduct of entropi by know on of them , is actual equal .
so , thi quantiti is call a mutual inform in order to bui i here .
and thi function ha some interest properti , first it is also non neg .
thi is easi to understand becaus the origin entropi is alwai not go to be lower than the possibl reduc condit entropi .
in other word , the condit entropi will never exce the origin entropi .
know some inform can alwai help us potenti , but will not hurt us in predict x .
the signal properti is that it is symmetr like addit entropi is not symmetr , mutual inform is , and the third properti is that it reach it minimum , zero , if and onli if the two random variabl ar complet independ .
that mean know on of them doe not tell us anyth about the other and thi last properti can be verifi by simpli look at the equat abov and it reach <num> if and onli the condit entropi of x y is exactli the same as origin entropi of x .
so that mean know why it did not help at all and that is when x and a y ar complet independ .
now when we fix x to rank differ ys us condit entropi would give the same order as rank base on mutual inform becaus in the function here , h x is fix becaus x is fix .
so rank base on mutual entropi is exactli the same as rank base on the condit entropi of x given y , but the mutual inform allow us to compar differ pair of x and y .
so , that is why mutual inform is more gener and in gener , more us .
so , let us examin the intuit of us mutual inform for syntagmat relat mine .
now , the question we ask forc that relat mine is , whenev eat occur , what other word also tend to occur ?
so thi question can be frame as a mutual inform question , that is , which word have high mutual inform wa eat , so comput the miss inform between eat and other word .
and if we do that , and it is basic a base on the same as condit we will see that word that ar strongli associ with eat , will have a high point .
wherea word that ar not relat will have lower mutual inform .
for thi , i will give some exampl here .
the mutual inform between eat and meat , which is the same as between meat and eat , becaus the inform is symmetr is expect to be higher than the mutual inform between eat and the , becaus know the doe not realli help us as a predictor .
it is similar , and know eat doe not help us predict , the as well .
and you also can easili see that the mutual inform between a word and itself is the largest , which is equal to the entropi of thi word and so , becaus in thi case the reduct is maximum becaus know on allow us to predict the other complet .
so the condit entropi is zero , therefor the mutual inform reach it maximum .
it is go to be larger , then ar equal to the machin volum eat in other word .
in other word pick ani other word and the comput pick between eat and that word .
you will not get ani inform larger the comput from eat and itself .
so now let us look at how to comput the mute inform .
now in order to do that , we often us a differ form of mutual inform , and we can mathemat rewrit the mutual inform into the form shown on thi slide .
where we essenti see a formula that comput what is call a kl diverg or diverg .
thi is anoth term in inform theori .
it measur the diverg between two distribut .
now , if you look at the formula , it is also sum over mani combin of differ valu of the two random variabl but insid the sum , mainli we ar do a comparison between two joint distribut .
the numer ha the joint , actual observ the joint distribut of the two random variabl .
the bottom part or the denomin can be interpret as the expect joint distribut of the two random variabl , if thei were independ becaus when two random variabl ar independ , thei ar join distribut is equal to the product of the two probabl .
so thi comparison will tell us whether the two variabl ar inde independ .
if thei ar inde independ then we would expect that the two ar the same , but if the numer is differ from the denomin , that would mean the two variabl ar not independ and that help measur the associ .
the sum is simpli to take into consider of all of the combin of the valu of these two random variabl .
in our case , each random variabl can choos on of the two valu , zero or on , so we have four combin here .
if we look at thi form of mutual inform , it show that the mutual inform match the diverg of the actual joint distribut from the expect distribut under the independ assumpt .
the larger thi diverg is , the higher the mutual inform would be .
so now let us further look at what ar exactli the probabl , involv in thi formula of mutual inform .
and here , thi is all the probabl involv , and it is easi for you to verifi that .
basic , we have first to probabl correspond to the presenc or absenc of each word .
so , for w1 , we have two probabl shown here .
thei should sum to on , becaus a word can either be present or absent .
in the segment , and similarli for the second word , we also have two probabl repres presenc or absenc of thi word , and there is some to y as well .
and final , we have a lot of join probabl that repres the scenario of co occurr of the two word , and thei ar shown here .
and thei sum to on becaus the two word can onli have these four possibl scenario .
either thei both occur , so in that case both variabl will have a valu of on , or on of them occur .
there ar two scenario .
in these two case on of the random variabl will be equal to on and the other will be zero and final we have the scenario when none of them occur .
thi is when the two variabl take a valu of zero .
so these ar the probabl involv in the calcul of mutual inform , over here .
onc we know how to calcul these probabl , we can easili calcul the new gene format .
it is also interest to know that there ar actual some relat or constraint among these probabl , and we alreadi saw two of them , right ?
so in the previou slide , that you have seen that the margin probabl of these word sum to on and we also have seen thi constraint , that sai the two word have these four scenario of co occurr , but we also have some addit constraint list in the bottom .
for exampl , thi on mean if we add up the probabl that we observ the two word occur togeth and the probabl when the first word occur and the second word doe not occur .
we get exactli the probabl that the first word is observ .
in other word , when the word is observ .
when the first word is observ , and there ar onli two scenario , depend on whether the second word is also observ .
so , thi probabl captur the first scenario when the second word actual is also observ , and thi captur the second scenario when the second word is not observ .
so , we onli see the first word , and it is easi to see the other equat also follow the same reason .
now these equat allow us to comput some probabl base on other probabl , and thi can simplifi the comput .
so more specif , if we know the probabl that a word is present , like in thi case , so if we know thi , and if we know the probabl of the presenc of the second word , then we can easili comput the absenc probabl , right ?
it is veri easi to us thi equat to do that , and so we take care of the comput of these probabl of presenc and absenc of each word .
now let's look at the distribut .
let us assum that we also have avail the probabl that thei occur togeth .
now it is easi to see that we can actual comput all the rest of these probabl base on these .
specif for exampl us thi equat we can comput the probabl that the first word occur and the second word did not , becaus we know these probabl in the box , and similarli us thi equat we can comput the probabl that we observ onli the second word .
word .
and then final , thi probabl can be calcul by us thi equat becaus now thi is known , and thi is also known , and thi is alreadi known , right .
so thi can be easier to calcul .
so now thi can be calcul .
so thi slide show that we onli need to know how to comput these three probabl that ar shown in the box , name the presenc of each word and the co occur of both word , in a segment .
in gener , we can us the empir count of event in the observ data to estim the probabl .
and a commonli us techniqu is call a maximum likelihood estim , where we simpli normal the observ account .
so if we do that , we can see , we can comput these probabl as follow .
for estim the probabl that we see a water current in a segment , we simpli normal the count of segment that contain thi word .
so let's first take a look at the data here .
on the right side , you see a list of some , hypothes the data .
these ar segment .
and in some segment you see both word occur , thei ar indic as on for both column .
in some other case onli on will occur , so onli that column ha on and the other column ha zero .
and in all , of cours , in some other case none of the word occur , so thei ar both zero .
and for estim these probabl , we simpli need to collect the three count .
so the three count ar first , the count of w1 .
and that's the total number of segment that contain word w1 .
it's just as the on in the column of w1 .
we can count how mani on we have seen there .
the segment count is for word <num> , and we just count the on in the second column .
and these will give us the total number of segment that contain w2 .
the third count is when both word occur .
so thi time , we're go to count the sentenc where both column have on .
and then , so thi would give us the total number of segment where we have seen both w1 and w2 .
onc we have these count , we can just normal these count by n , which is the total number of segment , and thi will give us the probabl that we need to comput origin inform .
now , there is a small problem , when we have zero count sometim .
and in thi case , we don't want a zero probabl becaus our data mai be a small sampl and in gener , we would believ that it's potenti possibl for a to avoid ani context .
so , to address thi problem , we can us a techniqu call smooth .
and that's basic to add some small constant to these count , and so that we don't get the zero probabl in ani case .
now , the best wai to understand smooth is imagin that we actual observ more data than we actual have , becaus we'll pretend we observ some pseudo segment .
i illustr on the top , on the right side on the slide .
and these pseudo segment would contribut addit count of these word so that no event will have zero probabl .
now , in particular we introduc the four pseudo segment .
each is weight at on quarter .
and these repres the four differ combin of occurr of thi word .
so now each event , each combin will have at least on count or at least a non zero count from thi pseudo segment .
so , in the actual segment that we'll observ , it's okai if we haven't observ all of the combin .
so more specif , you can see the <num> here after it come from the two on in the two pseudo segment , becaus each is weight at on quarter .
we add them up , we get <num> .
and similar to thi , <num> . <num> come from on singl pseudo segment that indic the two word occur togeth .
and of cours in the denomin we add the total number of pseudo segment that we add , in thi case , we ad a four pseudo segment .
each is weigh at on quarter so the total of the sum is , after the on .
so , that's why in the denomin you'll see a on there .
so , thi basic conclud the discuss of how to comput a these four syntagmat relat discoveri .
now , so to summar , syntagmat relat can gener be discov by measur correl between occurr of two word .
we've introduc the three concept from inform theori .
entropi , which measur the uncertainti of a random variabl x .
condit entropi , which measur the entropi of x given we know y .
and mutual inform of x and y , which match the entropi reduct of x due to know y , or entropi reduct of y due to know x .
thei ar the same .
so these three concept ar actual veri us for other applic as well .
that's why we spent some time to explain thi in detail .
but in particular , thei ar also veri us for discov syntagmat relat .
in particular , mutual inform is a princip wai for discov such a relat .
it allow us to have valu comput on differ pair of word that ar compar and so we can rank these pair and discov the strongest syntagmat from a collect of document .
now , note that there is some relat between syntagmat relat discoveri and relat discoveri .
so we alreadi discuss the possibl of us bm25 to achiev wait for term in the context to potenti also suggest the candid that have syntagmat relat with the candid word .
but here , onc we us mutual inform to discov syntagmat relat , we can also repres the context with thi mutual inform as weight .
so thi would give us anoth wai to repres the context of a word , like a cat .
and if we do the same for all the word , then we can cluster these word or compar the similar between these word base on their context similar .
so thi provid yet anoth wai to do term weight for paradigmat relat discoveri .
and so to summar thi whole part about word associ mine .
we introduc two basic associ , call a paradigmat and a syntagmat relat .
these ar fairli gener , thei appli to ani item in ani languag , so the unit don't have to be word , thei can be phrase or entiti .
we introduc multipl statist approach for discov them , mainli show that pure statist approach ar visibl , ar variabl for discov both kind of relat .
and thei can be combin to perform joint analysi , as well .
these approach can be appli to ani text with no human effort , mostli becaus thei ar base on count of word , yet thei can actual discov interest relat of word .
we can also us differ wai with defin context and segment , and thi would lead us to some interest variat of applic .
for exampl , the context can be veri narrow like a few word , around a word , or a sentenc , or mayb paragraph , as us differ context would allow to discov differ flavor of paradigmat relat .
and similarli , count co occurr us let's sai , visual inform to discov syntagmat relat .
we also have to defin the segment , and the segment can be defin as a narrow text window or a longer text articl .
and thi would give us differ kind of associ .
these discoveri associ can support mani other applic , in both inform retriev and text and data mine .
so here ar some recommend read , if you want to know more about the topic .
the first is a book with a chapter on colloc , which is quit relev to the topic of these lectur .
the second is an articl about us variou statist measur to discov lexic atom .
those ar phrase that ar non composit .
for exampl , hot dog is not realli a dog that's hot , blue chip is not a chip that's blue .
and the paper ha a discuss about some techniqu for discov such phrase .
the third on is a new paper on a unifi wai to discov both paradigmat relat and a syntagmat relat , us random work on word graph .
so , look at the text mine problem more close , we see that the problem is similar to gener data mine , except that we'll be focus more on text data .
and we're go to have text mine algorithm to help us to turn text data into action knowledg that we can us in real world , especi for decis make , or for complet whatev task that requir text data to support .
becaus , in gener , in mani real world problem of data mine we also tend to have other kind of data that ar non textual .
so a more gener pictur would be to includ non text data as well .
and for thi reason we might be concern with joint mine of text and non text data .
and so in thi cours we're go to focu more on text mine , but we're also go to also touch how do to joint analysi of both text data and non text data .
with thi problem definit we can now look at the landscap of the topic in text mine and analyt .
now thi slide show the process of gener text data in more detail .
more specif , a human sensor or human observ would look at the word from some perspect .
differ peopl would be look at the world from differ angl and thei'll pai attent to differ thing .
the same person at differ time might also pai attent to differ aspect of the observ world .
and so the human ar abl to perceiv the world from some perspect .
and that human , the sensor , would then form a view of the world .
and that can be call the observ world .
of cours , thi would be differ from the real world becaus of the perspect that the person ha taken can often be bias also .
now the observ world can be repres as , for exampl , entiti relat graph or in a more gener wai , us knowledg represent languag .
but in gener , thi is basic what a person ha in mind about the world .
and we don't realli know what exactli it look like , of cours .
but then the human would express what the person ha observ us a natur languag , such as english .
and the result is text data .
of cours a person could have us a differ languag to express what he or she ha observ .
in that case we might have text data of mix languag or differ languag .
the main goal of text mine is actual to revert thi process of gener text data .
we hope to be abl to uncov some aspect in thi process .
specif , we can think about mine , for exampl , knowledg about the languag .
and that mean by look at text data in english , we mai be abl to discov someth about english , some usag of english , some pattern of english .
so thi is on type of mine problem , where the result is some knowledg about languag which mai be us in variou wai .
if you look at the pictur , we can also then mine knowledg about the observ world .
and so thi ha much to do with mine the content of text data .
we're go to look at what the text data ar about , and then try to get the essenc of it or extract high qualiti inform about a particular aspect of the world that we're interest in .
for exampl , everyth that ha been said about a particular person or a particular entiti .
and thi can be regard as mine content to describ the observ world in the user's mind or the person's mind .
if you look further , then you can also imagin we can mine knowledg about thi observ , himself or herself .
so thi ha also to do with us text data to infer some properti of thi person .
and these properti could includ the mood of the person or sentiment of the person .
and note that we distinguish the observ word from the person becaus text data can't describ what the person ha observ in an object wai .
but the descript can be also subject with sentiment and so , in gener , you can imagin the text data would contain some factual descript of the world plu some subject comment .
so that's why it's also possibl to do text mine to mine knowledg about the observ .
final , if you look at the pictur to the left side of thi pictur , then you can see we can certainli also sai someth about the real world .
right ?
so inde we can do text mine to infer other real world variabl .
and thi is often call a predict analyt .
and we want to predict the valu of certain interest variabl .
so , thi pictur basic cover multipl type of knowledg that we can mine from text in gener .
when we infer other real world variabl we could also us some of the result from mine text data as intermedi result to help the predict .
for exampl , after we mine the content of text data we might gener some summari of content .
and that summari could be then us to help us predict the variabl of the real world .
now of cours thi is still gener from the origin text data , but i want to emphas here that often the process of text data to gener some featur that can help with the predict is veri import .
and that's why here we show the result of some other mine task , includ mine the content of text data and mine knowledg about the observ , can all be veri help for predict .
in fact , when we have non text data , we could also us the non text data to help predict , and of cours it depend on the problem .
in gener , non text data can be veri import for such predict task .
for exampl , if you want to predict stock price or chang of stock price base on discuss in the new articl or in social media , then thi is an exampl of us text data to predict some other real world variabl .
but in thi case , obvious , the histor stock price data would be veri import for thi predict .
and so that's an exampl of non text data that would be veri us for the predict .
and we're go to combin both kind of data to make the predict .
now non text data can be also us for analyz text by suppli context .
when we look at the text data alon , we'll be mostli look at the content and or opinion express in the text .
but text data gener also ha context associ .
for exampl , the time and the locat that associ ar with the text data .
and these ar us context inform .
and the context can provid interest angl for analyz text data .
for exampl , we might partit text data into differ time period becaus of the avail of the time .
now we can analyz text data in each time period and then make a comparison .
similarli we can partit text data base on locat or ani meta data that's associ to form interest comparison in area .
so , in thi sens , non text data can actual provid interest angl or perspect for text data analysi .
and it can help us make context sensit analysi of content or the languag usag or the opinion about the observ or the author of text data .
we could analyz the sentiment in differ context .
so thi is a fairli gener landscap of the topic in text mine and analyt .
in thi cours we're go to select cover some of those topic .
we actual hope to cover most of these gener topic .
first we're go to cover natur languag process veri briefli becaus thi ha to do with understand text data and thi determin how we can repres text data for text mine .
second , we're go to talk about how to mine word associ from text data .
and word associ is a form of us for lexic knowledg about a languag .
third , we're go to talk about topic mine and analysi .
and thi is onli on wai to analyz content of text , but it's a veri us wai of analyz content .
it's also on of the most us techniqu in text mine .
then we're go to talk about opinion mine and sentiment analysi .
so thi can be regard as on exampl of mine knowledg about the observ .
and final we're go to cover text base predict problem where we try to predict some real world variabl base on text data .
so thi slide also serv as a road map for thi cours .
and we're go to us thi as an outlin for the topic that we'll cover in the rest of thi cours .
thi lectur is about natur languag content analysi .
natur languag content analysi is the foundat of text mine .
so we're go to first talk about thi .
and in particular , natur languag process with a factor how we can present text data .
and thi determin what algorithm can be us to analyz and mine text data .
we're go to take a look at the basic concept in natur languag first .
and i'm go to explain these concept us a similar exampl that you've all seen here .
a dog is chase a boi on the playground .
now thi is a veri simpl sentenc .
when we read such a sentenc we don't have to think about it to get the mean of it .
but when a comput ha to understand the sentenc , the comput ha to go through sever step .
first , the comput need to know what ar the word , how to segment the word in english .
and thi is veri easi , we can just look at the space .
and then the comput will need the know the categori of these word , syntact categori .
so for exampl , dog is a noun , chase's a verb , boi is anoth noun etc .
and thi is call a lexic analysi .
in particular , tag these word with these syntact categori is call a part of speech tag .
after that the comput also need to figur out the relationship between these word .
so a and dog would form a noun phrase .
on the playground would be a preposit phrase , etc .
and there is certain wai for them to be connect togeth in order for them to creat mean .
some other combin mai not make sens .
and thi is call syntact pars , or syntact analysi , pars of a natur languag sentenc .
the outcom is a pars tree that you ar see here .
that tell us the structur of the sentenc , so that we know how we can interpret thi sentenc .
but thi is not semant yet .
so in order to get the mean we would have to map these phrase and these structur into some real world antithesi that we have in our mind .
so dog is a concept that we know , and boi is a concept that we know .
so connect these phrase that we know is understand .
now for a comput , would have to formal repres these entiti by us symbol .
so dog , d1 mean d1 is a dog .
boi , b1 mean b1 refer to a boi etc .
and also repres the chase action as a predic .
so , chase is a predic here with three argument , d1 , b1 , and p1 .
which is playground .
so thi formal rendit of the semant of thi sentenc .
onc we reach that level of understand , we might also make infer .
for exampl , if we assum there's a rule that sai if someon's be chase then the person can get scare , then we can infer thi boi might be scare .
thi is the infer mean , base on addit knowledg .
and final , we might even further infer what thi sentenc is request , or why the person who sai it in a sentenc , is sai the sentenc .
and so , thi ha to do with purpos of sai the sentenc .
thi is call speech act analysi or pragmat analysi .
which first to the us of languag .
so , in thi case a person sai thi mai be remind anoth person to bring back the dog .
so thi mean when sai a sentenc , the person actual take an action .
so the action here is to make a request .
now , thi slide clearli show that in order to realli understand a sentenc there ar a lot of thing that a comput ha to do .
now , in gener it's veri hard for a comput will do everyth , especi if you would want it to do everyth correctli .
thi is veri difficult .
now , the main reason why natur languag process is veri difficult , it's becaus it's design it will make human commun effici .
as a result , for exampl , with onli a lot of common sens knowledg .
becaus we assum all of us have thi knowledg , there's no need to encod thi knowledg .
that make commun effici .
we also keep a lot of ambigu , like , ambigu of word .
and thi is again , becaus we assum we have the abil to disambigu the word .
so , there's no problem with have the same word to mean possibl differ thing in differ context .
yet for a comput thi would be veri difficult becaus a comput doe not have the common sens knowledg that we do .
so the comput will be confus inde .
and thi make it hard for natur languag process .
inde , it make it veri hard for everi step in the slide that i show you earlier .
ambigu is a main killer .
mean that in everi step there ar multipl choic , and the comput would have to decid what the right choic and that decis can be veri difficult as you will see also in a moment .
and in gener , we need common sens reason in order to fulli understand the natur languag .
and comput todai don't yet have that .
that's why it's veri hard for comput to precis understand the natur languag at thi point .
so here ar some specif exampl of challeng .
think about the world level ambigu .
a word like design can be a noun or a verb , so we've got ambigu part of speech tag .
root also ha multipl mean , it can be of mathemat sens , like in the squar of , or can be root of a plant .
syntact ambigu refer to differ interpret of a sentenc in term structur .
so for exampl , natur languag process can actual be interpret in two wai .
so on is the ordinari mean that we will be get as we're talk about thi topic .
so , it's process of natur languag .
but there's is also anoth possibl interpret which is to sai languag process is natur .
now we don't gener have thi problem , but imagin for the comput to determin the structur , the comput would have to make a choic between the two .
anoth classic exampl is a man saw a boi with a telescop .
and thi ambigu li in the question who had the telescop ?
thi is call a preposit phrase attach ambigu .
mean where to attach thi preposit phrase with the telescop .
should it modifi the boi ?
or should it be modifi , saw , the verb .
anoth problem is anaphora resolut .
in john persuad bill to bui a tv for himself .
doe himself refer to john or bill ?
presupposit is anoth difficulti .
he ha quit smoke impli that he smoke befor , and we need to have such a knowledg in order to understand the languag .
becaus of these problem , the state of the art natur languag process techniqu can not do anyth perfectli .
even for the simplest part of speech tag , we still can not solv the whole problem .
the accuraci that ar list here , which is about <num> , wa just taken from some studi earlier .
and these studi obvious have to be us particular data set so the number here ar not realli meaning if you take it out of the context of the data set that ar us for evalu .
but i show these number mainli to give you some sens about the accuraci , or how well we can do thing like thi .
it doesn't mean ani data set accuraci would be precis <num> .
but , in gener , we can do pars speech tag fairli well although not perfect .
pars would be more difficult , but for partial pars , mean to get some phrase correct , we can probabl achiev <num> or better accuraci .
but to get the complet pars tree correctli is still veri , veri difficult .
for semant analysi , we can also do some aspect of semant analysi , particularli , extract of entiti and relat .
for exampl , recogn thi is the person , that's a locat , and thi person and that person met in some place etc .
we can also do word sens to some extent .
the occurr of root in thi sentenc refer to the mathemat sens etc .
sentiment analysi is anoth aspect of semant analysi that we can do .
that mean we can tag the sens as gener posit when it's talk about the product or talk about the person .
infer , howev , is veri hard , and we gener cannot do that for ani big domain and if it's onli feasibl for a veri limit domain .
and that's a gener difficult problem in artifici intellig .
speech act analysi is also veri difficult and we can onli do thi probabl for veri special case .
and with a lot of help from human to annot enough data for the comput to learn from .
so the slide also show that comput ar far from be abl to understand natur languag precis .
and that also explain why the text mine problem is difficult .
becaus we cannot reli on mechan approach or comput method to understand the languag precis .
therefor , we have to us whatev we have todai .
a particular statist machin learn method of statist analysi method to try to get as much mean out from the text as possibl .
and , later you will see that there ar actual mani such algorithm that can inde extract interest model from text even though we cannot realli fulli understand it .
mean of all the natur languag sentenc precis .
so here ar some specif exampl of what we can't do todai and part of speech tag is still not easi to do <num> correctli .
so in the exampl , he turn off the highwai vers he turn off the fan and the two off actual have somewhat a differ in their activ categori and also it veri difficult to get a complet the pars correct .
again , the exampl , a man saw a boi with a telescop can actual be veri difficult to pars depend on the context .
precis deep semant analysi is also veri hard .
for exampl , to defin the mean of own , precis is veri difficult in the sentenc , like john own a restaur .
so the state of the off can be summar as follow .
robust and gener nlp tend to be shallow while a deep understand doe not scale up .
for thi reason in thi cours , the techniqu that we cover ar in gener , shallow techniqu for analyz text data and mine text data and thei ar gener base on statist analysi .
so there ar robust and gener and thei ar in the in categori of shallow analysi .
so such techniqu have the advantag of be abl to be appli to ani text data in ani natur about ani topic .
but the downsid is that , thei don't give us a deeper understand of text .
for that , we have to reli on deeper natur languag analysi .
that typic would requir a human effort to annot a lot of exampl of analysi that would like to do and then comput can us machin learn techniqu and learn from these train exampl to do the task .
so in practic applic , we gener combin the two kind of techniqu with the gener statist and method as a backbon as the basi .
these can be appli to ani text data .
and on top of that , we're go to us human to , and you take more data and to us supervis machin learn to do some task as well as we can , especi for those import task to bring human into the loop to analyz text data more precis .
but thi cours will cover the gener statist approach that gener , don't requir much human effort .
so thei're practic , more us that some of the deeper analysi techniqu that requir a lot of human effort to annot the text todai .
so to summar , the main point we take ar first nlp is the foundat for text mine .
so obvious , the better we can understand the text data , the better we can do text mine .
comput todai ar far from be abl to understand the natur languag .
deep nlp requir common sens knowledg and infer .
thu , onli work for veri limit domain not feasibl for larg scale text mine .
shallow nlp base on statist method can be done in larg scale and is the main topic of thi cours and thei ar gener applic to a lot of applic .
thei ar in some sens also , more us techniqu .
in practic , we us statist nlp as the basi and we'll have human for help as need in variou wai .
thi lectur is about text represent .
in thi lectur we're go to discuss text represent and discuss how natur languag process can allow us to repres text in mani differ wai .
let's take a look at thi exampl sentenc again .
we can repres thi sentenc in mani differ wai .
first , we can alwai repres such a sentenc as a string of charact .
thi is true for all the languag .
when we store them in the comput .
when we store a natur languag sentenc as a string of charact .
we have perhap the most gener wai of repres text sinc we can alwai us thi approach to repres ani text data .
but unfortun us such a represent will not help us to semant analysi , which is often need for mani applic of text mine .
the reason is becaus we're not even recogn word .
so as a string we ar go to keep all of the space and these ascii symbol .
we can perhap count out what's the most frequent charact in the english text or the correl between those charact .
but we can't realli analyz semant , yet thi is the most gener wai of repres text becaus we hadn't us thi to repres ani natur languag or text .
if we try to do a littl bit more natur languag process by do word segment , then we can obtain a represent of the same text , but in the form of a sequenc of word .
so here we see that we can identifi word , like a dog is chase , etc .
now with thi level of represent we suddenli can do a lot of thing .
and thi is mainli becaus word ar the basic unit of human commun and natur languag .
so thei ar veri power .
by identifi word , we can for exampl , easili count what ar the most frequent word in thi document or in the whole collect , etc .
and these word can be us to form topic .
when we combin relat word togeth and some word posit and some word ar neg or we can also do analysi .
so repres text data as a sequenc of word open up a lot of interest analysi possibl .
howev , thi level of represent is slightli less gener than string of charact .
becaus in some languag , such as chines , it's actual not that easi to identifi all the word boundari , becaus in such a languag you see text as a sequenc of charact with no space in between .
so you have to reli on some special techniqu to identifi word .
in such a languag of cours then we might make mistak in segment word .
so the sequenc of word represent is not as robust as string of charact .
but in english , it's veri easi to obtain thi level of represent .
so we can do that all the time .
now if we go further to do in that round of process we can add a part of these text .
now onc we do that we can count , for exampl , the most frequent noun or what kind of noun ar associ with what kind of verb , etc .
so , thi open up a littl bit more interest opportun for further analysi .
note that i us a plu sign here becaus by repres text as a sequenc of part of speech tag , we don't necessarili replac the origin word sequenc written .
instead , we add thi as an addit wai or repres text data .
so now the data is repres as both a sequenc of word and a sequenc of part of speech tag .
thi enrich the represent of text data , and , thu also enabl a more interest analysi .
if we go further , then we'll be paus the sentenc to obtain a syntact structur .
now thi of cours will further open up more interest analysi of , for exampl , the write style or correct grammar mistak .
if we go further for semant analysi .
then we might be abl to recogn dog as an anim .
and we also can recogn boi as a person , and playground as a locat .
and we can further analys their relat .
for exampl , dog wa chase the boi , and boi is on the playground .
thi will add more entiti and relat , through entiti relat recreat .
at thi level , we can do even more interest thing .
for exampl , now we can counter easili the most frequent person that's manag thi whole collect of new articl .
or whenev you mention thi person you also tend to see mention of anoth person , etc .
so thi is veri a us represent .
and it's also relat to the knowledg graph that some of you mai have heard of that googl is do as a more semant wai of repres text data .
howev it's also less robust sequenc of word .
or even syntact analysi , becaus it's not alwai easi to identifi all the entiti with the right type and we might make mistak .
and relat ar even harder to find and we might make mistak .
thi make thi level of represent less robust , yet it's veri us .
now if we move further to logic group condit then we have predic and infer rule .
with infer rule we can infer interest deriv fact from the text .
so that's veri us but unfortun , thi level of represent is even less robust and we can make mistak .
and we can't do that all the time for all kind of sentenc .
and final speech act would add a yet anoth level of rendit of the intent of sai thi sentenc .
so in thi case it might be a request .
so know that would allow us to you know analyz more even more interest thing about the observ or the author of thi sentenc .
what's the intent of sai that ?
what scenario or what kind of action will be made ?
so thi is , anoth role of analysi that would be veri interest .
so thi pictur show that if we move down , we gener see more sophist and natur languag process techniqu will be us .
and unfortun such techniqu would requir more human effort .
and thei ar less accur .
that mean there ar mistak .
so if we analyz our text at the level that ar repres deeper analysi of languag then we have to toler error .
so that also mean it's still necessari to combin such deep analysi with shallow analysi base on , for exampl , sequenc of word .
on the right side , you see the arrow point down to indic that as we go down , with our represent of text is closer to knowledg represent in our mind and need for solv a lot of problem .
now , thi is desir becaus as we can repres text as a level of knowledg , we can easili extract the knowledg .
that's the purpos of text mine .
so , there wa a trade off here .
between do deeper analysi that might have error but would give us direct knowledg that can be extract from text .
and do shadow analysi which is more robust but wouldn't actual give us the necessari deeper represent of knowledg .
i should also sai that text data ar gener by human , and ar meant to be consum by human .
so as a result , in text data analysi , text mine , human plai a veri import role .
thei ar alwai in the loop , mean that we should optim a collabor of human and comput .
so , in that sens it's okai that comput mai not be abl to have complet accur represent of text data .
and pattern that ar extract from text data can be interpret by human .
and then human can guid the comput to do more accur analysi by annot more data , by provid featur to guid machin learn program , to make them work more effect .
so , as we explain the differ text represent tend to enabl differ analysi .
in particular , we can gradual add more and more deeper analysi result to repres text data .
and that would open up a more interest represent opportun and also analysi capac .
so , thi tabl summar what we have just seen .
so the first column show the text represent .
the second visual the gener of such a represent .
mean whether we can do thi kind of represent accur for all the text data or onli some of them .
and the third column show the enabl analysi techniqu .
and the final column show some exampl of applic that can be achiev through thi level of represent .
so let's take a look at them .
so as a stream text can onli be process by stream process algorithm .
it's veri robust , it's gener .
and there wa still some interest applic that can be down at thi level .
for exampl , compress of text .
doesn't necessarili need to know the word boundari .
although know word boundari might actual also help .
word base repetit is a veri import level of represent .
it's quit gener and rel robust , indic thei were a lot of analysi techniqu .
such as word relat analysi , topic analysi and sentiment analysi .
and there ar mani applic that can be enabl by thi kind of analysi .
for exampl , thesauru discoveri ha to do with discov relat word .
and topic and opinion relat applic ar abound .
and there ar , for exampl , peopl might be interest in know the major topic cover in the collect of text .
and thi can be the case in research literatur .
and scientist want to know what ar the most import research topic todai .
or custom servic peopl might want to know all our major complaint from their custom by mine their e mail messag .
and busi intellig peopl might be interest in understand consum' opinion about their product and the competitor' product to figur out what ar the win featur of their product .
and , in gener , there ar mani applic that can be enabl by the represent at thi level .
now , move down , we'll see we can gradual add addit represent .
by ad syntact structur , we can enabl , of cours , syntact graph analysi .
we can us graph mine algorithm to analyz syntact graph .
and some applic ar relat to thi kind of represent .
for exampl , stylist analysi gener requir syntact structur represent .
we can also gener the structur base featur .
and those ar featur that might help us classifi the text object into differ categori by look at the structur sometim in the classif .
it can be more accur .
for exampl , if you want to classifi articl into differ categori correspond to differ author .
you want to figur out which of the k author ha actual written thi articl , then you gener need to look at the syntact structur .
when we add entiti and relat , then we can enabl other techniqu such as knowledg graph and answer , or inform network and answer in gener .
and thi analysi enabl applic about entiti .
for exampl , discoveri of all the knowledg and opinion about real world entiti .
you can also us thi level represent to integr everyth about anyth from scale resourc .
final , when we add logic predic , that would enabl larg infer , of cours .
and thi can be veri us for integr analysi of scatter knowledg .
for exampl , we can also add ontolog on top of the , extract the inform from text , to make infer .
a good of exampl of applic in thi enabl by thi level of represent , is a knowledg assist for biologist .
and thi program that can help a biologist manag all the relev knowledg from literatur about a research problem such as understand function of gene .
and the comput can make infer about some of the hypothesi that the biologist might be interest .
for exampl , whether a gene ha a certain function , and then the intellig program can read the literatur to extract the relev fact , do compil and inform extract .
and then us a logic system to actual track that's the answer to research question about what gene ar relat to what function .
so in order to support thi level of applic we need to go as far as logic represent .
now , thi cours is cover techniqu mainli base on word base represent .
and these techniqu ar gener and robust and that's more wide us in variou applic .
in fact , in virtual all the text mine applic you need thi level of represent and then techniqu that support analysi of text in thi level .
but obvious all these other level can be combin and should be combin in order to support the sophist applic .
so to summar , here ar the major takeawai point .
text represent determin what kind of mine algorithm can be appli .
and there ar multipl wai to repres the text , string , word , syntact structur , entiti relat graph , knowledg predic , etc .
and these differ represent should in gener be combin in real applic to the extent we can .
for exampl , even if we cannot do accur represent of syntact structur , we can state that partial structur strictli .
and if we can recogn some entiti , that would be great .
so in gener we want to do as much as we can .
and when differ level ar combin togeth , we can enabl a richer analysi , more power analysi .
thi cours howev focus on word base represent .
such techniqu have also sever advantag , first of thei ar gener and robust , so thei ar applic to ani natur languag .
that's a big advantag over other approach that reli on more fragil natur languag process techniqu .
secondli , it doe not requir much manual effort , or sometim , it doe not requir ani manual effort .
so that's , again , an import benefit , becaus that mean that you can appli it directli to ani applic .
third , these techniqu ar actual surprisingli power and effect form in implic .
although not all of cours as i just explain .
now thei ar veri effect partli becaus the word ar invent by human as basic unit for commun .
so thei ar actual quit suffici for repres all kind of semant .
so that make thi kind of word base represent all so power .
and final , such a word base represent and the techniqu enabl by such a represent can be combin with mani other sophist approach .
so thei're not compet with each other .
thi lectur is about the word associ mine and analysi .
in thi lectur , we're go to talk about how to mine associ of word from text .
now thi is an exampl of knowledg about the natur languag that we can mine from text data .
here's the outlin .
we're go to first talk about what is word associ and then explain why discov such relat is us and final we're go to talk about some gener idea about how to mine word associ .
in gener there ar two word relat and these ar quit basic .
on is call a paradigmat relat .
the other is syntagmat relat .
a and b have paradigmat relat if thei can be substitut for each other .
that mean the two word that have paradigmat relat would be in the same semant class , or syntact class .
and we can in gener replac on by the other without affect the understand of the sentenc .
that mean we would still have a valid sentenc .
for exampl , cat and dog , these two word have a paradigmat relat becaus thei ar in the same class of anim .
and in gener , if you replac cat with dog in a sentenc , the sentenc would still be a valid sentenc that you can make sens of .
similarli mondai and tuesdai have paradigmat relat .
the second kind of relat is call syntagmat relat .
in thi case , the two word that have thi relat , can be combin with each other .
so a and b have syntagmat relat if thei can be combin with each other in a sentenc , that mean these two word ar semant relat .
so for exampl , cat and sit ar relat becaus a cat can sit somewher .
similarli , car and drive ar relat semant and thei can be combin with each other to convei mean .
howev , in gener , we can not replac cat with sit in a sentenc or car with drive in the sentenc to still get a valid sentenc , mean that if we do that , the sentenc will becom somewhat meaningless .
so thi is differ from paradigmat relat .
and these two relat ar in fact so fundament that thei can be gener to captur basic relat between unit in arbitrari sequenc .
and definit thei can be gener to describ relat of ani item in a languag .
so , a and b don't have to be word and thei can be phrase , for exampl .
and thei can even be more complex phrase than just a non phrase .
if you think about the gener problem of the sequenc mine then we can think about the unit be and the sequenc data .
then we think of paradigmat relat as relat that ar appli to unit that tend to occur in a singular locat in a sentenc , or in a sequenc of data element in gener .
so thei occur in similar locat rel to the neighbor in the sequenc .
syntagmat relat on the other hand is relat to co occurr element that tend to show up in the same sequenc .
so these two ar complimentari and ar basic relat of word .
and we're interest in discov them automat from text data .
discov such word relat ha mani applic .
first , such relat can be directli us for improv accuraci of mani nlp task , and thi is becaus thi is part of our knowledg about a languag .
so if you know these two word ar synonym , for exampl , and then you can help a lot of task .
and grammar learn can be also done by us such techniqu .
becaus if we can learn paradigmat relat , then we form class of word , syntact class for exampl .
and if we learn syntagmat relat , then we would be abl to know the rule for put togeth a larger express base on compon express .
so we learn the structur and what can go with what els .
word relat can be also veri us for mani applic in text retriev and mine .
for exampl , in search and text retriev , we can us word associ to modifi a queri , and thi can be us to introduc addit relat word into a queri and make the queri more effect .
it's often call a queri expans .
or you can us relat word to suggest relat queri to the user to explor the inform space .
anoth applic is to us word associ to automat construct the top of the map for brows .
we can have word as node and associ as edg .
a user could navig from on word to anoth to find inform in the inform space .
final , such word associ can also be us to compar and summar opinion .
for exampl , we might be interest in understand posit and neg opinion about the iphon <num> .
in order to do that , we can look at what word ar most strongli associ with a featur word like batteri in posit versu neg review .
such a syntagmat relat would help us show the detail opinion about the product .
so , how can we discov such associ automat ?
now , here ar some intuit about how to do that .
now let's first look at the paradigmat relat .
here we essenti can take advantag of similar context .
so here you see some simpl sentenc about cat and dog .
you can see thei gener occur in similar context , and that after all is the definit of paradigmat relat .
on the right side you can kind of see i extract expressli the context of cat and dog from thi small sampl of text data .
i've taken awai cat and dog from these sentenc , so that you can see just the context .
now , of cours we can have differ perspect to look at the context .
for exampl , we can look at what word occur in the left part of thi context .
so we can call thi left context .
what word occur befor we see cat or dog ?
so , you can see in thi case , clearli dog and cat have similar left context .
you gener sai hi cat or my cat and you sai also , my dog and hi dog .
so that make them similar in the left context .
similarli , if you look at the word that occur after cat and dog , which we can call right context , thei ar also veri similar in thi case .
of cours , it's an extrem case , where you onli see eat .
and in gener , you'll see mani other word , of cours , that can't follow cat and dog .
you can also even look at the gener context .
and that might includ all the word in the sentenc or in sentenc around thi word .
and even in the gener context , you also see similar between the two word .
so thi wa just a suggest that we can discov paradigmat relat by look at the similar of context of word .
so , for exampl , if we think about the follow question .
how similar ar context of cat and context of dog ?
in contrast how similar ar context of cat and context of comput ?
now , intuit , we're to imagin the context of cat and the context of dog would be more similar than the context of cat and context of the comput .
that mean , in the first case the similar valu would be high , between the context of cat and dog , where as in the second , the similar between context of cat and comput would be low becaus thei all not have a paradigmat relationship and imagin what word occur after comput in gener .
it would be veri differ from what word occur after cat .
so thi is the basic idea of what thi cover , paradigmat relat .
what about the syntagmat relat ?
well , here we're go to explor the correl occurr , again base on the definit of syntagmat relat .
here you see the same sampl of text .
but here we're interest in know what other word ar correl with the verb eat and what word can go with eat .
and if you look at the right side of thi slide and you see , i've taken awai the two word around eat .
i've taken awai the word to it left and also the word to it right in each sentenc .
and then we ask the question , what word tend to occur to the left of eat ?
and what word tend to occur to the right of eat ?
now think about thi question would help us discov syntagmat relat becaus syntagmat relat essenti captur such correl .
so the import question to ask for syntagmat relat is , whenev eat occur , what other word also tend to occur ?
so the question here ha to do with whether there ar some other word that tend to co occur togeth with each .
mean that whenev you see eat you tend to see the other word .
and if you don't see eat , probabl , you don't see other word often either .
so thi intuit can help discov syntagmat relat .
now again , consid exampl .
how help is occurr of eat for predict occurr of meat ?
right .
all right , so know whether eat occur in a sentenc would gener help us predict whether meat also occur inde .
and if we see eat occur in the sentenc , and that should increas the chanc that meat would also occur .
in contrast , if you look at the question in the bottom , how help is the occurr of eat for predict of occurr of text ?
becaus eat and text ar not realli relat , so know whether eat occur in the sentenc doesn't realli help us predict the weather , text also occur in the sentenc .
so thi is in contrast to the question about eat and meat .
thi also help explain that intuit behind the method of what discov syntagmat relat .
mainli we need to captur the correl between the occurr of two word .
so to summar the gener idea for discov word associ ar the follow .
for paradigmat relat , we present each word by it context .
and then comput it context similar .
we're go to assum the word that have high context similar to have paradigmat relat .
for syntagmat relat , we will count how mani time two word occur togeth in a context , which can be a sentenc , a paragraph , or a document even .
and we're go to compar their co occurr with their individu occurr .
we're go to assum word with high co occurr but rel low individu occurr to have syntagmat relat becaus thei attempt to occur togeth and thei don't usual occur alon .
note that the paradigmat relat and the syntagmat relat ar actual close relat in that paradigmat relat word tend to have syntagmat relat with the same word .
thei tend to be associ with the same word , and that suggest that we can also do join the discoveri of the two relat .
so these gener idea can be implement in mani differ wai .
and the cours won't cover all of them , but we will cover at least some of the method that ar effect for discov these relat .
thi lectur is about the paradigmat relat discoveri .
in thi lectur we ar go to talk about how to discov a particular kind of word associ call a paradigmat relat .
by definit , two word ar paradigmat relat if thei share a similar context .
name , thei occur in similar posit in text .
so natur our idea of discov such a relat is to look at the context of each word and then try to comput the similar of those context .
so here is an exampl of context of a word , cat .
here i have taken the word cat out of the context and you can see we ar see some remain word in the sentenc that contain cat .
now , we can do the same thing for anoth word like dog .
so in gener we would like to captur such a context and then try to assess the similar of the context of cat and the context of a word like dog .
so now the question is how can we formal repres the context and then defin the similar function .
so first , we note that the context actual contain a lot of word .
so , thei can be regard as a pseudo document , a imagin document , but there ar also differ wai of look at the context .
for exampl , we can look at the word that occur befor the word cat .
we can call thi context left1 context .
all right , so in thi case you will see word like my , hi , or big , a , the , et cetera .
these ar the word that can occur to left of the word cat .
so we sai my cat , hi cat , big cat , a cat , et cetera .
similarli , we can also collect the word that occur right after the word cat .
we can call thi context right1 , and here we see word like eat , at , is , ha , et cetera .
or , more gener , we can look at all the word in the window of text around the word cat .
here , let's sai we can take a window of <num> word around the word cat .
we call thi context window8 .
now , of cours , you can see all the word from left or from right , and so we'll have a bag of word in gener to repres the context .
now , such a word base represent would actual give us an interest wai to defin the perspect of measur the similar .
becaus if you look at just the similar of left1 , then we'll see word that share just the word in the left context , and we kind of ignor the other word that ar also in the gener context .
so that give us on perspect to measur the similar , and similarli , if we onli us the right1 context , we will captur thi narr from anoth perspect .
us both the left1 and right1 of cours would allow us to captur the similar with even more strict criteria .
so in gener , context mai contain adjac word , like eat and my , that you see here , or non adjac word , like saturdai , tuesdai , or some other word in the context .
and thi flexibl also allow us to match the similar in somewhat differ wai .
sometim thi is us , as we might want to captur similar base on gener content .
that would give us loos relat paradigmat relat .
wherea if you us onli the word immedi to the left and to the right of the word , then you like will captur word that ar veri much relat by their syntact categori and semant .
so the gener idea of discov paradigmat relat is to comput the similar of context of two word .
so here , for exampl , we can measur the similar of cat and dog base on the similar of their context .
in gener , we can combin all kind of view of the context .
and so the similar function is , in gener , a combin of similar on differ context .
and of cours , we can also assign weight to these differ similar to allow us to focu more on a particular kind of context .
and thi would be natur applic specif , but again , here the main idea for discov pardigmat relat word is to comput the similar of their context .
so next let's see how we exactli comput these similar function .
now to answer thi question , it is us to think of bag of word represent as vector in a vector space model .
now those of you who have been familiar with inform retriev or textual retriev techniqu would realiz that vector space model ha been us frequent for model document and queri for search .
but here we also find it conveni to model the context of a word for paradigmat relat discoveri .
so the idea of thi approach is to view each word in our vocabulari as defin on dimens in a high dimension space .
so we have n word in total in the vocabulari , then we have n dimens , as illustr here .
and on the bottom , you can see a frequenc vector repres a context , and here we see where eat occur <num> time in thi context , at occur <num> time , et cetera .
so thi vector can then be place in thi vector space model .
so in gener , we can repres a pseudo document or context of cat as on vector , d1 , and anoth word , dog , might give us a differ context , so d2 .
and then we can measur the similar of these two vector .
so by view context in the vector space model , we convert the problem of paradigmat relat discoveri into the problem of comput the vector and their similar .
so the two question that we have to address ar first , how to comput each vector , and that is how to comput xi or yi .
and the other question is how do you comput the similar .
now in gener , there ar mani approach that can be us to solv the problem , and most of them ar develop for inform retriev .
and thei have been shown to work well for match a queri vector and a document vector .
but we can adapt mani of the idea to comput a similar of context document for our purpos here .
so let's first look at the on plausibl approach , where we try to match the similar of context base on the expect overlap of word , and we call thi eowc .
so the idea here is to repres a context by a word vector where each word ha a weight that's equal to the probabl that a randomli pick word from thi document vector , is thi word .
so in other word , xi is defin as the normal account of word wi in the context , and thi can be interpret as the probabl that you would actual pick thi word from d1 if you randomli pick a word .
now , of cours these xi's would sum to on becaus thei ar normal frequenc , and thi mean the vector is actual probabl of the distribut over word .
so , the vector d2 can be also comput in the same wai , and thi would give us then two probabl distribut repres two context .
so , that address the problem how to comput the vector , and next let's see how we can defin similar in thi approach .
well , here , we simpli defin the similar as a dot product of two vector , and thi is defin as a sum of the product of the correspond element of the two vector .
now , it's interest to see that thi similar function actual ha a nice interpret , and that is thi .
dot product , in fact that give us the probabl that two randomli pick word from the two context ar ident .
that mean if we try to pick a word from on context and try to pick anoth word from anoth context , we can then ask the question , ar thei ident ?
if the two context ar veri similar , then we should expect we frequent will see the two word pick from the two context ar ident .
if thei ar veri differ , then the chanc of see ident word be pick from the two context would be small .
so thi intuit make sens , right , for measur similar of context .
now you might want to also take a look at the exact formula and see why thi can be interpret as the probabl that two randomli pick word ar ident .
so if you just stare at the formula to check what's insid thi sum , then you will see basic in each case it give us the probabl that we will see an overlap on a particular word , wi .
and where xi give us a probabl that we will pick thi particular word from d1 , and yi give us the probabl of pick thi word from d2 .
and when we pick the same word from the two context , then we have an ident pick , right so .
that's on possibl approach , eowc , extract overlap of word in context .
now as alwai , we would like to assess whether thi approach it would work well .
now of cours , ultim we have to test the approach with real data and see if it give us realli semant relat word .
realli give us paradigmat relat , but analyt we can also analyz thi formula a littl bit .
so first , as i said , it doe make sens , right , becaus thi formula will give a higher score if there is more overlap between the two context .
so that's exactli what we want .
but if you analyz the formula more carefulli , then you also see there might be some potenti problem , and specif there ar two potenti problem .
first , it might favor match on frequent term veri well , over match more distinct term .
and that is becaus in the dot product , if on element ha a high valu and thi element is share by both context and it contribut a lot to the overal sum , it might inde make the score higher than in anoth case , where the two vector actual have a lot of overlap in differ term .
but each term ha a rel low frequenc , so thi mai not be desir .
of cours , thi might be desir in some other case .
but in our case , we should intuit prefer a case where we match more differ term in the context , so that we have more confid in sai that the two word inde occur in similar context .
if you onli reli on on term and that's a littl bit question , it mai not be robust .
now the second problem is that it treat everi word equal , right .
so if you match a word like the and it will be the same as match a word like eat , but intuit we know match the isn't realli surpris becaus the occur everywher .
so match the is not as such strong evid as match what a word like eat , which doesn't occur frequent .
so thi is anoth problem of thi approach .
in the next chapter we ar go to talk about how to address these problem .
in thi lectur we continu discuss paradigmat relat discoveri .
earlier we introduc a method call expect overlap of word in context .
in thi method we repres each context by a word of vector that repres the probabl of a word in the context .
and we measur the similar by us the dot product which can be interpret as the probabl that two randomli pick word from the two context ar ident .
we also discuss the two problem of thi method .
the first is that it favor match on frequent term veri well over match more distinct term .
it put too much emphasi on match on term veri well .
the second is that it treat everi word equal .
even a common word like the would contribut equal as content word like eat .
so now we ar go to talk about how to solv thi problem .
more specif we're go to introduc some retriev heurist us in text retriev and these heurist can effect solv these problem as these problem also occur in text retriev when we match a queri with a document , so to address the first problem , we can us a sublinear transform of term frequenc .
that is , we don't have to us raw frequenc count of the term to repres the context .
we can transform it into some form that wouldn't emphas so much on the raw frequenc to address the problem , we can put more weight on rare term .
and that is , we ran reward a match a rare word .
and thi heurist is call idf term weight in text retriev .
idf stand for invers document frequenc .
so now we're go to talk about the two heurist in more detail .
first , let's talk about the tf transform .
that is , it'll convert the raw count of a word in the document into some weight that reflect our belief about how import thi word .
the document .
and so , that would be denot by tf of w and d .
that's shown in the y axi .
now , in gener , there ar mani wai to map that .
and let's first look at the the simpl wai of map .
in thi case , we're go to sai , well , ani non zero count will be map to on .
and the zero count will be map to zero .
so with thi map , all the frequenc will be map to onli two valu , zero or on .
and the map function is shown here as a flat line here .
thi is naiv becaus in order the frequenc of word , howev , thi actual ha advantag of emphas , match all the word in the context .
it doe not allow a frequent word to domin the match now the approach that we have taken earlier in the overlap account approach is a linear transform we basic take y as the same as x so we us the raw count as a represent and that creat the problem that we just talk about .
name , it emphas too much on match on frequent term .
match on frequent term can contribut a lot .
we can have a lot of other interest transform in between the two extrem .
and thei gener form a sub linear transform .
so for exampl , on a logarithm of the row count .
and thi will give us curv that look like thi that you ar see here .
in thi case , you can see the high frequenc count .
the high count ar penal a littl bit all right , so the curv is a sub linear curv .
and it bring down the weight of those realli high count .
and thi what we want becaus it prevent that kind of term from domin the score function .
now , there is also anoth interest transform call a bm25 transform , which as been shown to be veri effect for retriev .
and in thi transform we have a form that look like thi .
so it's k plu on multipli by x , divid by x plu k .
where k is a paramet .
x is the count .
the raw count of a word .
now the transform is veri interest , in that it can actual kind of go from on extrem to the other extrem by vari k , and it also is interest that it ha upper bound , k <num> in thi case .
so , thi put a veri strict constraint on high frequenc term , becaus their weight will never exce k <num> .
as we vari k , we can simul the two extrem .
so , when is set to zero , we roughli have the zero on vector .
wherea , when we set the k to a veri larg valu , it will behav more like , immedi transform .
so thi transform function is by far the most effect transform function for tax and retriev , and it also make sens for our problem set up .
so we just talk about how to solv the problem of overemphas a frequent , a frequent tongu .
now let's look at the second problem , and that is how we can penal popular term , match the is not surpris becaus the occur everywher .
but match eat would count a lot so how can we address that problem .
in thi case we can us the idf weight .
pop that's commonli us in retriev .
idf stand for invers document frequenc .
now frequenc mean the count of the total number of document that contain a particular word .
so here we show that the idf measur is defin as a logarithm function of the number of document that match a term or document frequenc .
so , k is the number of document contain a word , or document frequenc .
and m here is the total number of document in the collect .
the idf function is give a higher valu for a lower k , mean that it reward a rare term , and the maximum valu is log of m <num> .
that's when the word occur just onc in the context , so that's a veri rare term .
the rarest term in the whole collect .
the lowest valu you can see here is when k reach it maximum , which would be m .
all right so , that would be a veri low valu , close to zero in fact .
so , thi of cours measur is us in search .
where we natur have a collect .
in our case , what would be our collect ?
well , we can also us the context that we had collect for all the word as our collect .
and that is to sai , a word that's popul the collect in gener .
would also have a low idf becaus depend on the dataset we can construct the context vector in the differ wai .
but in the end , if a term is veri frequent origin data set .
then it will still be frequent the collect context document .
so how can we add these heurist to improv our similar function well here's on wai .
and there ar mani other wai that ar possibl .
but thi is a reason wai .
where we can adapt the bm25 retriev model for paradigmat relat mine .
so here , we defin , in thi case we defin the document vector as contain element repres normal bm25 valu .
so in thi normal function , we see , we take a sum over , sum of all the word .
and we normal the weight of each word by the sum of the weight of all the word .
and thi is to , again , ensur all the xi's will sum to <num> in thi vector .
so thi would be veri similar to what we had befor , in that thi vector is actual someth similar to a word distribut .
or the xi with sum to <num> .
now the weight of bm25 for each word is defin here .
and if you compar thi with our old definit where we just have a normal count , of thi on so we onli have thi on and the document len of the total count of word .
be that context document and that's what we had befor .
but now with the bm25 transform , we're introduc to someth els .
first off , becaus thi extra occurr of thi count is just to achiev the of normal .
but we also see we introduc the paramet k here .
and thi paramet is gener non activ number although zero is also possibl .
thi control the upper bound and the kind of all to what extent it simul the linear transform .
and so thi is on paramet , but we also see there wa anoth paramet here , b .
and thi would be within <num> an <num> .
and thi is a paramet to control length normal .
and in thi case , the normal formula ha averag document length here .
and thi is comput by take the averag of the length of all the document in the collect .
in thi case , all the length of all the context document .
that we ar consid .
so thi averag document will be a constant for ani given collect .
so it actual is onli affect the factor of the paramet b here becaus thi is a constant .
but i kept it here becaus it's constant and that's us in retriev where it would give us a stabil interpret of paramet b .
but , for our purpos it would be a constant .
so it would onli be affect the length normal togeth with paramet b .
now with thi definit then , we have a new wai to defin our document of vector .
and we can comput the vector d2 in the same wai .
the differ is that the high frequenc term will now have a somewhat lower weight .
and thi would help us control the influenc of these high frequenc term .
now , the idea can be ad here in the score function .
that mean we will introduc a wai for match each time .
you mai recal , thi is sum that indic all the possibl word that can be overlap between the two contact .
and the xi and the yi ar probabl of pick the word from both context , therefor , it indic how like we'll see a match on thi word .
now , idf would give us the import of match thi word .
a common word will be worth less than a rare word , and so we emphas more on match rare word now .
so , with thi modif , then the new function .
when like to address those two problem .
now interestingli , we can also us thi approach to discov syntagmat relat .
in gener , when we repres a term vector to replant a context with a term vector we would like see , some term have higher weight , and other term have lower weight .
depend on how we assign weight to these term , we might be abl to us these weight to discov the word that ar strongli associ with a candid of word in the context .
it's interest that we can also us thi context for similar function base on bm25 to discov syntagmat relat .
so , the idea is to us the convert implant of the context .
to see which term ar score high .
and if a term ha high weight , then that term might be more strongli relat to the candid word .
so let's take a look at the vector in more detail here .
and we have each xi defin as a normal weight of bm25 .
now thi weight alon onli reflect how frequent the word occur in the context .
but , we can't just sai an infrequ term in the context would be correl with the candid word becaus mani common word like the will occur frequent out of context .
but if we appli idf weight as you see here , we can then re weigh these term base on idf .
that mean the word that ar common , like the , will get penal .
so now the highest weight term will not be those common term becaus thei have lower idf .
instead , those term would be the term that ar frequent in the context but not frequent in the collect .
so those ar clearli the word that tend to occur in the context of the candid word , for exampl , cat .
so , for thi reason , the highli weight term in thi idea of weight vector can also be assum to be candid for syntagmat relat .
now , of cours , thi is onli a byproduct of how approach is for discov parathmat relat .
and in the next lectur , we're go to talk more about how to discov syntagmat relat .
but it clearli show the relat between discov the two relat .
and inde thei can be discuss .
discov in a join manner by leverag such associ , name syntact relat word that ar similar in , yeah it also show the relat between syntagmat relat discoveri and the paradgrat relat discoveri .
we mai be abl to leverag the relat to join the discoveri of two kind of relat .
thi also show some interest connect between the discoveri of syntagmat relat and the paradigmat relat .
specif those word that ar paradigmat relat tend to be have a syntagmat relat with the same word .
so to summar the main idea of what is cover paradigmat relat is to collect the context of a candid word to form a pseudo document , and thi is typic repres as a bag of word .
and then comput similar of the correspond context document of two candid word .
and then we can take the highli similar word pair and treat them as have paradigmat relat .
these ar the word that share similar context .
there ar mani differ wai to implement thi gener idea , and we just talk about some of the approach , and more specif we talk about us text retriev model to help us design effect similar function to comput the paradigmat relat .
more specif we have us the bm25 and idf weight to discov paradigmat relat .
and these approach also repres the state of the art .
in text retriev techniqu .
final , syntagmat relat can also be discov as a by product when we discov paradigmat relat .
thi lectur is about topic mine and analysi .
we're go to talk about it motiv and task definit .
in thi lectur we're go to talk about differ kind of mine task .
as you see on thi road map , we have just cover mine knowledg about languag , name discoveri of word associ such as paradigmat and relat and syntagmat relat .
now , start from thi lectur , we're go to talk about mine anoth kind of knowledg , which is content mine , and try to discov knowledg about the main topic in the text .
and we call that topic mine and analysi .
in thi lectur , we're go to talk about it motiv and the task definit .
so first of all , let's look at the concept of topic .
so topic is someth that we all understand , i think , but it's actual not that easi to formal defin .
roughli speak , topic is the main idea discuss in text data .
and you can think of thi as a theme or subject of a discuss or convers .
it can also have differ granular .
for exampl , we can talk about the topic of a sentenc .
a topic of articl , aa topic of paragraph or the topic of all the research articl in the research librari , right , so differ grand narr of topic obvious have differ applic .
inde , there ar mani applic that requir discoveri of topic in text , and thei're analyz then .
here ar some exampl .
for exampl , we might be interest in know about what ar twitter user ar talk about todai ?
ar thei talk about nba sport , or ar thei talk about some intern event , etc . ?
or we ar interest in know about research topic .
for exampl , on might be interest in know what ar the current research topic in data mine , and how ar thei differ from those five year ago ?
now thi involv discoveri of topic in data mine literatur and also we want to discov topic in todai's literatur and those in the past .
and then we can make a comparison .
we might also be also interest in know what do peopl like about some product like the iphon <num> , and what do thei dislik ?
and thi involv discov topic in posit opinion about iphon <num> and also neg review about it .
or perhap we're interest in know what ar the major topic debat in <num> presidenti elect ?
and all these have to do with discov topic in text and analyz them , and we're go to talk about a lot of techniqu for do thi .
in gener we can view a topic as some knowledg about the world .
so from text data we expect to discov a number of topic , and then these topic gener provid a descript about the world .
and it tell us someth about the world .
about a product , about a person etc .
now when we have some non text data , then we can have more context for analyz the topic .
for exampl , we might know the time associ with the text data , or locat where the text data were produc , or the author of the text , or the sourc of the text , etc .
all such meta data , or context variabl can be associ with the topic that we discov , and then we can us these context variabl help us analyz pattern of topic .
for exampl , look at topic over time , we would be abl to discov whether there's a trend topic , or some topic might be fade awai .
soon you ar look at topic in differ locat .
we might know some insight about peopl's opinion in differ locat .
so that's why mine topic is veri import .
now , let's look at the task of topic mine and analysi .
in gener , it would involv first discov a lot of topic , in thi case , k topic .
and then we also would like to know , which topic ar cover in which document , to what extent .
so for exampl , in document on , we might see that topic <num> is cover a lot , topic <num> and topic k ar cover with a small portion .
and other topic , perhap , ar not cover .
document two , on the other hand , cover topic <num> veri well , but it did not cover topic <num> at all , and it also cover topic k to some extent , etc . , right ?
so now you can see there ar gener two differ task , or sub task , the first is to discov k topic from a collect of text laid out .
what ar these k topic ?
okai , major topic in the text thei ar .
the second task is to figur out which document cover which topic to what extent .
so more formal , we can defin the problem as follow .
first , we have , as input , a collect of n text document .
here we can denot the text collect as c , and denot text articl as d i .
and , we gener also need to have as input the number of topic , k .
but there mai be techniqu that can automat suggest a number of topic .
but in the techniqu that we will discuss , which ar also the most us techniqu , we often need to specifi a number of topic .
now the output would then be the k topic that we would like to discov , in order as theta sub on through theta sub k .
also we want to gener the coverag of topic in each document of d sub i and thi is denot by pi  sub i j .
and pi  sub ij is the probabl of document d sub i cover topic theta sub j .
so obvious for each document , we have a set of such valu to indic to what extent the document cover , each topic .
and we can assum that these probabl sum to on .
becaus a document won't be abl to cover other topic outsid of the topic that we discuss , that we discov .
so now , the question is , how do we defin theta sub i , how do we defin the topic ?
now thi problem ha not been complet defin until we defin what is exactli theta .
so in the next few lectur , we're go to talk about differ wai to defin theta .
thi lectur is about the expect maxim algorithm or also call the em algorithm .
in thi lectur , we're go to continu the discuss of probabilist topic model .
in particular , we're go to introduc the em algorithm .
which is a famili of us algorithm for comput the maximum life or estim of mixtur model .
so , thi is now a familiar scenario of us two compon , the mixtur model to try to fact out the background word from on topic or word distribut .
yeah .
so , we're interest in comput thi estim and we're go to try to adjust these probabl valu to maxim the probabl of the observ document .
and know that we're assum all the other paramet ar known .
so , the onli thing unknown is these water properti , thi given by zero someth .
and in thi lectur , we're go to look into how to comput thi maximum like or estim .
now thi start with the idea of separ the word in the text data into two group .
on group will be explain by the background model .
the other group will be explain by the unknown topic order .
after all thi is the basic idea of the mixtur model .
but , suppos we actual know which word is from which distribut .
so that would mean , for exampl , these word , the , is , and we , ar known to be from thi background origin , distribut .
on the other hand , the other word , text mine , cluster , etcetera ar known to be from the topic word , distribut .
if you can see the color , that these ar show blue .
these blue word ar , thei ar assum to be from the topic word , distribut .
if we alreadi know how to separ these word .
then the problem of estim the word distribut would be extrem simpl , right ?
if you think about thi for a moment , you'll realiz that , well , we can simpli take all these word that ar known to be from thi word distribut , see that's a d and normal them .
so inde thi problem would be veri easi to solv if we had known which word ar from which it is written precis .
and thi is in fact , make thi model no longer a mysteri model becaus we can alreadi observ which of these distribut ha been us to gener which part of the data .
so we , actual go back to the singl order distribut problem .
and in thi case , let's call these word that ar known to be from theta d , a pseudo document of d prime .
and now all we have to do is just normal these word account for each word , w sub i .
and that's fairli straightforward , and it's just dictat by the maximum estim .
now , thi idea , howev , doesn't work becaus we in practic , don't realli know which word is from which distribut .
but thi give us an idea of perhap we can guess which word is from which distribut .
specif , given all the paramet , can we infer the distribut a word is from ?
so let's assum that we actual know tent probabl for these word in theta sub d .
so now all the paramet ar known for thi mysteri model .
now let's consid word , like a text .
so the question is , do you think text is more like , have been gener from theta sub d or from theta sub b ?
so , in other word , we ar to infer which distribut ha been us to gener thi text .
now , thi infer process is a typic of base an infer situat , where we have some prior about these two distribut .
so can you see what is our prior here ?
well , the prior here is the probabl of each distribut , right .
so the prior is given by these two probabl .
in thi case , the prior is sai that each model is equal like .
but we can imagin perhap a differ appli is possibl .
so thi is call a pry becaus thi is our guess of which distribut ha been us to gener the word .
befor we even observ the word .
so that's why we call it a pry .
if we don't observ the word we don't know what word ha been observ .
our best guess is to sai , well , thei're equal like .
so it's just like flip a coin .
now in basic infer , we typic them with our belief after we have observ the evid .
so what is the evid here ?
well , the evid here is the word text .
now that we know we're interest in the word text .
so text can be regard as evid .
and if we us base rule to combin the prior and the theta likelihood , what we will end up with is to combin the prior with the likelihood that you see here .
which is basic the probabl of the word text from each distribut .
and we see that in both case text is possibl .
note that even in the background it is still possibl , it just ha a veri small probabl .
so intuit what would be your guess see thi case ?
now if you're like mani other , you would guess text is probabl from c . subd it's more like from c . subd , why ?
and you will probabl see that it's becaus text ha a much higher probabl here by the c now sub d than by the background model which ha a veri small probabl .
and by thi we're go to sai well , text is more like from theta sub d .
so you see our guess of which distribut ha been us with the gener text would depend on how high the probabl of the data , the text , is in each word distribut .
we can do tent guess that distribut that give is a word higher probabl .
and thi is like to maxim the likelihood .
all right , so we ar go to choos a word that ha a higher likelihood .
so , in other word we ar go to compar these two probabl of the word given by each of these distribut .
but our guess must also be affect by the prior .
so we also need to compar these two prior .
why ?
becaus imagin if we adjust these probabl .
we're go to sai , the probabl of choos a background model is almost <num> .
now if we have that kind of strong prior , then that would affect your ga .
you might think , well , wait a moment , mayb texter could have been from the background as well .
although the probabl is veri small here the prior is veri high .
so in the end , we have to combin the two .
and the base formula provid us a solid and principl wai of make thi kind of guess to quantifi that .
so more specif , let's think about the probabl that thi word text ha been gener in fact from theta sub d .
well , in order for text to be gener from theta sub d , two thing must happen .
first , the theta sub d must have been select .
so , we have the select probabl here .
and secondli we also have to actual have observ the text from the distribut .
so , when we multipli the two togeth , we get the probabl that text ha in fact been gener from zero sub d .
similarli , for the background model and the probabl of gener text is anoth product of similar form .
now we also introduc late in the variabl z here to denot whether the word is from the background or the topic .
when z is <num> , it mean it's from the topic , theta sub d .
when it's <num> , it mean it's from the background , theta sub b .
so now we have the probabl that text is gener from each , then we can simpli normal them to have estim of the probabl that the word text is from theta sub d or from theta sub b .
and equival the probabl that z is equal to zero , given that the observ evid is text .
so thi is applic of base rule .
but thi step is veri crucial for understand the em hour .
becaus if we can do thi , then we would be abl to first , initi the paramet valu somewhat randomli .
and then , we're go to take a guess of these z valu and all , which distribut ha been us to gener which word .
and the initi the paramet valu would allow us to have a complet specif of the mixtur model , which allow us to appli bay' rule to infer which distribut is more like to gener each word .
and thi predict essenti help us to separ word from the two distribut .
although we can't separ them for sure , but we can separ then probabilist as shown here .
so thi is inde a gener idea of the expect maxim , or em , algorithm .
so in all the em algorithm we introduc a hidden variabl to help us solv the problem more easili .
in our case the hidden variabl is a binari variabl for each occurr of a word .
and thi binari variabl would indic whether the word ha been gener from <num> sub d or <num> sub p .
and here we show some possibl valu of these variabl .
for exampl , for the it's from background , the z valu is on .
and text on the other hand .
is from the topic then it's zero for z , etc .
now , of cours , we don't observ these z valu , we just imagin thei're all such .
valu of z attach to other word .
and that's why we call these hidden variabl .
now , the idea that we talk about befor for predict the word distribut that ha been us when we gener the word is it a predictor , the valu of thi hidden variabl ?
and , so , the em algorithm then , would work as follow .
first , we'll initi all the paramet with random valu .
in our case , the paramet ar mainli the probabl .
of a word , given by theta sub d .
so thi is an initi addit stage .
these initi valu would allow us to us base roll to take a guess of these z valu , so we'd guess these valu .
we can't sai for sure whether textt is from background or not .
but we can have our guess .
thi is given by thi formula .
it's call an e step .
and so the algorithm would then try to us the e step to guess these z valu .
after that , it would then invok anoth that's call m step .
in thi step we simpli take advantag of the infer z valu and then just group word that ar in the same distribut like these from that ground includ thi as well .
we can then normal the count to estim the probabl or to revis our estim of the paramet .
so let me also illustr that we can group the word that ar believ to have come from zero sub d , and that's text , mine algorithm , for exampl , and cluster .
and we group them togeth to help us re estim the paramet that we're interest in .
so these will help us estim these paramet .
note that befor we just set these paramet valu randomli .
but with thi guess , we will have somewhat improv estim of thi .
of cours , we don't know exactli whether it's zero or on .
so we're not go to realli do the split in a hard wai .
but rather we're go to do a softer split .
and thi is what happen here .
so we're go to adjust the count by the probabl that would believ thi word ha been gener by us the theta sub d .
and you can see thi , where doe thi come from ?
well , thi ha come from here , right ?
from the e step .
so the em algorithm would iter improv uur initi estim of paramet by us e step first and then m step .
the e step is to augment the data with addit inform , like z .
and the m step is to take advantag of the addit inform to separ the data .
to split the data account and then collect the right data account to re estim our paramet .
and then onc we have a new gener of paramet , we're go to repeat thi .
we ar go the e step again .
to improv our estim of the hidden variabl .
and then that would lead to anoth gener of re estim paramet .
for the word distribut that we ar interest in .
okai , so , as i said , the bridg between the two is realli the variabl z , hidden variabl , which indic how like thi water is from the top water distribut , theta sub p .
so , thi slide ha a lot of content and you mai need to .
paus the reader to digest it .
but thi basic captur the essenc of em algorithm .
start with initi valu that ar often random themself .
and then we invok e step follow by m step to get an improv set of paramet .
and then we repeat thi , so thi a hill climb algorithm that would gradual improv the estim of paramet .
as i will explain later there is some guarante for reach a local maximum of the log likelihood function .
so let take a look at the comput for a specif case , so these formula ar the em .
formula that you see befor , and you can also see there ar superscript , here , like here , n , to indic the gener of paramet .
like here for exampl we have n plu on .
that mean we have improv .
from here to here we have an improv .
so in thi set we have assum the two numer have equal probabl and the background model is null .
so what ar the relev of the statist ?
well these ar the word count .
so assum we have just four word , and their count ar like thi .
and thi is our background model that assign high probabl to common word like the .
and in the first iter , you can pictur what will happen .
well first we initi all the valu .
so here , thi probabl that we're interest in is normal into a uniform distribut of all the word .
and then the e step would give us a guess of the distribut that ha been us .
that will gener each word .
we can see we have differ probabl for differ word .
why ?
well , that's becaus these word have differ probabl in the background .
so even though the two distribut ar equal like .
and then our initi audit sai uniform distribut becaus of the differ in the background of the distribut , we have differ guess the probabl .
so these word ar believ to be more like from the topic .
these on the other hand ar less like .
probabl from background .
so onc we have these z valu , we know in the m step these probabl will be us to adjust the count .
so four must be multipli by thi <num> . <num> in order to get the alloc account toward the topic .
and thi is done by thi multipl .
note that if our guess sai thi is <num> if thi is on point zero , then we just get the full count of thi word for thi topic .
in gener it's not go to be on point zero .
so we're just go to get some percentag of thi count toward thi topic .
then we simpli normal these count to have a new gener of paramet estim .
so you can see , compar thi with the older on , which is here .
so compar thi with thi on and we'll see the probabl is differ .
not onli that , we also see some word that ar believ to have come from the topic will have a higher probabl .
like thi on , text .
and of cours , thi new gener of paramet would allow us to further adjust the infer latent variabl or hidden variabl valu .
so we have a new gener of valu , becaus of the e step base on the new gener of paramet .
and these new infer valu of zs will give us then anoth gener of the estim of probabl of the word .
and so on and so forth so thi is what would actual happen when we comput these probabl us the em algorithm .
as you can see in the last row where we show the log likelihood , and the likelihood is increas as we do the iter .
and note that these log likelihood is neg becaus the probabl is between <num> and <num> when you take a logarithm , it becom a neg valu .
now what's also interest is , you'll note the last column .
and these ar the invert word split .
and these ar the probabl that a word is believ to have come from on distribut , in thi case the topic distribut , all right .
and you might wonder whether thi would be also us .
becaus our main goal is to estim these word distribut .
so thi is our primari goal .
we hope to have a more discrimin order of distribut .
but the last column is also bi product .
thi also can actual be veri us .
you can think about that .
we want to us , is to for exampl is to estim to what extent thi document ha cover background word .
and thi , when we add thi up or take the averag we will kind of know to what extent it ha cover background versu content wa that ar not explain well by the background .
so , i just show you that empir the likelihood will converg , but theoret it can also be prove that em algorithm will converg to a local maximum .
so here's just an illustr of what happen and a detail explan .
thi requir more knowledg about that , some of that inequ , that we haven't realli cover yet .
so here what you see is on the x dimens , we have a c0 valu .
thi is a paramet that we have .
on the y axi we see the likelihood function .
so thi curv is the origin likelihood function , and thi is the on that we hope to maxim .
and we hope to find a c0 valu at thi point to maxim thi .
but in the case of mitsumoto we can not easili find an analyt solut to the problem .
so , we have to resolv the numer error , and the em algorithm is such an algorithm .
it's a hill climb algorithm .
that would mean you start with some random guess .
let's sai you start from here , that's your start point .
and then you try to improv thi by move thi to anoth point where you can have a higher likelihood .
so that's the ideal hill climb .
and in the em algorithm , the wai we achiev thi is to do two thing .
first , we'll fix a lower bound of likelihood function .
so thi is the lower bound .
see here .
and onc we fit the lower bound , we can then maxim the lower bound .
and of cours , the reason why thi work , is becaus the lower bound is much easier to optim .
so we know our current guess is here .
and by maxim the lower bound , we'll move thi point to the top .
to here .
right ?
and we can then map to the origin likelihood function , we find thi point .
becaus it's a lower bound , we ar guarante to improv thi guess , right ?
becaus we improv our lower bound and then the origin likelihood curv which is abov thi lower bound will definit be improv as well .
so we alreadi know it's improv the lower bound .
so we definit improv thi origin likelihood function , which is abov thi lower bound .
so , in our exampl , the current guess is paramet valu given by the current gener .
and then the next guess is the re estim paramet valu .
from thi illustr you can see the next guess is alwai better than the current guess .
unless it ha reach the maximum , where it will be stuck there .
so the two would be equal .
so , the e step is basic to comput thi lower bound .
we don't directli just comput thi likelihood function but we comput the length of the variabl valu and these ar basic a part of thi lower bound .
thi help determin the lower bound .
the m step on the other hand is to maxim the lower bound .
it allow us to move paramet to a new point .
and that's why em algorithm is guarante to converg to a local maximum .
now , as you can imagin , when we have mani local maxima , we also have to repeat the em algorithm multipl time .
in order to figur out which on is the actual global maximum .
and thi actual in gener is a difficult problem in numer optim .
so here for exampl had we start from here , then we gradual just climb up to thi top .
so , that's not optim , and we'd like to climb up all the wai to here , so the onli wai to climb up to thi gear is to start from somewher here or here .
so , in the em algorithm , we gener would have to start from differ point or have some other wai to determin a good initi start point .
to summar in thi lectur we introduc the em algorithm .
thi is a gener algorithm for comput maximum maximum likelihood estim of all kind of model , so not just for our simpl model .
and it's a hill climb algorithm , so it can onli converg to a local maximum and it will depend on initi point .
the gener idea is that we will have two step to improv the estim of .
in the e step we roughli how mani there ar by predict valu of us hidden variabl that we would us to simplifi the estim .
in our case , thi is the distribut that ha been us to gener the word .
in the m step then we would exploit such augment data which would make it easier to estim the distribut , to improv the estim of paramet .
here improv is guarante in term of the likelihood function .
note that it's not necessari that we will have a stabl converg of paramet valu even though the likelihood function is ensur to increas .
there ar some properti that have to be satisfi in order for the paramet also to convert into some stabl valu .
now here data augment is done probabilist .
that mean , we're not go to just sai exactli what's the valu of a hidden variabl .
but we're go to have a probabl distribut over the possibl valu of these hidden variabl .
so thi caus a split of count of event probabilist .
and in our case we'll split the word count between the two distribut .
thi lectur is about probabilist and latent semant analysi or plsa .
in thi lectur we're go to introduc probabilist latent semant analysi , often call plsa .
thi is the most basic topic model , also on of the most us topic model .
now thi kind of model can in gener be us to mine multipl topic from text document .
and prsa is on of the most basic topic model for do thi .
so let's first examin thi power in the e mail for more detail .
here i show a sampl articl which is a blog articl about hurrican katrina .
and i show some simpl topic .
for exampl govern respons , flood of the citi of new orlean .
donat and the background .
you can see in the articl we us word from all these distribut .
so we first for exampl see there's a critic of govern respons and thi is follow by discuss of flood of the citi and donat et cetera .
we also see background word mix with them .
so the overal of topic analysi here is to try to decod these topic behind the text , to segment the topic , to figur out which word ar from which distribut and to figur out first , what ar these topic ?
how do we know there's a topic about govern respons .
there's a topic about a flood in the citi .
so these ar the task at the top of the model .
if we had discov these topic can color these word , as you see here , to separ the differ topic .
then you can do a lot of thing , such as summar , or segment , of the topic , cluster of the sentenc etc .
so the formal definit of problem of mine multipl topic from text is shown here .
and thi is after a slide that you have seen in an earlier lectur .
so the input is a collect , the number of topic , and a vocabulari set , and of cours the text data .
and then the output is of two kind .
on is the topic categori , character .
theta i's .
each theta i is a word distribut .
and second , it's the topic coverag for each document .
these ar pi  sub i j's .
and thei tell us which document it cover .
which topic to what extent .
so we hope to gener these as output .
becaus there ar mani us applic if we can do that .
so the idea of plsa is actual veri similar to the two compon mixtur model that we have alreadi introduc .
the onli differ is that we ar go to have more than two topic .
otherwis , it is essenti the same .
so here i illustr how we can gener the text that ha multipl topic and natur in all case of probabilist model would want to figur out the likelihood function .
so we would also ask the question , what's the probabl of observ a word from such a mixtur model ?
now if you look at thi pictur and compar thi with the pictur that we have seen earlier , you will see the onli differ is that we have ad more topic here .
so , befor we have just on topic , besid the background topic .
but now we have more topic .
specif , we have k topic now .
all these ar topic that we assum that exist in the text data .
so the consequ is that our switch for choos a topic is now a multiwai switch .
befor it's just a two wai switch .
we can think of it as flip a coin .
but now we have multipl wai .
first we can flip a coin to decid whether we're talk about the background .
so it's the background lambda sub b versu non background .
<num> minu lambda sub b give us the probabl of actual choos a non background topic .
after we have made thi decis , we have to make anoth decis to choos on of these k distribut .
so there ar k wai switch here .
and thi is character by pi  , and thi sum to on .
thi is just the differ of design .
which is a littl bit more complic .
but onc we decid which distribut to us the rest is the same we ar go to just gener a word by us on of these distribut as shown here .
so now let look at the question about the likelihood .
so what's the probabl of observ a word from such a distribut ?
what do you think ?
now we've seen thi problem mani time now and if you can recal , it's gener a sum .
of all the differ possibl of gener a word .
so let's first look at how the word can be gener from the background mode .
well , the probabl that the word is gener from the background model is lambda multipli by the probabl of the word from the background mode .
model , right .
two thing must happen .
first , we have to have chosen the background model , and that's the probabl of lambda , of sub b .
then second , we must have actual obtain the word w from the background , and that's probabl of w given theta sub b .
okai , so similarli , we can figur out the probabl of observ the word from anoth topic .
like the topic theta sub k .
now notic that here's the product of three term .
and that's becaus of the choic of topic theta sub k , onli happen if two thing happen .
on is we decid not to talk about background .
so , that's a probabl of <num> minu lambda sub b .
second , we also have to actual choos theta sub k among these k topic .
so that's probabl of theta sub k , or pi  .
and similarli , the probabl of gener a word from the second .
the topic and the first topic ar like what you ar see here .
and so in the end the probabl of observ the word is just a sum of all these case .
and i have to stress again thi is a veri import formula to know becaus thi is realli kei to understand all the topic model and inde a lot of mixtur model .
so make sure that you realli understand the probabl of w is inde the sum of these term .
so , next , onc we have the likelihood function , we would be interest in know the paramet .
all right , so to estim the paramet .
but firstli , let's put all these togeth to have the complet likelihood of function for plsa .
the first line show the probabl of a word as illustr on the previou slide .
and thi is an import formula as i said .
so let's take a closer look at thi .
thi actual command all the import paramet .
so first of all we see lambda sub b here .
thi repres a percentag of background word that we believ exist in the text data .
and thi can be a known valu that we set empir .
second , we see the background languag model , and typic we also assum thi is known .
we can us a larg collect of text , or us all the text that we have avail to estim the world of distribut .
now next in the next stop thi formula .
excus me .
you see two interest kind of paramet , those ar the most import paramet .
that we ar .
so on is pi's .
and these ar the coverag of a topic in the document .
and the other is word distribut that character all the topic .
so the next line , then is simpli to plug thi in to calcul the probabl of document .
thi is , again , of the familiar form where you have a sum and you have a count of a word in the document .
and then log of a probabl .
now it's a littl bit more complic than the two compon .
becaus now we have more compon , so the sum involv more term .
and then thi line is just the likelihood for the whole collect .
and it's veri similar , just account for more document in the collect .
so what ar the unknown paramet ?
i alreadi said that there ar two kind .
on is coverag , on is word distribut .
again , it's a us exercis for you to think about .
exactli how mani paramet there ar here .
how mani unknown paramet ar there ?
now , try and think out that question will help you understand the model in more detail .
and will also allow you to understand what would be the output that we gener when us plsa to analyz text data ?
and these ar precis the unknown paramet .
so after we have obtain the likelihood function shown here , the next is to worri about the paramet estim .
and we can do the usual think , maximum likelihood estim .
so again , it's a constrain optim problem , like what we have seen befor .
onli that we have a collect of text and we have more paramet to estim .
and we still have two constraint , two kind of constraint .
on is the word distribut .
all the word must have probabl that's sum to on for on distribut .
the other is the topic coverag distribut and a document will have to cover precis these k topic so the probabl of cover each topic that would have to sum to <num> .
so at thi point though it's basic a well defin appli math problem , you just need to figur out the solut to optim problem .
there's a function with mani variabl .
and we need to just figur out the pattern of these variabl to make the function reach it maximum .
we can comput thi maximum estim by us the em algorithm .
so in the e step , we now have to introduc more hidden variabl becaus we have more topic , so our hidden variabl z now , which is a topic indic can take more than two valu .
so specif will take a k plu on valu , with b in the note the background .
and onc locat , to denot other k topic , right .
so , now the e step , as you can recal is your augment data , and by predict the valu of the hidden variabl .
so we're go to predict for a word , whether the word ha come from on of these k plu on distribut .
thi equat allow us to predict the probabl that the word w in document d is gener from topic zero sub j .
and the bottom on is the predict probabl that thi word ha been gener from the background .
note that we us document d here to index the word .
why ?
becaus whether a word is from a particular topic actual depend on the document .
can you see why ?
well , it's through the pi's .
the pi's ar ti to each document .
each document can have potenti differ pi's , right .
the pi's will then affect our predict .
so , the pi's ar here .
and thi depend on the document .
and that might give a differ guess for a word in differ document , and that's desir .
in both case we ar us the bay's rule , as i explain , basic assess the likelihood of gener word from each of thi divis and there's normal .
what about the m step ?
well , we mai recal the m step is we take advantag of the infer z valu .
to split the count .
and then collect the right count to re estim the paramet .
so in thi case , we can re estim our coverag of probabl .
and thi is re estim base on collect all the word in the document .
and that's why we have the count of the word in document .
and sum over all the word .
and then we're go to look at to what extent thi word belong to the topic theta sub j .
and thi part is our guess from each step .
thi tell us how like thi word is actual from theta sub j .
and when we multipli them togeth , we get the discount count that's locat for topic theta sub j .
and when we normal thi over all the topic , we get the distribut of all the topic to indic the coverag .
and similarli , the bottom on is the estim probabl of word for a topic .
and in thi case we ar us exact the same count , you can see thi is the same discount account , it tell us to what extend we should alloc thi word but then normal is differ .
becaus in thi case we ar interest in the word distribut , so we simpli normal thi over all the word .
thi is differ , in contrast here we normal the amount all the topic .
it would be us to take a comparison between the two .
thi give us differ distribut .
and these tell us how to improv the paramet .
and as i just explain , in both the formula is we have a maximum estim base on alloc word count now thi phenomena is actual gener phenomena in all the em algorithm .
in the m step , you gener with the comput expect an account of the event base on the e step result , and then you just and then count to four , particular normal it , typic .
so , in term of comput of thi em algorithm , we can actual just keep account variou event and then normal them .
and when we think thi wai , we also have a more concis wai of present the em algorithm .
it actual help us better understand the formula .
so i'm go to go over thi in some detail .
so as a algorithm we first initi all the unknown perimet randomli , all right .
so , in our case , we ar interest in all of those coverag perimet , pi's and award distribut , and we just randomli normal them .
thi is the initi step and then we will repeat until likelihood converg .
now how do we know whether likelihood converg ?
we can do comput likelihood at each step and compar the current likelihood with the previou likelihood .
if it doesn't chang much and we're go to sai it stop , right .
so , in each step we're go to do e step and m step .
in the e step we're go to do augment the data by predict the hidden variabl .
in thi case , the hidden variabl , z sub d , w , indic whether the word w in d is from a topic or background .
and if it's from a topic , which topic .
so if you look at the e step formula , essenti we're actual normal these count , sorri , these probabl of observ the word from each distribut .
so you can see , basic the predict of word from topic zero sub j is base on the probabl of select that theta sub j as a word distribut to gener the word .
multipli by the probabl of observ the word from that distribut .
and i said it's proport to thi becaus in the implement of em algorithm you can keep counter for thi quantiti , and in the end it just normal it .
so the normal here is over all the topic and then you would get a probabl .
now , in the m step , we do the same , and we ar go to collect these .
alloc account for each topic .
and we split word among the topic .
and then we're go to normal them in differ wai to obtain the real estim .
so for exampl , we can normal among all the topic to get the re estim of pi  , the coverag .
or we can re normal base on all the word .
and that would give us a word distribut .
so it's us to think algorithm in thi wai becaus when implement , you can just us variabl , but keep track of these quantiti in each case .
and then you just normal these variabl to make them distribut .
now i did not put the constraint for thi on .
and i intention leav thi as an exercis for you .
and you can see , what's the normal for thi on ?
it's of a slightli differ form but it's essenti the same as the on that you have seen here in thi on .
so in gener in the envis of em algorithm you will see you accumul the count , variou count and then you normal them .
so to summar , we introduc the plsa model .
which is a mixtur model with k unigram languag model repres k topic .
and we also ad a pre determin background languag model to help discov discrimin topic , becaus thi background languag model can help attract the common term .
and we select the maximum estim that we cant discov topic knowledg from text data .
in thi case plsa allow us to discov two thing , on is k word distribut , each on repres a topic and the other is the proport of each topic in each document .
and such detail character of coverag of topic in document can enabl a lot of photo analysi .
for exampl , we can aggreg the document in the particular pan period to assess the coverag of a particular topic in a time period .
that would allow us to gener the tempor chain of topic .
we can also aggreg topic cover in document associ with a particular author and then we can categor the topic written by thi author , etc .
and in addit to thi , we can also cluster term and cluster document .
in fact , each topic can be regard as a cluster .
so we alreadi have the term cluster .
in the higher probabl , the word can be regard as belong to on cluster repres by the topic .
similarli , document can be cluster in the same wai .
we can assign a document to the topic cluster that's cover most in the document .
so rememb , pi's indic to what extent each topic is cover in the document , we can assign the document to the topic cluster that ha the highest pi  .
and in gener there ar mani us applic of thi techniqu .
thi lectur is about topic mine and analysi .
we're go to talk about us a term as topic .
thi is a slide that you have seen in a earlier lectur where we defin the task of topic mine and analysi .
we also rais the question , how do we exactli defin the topic of theta ?
so in thi lectur , we're go to offer on wai to defin it , and that's our initi idea .
our idea here is defin a topic simpli as a term .
a term can be a word or a phrase .
and in gener , we can us these term to describ topic .
so our first thought is just to defin a topic as on term .
for exampl , we might have term like sport , travel , or scienc , as you see here .
now if we defin a topic in thi wai , we can then analyz the coverag of such topic in each document .
here for exampl , we might want to discov to what extent document on cover sport .
and we found that <num> of the content of document on is about sport .
and <num> is about the travel , etc .
we might also discov document two doe not cover sport at all .
so the coverag is zero , etc .
so now , of cours , as we discuss in the task definit for topic mine and analysi , we have two task .
on is to discov the topic .
and the second is to analyz coverag .
so let's first think about how we can discov topic if we repres each topic by a term .
so that mean we need to mine k topic term from a collect .
now there ar , of cours , mani differ wai of do that .
and we're go to talk about a natur wai of do that , which is also like effect .
so first of all , we're go to pars the text data in the collect to obtain candid term .
here candid term can be word or phrase .
let's sai the simplest solut is to just take each word as a term .
these word then becom candid topic .
then we're go to design a score function to match how good each term is as a topic .
so how can we design such a function ?
well there ar mani thing that we can consid .
for exampl , we can us pure statist to design such a score function .
intuit , we would like to favor repres term , mean term that can repres a lot of content in the collect .
so that would mean we want to favor a frequent term .
howev , if we simpli us the frequenc to design the score function , then the highest score term would be gener term or function term like the , etc .
those term occur veri frequent english .
so we also want to avoid have such word on the top so we want to penal such word .
but in gener , we would like to favor term that ar fairli frequent but not so frequent .
so a particular approach could be base on tf idf weight from retriev .
and tf stand for term frequenc .
idf stand for invers document frequenc .
we talk about some of these idea in the lectur about the discoveri of word associ .
so these ar statist method , mean that the function is defin mostli base on statist .
so the score function would be veri gener .
it can be appli to ani languag , ani text .
but when we appli such a approach to a particular problem , we might also be abl to leverag some domain specif heurist .
for exampl , in new we might favor titl word actual gener .
we might want to favor titl word becaus the author tend to us the titl to describ the topic of an articl .
if we're deal with tweet , we could also favor hashtag , which ar invent to denot topic .
so natur , hashtag can be good candid for repres topic .
anywai , after we have thi design score function , then we can discov the k topic term by simpli pick k term with the highest score .
now , of cours , we might encount situat where the highest score term ar all veri similar .
thei're semant similar , or close relat , or even synonym .
so that's not desir .
so we also want to have coverag over all the content in the collect .
so we would like to remov redund .
and on wai to do that is to do a greedi algorithm , which is sometim call a maxim margin relev rank .
basic , the idea is to go down the list base on our score function and gradual take term to collect the k topic term .
the first term , of cours , will be pick .
when we pick the next term , we're go to look at what term have alreadi been pick and try to avoid pick a term that's too similar .
so while we ar consid the rank of a term in the list , we ar also consid the redund of the candid term with respect to the term that we alreadi pick .
and with some threshold , then we can get a balanc of the redund remov and also high score of a term .
okai , so after thi that will get k topic term .
and those can be regard as the topic that we discov from the connect .
next , let's think about how we're go to comput the topic coverag pi  sub ij .
so look at thi pictur , we have sport , travel and scienc and these topic .
and now suppos you ar give a document .
how should we pick out coverag of each topic in the document ?
well , on approach can be to simpli count occurr of these term .
so for exampl , sport might have occur four time in thi thi document and travel occur twice , etc .
and then we can just normal these count as our estim of the coverag probabl for each topic .
so in gener , the formula would be to collect the count of all the term that repres the topic .
and then simpli normal them so that the coverag of each topic in the document would add to on .
thi form a distribut of the topic for the document to character coverag of differ topic in the document .
now , as alwai , when we think about idea for solv problem , we have to ask the question , how good is thi on ?
or is thi the best wai of solv problem ?
so now let's examin thi approach .
in gener , we have to do some empir evalu by us actual data set and to see how well it work .
well , in thi case let's take a look at a simpl exampl here .
and we have a text document that's about a nba basketbal game .
so in term of the content , it's about sport .
but if we simpli count these word that repres our topic , we will find that the word sport actual did not occur in the articl , even though the content is about the sport .
so the count of sport is zero .
that mean the coverag of sport would be estim as zero .
now of cours , the term scienc also did not occur in the document and it's estim is also zero .
and that's okai .
but sport certainli is not okai becaus we know the content is about sport .
so thi estim ha problem .
what's wors , the term travel actual occur in the document .
so when we estim the coverag of the topic travel , we have got a non zero count .
so it estim coverag will be non zero .
so thi obvious is also not desir .
so thi simpl exampl illustr some problem of thi approach .
first , when we count what word belong to to the topic , we also need to consid relat word .
we can't simpli just count the topic word sport .
in thi case , it did not occur at all .
but there ar mani relat word like basketbal , game , etc .
so we need to count the relat word also .
the second problem is that a word like star can be actual ambigu .
so here it probabl mean a basketbal star , but we can imagin it might also mean a star on the sky .
so in that case , the star might actual suggest , perhap , a topic of scienc .
so we need to deal with that as well .
final , a main restrict of thi approach is that we have onli on term to describ the topic , so it cannot realli describ complic topic .
for exampl , a veri special topic in sport would be harder to describ by us just a word or on phrase .
we need to us more word .
so thi exampl illustr some gener problem with thi approach of treat a term as topic .
first , it lack express power .
mean that it can onli repres the simpl gener topic , but it cannot repres the complic topic that might requir more word to describ .
second , it's incomplet in vocabulari coverag , mean that the topic itself is onli repres as on term .
it doe not suggest what other term ar relat to the topic .
even if we're talk about sport , there ar mani term that ar relat .
so it doe not allow us to easili count relat term to order , convers to coverag of thi topic .
final , there is thi problem of word sens disintegr .
a topic term or relat term can be ambigu .
for exampl , basketbal star versu star in the sky .
so in the next lectur , we're go to talk about how to solv the problem with of a topic .
thi lectur is about probabilist topic model for topic mine and analysi .
in thi lectur , we're go to continu talk about the topic mine and analysi .
we're go to introduc probabilist topic model .
so thi is a slide that you have seen earlier , where we discuss the problem with us a term as a topic .
so , to solv these problem intuit we need to us more word to describ the topic .
and thi will address the problem of lack of express power .
when we have more word that we can us to describ the topic , that we can describ complic topic .
to address the second problem we need to introduc weight on word .
thi is what allow you to distinguish subtl differ in topic , and to introduc semant relat word in a fuzzi manner .
final , to solv the problem of word ambigu , we need to split ambigu word , so that we can disambigu it topic .
it turn out that all these can be done by us a probabilist topic model .
and that's why we're go to spend a lot of lectur to talk about thi topic .
so the basic idea here is that , improv the replant of topic as on distribut .
so what you see now is the older replant .
where we replant each topic , it wa just on word , or on term , or on phrase .
but now we're go to us a word distribut to describ the topic .
so here you see that for sport .
we're go to us the word distribut over theoret speak all the word in our vocabulari .
so for exampl , the high probabl word here ar sport , game , basketbal , footbal , plai , star , etc .
these ar sport relat term .
and of cours it would also give a non zero probabl to some other word like troubl which might be relat to sport in gener , not so much relat to topic .
in gener we can imagin a non zero probabl for all the word .
and some word that ar not read and would have veri , veri small probabl .
and these probabl will sum to on .
so that it form a distribut of all the word .
now intuit , thi distribut repres a topic in that if we assembl word from the distribut , we tend to see word that ar readi to dispos .
you can also see , as a veri special case , if the probabl of the mass is concentr in entir on just on word , it's sport .
and thi basic degener to the symbol foundat of a topic wa just on word .
but as a distribut , thi topic of represent can , in gener , involv mani word to describ a topic and can model sever differ in semant of a topic .
similarli we can model travel and scienc with their respect distribut .
in the distribut for travel we see top word like attract , trip , flight etc .
wherea in scienc we see scientist , spaceship , telescop , or genom , and , you know , scienc relat term .
now that doesn't mean sport relat term will necessarili have zero probabl for scienc .
in gener we can imagin all of these word we have now zero probabl .
it's just that for a particular topic in some word we have veri , veri small probabl .
now you can also see there ar some word that ar share by these topic .
when i sai share it just mean even with some probabl threshold , you can still see on word occur much more topic .
in thi case i mark them in black .
so you can see travel , for exampl , occur in all the three topic here , but with differ probabl .
it ha the highest probabl for the travel topic , <num> . <num> .
but with much smaller probabl for sport and scienc , which make sens .
and similarli , you can see a star also occur in sport and scienc with reason high probabl .
becaus thei might be actual relat to the two topic .
so with thi replant it address the three problem that i mention earlier .
first , it now us multipl word to describ a topic .
so it allow us to describ a fairli complic topic .
second , it assign weight to term .
so now we can model sever differ of semant .
and you can bring in relat word togeth to model a topic .
third , becaus we have probabl for the same word in differ topic , we can disintegr the sens of word .
in the text to decod it's underli topic , to address all these three problem with thi new wai of repres a topic .
so now of cours our problem definit ha been refin just slightli .
the slight is veri similar to what you've seen befor except we have ad refin for what our topic is .
now each topic is word distribut , and for each word distribut we know that all the probabl should sum to on with all the word in the vocabulari .
so you see a constraint here .
and we still have anoth constraint on the topic coverag , name pi .
so all the pi  sub ij's must sum to on for the same document .
so how do we solv thi problem ?
well , let's look at thi problem as a comput problem .
so we clearli specifi it's input and output and illustr it here on thi side .
input of cours is our text data .
c is our collect but we also gener assum we know the number of topic , k .
or we hypothes a number and then try to bind k topic , even though we don't know the exact topic that exist in the collect .
and v is the vocabulari that ha a set of word that determin what unit would be treat as the basic unit for analysi .
in most case we'll us word as the basi for analysi .
and that mean each word is a uniqu .
now the output would consist of as first a set of topic repres by theta i's .
each theta i is a word distribut .
and we also want to know the coverag of topic in each document .
so that's .
that the same pi  ij that we have seen befor .
so given a set of text data we would like comput all these distribut and all these coverag as you have seen on thi slide .
now of cours there mai be mani differ wai of solv thi problem .
in theori , you can write the program to solv thi problem , but here we're go to introduc a gener wai of solv thi problem call a gener model .
and thi is , in fact , a veri gener idea and it's a principl wai of us statist model to solv text mine problem .
and here i dim the pictur that you have seen befor in order to show the gener process .
so the idea of thi approach is actual to first design a model for our data .
so we design a probabilist model to model how the data ar gener .
of cours , thi is base on our assumpt .
the actual data aren't necessarili gener thi wai .
so that gave us a probabl distribut of the data that you ar see on thi slide .
given a particular model and paramet that ar denot by lambda .
so thi templat of actual consist of all the paramet that we're interest in .
and these paramet in gener will control the behavior of the probabl risk model .
mean that if you set these paramet with differ valu and it will give some data point higher probabl than other .
now in thi case of cours , for our text mine problem or more precis topic mine problem we have the follow plan .
first of all we have theta i's which is a word distribut snd then we have a set of pi for each document .
and sinc we have n document , so we have n set of pi , and each set the pi  up .
the pi  valu will sum to on .
so thi is to sai that we first would pretend we alreadi have these word distribut and the coverag number .
and then we can see how we can gener data by us such distribut .
so how do we model the data in thi wai ?
and we assum that the data ar actual symbol drawn from such a model that depend on these paramet .
now on interest question here is to think about how mani paramet ar there in total ?
now obvious we can alreadi see n multipli by k paramet .
for pi's .
we also see k theta i's .
but each theta i is actual a set of probabl valu , right ?
it's a distribut of word .
so i leav thi as an exercis for you to figur out exactli how mani paramet there ar here .
now onc we set up the model then we can fit the model to our data .
mean that we can estim the paramet or infer the paramet base on the data .
in other word we would like to adjust these paramet valu .
until we give our data set the maximum probabl .
i just said , depend on the paramet valu , some data point will have higher probabl than other .
what we're interest in , here , is what paramet valu will give our data set the highest probabl ?
so i also illustr the problem with a pictur that you see here .
on the x axi i just illustr lambda , the paramet , as a on dimension variabl .
it's oversimplif , obvious , but it suffic to show the idea .
and the y axi show the probabl of the data , observ .
thi probabl obvious depend on thi set of lambda .
so that's why it vari as you chang the valu of lambda .
what we're interest here is to find the lambda star .
that would maxim the probabl of the observ data .
so thi would be , then , our estim of the paramet .
and these paramet , note that ar precis what we hope to discov from text data .
so we'd treat these paramet as actual the outcom or the output of the data mine algorithm .
so thi is the gener idea of us a gener model for text mine .
first , we design a model with some paramet valu to fit the data as well as we can .
after we have fit the data , we will recov some paramet valu .
we will us the specif paramet valu and those would be the output of the algorithm .
and we'll treat those as actual the discov knowledg from text data .
by vari the model of cours we can discov differ knowledg .
so to summar , we introduc a new wai of repres topic , name repres as word distribut and thi ha the advantag of us multipl word to describ a complic topic . it also allow us to assign weight on word so we have more than sever variat of semant .
we talk about the task of topic mine , and answer .
when we defin a topic as distribut .
so the import is a clash of text articl and a number of topic and a vocabulari set and the output is a set of topic .
each is a word distribut and also the coverag of all the topic in each document .
and these ar formal repres by theta i's and pi  i's .
and we have two constraint here for these paramet .
the first is the constraint on the word distribut .
in each word distribut the probabl of all the word must sum to <num> , all the word in the vocabulari .
the second constraint is on the topic coverag in each document .
a document is not allow to recov a topic outsid of the set of topic that we ar discov .
so , the coverag of each of these k topic would sum to on for a document .
we also introduc a gener idea of us a gener model for text mine .
and the idea here is , first we're design a model to model the gener of data .
we simpli assum that thei ar gener in thi wai .
and insid the model we emb some paramet that we're interest in denot by lambda .
and then we can infer the most like paramet valu lambda star , given a particular data set .
and we can then take the lambda star as knowledg discov from the text for our problem .
and we can adjust the design of the model and the paramet to discov variou kind of knowledg from text .
as you will see later in the other lectur .
thi lectur is about the overview of statist languag model , which cover proper model as special case .
in thi lectur we're go to give a overview of static languag model .
these model ar gener model that cover probabilist topic model as a special case .
so first off , what is a statist languag model ?
a statist languag model is basic a probabl distribut over word sequenc .
so , for exampl , we might have a distribut that give , todai is wednesdai a probabl of . <num> .
it might give todai wednesdai is , which is a non grammat sentenc , a veri , veri small probabl as shown here .
and similarli anoth sentenc , the eigenvalu is posit might get the probabl of . <num> .
so as you can see such a distribut clearli is context depend .
it depend on the context of discuss .
some word sequenc might have higher probabl than other but the same sequenc of word might have differ probabl in differ context .
and so thi suggest that such a distribut can actual categor topic such a model can also be regard as probabilist mechan for gener text .
and that just mean we can view text data as data observ from such a model .
for thi reason , we call such a model as gener model .
so , now given a model we can then assembl sequenc of word .
so , for exampl , base on the distribut that i have shown here on thi slide , when matter it sai assembl a sequenc like todai is wednesdai becaus it ha a rel high probabl .
we might often get such a sequenc .
we might also get the item valu as posit sometim with a smaller probabl and veri , veri occasion we might get todai is wednesdai becaus it's probabl is so small .
so in gener , in order to categor such a distribut we must specifi probabl valu for all these differ sequenc of word .
obvious , it's imposs to specifi that becaus it's imposs to enumer all of the possibl sequenc of word .
so in practic , we will have to simplifi the model in some wai .
so , the simplest languag model is call the unigram languag model .
in such a case , it wa simpli a the text is gener by gener each word independ .
but in gener , the word mai not be gener independ .
but after we make thi assumpt , we can significantli simplifi the languag more .
basic , now the probabl of a sequenc of word , w1 through wn , will be just the product of the probabl of each word .
so for such a model , we have as mani paramet as the number of word in our vocabulari .
so here we assum we have n word , so we have n probabl .
on for each word .
and then some to <num> .
so , now we assum that our text is a sampl drawn accord to thi word distribut .
that just mean , we're go to draw a word each time and then eventu we'll get a text .
so for exampl , now again , we can try to assembl word accord to a distribut .
we might get wednesdai often or todai often .
and some other word like eigenvalu might have a small probabl , etcetera .
but with thi , we actual can also comput the probabl of everi sequenc , even though our model onli specifi the probabl of word .
and thi is becaus of the independ .
so specif , we can comput the probabl of todai is wednesdai .
becaus it's just a product of the probabl of todai , the probabl of is , and probabl of wednesdai .
for exampl , i show some fake number here and when you multipli these number togeth you get the probabl that todai's wednesdai .
so as you can see , with n probabl , on for each word , we actual can character the probabl situat over all kind of sequenc of word .
and so , thi is a veri simpl model .
ignor the word order .
so it mai not be , in fact , in some problem , such as for speech recognit , where you mai care about the order of word .
but it turn out to be quit suffici for mani task that involv topic analysi .
and that's also what we're interest in here .
so when we have a model , we gener have two problem that we can think about .
on is , given a model , how like ar we to observ a certain kind of data point ?
that is , we ar interest in the sampl process .
the other is the estim process .
and that , is to think of the paramet of a model given , some observ the data and we're go to talk about that in a moment .
let's first talk about the sampl .
so , here i show two exampl of water distribut or unigram languag model .
the first on ha higher probabl for word like a text mine associ , it's separ .
now thi signal a topic about text mine becaus when we assembl word from such a distribut , we tend to see word that often occur in text mine contest .
so in thi case , if we ask the question about what is the probabl of gener a particular document .
then , we like will see text that look like a text mine paper .
of cours , the text that we gener by draw word .
thi distribut is unlik coher .
although , the probabl of gener attack mine publish in the top confer is non zero assum that no word ha a zero probabl in the distribut .
and that just mean , we can essenti gener all kind of text document includ veri meaning text document .
now , the second distribut show , on the bottom , ha differ than what wa high probabl .
so food healthi , etcetera .
so thi clearli indic a differ topic .
in thi case it's probabl about health .
so if we sampl a word from such a distribut , then the probabl of observ a text mine paper would be veri , veri small .
on the other hand , the probabl of observ a text that look like a food nutrit paper would be high , rel higher .
so that just mean , given a particular distribut , differ than the text .
now let's look at the estim problem now .
in thi case , we're go to assum that we have observ the data .
i will know exactli what the text data look like .
in thi case , let's assum we have a text mine paper .
in fact , it's abstract of the paper , so the total number of word is <num> .
and i've shown some count of individu word here .
now , if we ask the question , what is the most like languag model that ha been us to gener thi text data ?
assum that the text is observ from some languag model , what's our best guess of thi languag model ?
okai , so the problem now is just to estim the probabl of these word .
as i've shown here .
so what do you think ?
what would be your guess ?
would you guess text ha a veri small probabl , or a rel larg probabl ?
what about queri ?
well , your guess probabl would be depend on how mani time we have observ thi word in the text data , right ?
and if you think about it for a moment .
and if you ar like mani other , you would have guess that , well , text ha a probabl of <num> out of <num> becaus i've observ the text <num> time in the text that ha a total of <num> word .
and similarli , mine ha <num> out of <num> .
and queri ha a rel small probabl , just observ for onc .
so it's <num> out of <num> .
right , so that , intuit , is a reason guess .
but the question is , is thi our best guess or best estim of the paramet ?
of cours , in order to answer thi question , we have to defin what do we mean by best , in thi case , it turn out that our guess ar inde the best .
in some sens and thi is call maximum likelihood estim .
and it's the best thing that , it will give the observ data our maximum probabl .
mean that , if you chang the estim somehow , even slightli , then the probabl of the observ text data will be somewhat smaller .
and thi is call a maximum likelihood estim .
so now let's talk about the problem a littl bit more , and specif let's talk about the two differ wai of estim the paramet .
on is call the maximum likelihood estim that i alreadi just mention .
the other is bayesian estim .
so in maximum likelihood estim , we defin best as mean the data likelihood ha reach the maximum .
so formal it's given by thi express here , where we defin the estim as a arg max of the probabl of x given theta .
so , arg max here just mean it actual a function that will turn .
the argument that give the function maximum valu , add the valu .
so the valu of arg max is not the valu of thi function .
but rather , the argument that ha made it the function reach maximum .
so in thi case the valu of arg max is theta .
it's the theta that make the probabl of x , given theta , reach it's maximum .
so thi estim that in due it also make sens and it's often veri us , and it seek the premis that best explain the data .
but it ha a problem , when the data is too small becaus when the data point ar too small , there ar veri few data point .
the sampl is small , then if we trust data in entir and try to fit the data and then we'll be bias .
so in the case of text data , let's sai , all observ <num> word did not contain anoth word relat to text mine .
now , our maximum likelihood estim will give that word a zero probabl .
becaus give the non zero probabl would take awai probabl mass from some observ word .
which obvious is not optim in term of maxim the likelihood of the observ data .
but thi zero probabl for all the unseen word mai not be reason sometim .
especi , if we want the distribut to character the topic of text mine .
so on wai to address thi problem is actual to us bayesian estim , where we actual would look at the both the data , and our prior knowledg about the paramet .
we assum that we have some prior belief about the paramet .
now in thi case of cours , so we ar not go to look at just the data , but also look at the prior .
so the prior here is defin by p of theta , and thi mean , we will impos some prefer on certain theta's of other .
and by us bay rule , that i have shown here , we can then combin the likelihood function .
with the prior to give us thi posterior probabl of the paramet .
now , a full explan of bay rule , and some of these thing relat to bayesian reason , would be outsid the scope of thi cours .
but i just gave a brief introduct becaus thi is gener knowledg that might be us to you .
the bay rule is basic defin here , and allow us to write down on condit probabl of x given y in term of the condit probabl of y given x .
and you can see the two probabl ar differ in the order of the two variabl .
but often the rule is us for make infer of the variabl , so let's take a look at it again .
we can assum that p x encod our prior belief about x .
that mean befor we observ ani other data , that's our belief about x , what we believ some x valu have higher probabl than other .
and thi probabl of x given y is a condit probabl , and thi is our posterior belief about x .
becaus thi is our belief about x valu after we have observ the y .
given that we have observ the y , now what do we believ about x ?
now , do we believ some valu have higher probabl than other ?
now the two probabl ar relat through thi on , thi can be regard as the probabl of the observ evid y , given a particular x .
so you can think about x as our hypothesi , and we have some prior belief about which hypothesi to choos .
and after we have observ y , we will updat our belief and thi updat formula is base on the combin of our prior .
and the likelihood of observ thi y if x is inde true , so much for detour about bay rule .
in our case , what we ar interest in is infer the theta valu .
so , we have a prior here that includ our prior knowledg about the paramet .
and then we have the data likelihood here , that would tell us which paramet valu can explain the data well .
the posterior probabl combin both of them , so it repres a compromis of the the two prefer .
and in such a case , we can maxim thi posterior probabl .
to find thi theta that would maxim thi posterior probabl , and thi estim is call a maximum a posteriori , or map estim .
and thi estim is a more gener estim than the maximum likelihood estim .
becaus if we defin our prior as a noninform prior , mean that it's uniform over all the theta valu .
no prefer , then we basic would go back to the maximum likelihood estim .
becaus in such a case , it's mainli go to be determin by thi likelihood valu , the same as here .
but if we have some not inform prior , some bia toward the differ valu then map estim can allow us to incorpor that .
but the problem here of cours , is how to defin the prior .
there is no free lunch and if you want to solv the problem with more knowledg , we have to have that knowledg .
and that knowledg , ideal , should be reliabl .
otherwis , your estim mai not necessarili be more accur than that maximum likelihood estim .
so , now let's look at the bayesian estim in more detail .
so , i show the theta valu as just a on dimens valu and that's a simplif of cours .
and so , we're interest in which variabl of theta is optim .
so now , first we have the prior .
the prior tell us that some of the variabl ar more like the other would believ .
for exampl , these valu ar more like than the valu over here , or here , or other place .
so thi is our prior , and then we have our theta likelihood .
and in thi case , the theta also tell us which valu of theta ar more like .
and that just mean loos syllabl can best expand our theta .
and then when we combin the two we get the posterior distribut , and that's just a compromis of the two .
it would sai that it's somewher in between .
so , we can now look at some interest point that is made of .
thi point repres the mode of prior , that mean the most like paramet valu accord to our prior , befor we observ ani data .
thi point is the maximum likelihood estim , it repres the theta that give the theta of maximum probabl .
now thi point is interest , it's the posterior mode .
it's the most like valu of the theta given by the posterior of thi .
and it repres a good compromis of the prior mode and the maximum likelihood estim .
now in gener in bayesian infer , we ar interest in the distribut of all these paramet addit as you see here .
if there's a distribut over see how valu that you can see .
here , p of theta given x .
so the problem of bayesian infer is to infer thi posterior , thi regim , and also to infer other interest quantiti that might depend on theta .
so , i show f of theta here as an interest variabl that we want to comput .
but in order to comput thi valu , we need to know the valu of theta .
in bayesian infer , we treat theta as an uncertain variabl .
so we think about all the possibl variabl of theta .
therefor , we can estim the valu of thi function f as extract valu of f , accord to the posterior distribut of theta , given the observ evid x .
as a special case , we can assum f of theta is just equal to theta .
in thi case , we get the expect valu of the theta , that's basic the posterior mean .
that give us also on point of theta , and it's sometim the same as posterior mode , but it's not alwai the same .
so , it give us anoth wai to estim the paramet .
so , thi is a gener illustr of bayesian estim and it an influenc .
and later , you will see thi can be us for topic mine where we want to inject the sum prior knowledg about the topic .
so to summar , we've us the languag model which is basic probabl distribut over text .
it's also call a gener model for text data .
the simplest languag model is unigram languag model , it's basic a word distribut .
we introduc the concept of likelihood function , which is the probabl of the a data given some model .
and thi function is veri import , given a particular set of paramet valu thi function can tell us which x , which data point ha a higher likelihood , higher probabl .
given a data sampl x , we can us thi function to determin which paramet valu would maxim the probabl of the observ data , and thi is the maximum livelihood estim .
we also talk about the bayesian estim or infer .
in thi case we , must defin a prior on the paramet p of theta .
and then we're interest in comput the posterior distribut of the paramet , which is proport to the prior and the likelihood .
and thi distribut would allow us then to infer ani deriv that is from theta .
thi lectur is a continu discuss of probabilist topic model .
in thi lectur , we're go to continu discuss probabilist model .
we're go to talk about a veri simpl case where we ar interest in just mine on topic from on document .
so in thi simpl setup , we ar interest in analyz on document and try to discov just on topic .
so thi is the simplest case of topic model .
the input now no longer ha k , which is the number of topic becaus we know there is onli on topic and the collect ha onli on document , also .
in the output , we also no longer have coverag becaus we assum that the document cover thi topic <num> .
so the main goal is just to discov the world of probabl for thi singl topic , as shown here .
as alwai , when we think about us a gener model to solv such a problem , we start with think about what kind of data we ar go to model or from what perspect we're go to model the data or data represent .
and then we're go to design a specif model for the gener of the data , from our perspect .
where our perspect just mean we want to take a particular angl of look at the data , so that the model will have the right paramet for discov the knowledg that we want .
and then we'll be think about the microfunct or write down the microfunct to captur more formal how like a data point will be obtain from thi model .
and the likelihood function will have some paramet in the function .
and then we argu our interest in estim those paramet for exampl , by maxim the likelihood which will lead to maximum likelihood estim .
these estim paramet will then becom the output of the mine hour , which mean we'll take the estim paramet as the knowledg that we discov from the text .
so let's look at these step for thi veri simpl case .
later we'll look at thi procedur for some more complic case .
so our data , in thi case is , just a document which is a sequenc of word .
each word here is denot by x sub i .
our model is a unigram languag model .
a word distribut that we hope to denot a topic and that's our goal .
so we will have as mani paramet as mani word in our vocabulari , in thi case m .
and for conveni we're go to us theta sub i to denot the probabl of word w sub i .
and obvious these theta sub i's will sum to <num> .
now what doe a likelihood function look like ?
well , thi is just the probabl of gener thi whole document , that given such a model .
becaus we assum the independ in gener each word so the probabl of the document will be just a product of the probabl of each word .
and sinc some word might have repeat occurr .
so we can also rewrit thi product in a differ form .
so in thi line , we have rewritten the formula into a product over all the uniqu word in the vocabulari , w sub <num> through w sub m .
now thi is differ from the previou line .
well , the product is over differ posit of word in the document .
now when we do thi transform , we then would need to introduc a counter function here .
thi denot the count of word on in document and similarli thi is the count of word of n in the document becaus these word might have repeat occurr .
you can also see if a word did not occur in the document .
it will have a zero count , therefor that correspond term will disappear .
so thi is a veri us form of write down the likelihood function that we will often us later .
so i want you to pai attent to thi , just get familiar with thi notat .
it's just to chang the product over all the differ word in the vocabulari .
so in the end , of cours , we'll us theta sub i to express thi likelihood function and it would look like thi .
next , we're go to find the theta valu or probabl of these word that would maxim thi likelihood function .
so now let take a look at the maximum likelihood estim problem more close .
thi line is copi from the previou slide .
it's just our likelihood function .
so our goal is to maxim thi likelihood function .
we will find it often easi to maxim the local likelihood instead of the origin likelihood .
and thi is pure for mathemat conveni becaus after the logarithm transform our function will becom a sum instead of product .
and we also have constraint over these these probabl .
the sum make it easier to take deriv , which is often need for find the optim solut of thi function .
so pleas take a look at thi sum again , here .
and thi is a form of a function that you will often see later also , the more gener topic model .
so it's a sum over all the word in the vocabulari .
and insid the sum there is a count of a word in the document .
and thi is macro by the logarithm of a probabl .
so let's see how we can solv thi problem .
now at thi point the problem is pure a mathemat problem becaus we ar go to just the find the optim solut of a constrain maxim problem .
the object function is the likelihood function and the constraint is that all these probabl must sum to on .
so , on wai to solv the problem is to us lagrang multipli approac .
now thi command is beyond the scope of thi cours but sinc lagrang multipli is a veri us approach , i also would like to just give a brief introduct to thi , for those of you who ar interest .
so in thi approach we will construct a lagrang function , here .
and thi function will combin our object function with anoth term that encod our constraint and we introduc lagrang multipli here , lambda , so it's an addit paramet .
now , the idea of thi approach is just to turn the constraint optim into , in some sens , an unconstrain optim problem .
now we ar just interest in optim thi lagrang function .
as you mai recal from calculu , an optim point would be achiev when the deriv is set to zero .
thi is a necessari condit .
it's not suffici , though .
so if we do that you will see the partial deriv , with respect to theta i here , is equal to thi .
and thi part come from the deriv of the logarithm function and thi lambda is simpli taken from here .
and when we set it to zero we can easili see theta sub i is relat to lambda in thi wai .
sinc we know all the theta i's must a sum to on we can plug thi into thi constraint , here .
and thi will allow us to solv for lambda .
and thi is just a net sum of all the count .
and thi further allow us to then solv the optim problem , eventu , to find the optim set for theta sub i .
and if you look at thi formula it turn out that it's actual veri intuit becaus thi is just the normal count of these word by the document ns , which is also a sum of all the count of word in the document .
so , after all thi mess , after all , we have just obtain someth that's veri intuit and thi will be just our intuit where we want to maxim the data by assign as much probabl mass as possibl to all the observ the word here .
and you might also notic that thi is the gener result of maximum likelihood rais estim .
in gener , the estim would be to normal count and it's just sometim the count have to be done in a particular wai , as you will also see later .
so thi is basic an analyt solut to our optim problem .
in gener though , when the likelihood function is veri complic , we're not go to be abl to solv the optim problem by have a close form formula .
instead we have to us some numer algorithm and we're go to see such case later , also .
so if you imagin what would we get if we us such a maximum likelihood estim to estim on topic for a singl document d here ?
let's imagin thi document is a text mine paper .
now , what you might see is someth that look like thi .
on the top , you will see the high probabl word tend to be those veri common word , often function word in english .
and thi will be follow by some content word that realli character the topic well like text , mine , etc .
and then in the end , you also see there is more probabl of word that ar not realli relat to the topic but thei might be extran mention in the document .
as a topic represent , you will see thi is not ideal , right ?
that becaus the high probabl word ar function word , thei ar not realli character the topic .
so my question is how can we get rid of such common word ?
now thi is the topic of the next modul .
we're go to talk about how to us probabilist model to somehow get rid of these common word .
thi lectur is about the mixtur of unigram languag model .
in thi lectur we will continu discuss probabilist topic model .
in particular , what we introduc a mixtur of unigram languag model .
thi is a slide that you have seen earlier .
where we talk about how to get rid of the background word that we have on top of for on document .
so if you want to solv the problem , it would be us to think about why we end up have thi problem .
well , thi obvious becaus these word ar veri frequent in our data and we ar us a maximum likelihood to estim .
then the estim obvious would have to assign high probabl for these word in order to maxim the likelihood .
so , in order to get rid of them that would mean we'd have to do someth differ here .
in particular we'll have to sai thi distribut doesn't have to explain all the word in the tax data .
what were go to sai is that , these common word should not be explain by thi distribut .
so on natur wai to solv the problem is to think about us anoth distribut to account for just these common word .
thi wai , the two distribut can be mix togeth to gener the text data .
and we'll let the other model which we'll call background topic model to gener the common word .
thi wai our target topic theta here will be onli gener the common handl word that ar characteris the content of the document .
so , how doe thi work ?
well , it is just a small modif of the previou setup where we have just on distribut .
sinc we now have two distribut , we have to decid which distribut to us when we gener the word .
each word will still be a sampl from on of the two distribut .
text data is still gener the same wai .
name , look at the gener of the on word at each time and eventu we gener a lot of word .
when we gener the word , howev , we're go to first decid which of the two distribut to us .
and thi is control by anoth probabl , the probabl of theta sub d and the probabl of theta sub b here .
so thi is a probabl of enact the topic word of distribut .
thi is the probabl of enact the background word of distribut denot by theta sub b .
on thi case i just give exampl where we can set both to <num> .
so you're go to basic flip a coin , a fair coin , to decid what you want to us .
but in gener these probabl don't have to be equal .
so you might bia toward us on topic more than the other .
so now the process of gener a word would be to first we flip a coin .
base on these probabl choos each model and if let's sai the coin show up as head , which mean we're go to us the topic two word distribut .
then we're go to us thi word distribut to gener a word .
otherwis we might be go slow thi path .
and we're go to us the background word distribut to gener a word .
so in such a case , we have a model that ha some uncertainti associ with the us of a word distribut .
but we can still think of thi as a model for gener text data .
and such a model is call a mixtur model .
so now let's see .
in thi case , what's the probabl of observ a word w ?
now here i show some word .
like the and text .
so as in all case , onc we setup a model we ar interest in comput the likelihood function .
the basic question is , so what's the probabl of observ a specif word here ?
now we know that the word can be observ from each of the two distribut , so we have to consid two case .
therefor it's a sum over these two case .
the first case is to us the topic for the distribut to gener the word .
and in such a case then the probabl would be theta sub d , which is the probabl of choos the model multipli by the probabl of actual observ the word from that model .
both event must happen in order to observ .
we first must have choos the topic theta sub d and then , we also have to actual have sampl the word the from the distribut .
and similarli , the second part account for a differ wai of gener the word from the background .
now obvious the probabl of text the same is all similar , right ?
so we also can see the two wai of gener the text .
and in each case , it's a product of the probabl of choos a particular word is multipli by the probabl of observ the word from that distribut .
now whether you will see , thi is actual a gener form .
so might want to make sure that you have realli understood thi express here .
and you should convinc yourself that thi is inde the probabl of obsolet text .
so to summar what we observ here .
the probabl of a word from a mixtur model is a gener sum of differ wai of gener the word .
in each case , it's a product of the probabl of select that compon model .
multipli by the probabl of actual observ the data point from that compon of the model .
and thi is someth quit gener and you will see thi occur often later .
so the basic idea of a mixtur model is just to retriev thesetwo distribut togeth as on model .
so i us a box to bring all these compon togeth .
so if you view thi whole box as on model , it's just like ani other gener model .
it would just give us the probabl of a word .
but the wai that determin thi probabl is quit the differ from when we have just on distribut .
and thi is basic a more complic mixtur model .
so the more complic is more than just on distribut .
and it's call a mixtur model .
so as i just said we can treat thi as a gener model .
and it's often us to think of just as a likelihood function .
the illustr that you have seen befor , which is dimmer now , is just the illustr of thi gener model .
so mathemat , thi model is noth but to just defin the follow gener model .
where the probabl of a word is assum to be a sum over two case of gener the word .
and the form you ar see now is a more gener form that what you have seen in the calcul earlier .
well i just us the symbol w to denot ani water but you can still see thi is basic first a sum .
right ?
and thi sum is due to the fact that the water can be gener in much more wai , two wai in thi case .
and insid a sum , each term is a product of two term .
and the two term ar first the probabl of select a compon like of d second , the probabl of actual observ the word from thi compon of the model .
so thi is a veri gener descript of all the mixtur model .
i just want to make sure that you understand thi becaus thi is realli the basi for understand all kind of on top model .
so now onc we setup model .
we can write down that like function as we see here .
the next question is , how can we estim the paramet , or what to do with the paramet .
given the data .
well , in gener , we can us some of the text data to estim the model paramet .
and thi estim would allow us to discov the interest knowledg about the text .
so you , in thi case , what do we discov ?
well , these ar present by our paramet and we will have two kind of paramet .
on is the two word distribut , that result in topic , and the other is the coverag of each topic in each .
the coverag of each topic .
and thi is determin by probabl of c less of d and probabl of theta , so thi is to on .
now , what's interest is also to think about special case like when we send on of them to want what would happen ?
well with the other , with the zero right ?
and if you look at the likelihood function , it will then degener to the special case of just on distribut .
okai so you can easili verifi that by assum on of these two is <num> and the other is zero .
so in thi sens , the mixtur model is more gener than the previou model where we have just on distribut .
it can cover that as a special case .
so to summar , we talk about the mixtur of two unigram languag model and the data we're consid here is just on document .
and the model is a mixtur model with two compon , two unigram lm model , specif theta sub d , which is intend to denot the topic of document d , and theta sub b , which is repres a background topic that we can set to attract the common word becaus common word would be assign a high probabl in thi model .
so the paramet can be collect call lambda which i show here you can again think about the question about how mani paramet ar we talk about exactli .
thi is usual a good exercis to do becaus it allow you to see the model in depth and to have a complet understand of what's go on thi model .
and we have mix weight , of cours , also .
so what doe a likelihood function look like ?
well , it look veri similar to what we had befor .
so for the document , first it's a product over all the word in the document exactli the same as befor .
the onli differ is that insid here now it's a sum instead of just on .
so you might have recal befor we just had thi on there .
but now we have thi sum becaus of the mixtur model .
and becaus of the mixtur model we also have to introduc a probabl of choos that particular compon of distribut .
and so thi is just anoth wai of write , and by us a product over all the uniqu word in our vocabulari instead of have that product over all the posit in the document .
and thi form where we look at the differ and uniqu word is a commut that form for comput the maximum likelihood estim later .
and the maximum likelihood estim is , as usual , just to find the paramet that would maxim the likelihood function .
and the constraint here ar of cours two kind .
on is what ar probabl in each must sum to <num> the other is the choic of each must sum to <num> .
thi lectur is about mixtur model estim .
in thi lectur we're go to continu discuss probabilist topic model .
in particular , we're go to talk about how to estim the paramet of a mixtur model .
so let's first look at our motiv for us a mixtur model .
and we hope to factor out the background word .
from the top word equat .
the idea is to assum that the text data actual contain two kind of word .
on kind is from the background here .
so , the is , we , etc .
and the other kind is from our pop board distribut that we ar interest in .
so in order to solv thi problem of factor out background word , we can set up our mixtur model as fals .
we're go to assum that we alreadi know the paramet of all the valu for all the paramet in the mixtur model , except for the water distribut of which is our target .
so thi is a case of custom a probabilist model so that we embed a known variabl that we ar interest in .
but we're go to simplifi other thing .
we're go to assum we have knowledg abov other .
and thi is a power wai of custom a model .
for a particular need .
now you can imagin , we could have assum that we also don't know the background word .
but in thi case , our goal is to factor out precis those high probabl background word .
so we assum the background model is alreadi fix .
and on problem here is how can we adjust theta sub d in order to maxim the probabl of the observ document here and we assum all the other perimet ar now .
now although we design the model holist .
to try to factor out these background word .
it's unclear whether , if we us maximum write or estim .
we will actual end up have a whole distribut where the common word like the would inde have smaller probabl than befor .
now in thi case it turn out the answer is ye .
and when we set up the probabl in thi wai , when we us maximum likelihood or we will end up have a word distribut where the us common word would be factor out .
by the us of the background rule of distribut .
so to understand why thi is so , it's us to examin the behavior of a mixtur model .
so we're go to look at a veri veri simpl case .
in order to understand some interest behavior of a mixtur model .
the observ pattern here actual ar generaliz to mixtur model in gener .
but it's much easier to understand thi behavior when we us a veri simpl case like what we ar see here .
so specif in thi case , let's assum that the probabl choos each of the two model is exactli the same .
so we're go to flip a fair coin to decid which model to us .
furthermor , we're go to assum there ar .
precis two word , the and text .
obvious thi is a veri naiv oversimplif of the actual text , but again , it's us to examin the behavior in such a special case .
so we further assum that the background model give probabl of <num> toward the end text <num> .
now , let also assum that our data is extrem simpl the document ha just two word text and the so now let right down the likeabl function in such a case .
first , what's the probabl of text , and what's the probabl of the .
i hope by thi point you'll be abl to write it down .
so the probabl of text is basic the sum over two case , where each case correspond with to each of the order distribut and it account for the two wai of gener text .
and insid each case , we have the probabl of choos the model , which is <num> multipli by the probabl of observ text from that model .
similarli , the , would have a probabl of the same form , just what is differ is the exact probabl .
so natur our later function is just a product of the two .
so it's veri easi to see that , onc you understand what's the probabl of each word .
which is also why it's so import to understand what's exactli the probabl of observ each word from such a mixtur model .
now , the interest question now is , how can we then optim thi likelihood ?
well , you will note that there ar onli two variabl .
thei ar precis the two probabl of the two word .
text given by theta sub d .
and thi is becaus we have assum that all the other paramet ar known .
so , now the question is a veri simpl algebra question .
so , we have a simpl express with two variabl and we hope to choos the valu of these two variabl to maxim thi function .
and the exercis that we have seen some simpl algebra problem .
note that the two probabl must sum to on , so there's some constraint .
if there were no constraint of cours , we would set both probabl to their maximum valu which would be on , to maxim , but we can't do that becaus text then the must sum to on .
we can't give both a probabl of on .
so , now the question is how should we alloc the probabl and the math between the two word .
what do you think ?
now , it would be us to look at thi formula for a moment , and to see what , intuit , what we do in order to do set these probabl to maxim the valu of thi function .
okai , if we look into thi further , then we see some interest behavior of the two compon model in that thei will be collabor to maxim the probabl of the observ data .
which is dictat by the maximum likelihood estim .
but thei ar also compet in some wai , and in particular , thei would be compet on the word .
and thei would tend to back high probabl on differ word to avoid thi competit in some sens or to gain advantag in thi competit .
so again , look at thi object function and we have a constraint on the two probabl .
now , if you look at the formula intuit , you might feel that you want to set the probabl of text to be somewhat larger .
and thi induc can be work support by mathemat fact , which is when the sum of two variabl is a constant then the product of them which is maximum when thei ar equal , and thi is a fact we know from algebra .
now if we plug that it would mean that we have to make the two probabl equal .
and when we make them equal and then if we consid the constraint it will be easi to solv thi problem , and the solut is the probabl of tax will be . <num> and probabl is . <num> .
the probabl of text is now much larger than probabl of the , and thi is not the case when have just on distribut .
and thi is clearli becaus of the us of the background model , which assign the veri high probabl to the and low probabl to text .
and if you look at the equat you will see obvious some interact of the two distribut here .
in particular , you will see in order to make them equal .
and then the probabl assign by theta sub d must be higher for a word that ha a smaller probabl given by the background .
thi is obviou from examin thi equat .
becaus the background part is weak for text .
it's small .
so in order to compens for that , we must make the probabl for text given by theta sub d somewhat larger , so that the two side can be balanc .
so thi is in fact a veri gener behavior of thi model .
and that is , if on distribut assign a high probabl to on word than anoth , then the other distribut would tend to do the opposit .
basic it would discourag other distribut to do the same and thi is to balanc them out so we can account for all kind of word .
and thi also mean that by us a background model that is fix into assign high probabl through background word .
we can inde encourag the unknown topic on of thi to assign smaller probabl for such common word .
instead put more probabl than thi on the content word , that cannot be explain well by the background model .
mean that thei have a veri small probabl from the background motor like text here .
now let look at anoth behaviour of the mix model and in thi case let look at the respons to data frequenc .
so what you ar see now is basic the likelihood of function for the two word document and we now in thi case the solut is text .
a probabl of <num> and the a probabl of <num> .
now it's interest to think about a scenario where we start ad more word to the document .
so what would happen if we add mani the's to the document ?
now thi would chang the game , right ?
so , how ?
well , pictur , what would the likelihood function look like now ?
well , it start with the likelihood function for the two word , right ?
as we add more word , we know that .
but we have to just multipli the likelihood function by addit term to account for the addit .
occurr of that .
sinc in thi case , all the addit term ar the , we're go to just multipli by thi term .
right ?
for the probabl of the .
and if we have anoth occurr of the , we'd multipli again by the same term , and so on and forth .
add as mani term as the number of the's that we add to the document , d' .
now thi obvious chang the likelihood function .
so what's interest is now to think about how would that chang our solut ?
so what's the optim solut now ?
now , intuit you'd know the origin solut , pull the <num> versu pull the , will no longer be optim for thi new function .
right ?
but , the question is how should we chang it .
what gener is to sum to on .
so he know we must take awai some probabl the mass from on word and add the probabl mass to the other word .
the question is which word to have reduc the probabl and which word to have a larger probabl .
and in particular , let's think about the probabl of the .
should it be increas to be more than <num> ?
or should we decreas it to less than <num> ?
what do you think ?
now you might want to paus the video a moment to think more about .
thi question .
becaus thi ha to do with understand of import behavior of a mixtur model .
and inde , other maximum likelihood estim .
now if you look at the formula for a moment , then you will see it seem like anoth object function is more influenc by the than text .
befor , each comput .
so now as you can imagin , it would make sens to actual assign a smaller probabl for text and lock it .
to make room for a larger probabl for the .
why ?
becaus the is repeat mani time .
if we increas it a littl bit , it will have more posit impact .
wherea a slight decreas of text will have rel small impact becaus it occur just on , right ?
so thi mean there is anoth behavior that we observ here .
that is high frequenc word gener with high probabl from all the distribut .
and , thi is no surpris at all , becaus after all , we ar maxim the likelihood of the data .
so the more a word occur , then it make more sens to give such a word a higher probabl becaus the impact would be more on the likelihood function .
thi is in fact a veri gener phenomenon of all the maximum likelihood estim .
but in thi case , we can see as we see more occurr of a term , it also encourag the unknown distribut theta sub d to assign a somewhat higher probabl to thi word .
now it's also interest to think about the impact of probabl of theta sub b .
the probabl of choos on of the two compon model .
now we've been so far assum that each model is equal like .
and that give us <num> .
but you can again look at thi likelihood function and try to pictur what would happen if we increas the probabl of choos a background model .
now you will see these term for the , we have a differ form where the probabl that would be even larger becaus the background ha a high probabl for the word and the coeffici in front of <num> which is now <num> would be even larger .
when thi is larger , the overal result would be larger .
and that also make thi the less import for theta sub d to increas the probabl befor the .
becaus it's alreadi veri larg .
so the impact here of increas the probabl of the is somewhat regul by thi coeffici , the point of i .
if it's larger on the background , then it becom less import to increas the valu .
so thi mean the behavior here , which is high frequenc word tend to get the high probabl , ar effect or regular somewhat by the probabl of choos each compon .
the more like a compon is be chosen .
it's more import that to have higher valu for these frequent word .
if you have a variou small probabl of be chosen , then the incent is less .
so to summar , we have just discuss the mixtur model .
and we discuss that the estim problem of the mixtur model and particular with thi discuss some gener behavior of the estim and that mean we can expect our estim to captur these infus .
first everi compon model attempt to assign high probabl to high frequent their word in the data .
and thi is to collabor maxim likelihood .
second , differ compon model tend to bet high probabl on differ word .
and thi is to avoid a competit or wast of probabl .
and thi would allow them to collabor more effici to maxim the likelihood .
so , the probabl of choos each compon regul the collabor and the competit between compon model .
it would allow some compon model to respond more to the chang , for exampl , of frequenc of the theta point in the data .
we also talk about the special case of fix on compon to a background word distribut , right ?
and thi distribut can be estim by us a collect of document , a larg collect of english document , by us just on distribut and then we'll just have normal frequenc of term to give us the probabl of all these word .
now when we us such a special mixtur model , we show that we can effect get rid of that on word in the other compon .
and that would make thi cover topic more discrimin .
thi is also an exampl of impos a prior on the model paramet and the prior here basic mean on model must be exactli the same as the background languag model and if you recal what we talk about in bayesian estim , and thi prior will allow us to favor a model that is consist with our prior .
in fact , if it's not consist we're go to sai the model is imposs .
so it ha a zero prior probabl .
that effect exclud such a scenario .
thi is also issu that we'll talk more later .
thi lectur is about , opinion mine and sentiment analysi , cover , motiv .
in thi lectur , we're go to start , talk about , mine a differ kind of knowledg .
name , knowledg about the observ or human that have gener the text data .
in particular , we're go to talk about the opinion mine and sentiment analysi .
as we discuss earlier , text data can be regard as data gener from human as subject sensor .
in contrast , we have other devic such as video record that can report what's happen in the real world object to gener the viewer data for exampl .
now the main differ between test data and other data , like video data , is that it ha rich opinion , and the content tend to be subject becaus it's gener from human .
now , thi is actual a uniqu advantag of text data , as compar with other data , becaus the offic is a great opportun to understand the observ .
we can mine text data to understand their opinion .
understand peopl's prefer , how peopl think about someth .
so thi lectur and the follow lectur will be mainli about how we can mine and analyz opinion buri in a lot of text data .
so let's start with the concept of opinion .
it's not that easi to formal defin opinion , but mostli we would defin opinion as a subject statement describ what a person believ or think about someth .
now , i highlight quit a few word here .
and that's becaus it's worth think a littl bit more about these word .
and that will help us better understand what's in an opinion .
and thi further help us to defin opinion more formal .
which is alwai need to comput to resolv the problem of opinion mine .
so let's first look at the kei word of subject here .
thi is in contrast with object statement or factual statement .
those statement can be prove right or wrong .
and thi is a kei differenti factor from opinion which tend to be not easi to prove wrong or right , becaus it reflect what the person think about someth .
so in contrast , object statement can usual be prove wrong or correct .
for exampl , you might sai thi comput ha a screen and a batteri .
now that's someth you can check .
it's either have a batteri or not .
but in contrast with thi , think about the sentenc such as , thi laptop ha the best batteri or thi laptop ha a nice screen .
now these statement ar more subject and it's veri hard to prove whether it's wrong or correct .
so opinion , is a subject statement .
and next let look at the keyword person here .
and that indic that is an opinion holder .
becaus when we talk about opinion , it's about an opinion held by someon .
and then we notic that there is someth here .
so that is the target of the opinion .
the opinion is express on thi someth .
and now , of cours , believ or think impli that an opinion will depend on the cultur or background and the context in gener .
becaus a person might think differ in a differ context .
peopl from differ background mai also think in differ wai .
so thi analysi show that there ar multipl element that we need to includ in order to character opinion .
so , what's a basic opinion represent like ?
well , it should includ at least three element , right ?
firstli , it ha to specifi what's the opinion holder .
so whose opinion is thi ?
second , it must also specifi the target , what's thi opinion about ?
and third , of cours , we want opinion content .
and so what exactli is opinion ?
if you can identifi these , we get a basic understand of opinion and can alreadi be us sometim .
you want to understand further , we want enrich opinion represent .
and that mean we also want to understand that , for exampl , the context of the opinion and what situat wa the opinion express .
for exampl , what time wa it express ?
we , also , would like to , peopl understand the opinion sentiment , and thi is to understand that what the opinion tell us about the opinion holder's feel .
for exampl , is thi opinion posit , or neg ?
or perhap the opinion holder wa happi or wa sad , and so such understand obviou to those beyond just extract the opinion content , it need some analysi .
so let's take a simpl exampl of a product review .
in thi case , thi actual express the opinion holder , and express the target .
so it obvious what opinion holder and that's just review and it also often veri clear what the opinion target and that's the product review for exampl iphon <num> .
when the review is post usual you can't such inform easier .
now the content , of cours , is a review text that's , in gener , also easi to obtain .
so you can see product review ar fairli easi to analyz in term of obtain a basic opinion of represent .
but of cours , if you want to get more inform , you might know the context , for exampl .
the review wa written in <num> .
or , we want to know that the sentiment of thi review is posit .
so , thi addit understand of cours add valu to mine the opinion .
now , you can see in thi case the task is rel easi and that's becaus the opinion holder and the opinion target have alreadi been identifi .
now let's take a look at the sentenc in the new .
in thi case , we have a implicit holder and a implicit target .
and the tasker is in gener harder .
so , we can identifi opinion holder here , and that's the governor of connecticut .
we can also identifi the target .
so on target is hurrican sandi , but there is also anoth target mention which is hurrican of <num> .
so what's the opinion ?
well , there's a neg sentiment here that's indic by word like bad and worst .
and we can also , then , identifi context , new england in thi case .
now , unlik in the playoff review , all these element must be extract by us natur ram process techniqu .
so , the task is much harder .
and we need a deeper natur languag process .
and these exampl also suggest that a lot of work can be easi to done for product review .
that's inde what ha happen .
analyz and assembl new is still quit difficult , it's more difficult than the analysi of opinion in product review .
now there ar also some other interest variat .
in fact , here we're go to examin the variat of opinion , more systemat .
first , let's think about the opinion holder .
the holder could be an individu or it could be group of peopl .
sometim , the opinion wa from a committe .
or from a whole countri of peopl .
opinion target account will vari a lot .
it can be about on entiti , a particular person , a particular product , a particular polici , ect .
but it could be about a group of product .
could be about the product from a compani in gener .
could also be veri specif about on attribut , though .
an attribut of the entiti .
for exampl , it's just about the batteri of iphon .
it could be someon els's opinion .
and on person might comment on anoth person's opinion , etc .
so , you can see there is a lot of variat here that will caus the problem to vari a lot .
now , opinion content , of cours , can also vari a lot on the surfac , you can identifi on sentenc opinion or on phrase opinion .
but you can also have longer text to express an opinion , like the whole articl .
and furthermor we identifi the variat in the sentiment or emot damag that's abov the feed of the opinion holder .
so , we can distinguish a posit versu neg or mutual or happi versu sad , separ .
final , the opinion context can also vari .
we can have a simpl context , like differ time or differ locat .
but there could be also complex context , such as some background of topic be discuss .
so when opinion is express in particular discours context , it ha to be interpret in differ wai than when it's express in anoth context .
so the context can be veri to entir discours context of the opinion .
from comput perspect , we're mostli interest in what opinion can be extract from text data .
so , it turn out that we can also differenti , distinguish , differ kind of opinion in text data from comput perspect .
first , the observ might make a comment about opinion target , observ the word so in case we have the author's opinion .
for exampl , i don't like thi phone at all .
and that's an opinion of thi author .
in contrast , the text might also report opinion about other .
so the person could also make observ about anoth person's opinion and report thi opinion .
so for exampl , i believ he love the paint .
and that opinion is realli about the it is realli express by anoth person here .
so , it doesn't mean thi author love that paint .
so clearli , the two kind of opinion need to be analyz in differ wai , and sometim in product review , you can see , although mostli the opinion ar fals from thi review .
sometim , a review might mention opinion of hi friend or her friend .
anoth complic is that there mai be indirect opinion or infer opinion that can be obtain .
by make infer on what's express in the text that might not necessarili look like opinion .
for exampl , on statement that might be , thi phone ran out of batteri in just on hour .
now , thi is in a wai a factual statement becaus it's either true or fals , right ?
you can even verifi that , but from thi statement , on can also infer some neg opinion about the qualiti of the batteri of thi phone , or the feel of the opinion holder about the batteri .
the opinion holder clearli wish that the batteri do last longer .
so these ar interest variat that we need to pai attent to when we extract opinion .
also , for thi reason about indirect opinion , it's often also veri us to extract whatev the person ha said about the product , and sometim factual sentenc like these ar also veri us .
so , from a practic viewpoint , sometim we don't necessarili extract the subject of sentenc .
instead , again , all the sentenc that ar about the opinion ar us for understand the person or understand the product that we commend .
so the task of opinion mine can be defin as take textual input to gener a set of opinion represent .
each represent we should identifi opinion holder , target , content , and the context .
ideal we can also infer opinion sentiment from the comment and the context to better understand .
the opinion .
now often , some element of the represent ar alreadi known .
i just gave a good exampl in the case of product we'd us where the opinion holder and the opinion target ar often expressli identifi .
and that's not why thi turn out to be on of the simplest opinion mine task .
now , it's interest to think about the other task that might be also simpl .
becaus those ar the case where you can easili build applic by us opinion mine techniqu .
so now that we have talk about what is opinion mine , we have defin the task .
let's also just talk a littl bit about why opinion mine is veri import and why it's veri us .
so here , i identifi three major reason , three broad reason .
the first is it can help decis support .
it can help us optim our decis .
we often look at other peopl's opinion , look at read the review in order to make a decis like bui a product or us a servic .
we also would be interest in other opinion when we decid whom to vote for exampl .
and polici maker , mai also want to know peopl's opinion when design a new polici .
so that's on gener , kind of , applic .
and it's veri broad , of cours .
the second applic is to understand peopl , and thi is also veri import .
for exampl , it could help understand peopl's prefer .
and thi could help us better serv peopl .
for exampl , we optim a product search engin or optim a recommend system if we know what peopl ar interest in , what peopl think about product .
it can also help with advertis , of cours , and we can have target advertis if we know what kind of peopl tend to like what kind of plot .
now the third kind of applic can be call voluntari survei .
now thi is most import research that us to be done by do survei , do manual survei .
question , answer it .
peopl need to feel inform to answer their question .
now thi is directli relat to human as sensor , and we can usual aggreg opinion from a lot of human through kind of assess the gener opinion .
now thi would be veri us for busi intellig where manufactur want to know where their product have advantag over other .
what ar the win featur of their product , win featur of competit product .
market research ha to do with understand consum oppinion .
and thi creat veri us direct for that .
data driven social scienc research can benefit from thi becaus thei can do text mine to understand the peopl's opinion .
and if you can aggreg a lot of opinion from social media , from a lot of , popular inform then you can actual do some studi of some question .
for exampl , we can studi the behavior of peopl on social media on social network .
and these can be regard as voluntari survei done by those peopl .
in gener , we can gain a lot of advantag in ani predict task becaus we can leverag the text data as extra data abov ani problem .
and so we can us text base predict techniqu to help you make predict or improv the accuraci of predict .
thi lectur is about us a time seri as context to potenti discov causal topic in text .
in thi lectur , we're go to continu discuss contextu text mine .
in particular , we're go to look at the time seri as a context for analyz text , to potenti discov causal topic .
as usual , it start with the motiv .
in thi case , we hope to us text mine to understand a time seri .
here , what you ar see is dow jone industri averag stock price curv .
and you'll see a sudden drop here .
right .
so on would be interest know what might have caus the stock market to crash .
well , if you know the background , and you might be abl to figur it out if you look at the time stamp , or there ar other data that can help us think about .
but the question here is can we get some clue about thi from the companion new stream ?
and we have a lot of new data that gener dure that period .
so if you do that we might actual discov the crash .
after it happen , at the time of the septemb <num> attack .
and that's the time when there is a sudden rise of the topic about septemb <num> happen in new articl .
here's anoth scenario where we want to analyz the presidenti elect .
and thi is the time seri that ar from the presidenti predict market .
for exampl , i write a trunk of market would have stock for each candid .
and if you believ on candid that will win then you tend to bui the stock for that candid , caus the price of that candid to increas .
so , that's a nice wai to actual do survei of peopl's opinion about these candid .
now , suppos you see someth drop of price for on candid .
and you might also want to know what might have caus the sudden drop .
or in a social scienc studi , you might be interest in know what method in thi elect , what issu realli matter to peopl .
now again in thi case , we can look at the companion new stream and ask for the question .
ar there ani clue in the new stream that might provid insight about thi ?
so for exampl , we might discov the mention of tax cut ha been increas sinc that point .
so mayb , that's relat to the drop of the price .
so all these case ar special case of a gener problem of joint analysi of text and a time seri data to discov causal topic .
the input in thi case is time seri plu text data that ar produc in the same time period , the companion text stream .
and thi is differ from the standard topic model , where we have just to text collect .
that's why we see time seri here , it serv as context .
now , the output that we want to gener is the topic whose coverag in the text stream ha strong correl with the time seri .
for exampl , whenev the topic is manag the price tend to go down , etc .
now we call these topic causal topic .
of cours , thei're not , strictli speak , causal topic .
we ar never go to be abl to verifi whether thei ar causal , or there's a true causal relationship here .
that's why we put causal in quotat mark .
but at least thei ar correl topic that might potenti explain the caus and human can certainli further analyz such topic to understand the issu better .
and the output would contain topic just like in topic model .
but we hope that these topic ar not just the regular topic with .
these topic certainli don't have to explain the data of the best in text , but rather thei have to explain the data in the text .
mean that thei have to reprehend the meaning topic in text .
cement but also more importantli , thei should be correl with extern hand seri that's given as a context .
so to understand how we solv thi problem , let's first adjust to solv the problem with reactiv topic model , for exampl prsa .
and we can appli thi to text stream and with some extens like a cprsa or contextu prsa .
then we can discov these topic in the correl and also discov their coverag over time .
so , on simpl solut is , to choos the topic from thi set that have the strongest correl with the extern time seri .
but thi approach is not go to be veri good .
why ?
becaus awar pictur to the topic is that thei will discov by prsa or lda .
and that mean the choic of topic will be veri limit .
and we know these model try to maxim the likelihood of the text data .
so those topic tend to be the major topic that explain the text data well .
aand thei ar not necessarili correl with time seri .
even if we get the best on , the most correl topic might still not be so interest from causal perspect .
so here in thi work site here , a better approach wa propos .
and thi approach is call iter causal topic model .
the idea is to do an iter adjust of topic , discov by topic model us time seri to induc a product .
so here's an illustr on how thi work , how thi work .
take the text stream as input and then appli regular topic model to gener a number of topic .
let's sai four topic .
shown here .
and then we're go to us extern time seri to assess which topic is more causal relat or correl with the extern time seri .
so we have someth that rank them .
and we might think that topic on and topic four ar more correl and topic two and topic three ar not .
now we could have stop here and that would be just like what the simpl approach that i talk about earlier then we can get to these topic and call them causal topic .
but as i also explain that these topic ar unlik veri good becaus thei ar gener topic that explain the whole text connect .
thei ar not necessari .
the best topic ar correl with our time seri .
so what we can do in thi approach is to first zoom into word level and we can look into each word and the top rank word list for each topic .
let's sai we take topic <num> as the target examin .
we know topic <num> is correl with the time seri .
or is at least the best that we could get from thi set of topic so far .
and we're go to look at the word in thi topic , the top word .
and if the topic is correl with the time seri , there must be some word that ar highli correl with the time seri .
so here , for exampl , we might discov w1 and w3 ar posit correl with time seri , but w2 and w4 ar neg correl .
so , as a topic , and it's not good to mix these word with differ correl .
so we can then for the separ of these word .
we ar go to get all the red word that indic posit correl .
w1 and w3 .
and we're go to also get anoth sub topic .
if you want .
that repres a neg correl word , w2 and w4 .
now , these subtop , or these variat of topic , base on the correl analysi , ar topic that ar still quit relat to the origin topic , topic <num> .
but thei ar alreadi deviat , becaus of the us of time seri inform for bia select of word .
so then in some sens , well we should expect so , some sens more correl with the time seri than the origin topic <num> .
becaus the topic <num> ha mix word , here we separ them .
so each of these two subtop can be expect to be better coher in thi time seri .
howev , thei mai not be so coher as it mention .
so the idea here is to go back to topic model by us these each as a prior to further guid the topic model .
and that's to sai we ask our topic model now discov topic that ar veri similar to each of these two subtop .
and thi will caus a bia toward more correl to the topic wa a time seri .
of cours then we can appli topic model to get anoth gener of topic .
and that can be further ran to the base of the time seri to set after the highli correl topic .
and then we can further analyz the compon at work in the topic and then try to analyz . word level correl .
and then get the even more correl subtop that can be further fed into the process as prior to drive the topic of model discoveri .
so thi whole process is just a heurist wai of optim causal and coher , and that's our ultim goal .
right ?
so here you see the pure topic model will be veri good at maxim topic coher , the topic will be all meaning .
if we onli us causal test , or correl measur , then we might get a set word that ar strongli correl with time seri , but thei mai not necessarili mean anyth .
it might not be cementr connect .
so , that would be at the other extrem , on the top .
now , the ideal is to get the causal topic that's score high , both in topic coher and also causal relat .
in thi approach , it can be regard as an altern wai to maxim both sine engin .
so when we appli the topic model we're maxim the coher .
but when we decompos the topic model word into set of word that ar veri strong correl with the time seri .
we select the most strongli correl word with the time seri .
we ar push the model back to the causal dimens to make it better in causal score .
and then , when we appli the select word as a prior to guid a topic model , we again go back to optim the coher .
becaus topic model , we ensur the next gener of topic to be coher and we can iter when thei're optim in thi wai as shown on thi pictur .
so the onli i think a compon that you haven't seen such a framework is how to measur the causal .
becaus the rest is just talk more on .
so let's have a littl bit of discuss of that .
so here we show that .
and let's sai we have a topic about govern respons here .
and then we just talk more of we can get coverag of the topic over time .
so , we have a time seri , x sub t .
now , we also have , ar give a time seri that repres extern inform .
it's a non text time seri , y sub t .
it's the stock price .
now the the question here is doe xt caus yt ?
well in other word , we want to match the causal relat between the two .
or mayb just measur the correl of the two .
there ar mani measur that we can us in thi framework .
for exampl , pair in correl is a common us measur .
and we got to consid time lag here so that we can try to captur causal relat .
us somewhat past data and us the data in the past to try to correl with the data on point of y that repres the futur , for exampl .
and by introduc such lag , we can hopefulli captur some causal relat by even us correl measur like person correl .
but a common us , the measur for causal here is granger causal test .
and the idea of thi test is actual quit simpl .
basic you're go to have all the regress model to us the histori inform of y to predict itself .
and thi is the best we could without ani other inform .
so we're go to build such a model .
and then we're go to add some histori inform of x into such model .
to see if we can improv the predict of y .
if we can do that with a statist signific differ .
then we just sai x ha some causal infer on y , or otherwis it wouldn't have causal improv of predict of y .
if , on the other hand , the differ is insignific and that would mean x doe not realli have a caus or relat why .
so that's the basic idea .
now , we don't have time to explain thi in detail so you could read , but you would read at thi cite refer here to know more about thi measur .
it's a veri conveni us measur .
ha mani applic .
so next , let's look at some simpl result gener by thi approach .
and here the data is the new york time and in the time period of june <num> through decemb of <num> .
and here the time seri we us is stock price of two compani .
american airlin and appl and the goal is to see if we inject the sum time seri contest , whether we can actual get topic that ar wise for the time seri .
imagin if we don't us ani input , we don't us ani context .
then the topic from new york time discov by prsa would be just gener topic that peopl talk about in new .
all right .
those major topic in the new event .
but here you see these topic ar inde bias toward each time seri .
and particularli if you look at the underlin word here in the american airlin result , and you see airlin , airport , air , unit trade , or terror , etc .
so it clearli ha topic that ar more correl with the extern time seri .
on the right side , you see that some of the topic ar clearli relat to appl , right .
so you can see comput , technolog , softwar , internet , com , web , etc .
so that just mean the time seri ha effect serv as a context to bia the discoveri of topic .
from anoth perspect , these result help us on what peopl have talk about in each case .
so not just the peopl , what peopl have talk about , but what ar some topic that might be correl with their stock price .
and so these topic can serv as a start point for peopl to further look into issu and you'll find the true causal relat .
here ar some other result from analyz presidenti elect time seri .
the time seri data here is from iowa electron market .
and that's a predict market .
and the data is the same .
new york time from mai <num> to octob <num> .
that's for <num> presidenti campaign elect .
now , what you see here ar the top three word in signific topic from new york time .
and if you look at these topic , and thei ar inde quit relat to the campaign .
actual the issu ar veri much relat to the import issu of thi presidenti elect .
now here i should mention that the text data ha been filter by us onli the articl that mention these candid name .
it's a subset of these new articl .
veri differ from the previou experi .
but the result here clearli show that the approach can uncov some import issu in that presidenti elect .
so tax cut , oil energi , abort and gun control ar all known to be import issu in that presidenti elect .
and that wa support by some literatur in polit scienc .
and also i wa discuss wikipedia , right .
so basic the result show that the approach can effect discov possibl causal topic base on the time seri data .
so there ar two suggest read here .
on is the paper about thi iter topic model with time seri feedback .
where you can find more detail about how thi approach work .
and the second on is read about granger casual text .
so in the end , let's summar the discuss of text base predict .
now , text base predict is gener veri us for big data applic that involv text .
becaus thei can help us inform new knowledg about the world .
and the knowledg can go beyond what's discuss in the text .
as a result can also support optim of our decis make .
and thi ha a wider spread applic .
text data is often combin with non text data for predict .
becaus , for thi purpos , the predict purpos , we gener would like to combin non text data and text data togeth , as much cruel as possibl for predict .
and so as a result dure the analysi of text and non text is veri necessari and it's also veri us .
now when we analyz text data togeth with non text data , we can see thei can help each other .
so non text data , provid a context for mine text data , and we discuss a number of techniqu for contextu text mine .
and on the other hand , a text data can also help interpret pattern discov from non text data , and thi is call a pattern annot .
in gener , thi is a veri activ research topic , and there ar new paper be publish .
and there ar also mani open challeng that have to be solv .
thi lectur is a summari of thi whole cours .
first , let's revisit the topic that we cover in thi cours .
in the begin , we talk about the natur languag process and how it can enrich text represent .
we then talk about how to mine knowledg about the languag , natur languag us to express the , what's observ the world in text and data .
in particular , we talk about how to mine word associ .
we then talk about how to analyz topic in text .
how to discov topic and analyz them .
thi can be regard as knowledg about observ world , and then we talk about how to mine knowledg about the observ and particularli talk about the , how to mine opinion and do sentiment analysi .
and final , we will talk about the text base predict , which ha to do with predict valu of other real world variabl base on text data .
and in discuss thi , we will also discuss the role of non text data , which can contribut addit predictor for the predict problem , and also can provid context for analyz text data , and in particular we talk about how to us context to analyz topic .
so here ar the kei high level take awai messag from thi cost .
i go to go over these major topic and point out what ar the kei take awai messag that you should rememb .
first the nlp and text represent .
you should realiz that nlp is alwai veri import for ani text replic becaus it enrich text represent .
the more nlp the better text represent we can have .
and thi further enabl more accur knowledg discoveri , to discov deeper knowledg , buri in text .
howev , the current estat of art of natur energi process is , still not robust enough .
so , as an result , the robust text mine technolog todai , tend to be base on world and tend to reli a lot on statist analysi , as we've discuss in thi cours .
and you mai recal we've mostli us word base represent .
and we've reli a lot on statist techniqu , statist learn techniqu particularli .
in word associ mine and analysi the import point first , we ar introduc the two concept for two basic and complementari relat of word , paradigmat and syntagmat relat .
these ar actual veri gener relat between element sequenc .
if you take it as mean element that occur in similar context in the sequenc and element that tend to co occur with each other .
and these relat might be also meaning for other sequenc of data .
we also talk a lot about test the similar then we discuss how to discov paradynam similar compar the context of word discov word that share similar context .
at that point level , we talk about repres text data with a vector space model .
and we talk about some retriev techniqu such as bm25 for measur similar of text and for assign weight to term , tf idf weight , et cetera .
and thi part is well connect to text retriev .
there ar other techniqu that can be relev here also .
the next point is about co occurr analysi of text , and we introduc some inform theori concept such as entropi , condit entropi , and mutual inform .
these ar not onli veri us for measur the co occurr of word , thei ar also veri us for analyz other kind of data , and thei ar us for , for exampl , for featur select in text categor as well .
so thi is anoth import concept , good to know .
and then we talk about the topic mine and analysi , and that's where we introduc in the probabilist topic model .
we spent a lot of time to explain the basic topic model , plsa in detail and thi is , those ar the basic for understand lda which is .
theoret , a more opinion model , but we did not have enough time to realli go in depth in introduc lda .
but in practic , plsa seem as effect as lda and it's simpler to implement and it's also more effici .
in thi part of wilson video is some gener concept that would be us to know , on is gener model , and thi is a gener method for model text data and model other kind of data as well .
and we talk about the maximum life eras data , the em algorithm for solv the problem of comput maximum estim .
so , these ar all gener techniqu that tend to be veri us in other scenario as well .
then we talk about the text cluster and the text categor .
those ar two import build block in ani text mine applic system .
in text with cluster we talk about how we can solv the problem by us a slightli differ mixtur modul than the probabilist topic model .
and we then also prefer to view the similar base approach to test for cuss word .
in categor we also talk about the two kind of approach .
on is gener classifi that reli on to base word to infer the condit of or probabl of a categori given text data , in deeper we'll introduc you should us base in detail .
thi is the practic us for techniqu , for a lot of text , capit task .
we also introduc the some discrimin classifi , particularli logist regress , can nearest labor and sbn .
thei also veri import , thei ar veri popular , thei ar veri us for text capit as well .
in both part , we'll also discuss how to evalu the result .
evalu is quit import becaus if the match that you us don't realli reflect the volatil of the method then it would give you mislead result so it veri import to get the variat right .
and we talk about variat of categor in detail wa a lot of specif measur .
then we talk about the sentiment analysi and the paradigm and that's where we introduc sentiment classif problem .
and although it's a special case of text recalcul , but we talk about how to extend or improv the text recalcul method by us more sophist featur that would be need for sentiment analysi .
we did a review of some common us for complex featur for text analysi , and then we also talk about how to captur the order of these categori , in sentiment classif , and in particular we introduc ordin logist regress then we also talk about latent aspect rate analysi .
thi is an unsupervis wai of us a gener model to understand and review data in more detail .
in particular , it allow us to understand the compos rate of a review on differ aspect of a topic .
so given text review with overal rate , the method allow even further rate on differ aspect .
and it also allow us to infer , the viewer lai their weight on these aspect or which aspect ar more import to a viewer can be reveal as well .
and thi enabl a lot of interest applic .
final , in the discuss of predict , we mainli talk about the joint mine of text and non text data , as thei ar both veri import for predict .
we particularli talk about how text data can help non text data and vice versa .
in the case of us non text data to help text data analysi , we talk about the contextu text mine .
we introduc the contextu plsa as a gener or gener model of plsa to allow us to incorpor the context of variabl , such as time and locat .
and thi is a gener wai to allow us to reveal a lot of interest topic of pattern in text data .
we also introduc the net plsa , in thi case we us social network or network in gener of text data to help analyz puppet .
and final we talk about how can be us as context to mine potenti causal topic in text layer .
now , in the other wai of us text to help interpret pattern discov from lam text data , we did not realli discuss anyth in detail but just provid a refer but i should stress that that's after a veri import direct to know about , if you want to build a practic text mine system , becaus understand and interpret pattern is quit import .
so thi is a summari of the kei take awai messag , and i hope these will be veri us to you for build ani text mine applic or to you for the start of these algorithm .
and thi should provid a good basi for you to read from your research paper , to know more about more of allow for other organ or to invent new hour in yourself .
so to know more about thi topic , i would suggest you to look into other area in more depth .
and dure thi short period of time of thi cours , we could onli touch the basic concept , basic principl , of text mine and we emphas the coverag of practic algorithm .
and thi is after the cost of cover algorithm and in mani case we omit the discuss of a lot of algorithm .
so to learn more about the subject you should definit learn more about the natur languag process becaus thi is foundat for all text base applic .
the more nlp you can do , the better the addit text that you can get , and then the deeper knowledg you can discov .
so thi is veri import .
the second area you should look into is the statist machin learn .
and these techniqu ar now the backbon techniqu for not just text analysi applic but also for nlp .
a lot of nlp techniqu ar nowadai actual base on supervis machineri .
so , thei ar veri import becaus thei ar a kei to also understand some advanc nlp techniqu and natur thei will provid more tool for do text analysi in gener .
now , a particularli interest area , call deep learn ha attract a lot of attent recent .
it ha also shown promis in mani applic area , especi in speech and vision , and ha been appli to text data as well .
so , for exampl , recent there ha work on us deep learn to do segment analysi to achiev better accuraci .
so that's on exampl of techniqu that we weren't abl to cover , but that's also veri import .
and the other area that ha emerg in statu learn is the water and bare techniqu , where thei can learn better recognit of word .
and then these better recognit will allow you confus similar of word .
as you can see , thi provid directli a wai to discov the paradigmat relat of word .
and result that peopl have got , so far , ar veri impress .
that's anoth promis techniqu that we did not have time to touch , but , of cours , whether these new techniqu would lead to practic us techniqu that work much better than the current technolog is still an open question that ha to be examin .
and no seriou evalu ha been done yet .
in , for exampl , examin the practic valu of word embed , other than word similar and basic evalu .
but nevertheless , these ar advanc techniqu that sure will make impact in text mine in the futur .
so it veri import to know more about these .
statist learn is also the kei to predict model which is veri crucial for mani big data applic and we did not talk about that predict model compon but thi is mostli about the regress or categor techniqu and thi is anoth reason why statist learn is import .
we also suggest that you learn more about data mine , and that's simpli becaus gener data mine algorithm can alwai be appli to text data , which can be regard as as special case of gener data .
so there ar mani applic of data mine techniqu .
in particular for exampl , pattern discoveri would be veri us to gener the interest featur for test analysi and the reason that an inform network that mine techniqu can also be us to analyz text inform at work .
so these ar all good to know .
in order to develop effect text analysi techniqu .
and final , we also recommend you to learn more about the text retriev , inform retriev , of search engin .
thi is especi import if you ar interest in build practic text applic system .
and a search end would be an essenti system compon in ani text base applic .
and that's becaus text data ar creat for human to consum .
so human ar at the best posit to understand text data and it's import to have human in the loop in big text data applic , so it can in particular help text mine system in two wai .
on is through effect reduc the data size from a larg collect to a small collect with the most relev text data that onli matter for the particular interpret .
so the other is to provid a wai to annot it , to explain parent , and thi ha to do with knowledg provid .
onc we discov some knowledg , we have to figur out whether or not the discoveri is realli reliabl .
so we need to go back to the origin text to verifi that .
and that is why the search engin is veri import .
moreov , some techniqu of inform retriev , for exampl bm25 , vector space and ar also veri us for text data mine .
we onli mention some of them , but if you know more about text retriev you'll see that there ar mani techniqu that ar us for it .
anoth techniqu that it's us for is index techniqu that enabl quick respons of search engin to a user's queri , and such techniqu can be veri us for build effici text mine system as well .
so , final , i want to remind you of thi big pictur for har big text data that i show you at your begin of the semest .
so in gener , to deal with a big text applic system , we need two kind text , text retriev and text mine .
and text retriev , as i explain , is to help convert big text data into a small amount of most relev data for a particular problem , and can also help provid knowledg proven , help interpret pattern later .
text mine ha to do with further analyz the relev data to discov the action knowledg that can be directli us for decis make or mani other task .
so thi cours cover text mine .
and there's a companion cours call text retriev and search engin that cover text retriev .
if you haven't taken that cours , it would be us for you to take it , especi if you ar interest in build a text cach system .
and take both cours will give you a complet set of practic skill for build such a system .
so in i just would like to thank you for take thi cours .
i hope you have learn us knowledg and skill in test mine and as you see from our discuss there ar a lot of opportun for thi kind of techniqu and there ar also a lot of open channel .
so i hope you can us what you have learn to build a lot of us for applic will benefit societi and to also join the research commun to discov new techniqu for text mine and benefit .
thank you .
thi lectur is about the sentiment classif .
if we assum that most of the element in the opinion represent ar all readi known , then our onli task mai be just a sentiment classif , as shown in thi case .
so suppos we know who's the opinion holder and what's the opinion target , and also know the content and the context of the opinion , then we mainli need to decid the opinion sentiment of the review .
so thi is a case of just us sentiment classif for understand opinion .
sentiment classif can be defin more specif as follow .
the input is opinion text object , the output is typic a sentiment label , or a sentiment tag , and that can be design in two wai .
on is polar analysi , where we have categori such as posit , neg , or neutral .
the other is emot analysi that can go beyond a polar to character the feel of the opinion holder .
in the case of polar analysi , we sometim also have numer rate as you often see in some review on the web .
five might denot the most posit , and on mayb the most neg , for exampl .
in gener , you have just disk holder categori to character the sentiment .
in emot analysi , of cours , there ar also differ wai for design the categori .
the six most frequent us categori ar happi , sad , fear , angri , surpris , and disgust .
so as you can see , the task is essenti a classif task , or categor task , as we've seen befor , so it's a special case of text categor .
thi also mean ani textual categor method can be us to do sentiment classif .
now of cours if you just do that , the accuraci mai not be good becaus sentiment classif doe requir some improv over regular text categor techniqu , or simpl text categor techniqu .
in particular , it need two kind of improv .
on is to us more sophist featur that mai be more appropri for sentiment tag as i will discuss in a moment .
the other is to consid the order of these categori , and especi in polar analysi , it's veri clear there's an order here , and so these categori ar not all that independ .
there's order among them , and so it's us to consid the order .
for exampl , we could us ordin regress to do that , and that's someth that we'll talk more about later .
so now , let's talk about some featur that ar often veri us for text categor and text mine in gener , but some of them ar especi also need for sentiment analysi .
so let's start from the simplest on , which is charact n gram .
you can just have a sequenc of charact as a unit , and thei can be mix with differ n's , differ length .
all right , and thi is a veri gener wai and veri robust wai to repres the text data .
and you could do that for ani languag , pretti much .
and thi is also robust to spell error or recognit error , right ?
so if you misspel a word by on charact and thi represent actual would allow you to match thi word when it occur in the text correctli .
right , so misspel the word and the correct form can be match becaus thei contain some common n gram of charact .
but of cours such a recommend would not be as discrimin as word .
so next , we have word n gram , a sequenc of word and again , we can mix them with differ n's .
unigram's ar actual often veri effect for a lot of text process task , and it's mostli becaus word ar word design featur by human for commun , and so thei ar often good enough for mani task .
but it's not good , or not suffici for sentiment analysi clearli .
for exampl , we might see a sentenc like , it's not good or it's not as good as someth els , right ?
so in such a case if you just take a good and that would suggest posit that's not good , all right so it's not accur .
but if you take a bigram , not good togeth , and then it's more accur .
so longer n gram ar gener more discrimin , and thei're more specif .
if you match it , and it sai a lot , and it's accur it's unlik , veri ambigu .
but it mai caus overfit becaus with such veri uniqu featur that machin orient program can easili pick up such featur from the train set and to reli on such uniqu featur to distinguish the categori .
and obvious , that kind of classifi , on would gener word to futur there when such discrimin featur will not necessarili occur .
so that's a problem of overfit that's not desir .
we can also consid part of speech tag , n gram if we can do part of speech tag an , for exampl , adject noun could form a pair .
we can also mix n gram of word and n gram of part of speech tag .
for exampl , the word great might be follow by a noun , and thi could becom a featur , a hybrid featur , that could be us for sentiment analysi .
so next we can also have word class .
so these class can be syntact like a part of speech tag , or could be semant , and thei might repres concept in the thesauru or ontolog , like wordnet .
or thei can be recogn the name entiti , like peopl or place , and these categori can be us to enrich the present as addit featur .
we can also learn word cluster and parod , for exampl , we've talk about the mine associ of word .
and so we can have cluster of paradigmat relat word or syntaxmat relat word , and these cluster can be featur to supplement the word base represent .
furthermor , we can also have frequent pattern syntax , and these could be frequent word set , the word that form the pattern do not necessarili occur togeth or next to each other .
but we'll also have locat where the word my occur more close togeth , and such pattern provid a more discrimin featur than word obvious .
and thei mai also gener better than just regular n gram becaus thei ar frequent .
so you expect them to occur also in test data .
so thei have a lot of advantag , but thei might still face the problem of overfeed as the featur becom more complex .
thi is a problem in gener , and the same is true for pars tree base featur , when you can us a pars tree to deriv featur such as frequent subtre , or path , and those ar even more discrimin , but thei're also ar more like to caus over fit .
and in gener , pattern discoveri algorithm's ar veri us for featur construct becaus thei allow us to search in a larg space of possibl featur that ar more complex than word that ar sometim us .
so in gener , natur languag process is veri import that thei deriv complex featur , and thei can enrich text represent .
so for exampl , thi is a simpl sentenc that i show you a long time ago in anoth lectur .
so from these word we can onli deriv simpl word n gram , represent or charact n gram .
but with nlp , we can enrich the represent with a lot of other inform such as part of speech tag , pars tree or entiti , or even speech act .
now with such enrich inform of cours , then we can gener a lot of other featur , more complex featur like a mix gram of a word and the part of speech tag , or even a part of a pars tree .
so in gener , featur design actual affect categor accuraci significantli , and it's a veri import part of ani machin learn applic .
in gener , i think it would be most effect if you can combin machin learn , error analysi , and domain knowledg in design featur .
so first you want to us the main knowledg , your understand of the problem , the design seed featur , and you can also defin a basic featur space with a lot of possibl featur for the machin learn program to work on , and machin can be appli to select the most effect featur or construct the new featur .
that's featur learn , and these featur can then be further analyz by human through error analysi .
and you can look at the categor error , and then further analyz what featur can help you recov from those error , or what featur caus overfit and caus those error .
and so thi can lead into featur valid that will revis the featur set , and then you can iter .
and we might consid us a differ featur space .
so nlp enrich text recognit as i just said , and becaus it enrich the featur space , it allow much larger such a space of featur and there ar also mani , mani more featur that can be veri us for a lot of task .
but be care not to us a lot of categori featur becaus it can caus overfit , or otherwis you would have to train care not to let overflow happen .
so a main challeng in design featur , a common challeng is to optim a trade off between exhaust and the specif , and thi trade off turn out to be veri difficult .
now exhaust mean we want the featur to actual have high coverag of a lot of document .
and so in that sens , you want the featur to be frequent .
specif requir the featur to be discrimin , so natur infrequ the featur tend to be more discrimin .
so thi realli caus a trade off between frequent versu infrequ featur .
and that's why a featur design is usual odd .
and that's probabl the most import part in machin learn ani problem in particularli in our case , for text categor or more specif the senit classif .
thi lectur is about the ordin logist regress for sentiment analysi .
so , thi is our problem set up for a typic sentiment classif problem .
or more specif a rate predict .
we have an opinion text document d as input , and we want to gener as output , a rate in the rang of <num> through k so it's a discret rate , and thi is a categor problem .
we have k categori here .
now we could us a regular text for categor techniqu to solv thi problem .
but such a solut would not consid the order and depend of the categori .
intuit , the featur that can distinguish categori <num> from <num> , or rather rate <num> from <num> , mai be similar to those that can distinguish k from k <num> .
for exampl , posit word gener suggest a higher rate .
when we train categor problem by treat these categori as independ we would not captur thi .
so what's the solut ?
well in gener we can order to classifi and there ar mani differ approach .
and here we're go to talk about on of them that call ordin logist regress .
now , let's first think about how we us logist regress for a binari sentiment .
a categor problem .
so suppos we just want to distinguish a posit from a neg and that is just a two categori categor problem .
so the predictor ar repres as x and these ar the featur .
and there ar m featur all togeth .
the featur valu is a real number .
and thi can be represent of a text document .
and why it ha two valu , binari respons variabl <num> or <num> .
<num> mean x is posit , <num> mean x is neg .
and then of cours thi is a standard two categori categor problem .
we can appli logist regress .
you mai recal that in logist regress , we assum the log of probabl that the y is equal to on , is assum to be a linear function of these featur , as shown here .
so thi would allow us to also write the probabl of y equal on , given x in thi equat that you ar see on the bottom .
so that's a logist function and you can see it relat thi probabl to , probabl that y <num> to the featur valu .
and of cours beta i's ar paramet here , so thi is just a direct applic of logist regress for binari categor .
what if we have multipl categori , multipl level ?
well we have to us such a binari logist regress problem to solv thi multi level rate predict .
and the idea is we can introduc multipl binari class file .
in each case we ask the class file to predict the , whether the rate is j or abov , or the rate's lower than j .
so when yj is equal to <num> , it mean rate is j or abov .
when it's <num> , that mean the rate is lower than j .
so basic if we want to predict a rate in the rang of <num> k , we first have on classifi to distinguish a k versu other .
and that's our classifi on .
and then we're go to have anoth classifi to distinguish it .
at k <num> from the rest .
that's classifi <num> .
and in the end , we need a classifi to distinguish between <num> and <num> .
so altogeth we'll have k <num> classifi .
now if we do that of cours then we can also solv thi problem and the logist regress program will be also veri straight forward as you have just seen on the previou slide .
onli that here we have more paramet .
becaus for each classifi , we need a differ set of paramet .
so now the logist regress classifi index by j , which correspond to a rate level .
and i have also us of j to replac beta <num> .
and thi is to .
make the notat more consist , than wa what we can show in the ordin logist regress .
so here we now have basic k minu on regular logist regress classifi .
each ha it's own set of paramet .
so now with thi approach , we can now do rate as follow .
after we have train these k <num> logist regress classifi , separ of cours , then we can take a new instanc and then invok a classifi sequenti to make the decis .
so first let look at the classifi that correspond to level of rate k .
so thi classifi will tell us whether thi object should have a rate of k or about .
if probabl accord to thi logist regress classifi is larger than point five , we're go to sai ye .
the rate is k .
now , what if it's not as larg as twenti five ?
well , that mean the rate's below k , right ?
so now , we need to invok the next classifi , which tell us whether it's abov k minu on .
it's at least k minu on .
and if the probabl is larger than twenti five , then we'll sai , well , then it's k <num> .
what if it sai no ?
well , that mean the rate would be even below k <num> .
and so we're go to just keep invok these classifi .
and here we hit the end when we need to decid whether it's two or on .
so thi would help us solv the problem .
right ?
so we can have a classifi that would actual give us a predict of a rate in the rang of <num> through k .
now unfortun such a strategi is not an optim wai of solv thi problem .
and specif there ar two problem with thi approach .
so these equat ar the same as .
you have seen befor .
now the first problem is that there ar just too mani paramet .
there ar mani paramet .
now , can you count how mani paramet do we have exactli here ?
now thi mai be a interest exercis .
to do .
so you might want to just paus the video and try to figur out the solut .
how mani paramet do i have for each classifi ?
and how mani classifi do we have ?
well you can see the , and so it is that for each classifi we have n plu on paramet , and we have k minu on classifi all togeth , so the total number of paramet is k minu on multipli by n plu on .
that's a lot .
a lot of paramet , so when the classifi ha a lot of paramet , we would in gener need a lot of data out to actual help us , train data , to help us decid the optim paramet of such a complex model .
so that's not ideal .
now the second problem is that these problem , these k minu <num> plu five , ar not realli independ .
these problem ar actual depend .
in gener , word that ar posit would make the rate higher for ani of these classifi .
for all these classifi .
so we should be abl to take advantag of thi fact .
now the idea of ordin logist regress is precis that .
the kei idea is just the improv over the k <num> independ logist regress classifi .
and that idea is to tie these beta paramet .
and that mean we ar go to assum the beta paramet .
these ar the paramet that indic the infer of those weight .
and we're go to assum these beta valu ar the same for all the k <num> paramet .
and thi just encod our intuit that , posit word in gener would make a higher rate more like .
so thi is intuit assumpt , so reason for our problem setup .
and we have thi order in these categori .
now in fact , thi would allow us to have two posit benefit .
on is it's go to reduc the number of famili significantli .
and the other is to allow us to share the train data .
becaus all these paramet ar similar to be equal .
so these train data , for differ classifi can then be share to help us set the optim valu for beta .
so we have more data to help us choos a good beta valu .
so what's the consequ , well the formula would look veri similar to what you have seen befor onli that , now the beta paramet ha just on index that correspond to the featur .
it no longer ha the other index that correspond to the level of rate .
so that mean we tie them togeth .
and there's onli on set of better valu for all the classifi .
howev , each classifi still ha the distinct r for valu .
the r for paramet .
except it's differ .
and thi is of cours need to predict the differ level of rate .
so r for sub j is differ it depend on j , differ than j , ha a differ r valu .
but the rest of the paramet , the beta i's ar the same .
so now you can also ask the question , how mani paramet do we have now ?
again , that's an interest question to think about .
so if you think about it for a moment , and you will see now , the param , we have far fewer paramet .
specif we have m plu k minu on .
becaus we have m , beta valu , and plu k minu on of our valu .
so let's just look basic , that's basic the main idea of ordin logist regress .
so , now , let's see how we can us such a method to actual assign rate .
it turn out that with thi , thi idea of ty all the paramet , the beta valu .
we also end up by have a similar wai to make decis .
and more specif now , the criteria whether the predictor probabl ar at least <num> abov , and now is equival to whether the score of the object is larger than or equal to neg author of j , as shown here .
now , the score function is just take the linear combin of all the featur with the divid beta valu .
so , thi mean now we can simpli make a decis of rate , by look at the valu of thi score function , and see which bracket it fall into .
now you can see the gener decis rule is thu , when the score is in the particular rang of all of our valu , then we will assign the correspond rate to that text object .
so in thi approach , we're go to score the object by us the featur and train paramet valu .
thi score will then be compar with a set of train alpha valu to see which rang the score is in .
and then , us the rang , we can then decid which rate the object should be get .
becaus , these rang of alpha valu correspond to the differ level of rate , and that's from the wai we train these alpha valu .
each is ti to some level of rate .
thi lectur is about the latent aspect rate analysi for opinion mine and sentiment analysi .
in thi lectur , we're go to continu discuss opinion mine and sentiment analysi .
in particular , we're go to introduc latent aspect rate analysi which allow us to perform detail analysi of review with overal rate .
so , first is motiv .
here ar two review that you often see in the net about the hotel .
and you see some overal rate .
in thi case , both review have given five star .
and , of cours , there ar also review that ar in text .
now , if you just look at these review , it's not veri clear whether the hotel is good for it locat or for it servic .
it's also unclear why a review like thi hotel .
what we want to do is to decompos thi overal rate into rate on differ aspect such as valu , room , locat , and servic .
so , if we can decompos the overal rate , the rate on these differ aspect , then , we can obtain a more detail understand of the review's opinionsabout the hotel .
and thi would also allow us to rank hotel along differ dimens such as valu or room .
but , in gener , such detail understand will reveal more inform about the user's prefer , review's prefer .
and also , we can understand better how the review view thi hotel from differ perspect .
now , not onli do we want to infer these aspect rate , we also want to infer the aspect weight .
so , some review mai care more about valu as oppos to the servic .
and that would be a case .
like what's shown on the left for the weight distribut , where you can see a lot of weight is place on valu .
but other care more for servic .
and therefor , thei might place more weight on servic than valu .
the reason why thi is also import is becaus , do you think about a five star on valu , it might still be veri expens if the review care a lot about servic , right ?
for thi kind of servic , thi price is good , so the review might give it a five star .
but if a review realli care about the valu of the hotel , then the five star , most like , would mean realli cheap price .
so , in order to interpret the rate on differ aspect accur , we also need to know these aspect weight .
when thei're combin togeth , we can have a more detail understand of the opinion .
so the task here is to get these review and their overal rate as input , and then , gener both the aspect rate , the compos aspect rate , and the aspect rate as output .
and thi is a problem call latent aspect rate analysi .
so the task , in gener , is given a set of review articl about the topic with overal rate , and we hope to gener three thing .
on is the major aspect comment on in the review .
second is rate on each aspect , such as valu and room servic .
and third is the rel weight place on differ aspect by the review .
and thi task ha a lot of applic , and if you can do thi , and it will enabl a lot of applic .
i just list some here .
and later , i will show you some result .
and , for exampl , we can do opinion base entiti rank .
we can gener an aspect level opinion summari .
we can also analyz review prefer , compar them or compar their prefer on differ hotel .
and we can do person recommend of product .
so , of cours , the question is how can we solv thi problem ?
now , as in other case of these advanc topic , we won t have time to realli cover the techniqu in detail .
but i m go to give you a brisk , basic introduct to the techniqu develop for thi problem .
so , first step , we ar go to talk about how to solv the problem in two stage .
later , we ar go to also mention that we can do thi in the unifi model .
now , take thi review with the overal rate as input .
what we want to do is , first , we're go to segment the aspect .
so we're go to pick out what word ar talk about locat , and what word ar talk about room condit , etc .
so with thi , we would be abl to obtain aspect segment .
in particular , we're go to obtain the count of all the word in each segment , and thi is denot by c sub i of w and d .
now thi can be done by us seed word like locat and room or price to retriev the in the segment .
and then , from those segment , we can further mine correl word with these seed word and that would allow us to segment the text into segment , discuss differ aspect .
but , of cours , later , as we will see , we can also us model to do the segment .
but anywai , that's the first stage , where the obtain the council of word in each segment .
in the second stage , which is call latent rate regress , we're go to us these word and their frequenc in differ aspect to predict the overal rate .
and thi predict happen in two stage .
in the first stage , we're go to us the and the weight of these word in each aspect to predict the aspect rate .
so , for exampl , if in your discuss of locat , you see a word like , amaz , mention mani time , and it ha a high weight .
for exampl , here , <num> .
then , it will increas the aspect rate for locat .
but , anoth word like , far , which is an act weight , if it's mention mani time , and it will decreas the rate .
so the aspect rate , assum that it will be a weight combin of these word frequenc where the weight ar the sentiment weight of the word .
of cours , these sentiment weight might be differ for differ aspect .
so we have , for each aspect , a set of term sentiment weight as shown here .
and that's in order by beta sub i and w .
in the second stage or second step , we're go to assum that the overal rate is simpli a weight combin of these aspect rate .
so we're go to assum we have aspect weight to the sub i of d , and thi will be us to take a weight averag of the aspect rate , which ar denot by r sub i of d .
and we're go to assum the overal rate is simpli a weight averag of these aspect rate .
so thi set up allow us to predict the overal rate base on the observ frequenc .
so on the left side , you will see all these observ inform , the r sub d and the count .
but on the right side , you see all the inform in that rang is actual latent .
so , we hope to discov that .
now , thi is a typic case of a gener model where would emb the interest variabl in the gener model .
and then , we're go to set up a gener probabl for the overal rate given the observ word .
and then , of cours , we can adjust these paramet valu includ beta rs and alpha is in order to maxim the probabl of the data .
in thi case , the condit probabl of the observ rate given the document .
so we have seen such case befor in , for exampl , pisa , where we predict a text data .
but here , we're predict the rate , and the paramet , of cours , ar veri differ .
but we can see , if we can uncov these paramet , it would be nice , becaus r sub i of d is precis as the rate that we want to get .
and these ar the compos rate on differ aspect .
sub i d is precis the aspect weight that we hope to get as a byproduct , that we also get the beta factor , and these ar the factor , the sentiment weight of word .
so more formal , the data we ar model here is a set of review document with overal rate .
and each review document denot by a d , and the overal rate denot by r sub d .
and d pre segment turn into k aspect segment .
and we're go to us ci w , d to denot the count of word w in aspect segment i .
of cours , it's zero if the word doesn't occur in the segment .
now , the model is go to predict the rate base on d .
so , we're interest in the provision problem of r sub d given d .
and thi model is set up as follow .
so r sub d is assum the two follow a normal distribut doesn't mean that denot actual await the averag of the aspect of rate r sub i of d as shown here .
thi normal distribut is a varianc of data squar .
now , of cours , thi is just our assumpt .
the actual rate is not necessarili anyth thing thi wai .
but as alwai , when we make thi assumpt , we have a formal wai to model the problem and that allow us to comput the interest in quantiti .
in thi case , the aspect rate and the aspect weight .
now , the aspect rate as you see on the is assum that will be a weight of sum of these weight .
where the weight is just the of the weight .
so as i said , the overal rate is assum to be a weight averag of aspect rate .
now , these other valu , r for sub i of d , or denot togeth by other vector that depend on d is that the token of specif weight .
and we ar go to assum that thi vector itself is drawn from anoth multivari gaussian distribut , with mean denot by a mu factor , and covari metric sigma here .
now , so thi mean , when we gener our overal rate , we're go to first draw a set of other valu from thi multivari gaussian prior distribut .
and onc we get these other valu , we're go to us then the weight averag of aspect rate as the mean here to us the normal distribut to gener the overal rate .
now , the aspect rate , as i just said , is the sum of the sentiment weight of word in aspect , note that here the sentiment weight ar specif to aspect .
so , beta is index by i , and that's for aspect .
and that give us a wai to model differ segment of a word .
thi is neither becaus the same word might have posit sentiment for anoth aspect .
it's also us for see what paramet we have here beta sub i and w give us the aspect specif sentiment of w .
so , obvious , that's on of the import paramet .
but , in gener , we can see we have these paramet , beta valu , the delta , and the mu , and sigma .
so , next , the question is , how can we estim these paramet and , so we collect denot all the paramet by lambda here .
now , we can , as usual , us the maximum likelihood estim , and thi will give us the set of these paramet , that with a maxim observ rate condit of their respect review .
and of , cours , thi would then give us all the us variabl that we ar interest in comput .
so , more specif , we can now , onc we estim the paramet , we can easili comput the aspect rate , for aspect the i or sub i of d .
and that's simpli to take all of the word that occur in the segment , i , and then take their count and then multipli that by the center of the weight of each word and take a sum .
so , of cours , thi time would be zero for word that ar not occur in and that's why were go to take the sum of all the word in the vocabulari .
now what about the s factor weight ?
alpha sub i of d , well , it's not part of our paramet .
right ?
so we have to us that to comput it .
and in thi case , we can us the maximum a posteriori to comput thi alpha valu .
basic , we're go to maxim the product of the prior of alpha accord to our assum multivari gaussian distribut and the likelihood .
in thi case , the likelihood rate is the probabl of gener thi observ overal rate given thi particular alpha valu and some other paramet , as you see here .
so for more detail about thi model , you can read thi paper cite here .
thi lectur is a continu discuss of latent aspect rate analysi .
earlier , we talk about how to solv the problem of lara in two stage .
but we first do segment of differ aspect .
and then we us a latent regress model to learn the aspect rate and then later the weight .
now it's also possibl to develop a unifi gener model for solv thi problem , and that is we not onli model the gener over rate base on text .
we also model the gener of text , and so a natur solut would be to us topic model .
so given the entiti , we can assum there ar aspect that ar describ by word distribut .
topic .
and then we an us a topic model to model the gener of the review text .
i will assum word in the review text ar drawn from these distribut .
in the same wai as we assum for gener model like prsa .
and then we can then plug in the latent regress model to us the text to further predict the overr .
and that mean when we first predict the aspect rate and then combin them with aspect weight to predict the overal rate .
so thi would give us a unifi gener model , where we model both the gener of text and the overal readi condit on text .
so we don't have time to discuss thi model in detail as in mani other case in thi part of the caus where we discuss the cut edg topic , but there's a refer site here where you can find more detail .
so now i'm go to show you some simpl result that you can get by us these kind of gener model .
first , it's about rate decomposit .
so here , what you see ar the decompos rate for three hotel that have the same overal rate .
so if you just look at the overal rate , you can't realli tell much differ between these hotel .
but by decompos these rate into aspect rate we can see some hotel have higher rate for some dimens , like valu , but other might score better in other dimens , like locat .
and so thi can give you detail opinion at the aspect level .
now here , the ground truth is shown in the parenthesi , so it also allow you to see whether the predict is accur .
it's not alwai accur but it's mostli still reflect some of the trend .
the second result you compar differ review on the same hotel .
so the tabl show the decompos rate for two review about same hotel .
again their high level overal rate ar the same .
so if you just look at the overal rate , you don't realli get that much inform about the differ between the two review .
but after you decompos the rate , you can see clearli that thei have high score on differ dimens .
so thi show that model can review differ in opinion of differ review and such a detail understand can help us understand better about review and also better about their feedback on the hotel .
thi is someth veri interest , becaus thi is in some sens some byproduct .
in our problem formul , we did not realli have to do thi .
but the design of the gener model ha thi compon .
and these ar sentiment weight for word in differ aspect .
and you can see the highli weight word versu the neg load weight word here for each of the four dimens .
valu , room , locat , and cleanli .
the top word clearli make sens , and the bottom word also make sens .
so thi show that with thi approach , we can also learn sentiment inform directli from the data .
now , thi kind of lexicon is veri us becaus in gener , a word like long , let's sai , mai have differ sentiment polar for differ context .
so if i sai the batteri life of thi laptop is long , then that's posit .
but if i sai the reboot time for the laptop is long , that's bad , right ?
so even for review about the same product , laptop , the word long is ambigu , it could mean posit or it could mean neg .
but thi kind of lexicon , that we can learn by us thi kind of gener model , can show whether a word is posit for a particular aspect .
so thi is clearli veri us , and in fact such a lexicon can be directli us to tag other review about hotel or tag comment about hotel in social media like tweet .
and what's also interest is that sinc thi is almost complet unsupervis , well assum the review whose overal rate ar avail and then thi can allow us to learn form potenti larger amount of data on the internet to reach sentiment lexicon .
and here ar some result to valid the prefer word .
rememb the model can infer wether a review care more about servic or the price .
now how do we know whether the infer weight ar correct ?
and thi pose a veri difficult challeng for evalu .
now here we show some interest wai of evalu .
what you see here ar the price of hotel in differ citi , and these ar the price of hotel that ar favor by differ group of review .
the top ten ar the review wa the highest infer valu to other aspect ratio .
so for exampl valu versu locat , valu versu room , etcetera .
now the top ten of the review that have the highest ratio by thi measur .
and that mean these review tend to put a lot of weight on valu as compar with other dimens .
so that mean thei realli emphas on valu .
the bottom ten on the other hand of the review .
the lowest ratio , what doe that mean ?
well it mean these review have put higher weight on other aspect than valu .
so those ar peopl that care about anoth dimens and thei didn't care so much the valu in some sens , at least as compar with the top ten group .
now these ratio ar comput base on the infer weight from the model .
so now you can see the averag price of hotel favor by top ten review ar inde much cheaper than those that ar favor by the bottom ten .
and thi provid some indirect wai of valid the infer weight .
it just mean the weight ar not random .
thei ar actual meaning here .
in comparison , the averag price in these three citi , you can actual see the top ten tend to have below averag in price , wherea the bottom half , where thei care a lot about other thing like a servic or room condit tend to have hotel that have higher price than averag .
so with these result we can build a lot of interest applic .
for exampl , a direct applic would be to gener the rate aspect , the summari , and becaus of the decomposit we have now gener the summari for each aspect .
the posit sentenc the neg sentenc about each aspect .
it's more inform than origin review that just ha an overal rate and review text .
here ar some other result about the aspect that's cover from review with no rate .
these ar mp3 review , and these result show that the model can discov some interest aspect .
comment on low overal rate versu those higher overal per rate .
and thei care more about the differ aspect .
or thei comment more on the differ aspect .
so that can help us discov for exampl , consum' trend in appreci differ featur of product .
for exampl , on might have discov the trend that peopl tend to like larger screen of cell phone or light weight of laptop , etcetera .
such knowledg can be us for manufactur to design their next gener of product .
here ar some interest result on analyz user rate behavior .
so what you see is averag weight along differ dimens by differ group of review .
and on the left side you see the weight of viewer that like the expens hotel .
thei gave the expens hotel <num> star , and you can see their averag rate tend to be more for some servic .
and that suggest that peopl like expens hotel becaus of good servic , and that's not surpris .
that's also anoth wai to valid it by infer weight .
if you look at the right side where , look at the column of <num> star .
these ar the review that like the cheaper hotel , and thei gave cheaper hotel five star .
as we expect and thei put more weight on valu , and that's why thei like the cheaper hotel .
but if you look at the , when thei didn't like expens hotel , or cheaper hotel , then you'll see that thei tend to have more weight on the condit of the room clean .
so thi show that by us thi model , we can infer some inform that's veri hard to obtain even if you read all the review .
even if you read all the review it's veri hard to infer such prefer or such emphasi .
so thi is a case where text mine algorithm can go beyond what human can do , to review interest pattern in the data .
and thi of cours can be veri us .
you can compar differ hotel , compar the opinion from differ consum group , in differ locat .
and of cours , the model is gener .
it can be appli to ani review with overal rate .
so thi is a veri us techniqu that can support a lot of text mine applic .
final the result of appli thi model for person rank or recommend of entiti .
so becaus we can infer the review weight on differ dimens , we can allow a user to actual sai what do you care about .
so for exampl , i have a queri here that show <num> of the weight should be on valu and <num> on other .
so that just mean i don't care about other aspect .
i just care about get a cheaper hotel .
my emphasi is on the valu dimens .
now what we can do with such queri is we can us review that we believ have a similar prefer to recommend a hotel for you .
how can we know that ?
well , we can infer the weight of those review on differ aspect .
we can find the review whose weight ar more precis , of cours infer rate ar similar to your .
and then us those review to recommend hotel for you and thi is what we call person or rather queri specif recommend .
now the non person recommend now shown on the top , and you can see the top result gener have much higher price , than the lower group and that's becaus when the review's care more about the valu as dictat by thi queri thei tend to realli favor low price hotel .
so thi is yet anoth applic of thi techniqu .
it show that by do text mine we can understand the user better .
and onc we can handl user better we can solv these user better .
so to summar our discuss of opinion mine in gener , thi is a veri import topic and with a lot of applic .
and as a text sentiment analysi can be readili done by us just text categor .
but standard techniqu tend to not be enough .
and so we need to have enrich featur implement .
and we also need to consid the order of those categori .
and we'll talk about ordin regress for some of these problem .
we have also assum that the gener model ar power for mine latent user prefer .
thi in particular in the gener model for mine latent regress .
and we emb some interest prefer inform and send the weight of word in the model as a result we can learn most us inform when fit the model to the data .
now most approach have been propos and evalu .
for product review , and that wa becaus in such a context , the opinion holder and the opinion target ar clear .
and thei ar easi to analyz .
and there , of cours , also have a lot of practic applic .
but opinion mine from new and social media is also import , but that's more difficult than analyz review data , mainli becaus the opinion holder and opinion target ar all interest .
so that call for natur manag process techniqu to uncov them accur .
here ar some suggest read .
the first two ar small book that ar of some us of thi topic , where you can find a lot of discuss about other variat of the problem and techniqu propos for solv the problem .
the next two paper about gener model for rate the aspect rate analysi .
the first on is about solv the problem us two stage , and the second on is about a unifi model where the topic model is integr with the regress model to solv the problem us a unifi model .
thi lectur is about the text base predict .
in thi lectur , we're go to start talk about the mine a differ kind of knowledg , as you can see here on thi slide .
name we're go to us text data to infer valu of some other variabl in the real world that mai not be directli relat to the text .
or onli remot relat to text data .
so thi is veri differ from content analysi or topic mine where we directli character the content of text .
it's also differ from opinion mine or sentiment analysi , which still have to do is character mostli the content .
onli that we focu more on the subject of content which reflect what we know about the opinion holder .
but thi onli provid limit review of what we can predict .
in thi lectur and the follow lectur , we're go to talk more about how we can predict more inform about the world .
how can we get the sophist pattern of text togeth with other kind of data ?
it would be us first to take a look at the big pictur of predict , and data mine in gener , and i call thi data mine loop .
so the pictur that you ar see right now is that there ar multipl sensor , includ human sensor , to report what we have seen in the real world in the form of data .
of cours the data in the form of non text data , and text data .
and our goal is to see if we can predict some valu of import real world variabl that matter to us .
for exampl , someon's hous condit , or the weather , or etc .
and so these variabl would be import becaus we might want to act on that .
we might want to make decis base on that .
so how can we get from the data to these predict valu ?
well in gener we'll first have to do data mine and analysi of the data .
becaus we , in gener , should treat all the data that we collect in such a predict problem set up .
we ar veri much interest in joint mine of non text and text data , which should combin all the data togeth .
and then , through analysi , gener there ar multipl predictor of thi interest variabl to us .
and we call these featur .
and these featur can then be put into a predict model , to actual predict the valu of ani interest variabl .
so thi then allow us to chang the world .
and so thi basic is the gener process for make a predict base on data , includ the test data .
now it's import to emphas that a human actual plai a veri import role in thi process .
especi becaus of the involv of text data .
so human first would be involv in the mine of the data .
it would control the gener of these featur .
and it would also help us understand the text data , becaus text data ar creat to be consum by human .
human ar the best in consum or interpret text data .
but when there ar , of cours , a lot of text data then machin have to help and that's why we need to do text data mine .
sometim machin can see pattern in a lot of data that human mai not see .
but in gener human would plai an import role in analyz some text data , or applic .
next , human also must be involv in predict model build and adjust or test .
so in particular , we will have a lot of domain knowledg about the problem of predict that we can build into thi predict model .
and then next , of cours , when we have predict valu for the variabl , then human would be involv in take action to chang a word or make decis base on these particular valu .
and final it's interest that a human could be involv in control the sensor .
and thi is so that we can adjust to the sensor to collect the most us data for predict .
so that's why i call thi data mine loop .
becaus as we perturb the sensor , it'll collect the new data and more us data then we will obtain more data for predict .
and thi data gener will help us improv the predict accuraci .
and in thi loop , human will recogn what addit data will need to be collect .
and machin , of cours , help human identifi what data should be collect next .
in gener , we want to collect data that is most us for learn .
and there wa actual a subarea in machin learn call activ learn that ha to do with thi .
how do you identifi data point that would be most help in machin learn program ?
if you can label them , right ?
so , in gener , you can see there is a loop here from data acquisit to data analysi .
or data mine to predict of valu .
and to take action to chang the word , and then observ what happen .
and then you can then decid what addit data have to be collect by adjust the sensor .
or from the predict arrow , you can also note what addit data we need to acquir in order to improv the accuraci of predict .
and thi big pictur is actual veri gener and it's reflect a lot of import applic of big data .
so , it's us to keep that in mind while we ar look at some text mine techniqu .
so from text mine perspect and we're interest in text base predict .
of cours , sometim text alon can make predict .
and thi is most us for predict about human behavior or human prefer or opinion .
but in gener text data will be put togeth as non text data .
so the interest question here would be , first , how can we design effect predictor ?
and how do we gener such effect predictor from text ?
and thi question ha been address to some extent in some previou lectur where we talk about what kind of featur we can design for text data .
and it ha also been address to some extent by talk about the other knowledg that we can mine from text .
so , for exampl , topic mine can be veri us to gener the pattern or topic base indic or predictor that can be further fed into a predict model .
so topic can be intermedi recognit of text .
that would allow us to do design high level featur or predictor that ar us for predict of some other variabl .
it mai be also gener from origin text data , it provid a much better implement of the problem and it serv as more effect predictor .
and similarli similar analysi can lead to such predictor , as well .
so , those other data mine or text mine algorithm can be us to gener predictor .
the other question is , how can we join the mine text and non text data togeth ?
now , thi is a question that we have not address yet .
so , in thi lectur , and in the follow lectur , we're go to address thi problem .
becaus thi is where we can gener much more enrich featur for predict .
and allow us to review a lot of interest knowledg about the world .
these pattern that ar gener from text and non text data themselv can sometim , alreadi be us for predict .
but , when thei ar put togeth with mani other predictor thei can realli help improv the predict .
basic , you can see text base predict can actual serv as a unifi framework to combin mani text mine and analysi techniqu .
includ topic mine and ani content mine techniqu or segment analysi .
the goal here is mainli to evok valu of real world variabl .
but in order to achiev the goal we can do some other prepar .
and these ar subtask .
so on subtask could mine the content of text data , like topic mine .
and the other could be to mine knowledg about the observ .
so sentiment analysi , opinion .
and both can help provid predictor for the predict problem .
and of cours we can also add non text data directli to the predict model , but then non text data also help provid a context for text analyst .
and that further improv the topic mine and the opinion analysi .
and such improv often lead to more effect predictor for our problem .
it would enlarg the space of pattern of opinion of topic that we can mine from text and that we'll discuss more later .
so the joint analysi of text and non text data can be actual understood from two perspect .
on perspect , we have non text can help with testimoni .
becaus non text data can provid a context for mine text data provid a wai to partit data in differ wai .
and thi lead to a number of type of techniqu for contextu type of mine .
and that's the mine text in the context defin by non text data .
and you see thi refer here , for a larg bodi of work , in thi direct .
and i will need to highlight some of them , in the next lectur .
now , the other perspect is text data can help with non text data mine as well .
and thi is becaus text data can help interpret pattern discov from non text data .
let's sai you discov some frequent pattern from non text data .
now we can us the text data associ with instanc where the pattern occur as well as text data that is associ with instanc where the pattern doesn't look up .
and thi give us two set of text data .
and then we can see what's the differ .
and thi differ in text data is interpret becaus text content is easi to digest .
and that differ might suggest some mean for thi pattern that we found from non text data .
so , it help interpret such pattern .
and thi techniqu is call pattern annot .
and you can see thi refer list here for more detail .
so here ar the refer that i just mention .
the first is refer for pattern annot .
the second is , qiaozhu mei's dissert on contextu text mine .
it contain a larg bodi of work on contextu text mine techniqu .
thi lectur is about the contextu text mine .
contextu text mine is relat to multipl kind of knowledg that we mine from text data , as i'm show here .
it's relat to topic mine becaus you can make topic associ with context , like time or locat .
and similarli , we can make opinion mine more contextu , make opinion connect to context .
it's relat to text base predict becaus it allow us to combin non text data with text data to deriv sophist predictor for the predict problem .
so more specif , why ar we interest in contextu text mine ?
well , that's first becaus text often ha rich context inform .
and thi can includ direct context such as meta data , and also indirect context .
so , the direct context can grow the meta data such as time , locat , author , and sourc of the text data .
and thei're almost alwai avail to us .
indirect context refer to addit data relat to the meta data .
so for exampl , from offic , we can further obtain addit context such as social network of the author , or the author's ag .
such inform is not in gener directli relat to the text , yet through the process , we can connect them .
there could be other text data from the same sourc , as thi on through the other text can be connect with thi text as well .
so in gener , ani relat data can be regard as context .
so there could be remov or rate for context .
and so what's the us ?
what is text context us for ?
well , context can be us to partit text data in mani interest wai .
it can almost allow us to partit text data in other wai as we need .
and thi is veri import becaus thi allow us to do interest compar analys .
it also in gener , provid mean to the discov topic , if we associ the text with context .
so here's illustr of how context can be regard as interest wai of partit of text data .
so here i just show some research paper publish in differ year .
on differ venu , differ confer name here list on the bottom like the sigir or acl , etc .
now such text data can be partit in mani interest wai becaus we have context .
so the context here just includ time and the confer venu .
but perhap we can includ some other variabl as well .
but let's see how we can partit thi interest of wai .
first , we can treat each paper as a separ unit .
so in thi case , a paper id and the , each paper ha it own context .
it's independ .
but we can also treat all the paper within <num> as on group and thi is onli possibl becaus of the avail of time .
and we can partit data in thi wai .
thi would allow us to compar topic for exampl , in differ year .
similarli , we can partit the data base on the menu .
we can get all the sigir paper and compar those paper with the rest .
or compar sigir paper with kdd paper , with acl paper .
we can also partit the data to obtain the paper written by author in the u . s . , and that of cours , us addit context of the author .
and thi would allow us to then compar such a subset with anoth set of paper written by also seen in other countri .
or we can obtain a set of paper about text mine , and thi can be compar with paper about anoth topic .
and note that these partit can be also intersect with each other to gener even more complic partit .
and so in gener , thi enabl discoveri of knowledg associ with differ context as need .
and in particular , we can compar differ context .
and thi often give us a lot of us knowledg .
for exampl , compar topic over time , we can see trend of topic .
compar topic in differ context can also reveal differ about the two context .
so there ar mani interest question that requir contextu text mine .
here i list some veri specif on .
for exampl , what topic have been get increas attent recent in data mine research ?
now to answer thi question , obvious we need to analyz text in the context of time .
so time is context in thi case .
is there ani differ in the respons of peopl in differ region to the event , to ani event ?
so thi is a veri broad an answer to thi question .
in thi case of cours , locat is the context .
what ar the common research interest of two research ?
in thi case , author can be the context .
is there ani differ in the research topic publish by author in the usa and those outsid ?
now in thi case , the context would includ the author and their affili and locat .
so thi goe beyond just the author himself or herself .
we need to look at the addit inform connect to the author .
is there ani differ in the opinion of all the topic express on on social network and anoth ?
in thi case , the social network of author and the topic can be a context .
other topic in new data that ar correl with sudden chang in stock price .
in thi case , we can us a time seri such as stock price as context .
what issu matter in the <num> presidenti campaign , or presidenti elect ?
now in thi case , time serv again as context .
so , as you can see , the list can go on and on .
basic , contextu text mine can have mani applic .
thi lectur is about a specif techniqu for contextu text mine call contextu probabilist latent semant analysi .
in thi lectur , we're go to continu discuss contextu text mine .
and we're go to introduc contextu probablitist latent semant analysi as exchang of po for do contextu text mine .
recal that in contextu text mine we hope to analyz topic in text , in consider of the context so that we can associ the topic with a properti of the context were interest .
so in thi approach , contextu probabilist latent semant analysi , or cplsa , the main idea is to express to the add interest context variabl into a gener model .
recal that befor when we gener the text we gener assum we'll start with some topic , and then assembl word from some topic .
but here , we're go to add context variabl , so that the coverag of topic , and also the content of topic would be ti in context .
or in other word , we're go to let the context influenc both coverag and the content of a topic .
the consequ that thi will enabl us to discov contextu topic .
make the topic more interest , more meaning .
becaus we can then have topic that can be interpret as specif to a particular context that we ar interest in .
for exampl , a particular time period .
as an extens of plsa model , cplsa doe the follow chang .
firstli it would model the condit likelihood of text given context .
that clearli suggest that the gener of text would then depend on context , and that allow us to bring context into the gener model .
secondli , it make two specif assumpt about the depend of topic on context .
on is to assum that depend on the context , depend on differ time period or differ locat , we assum that there ar differ view of a topic or differ version of word descript that character a topic .
and thi assumpt allow us to discov differ variat of the same topic in differ context .
the other is that we assum the topic coverag also depend on the context .
that mean depend on the time or locat , we might cover topic differ .
again , thi depend would then allow us to captur the associ of topic with specif context .
we can still us the em algorithm to solv the problem of paramet estim .
and in thi case , the estim paramet would natur contain context variabl .
and in particular , a lot of condit probabl of topic given certain context .
and thi is what allow you to do contextu text mine .
so thi is the basic idea .
now , we don't have time to introduc thi model in detail , but there ar refer here that you can look into to know more detail .
here i just want to explain the high level idea in more detail .
particularli i want to explain the gener process .
of text data that ha context associ in such a model .
so as you see here , we can assum there ar still multipl topic .
for exampl , some topic might repres a theme like a govern respons , donat or the citi of new orlean .
now thi exampl is in the context of hurrican katrina and that hit new orlean .
now as you can see we assum there ar differ view associ with each of the topic .
and these ar shown as view <num> , view <num> , view <num> .
each view is a differ version of word distribut .
and these view ar ti to some context variabl .
for exampl , ti to the locat texa , or the time juli <num> , or the occup of the author be a sociologist .
now , on the right side , now we assum the document ha context inform .
so the time is known to be juli <num> .
the locat is texa , etc .
and such context inform is what we hope to model as well .
so we're not go to just model the text .
and so on idea here is to model the variat of top content and variou content .
and thi give us differ view of the water distribut .
now on the bottom you will see the theme coverag of top coverag might also vari accord to these context becaus in the case of a locat like texa , peopl might want to cover the red topic more .
that's new orlean .
that's visual here .
but in a certain time period , mayb particular topic and will be cover more .
so thi variat is also consid in cplsa .
so to gener the searcher document with context , with first also choos a view .
and thi view of cours now could be from ani of these context .
let's sai , we have taken thi view that depend on the time .
in the middl .
so now , we will have a specif version of word distribut .
now , you can see some probabl of word for each topic .
now , onc we have chosen a view , now the situat will be veri similar to what happen in standard prsa we assum we have got word distribut associ with each topic , right ?
and then next , we will also choos a coverag from the bottom , so we're go to choos a particular coverag , and that coverag , befor is fix in plsa , and assign to a particular document .
each document ha just on coverag distribut .
now here , becaus we consid context , so the distribut of topic or the coverag of topic can vari depend on the context that ha influenc the coverag .
so , for exampl , we might pick a particular coverag .
let's sai in thi case we pick a document specif coverag .
now with the coverag and these word distribut we can gener a document in exactli the same wai as in plsa .
so what it mean , we're go to us the coverag to choos a topic , to choos on of these three topic .
let's sai we have pick the yellow topic .
then we'll draw a word from thi particular topic on the top .
okai , so we might get a word like govern .
and then next time we might choos a differ topic , and we'll get donat , etc .
until we gener all the word .
and thi is basic the same process as in plsa .
so the main differ is when we obtain the coverag .
and the word distribut , we let the context influenc our choic so in other word we have extra switch that ar ti to these contact that will control the choic of differ view of topic and the choic of coverag .
and natur the model we have more paramet to estim .
but onc we can estim those paramet that involv the context , then we will be abl to understand the context specif view of topic , or context specif coverag of topic .
and thi is precis what we want in contextu text mine .
so here ar some simpl result .
from us such a model .
not necessari exactli the same model , but similar model .
so on thi slide you see some sampl result of compar new articl about iraq war and afghanistan war .
now we have about <num> articl on iraq wa , r and <num> articl on afghanistan war .
and in thi case , the goal is to review the common topic .
it's cover in both set of articl and the differ of variat of the topic in each of the two collect .
so in thi case the context is explicitli specifi by the topic or collect .
and we see the result here show that there is a common theme that's correspond to cluster <num> here in thi column .
and there is a common theme indict that unit nation is involv in both war .
it's a common topic cover in both set of articl .
and that's indic by the high probabl word shown here , unit and nation .
now if you know the background , of cours thi is not surpris and thi topic is inde veri relev to both war .
if you look at the column further and then what's interest's that the next two cell of word distribut actual tell us collect specif variat of the topic of unit nation .
so it indic that the iraq war , unit nation wa more involv in weapon faction , wherea in the afghanistan war it wa more involv in mayb aid to northern allianc .
it's a differ variat of the topic of unit nation .
so thi show that by bring the context .
in thi case differ the wall or differ the collect of text .
we can have topic variat ti to these context , to review the differ of coverag of the unit nation in the two war .
now similarli if you look at the second cluster class two , it ha to do with the kill of peopl , and , again , it's not surpris if you know the background about war .
all the war involv kill of peopl , but imagin if you ar not familiar with the text collect .
we have a lot of text articl , and such a techniqu can reveal the common topic cover in both set of articl .
it can be us to review common topic in multipl set of articl as well .
if you look at of cours in that column of cluster two , you see variat of kill of peopl and that correspond to differ context and here is anoth exampl of result obtain from blog articl about hurrican katrina .
in thi case , what you see here is visual of the trend of topic over time .
and the top on show just the tempor trend of two topic .
on is oil price , and on is about the flood of the citi of new orlean .
now these topic ar obtain from blog articl about hurrican katrina .
and peopl talk about these topic .
and end up teach to some other topic .
but the visualis show that with thi techniqu , we can have condit distribut of time .
given a topic .
so thi allow us to plot thi condit probabl the curv is like what you're see here .
we see that , initi , the two curv track each other veri well .
but later we see the topic of new orlean wa mention again but oil price wa not .
and thi turn out to be the time period when anoth hurrican , hurrican rita hit the region .
and that appar trigger more discuss about the flood of the citi .
the bottom curv show the coverag of thi topic about flood of the citi by block articl in differ locat .
and it also show some shift of coverag that might be relat to peopl's migrat from the state of louisiana to texa for exampl .
so in thi case we can see the time can be us as context to review trend of topic .
these ar some addit result on spacial pattern .
in thi case it wa about the topic of govern respons .
and there wa some critic about the slow respons of govern in the case of hurrican katrina .
and the discuss now is cover in differ locat .
and these visual show the coverag in differ week of the event .
and initi it's cover mostli in the victim state , in the south , but then gradual spread into other locat .
but in week four , which is shown on the bottom left , we see a pattern that's veri similar to the first week on the top left .
and that's when again hurrican rita hit in the region .
so such a techniqu would allow us to us locat as context to examin their issu of topic .
and of cours the moral is complet gener so you can appli thi to ani other connect of text .
to review spatial tempor pattern .
hi view found anoth applic of thi kind of model , where we look at the us of the model for event impact analysi .
so here we're look at the research articl inform retriev .
ir , particularli sigir paper .
and the topic we ar focus on is about the retriev model .
and you can see the top word with high probabl about thi model on the left .
and then we hope to examin the impact of two event .
on is a start of trec , for text and retriev confer .
thi is a major evalu sponsor by u . s .
govern , and wa launch in <num> or around that time .
and that is known to have made a impact on the topic of research inform retriev .
the other is the public of a semin paper , by croft and port .
thi is about a languag model approach to inform retriev .
it's also known to have made a high impact on inform retriev research .
so we hope to us thi kind of model to understand impact .
the idea here is simpli to us the time as context .
and us these event to divid the time period into a period befor .
for the event and anoth after thi event .
and then we can compar the differ of the topic .
the and the variat , etc .
so in thi case , the result show befor track the studi of retriev model wa mostli a vector space model , boolean model etc .
but the after trec , appar the studi of retriev model have involv a lot of other word .
that seem to suggest some differ retriev task , so for exampl , email wa us in the enterpris search task and subtop retriev wa anoth task later introduc by trec .
on the bottom , we see the variat that ar correl with the propag of the languag model paper .
befor , we have those classic probabl risk model , logic model , boolean etc . , but after <num> , we see clear domin of languag model as probabilist model .
and we see word like languag model , estim of paramet , etc .
so thi techniqu here can us event as context to understand the impact of event .
again the techniqu is gener so you can us thi to analyz the impact of ani event .
here ar some suggest read .
the first is paper about simpl stage of psi to label cross collect comparison .
it's to perform compar text mine to allow us to extract common topic share by multipl collect .
and there ar variat in each collect .
the second on is the main paper about the cplsa model .
wa a discuss of a lot of applic .
the third on ha a lot of detail about the special tempor pattern for the hurrican katrina exampl .
thi lectur is about how to mine text data with social network as context .
in thi lectur we're go to continu discuss contextu text mine .
in particular , we're go to look at the social network of other as context .
so first , what's our motiv for us network context for analysi of text ?
the context of a text articl can form a network .
for exampl the author of research articl might form collabor network .
but author of social media content might form social network .
for exampl , in twitter peopl might follow each other .
or in facebook as peopl might claim friend of other , etc .
so such context connect the content of the other .
similarli , locat associ with text can also be connect to form geograph network .
but in gener you can can imagin the metadata of the text data can form some kind of network if thei have some relat .
now there is some benefit in jointli analyz text and it social network context or network context in gener .
and that's becaus we can us network to impos some constraint on topic of text .
so for exampl it's reason to assum that author connect in collabor network tend to write about the similar topic .
so such heurist can be us to guid us in analyz topic .
text also can help character the content associ with each subnetwork .
and thi is to sai that both kind of data , the network and text , can help each other .
so for exampl the differ in opinion express that ar in two subnetwork can be review by do thi type of joint analysi .
so here briefli you could us a model call a network supervis topic model .
in thi slide we're go to give some gener idea .
and then in the next slide we're go to give some more detail .
but in gener in thi part of the cours we don't have enough time to cover these frontier topic in detail .
but we provid refer that would allow you to read more about the topic to know the detail .
but it should still be us to know the gener idea .
and to know what thei can do to know when you might be abl to us them .
so the gener idea of network supervis topic model is the follow .
let's start with view the regular topic model .
like if you had an lda as sort optim problem .
of cours , in thi case , the optim object function is a likelihood function .
so we often us maximum likelihood estim to obtain the paramet .
and these paramet will give us us inform that we want to obtain from text data .
for exampl , topic .
so we want to maxim the probabl of test that ar given the paramet gener denot by number .
the main idea of incorpor network is to think about the constraint that can be impos base on the network .
in gener , the idea is to us the network to impos some constraint on the model paramet , lambda here .
for exampl , the text at adjac node of the network can be similar to cover similar topic .
inde , in mani case , thei tend to cover similar topic .
so we mai be abl to smooth the topic distribut on the graph on the network so that adjac node will have veri similar topic distribut .
so thei will share a common distribut on the topic .
or have just a slight variat of the topic of distribut , of the coverag .
so , technic , what we can do is simpli to add a network and us the regular to the likelihood of object function as shown here .
so instead of just optim the probabl of test data given paramet lambda , we're go to optim anoth function f .
thi function combin the likelihood with a regular function call r here .
and the regular defin the the paramet lambda and the network .
it tell us basic what kind of paramet ar prefer from a network constraint perspect .
so you can easili see thi is in effect implement the idea of impos some prior on the model paramet .
onli that we're not necessari have a probabilist model , but the idea is the same .
we're go to combin the two in on singl object function .
so , the advantag of thi idea is that it's quit gener .
here the top model can be ani gener model for text .
it doesn't have to be plsa or lea , or the current topic model .
and similarli , the network can be also in a network .
ani graph that connect these text object .
thi regular can also be ani regular .
we can be flexibl in captur differ heurist that we want to captur .
and final , the function f can also vari , so there can be mani differ wai to combin them .
so , thi gener idea is actual quit , quit power .
it offer a gener approach to combin these differ type of data in singl optim framework .
and thi gener idea can realli be appli for ani problem .
but here in thi paper refer here , a particular instanti call a netplsa wa start .
in thi case , it's just for instanti of plsa to incorpor thi simpl constraint impos by network .
and the prior here is the neighbor on the network must have similar topic distribut .
thei must cover similar topic in similar wai .
and that's basic what it sai in english .
so technic we just have a modifi object function here .
let's defin both the text you can actual see in the network graph g here .
and if you look at thi formula , you can actual recogn some part fairli familiarli .
becaus thei ar , thei should be fairli familiar to you by now .
so can you recogn which part is the likelihood for the test given the topic model ?
well if you look at it , you will see thi part is precis the plsa log likelihood that we want to maxim when we estim paramet for plsa alon .
but the second equat show some addit constraint on the paramet .
and in particular , we'll see here it's to measur the differ between the topic coverag at node u and node v .
the two adjac node on the network .
we want their distribut to be similar .
so here we ar comput the squar of their differ and we want to minim thi differ .
and note that there's a neg sign in front of thi sum , thi whole sum here .
so thi make it possibl to find the paramet that ar both to maxim the plsa log likelihood .
that mean the paramet will fit the data well and , also to respect that thi constraint from the network .
and thi is the neg sign that i just mention .
becaus thi is an neg sign , when we maxim thi object in function we'll actual minim thi statement term here .
so if we look further in thi pictur we'll see the result will weight of edg between u and v here .
and that space from out network .
if you have a weight that sai well , these two node ar strong collabor of research .
these two ar strong connect between two peopl in a social network .
and thei would have weight .
then that mean it would be more import that thei're topic coverag ar similar .
and that's basic what it sai here .
and final you see a paramet lambda here .
thi is a new paramet to control the influenc of network constraint .
we can see easili , if lambda is set to <num> , we just go back to the standard plsa .
but when lambda is set to a larger valu , then we will let the network influenc the estim model more .
so as you can see , the effect here is that we're go to do basic plsa .
but we're go to also try to make the topic coverag on the two node that ar strongli connect to be similar .
and we ensur their coverag ar similar .
so here ar some of the sever result , from that paper .
thi is slide show the record result of us plsa .
and the data here is dblp data , bibliograph data , about research articl .
and the experi have to do with us four commun of applic .
ir inform retriev .
dm stand for data mine .
ml for machineri and web .
there ar four commun of articl , and we were hope to see that the topic mine can help us uncov these four commun .
but from these assembl topic that you have seen here that ar gener by plsa .
and plsa is unabl to gener the four commun that correspond to our intuit .
the reason wa becaus thei ar all mix togeth and there ar mani word that ar share by these commun .
so it's not that easi to us four topic to separ them .
if we us more topic , perhap we will have more coher topic .
but what's interest is that if we us the netplsa where the network , the collabor network in thi case of author is us to impos constraint .
and in thi case we also us four topic .
but ned pierr said we gave much more meaning topic .
so here we'll see that these topic correspond well to the four commun .
the first is inform retriev .
the second is data mine .
third is machin learn .
and the fourth is web .
so that separ wa mostli becaus of the influenc of network where with leverag is a collabor network inform .
essenti the peopl that form a collabor network would then be kind of assum to write about similar topic .
and that's why we're go to have more coher topic .
and if you just listen to text data alon base on the occurr , you won't get such coher topic .
even though a topic model , like plsa or lda also should be abl to pick up co occur word .
so in gener the topic that thei gener repres word that co occur each other .
but still thei cannot gener such a coher result as netplsa , show that the network contest is veri us here .
now a similar model could have been also us to to character the content associ with each subnetwork of collabor .
so a more gener view of text mine in context of network is you treat text as live in a rich inform network environ .
and that mean we can connect all the relat data togeth as a big network .
and text data can be associ with a lot of structur in the network .
for exampl , text data can be associ with the node of the network , and that's basic what we just discuss in the netplsa .
but text data can be associ with ag as well , or path or even subnetwork .
and such a wai to repres text that ar in the big environ of all the context inform is veri power .
becaus it allow to analyz all the data , all the inform togeth .
and so in gener , analysi of text should be us the entir network inform that's relat to the text data .
so here's on suggest read .
and thi is the paper about netplsa where you can find more detail about the model and how to make such a model .
