up until now i've refer to the model that we're us as maximum entropi model .
but actual what we defin is these likelihood maxim model which ar defin with , in a certain exponenti model form .
in thi part i want to show why these model ar also refer to as maximum entropi model and what the motiv intuit is for the maximum entropi idea .
the motiv intuit for maximum entropi model is that in gener there ar ton of probabl distribut out there , most of which ar veri spike and specif , and would tend to overfit on particular data item .
what we want to do is to find a distribut that's as uniform as possibl except in place where we know that there's some reason to believ that the probabl distribut isn't uniform .
uniform can be thought of as high entropi .
we search for distribut which have the properti we desir , but which also have high entropi .
so thi is kind of embodi a statement of thoma jefferson's .
ignor is prefer to error , and he is less remot from the truth who believ noth than he who believ what is wrong .
so what we're want to do is not have ani belief in our model that we haven't particularli state .
to the extent that we haven't state ani constraint in our model , we want the probabl to be uniform as possibl .
so entropi is a quantiti that measur the uncertainti of a distribut .
so if we have an event x which ha some probabl , we can work out the surpris of that event by take the log of the invers of the probabl .
so if you see an event with a veri , which you think ha a veri small probabl of happen , your surpris is then great .
if you thought it had zero probabl of happen , your surpris is infinit .
on the other hand , if the event had a veri high probabl of happen , your surpris is small .
in particular , if you think it had probabl on , then your surpris is zero .
we can then work out the entropi of a distribut by take the expect over the surpris , which can be written out in thi form here .
and thi give us the equat for entropi .
so here's an exampl of that with the simpl case of flip a possibl weight coin .
so if the coin alwai come down head , or alwai come down tail , then there's no entropi in the distribut .
and the entropi is maxim when the coin is equal like to come down head or tail .
let me go through some concret exampl that show what happen when we try and maxim the entropi of a distribut , i . e . , to minim the commit .
so we're start off with no belief , and then in some wai we're go to want a probabl distribut that resembl some refer distribut , which for us we're go to be take straight from the observ data .
so what we're go to sai in our model is that we want to maxim entropi subject to featur base constraint .
and precis what our featur base constraint ar is to sai that the expect of the valu of featur in the model should be the same as the empir expect of those featur over our observ data .
so everi time that we add featur that , that put constraint into the model and therefor it lower the maximum entropi , but on the other hand , it rais the likelihood of the observ data .
it take the distribut further from uniform , but it bring the distribut closer to the data .
so here is a veri simpl exampl of that .
so here wa our unconstrain entropi distribut , which ha it maximum right here .
if we put in the constraint that said the probabl of the head wa <num> , then we've constrain the distribut to just a singl point .
and so at thi point the constrain distribut ha a lower maximum entropi than we had befor .
that exampl in on dimens is so simpl it's hard to see much .
so , let's do a slightli more complex exampl in two dimens .
so here we have two , two probabl , the probabl of head , and the probabl of tail .
so let's just assum those ar two number between zero and on and we haven't even model the fact that head and tail ar in complementari distribut .
well then , if we model thi entropi surfac , that we find that the maximum entropi come about when the probabl of p h and p t is around there , and well why is that , well that's becaus for the compon of the entropi distribut minu x log x , thei have their optimum at the valu of on on e , which is about . <num> , . <num> or someth like that .
so that's not what we normal see when we see the entropi pictur for a coin .
and that's becaus , normal , we immedi put in thi constraint sai that head and tail ar in complementari distribut .
the sum of their probabl have to add up to on .
and so then , onc we do that , we've constrain the space by sai we have to be somewher along thi line .
and then we're in the situat that entropi is maxim by have the probabl of head equal the probabl of tail equal a half .
but again , we could sort of stick in on more constraint , and sai the probabl of head is . <num> .
and then again , we've constrain the distribut down to a singl feasibl point .
and the point to notic is that , with each of these constraint , the maximum entropi of the model ha gone lower .
so here , we're at the true maximum of the function .
now we ar sort of awai from the true maximum of the function , but still at a reason high entropi point .
and here we've wander even further from the maximum of the function .
so our maximum entropi is go down , but we ar model fact about the situat in the world that we want to model .
let's look at a concret exampl that's a littl bit closer to an actual languag problem now .
let's suppos we have thi event space .
so we have part of speech , where here we just have six part of speech noun , plural noun , proper noun , proper plural noun , and two verb part of speech .
and thi wa our empir data .
so that we saw <num> differ event and of those the most common thing we saw were proper noun , which were actual two third of the data , and then we saw some regular noun and we saw a few verb .
right , so if we have a probabl for each of these event , and we just sai maxim , and thei're sort of p1 , p2 , p3 .
and we sai maxim those probabl .
again , the maximum entropi distribut is by set each of them to the valu <num> e .
but that's not what we want .
we want to sai that thi is a set of categor probabl .
and so then the maximum entropi distribut is to sai that each of those probabl is on sixth .
uniform is the maximum entropi distribut in categor distribut .
but that's too uniform given what happen in the data .
the noun ar much more common than verb in our data , and so we're gonna add a featur that ha valu on if the tag is a noun and zero otherwis .
well the expect valu of that featur is <num> over <num> , eight ninth .
that's just us the data from the previou slide .
so if we add that constraint , and then sai what's the maximum entropi distribut , it's go to set the probabl so that thi constraint is satisfi .
the sum of these probabl equal <num> over <num> , but within that categori probabl mass is still gonna be distribut uniformli becaus that's the maximum entropi distribut .
similarli , within the verb class you're gonna get the remain probabl mass distribut uniformli .
at that point we might notic that proper noun ar much more frequent that common noun , so we can add a second constraint that is a featur that ha valu on when the tag is a proper noun and the expect of that featur is two third , two third of the data ar proper noun , as we note befor .
if we put that into the model , we then get the expect of that featur observ , that two third of the probabl mass goe to proper noun which is again distribut uniformli .
we still have the featur that <num> <num> of the probabl mass goe to some kind of noun .
so the remaind is distribut evenli among the other noun and thi is as befor .
now , of cours we could keep on refin our model .
we could , for exampl , sai add a featur to sai that singular noun compris a certain amount of the data .
so singular noun compris eighteen thirti sixth of the data .
we can add that constraint in and eventu if we ad enough constraint , we'd forc the distribut to be exactli the same as the empir distribut .
it's veri easi to see that maximum entropi model ar convex model , so what's the idea of a convex function ?
the idea of a convex function , here f , is that if you take the function valu at the weight mean of ani set of point , then that function valu is greater than what you do if you take the function valu at those point , and then weight those function valu that you found , that we're kind of abov it up here .
and so that's distinguish from someth like a non convex function where you could have these local minima and so then the function valu of thi averag point is beneath what the function valu is if you just take the averag of the valu of the two greater point .
convex guarante that a function ha a singl global maximum becaus ani higher point of the function ar greedili reachabl .
in the maximum entropi formul it's easi to see that we have a convex function , so we can start of by show that the entropi function is convex .
and so we have that minu x log x is a convex function .
and so therefor if we take a sum and we drew it over here that's a convex function .
if we then take a sum of convex function , that's alwai convex .
then after that , what we're do is ad constraint to the function .
and so the feasibl region of a constrain entropi function is a linear subspac of it , which then also ha to be convex .
so , like , when we put a linear subspac constraint right here through our entropi surfac , that we've still got a convex function come out of it .
and so , i'm not gonna show it here , but the same is true for our origin maximum likelihood present , that we get a convex function .
okai .
well , i hope you now understand where the name maxent model come from and what thei kei idea of the maximum entropi principl is .
let's go through a few exampl now that show how maximum entropi model behav .
and in particular , we see how maximum entropi model don't doubl count featur in the same wai that the naiv bay model did .
so , for these exampl , we're assum thi teeni empir data set here .
so that there ar two featur , each of which can have two valu .
so there's littl or big b and there's littl or big a , and we've got thi distribut of the six data point .
and so these ar the featur that we're go to put in our model .
so the first featur is to sai that we have a probabl distribut , and so onc we put in that featur , the maximum entropi model is to sai okai , give a quarter of the probabl to each outcom .
now the second constraint we're go to put in the model is that we're go to sai , well , but wait a minut there ar a lot of big a's in thi data .
so our second featur is thi red featur .
and the expect of that red featur in the empir data is two third .
so we add a paramet to the model that captur that .
and so then the distribut that we get is that two third of the data ha to be in thi column , and so each of these cell is uniformli on third , on third .
well suppos what happen , is we just add a second featur to the model which is actual look at exactli the same thing and sai that the exp , expect of get a big a must be two third .
well , we now have two paramet weight , lambda prime and lambda doubl prime .
but what we're gonna optim the model to do is to sai , well , the sum of the expect in thi column must be two third .
and so , what is gonna happen , actual , is that the sum of lambda prime and lambda doubl prime is go to be the same as the valu of the old lambda a .
and we get exactli the same probabl distribut right here .
the effect of that in maximum entropi model is that featur that duplic the evid of other featur tend to not get much weight .
so , here's an exampl from our name entiti model .
so , what we're do is predict the name entiti that goe on the next token here .
so we're tag along a word at a time and we're do at grace road , and we've said that at is other , and we're go to want to give a name entiti tag to grace .
and our two candid in these exampl ar person or locat .
so you could reason think that grace is a good , the word grace is a good indic of someth be a person and should therefor have a high weight for person .
and if you look what's actual in the model , well it doe have a posit vote right here for grace be a person .
but it's a fairli weak posit vote .
and a lot of reason why it's a weak posit vote is actual just know the letter , the word start with g , is actual a rather strong indic that someth is a person and so the current word featur is a special case of thi more gener featur , that the begin of the word start with g .
and so most of the weight goe to there , and there's onli a littl bit of posit weight on know what the current word is .
okai , now let's look at a more interest exampl , where we have featur that overlap but aren't exactli the same .
so thi time , here's our empir data that we want to model these three point of empir data .
so , as befor we start off by constrain the valu to probabl and then as befor we put in thi red featur whose expect is still two third in the empir data and so that give us the same distribut befor with the same paramet weight lambda a .
okai , but what now if we want to an extra constraint to our model to captur the fact that there's data over here .
well the obviou thing to do is to notic that capit b also ha an expect of two third in the observ data .
so let's add in a featur that's true if a data point ha a capit b .
and let's add that to our model with a weight of lambda b .
you might be hope that that would mean that we model the data perfectli , but actual we don't .
the maximum entropi distribut is thi on here where we get <num> 9th chanc of littl b littl a , so we've smooth the model a littl which is mayb good , but we don't get a uniform distribut over the other point .
we get weight of <num> 9th in these two cell and <num> 9th in thi cell .
so you're expect almost half the probabl mass to got to get in big a big b .
if you think about it in term of the paramet weight down here it's kind of obviou what's happen .
that for the case of big a , big b , both of our featur fire and so the weight that goe into the maxent formul is the sum of lambda a and lambda b , and therefor it probabl ha to be much higher than the probabl that ar assign to just lambda a and lambda b alon .
now there's no weight over here so you might have thought that that should still be probabl zero , but rememb that we do have to be observ the empir expect of the model .
so the constraint of our model ar that the probabl here should sum to <num> <num> and the probabl here should sum to <num> <num> .
and then we've ad in thi constraint , that the probabl of thi cell should be doubl the probabl of thi cell and if you work through those constraint , the maximum likelihood solut is precis the on that's shown here , with four ninth , two ninth , two ninth and on ninth .
so what that show is that maximum entropi model don't model for free what statistician call interact term .
that if we want to sai someth special about how the culmin of have big a and big b behav , we have to do that by put in extra featur that model that .
so here we have the same data as befor .
and we start off with the same two featur as befor .
on wai we can fix our distribut is that we can add in a third featur , which is true if both a and b ar capit .
well the expect of that featur in the observ data is <num> <num> .
and so , that featur then determin that the probabl of thi cell must be on third .
well , given our other featur constraint come off these other column , here and here , then what we get is that the probabl ar on third in each cell and we exactli model the empir distribut includ of cours now that we do get probabl zero for the remain cell .
i mean of cours , that's not the onli wai that thi could have been achiev .
we could have instead , assign a differ featur f4 , which is true in ani of the situat of big a big b , big a littl b , or littl a big b .
so , ani of those three situat , it ha the valu on and zero otherwis .
and we can make a model with thi featur alon , and it would also give exactli the right distribut .
so in gener the thing to take awai from here is that a lot of the time we want to put in featur that model interact term or that model set of data .
i mean in particular natur languag context commonli what you want is to have featur that model natur class , so someth like a featur for the charact is a digit , or the charact is upper case , or the charact is the letter e , regardless of whether lowercas or uppercas .
that those kind of natur class make good featur becaus thei'll caus the model to gener in good wai .
how though do we find out which featur we want to put in our model ?
so , insid statist , when you're look at logist regress model , and .
maximum entropi model ar basic equival to multiclass logist regress model .
what it's standard to do is to do a greedi stepwis search over the space of all possibl interact term .
i mean , that is , you don't evalu everi possibl subset of featur .
'caus that number of possibl subset of featur is exponenti .
but you start with a null model , and you on by on , add in the featur that is the most us out of all of the featur that aren't yet in the model , until you find a good model .
that work reason well on tradit statist case where you have mayb ten or twenti featur but for the kind of case that we do in natur languag process we commonli us templat to gener thousand and thousand of featur .
so , for instanc , if we just have a , what is the current word , featur , that's a featur that might have <num> , <num> valu over a typic train set .
but we don't onli want that featur .
we want featur like , what is the previou word , what is the next word and often we want higher order featur like what is the word pair of the previou word and the current word .
so veri commonli we get model with million of featur and inde it's those model with million of featur that optim the perform of our system .
and well , we have million of featur we just can't be afford to train million upon million of model to try and work out what's the roughli optim set of featur even do it in a greedi fashion , and so , therefor , in nlp it's actual normal that which featur ar put into the model , which interact term ar put into the model is just be determin by hand base on linguist intuit .
not alwai , there ha been some work that ha been look at automat wai to find good featur interact , though , even that work is have to do some fairli heurist thing to make up for the fact that the search space is so big in thi case .
now here's an exampl show featur interact at work .
here we have again thi same exampl with at grace road .
and so we have , what is the previou class ?
and so in our exampl the previou class wa other .
and so , what thi sai is that , if the previou class wa other , it's veri unlik that the next word is a name entiti .
those ar strongli neg weight featur .
and that's just realli becaus most word aren't name entiti , and if you've just seen what came befor you wasn't a name entiti , probabl the next word isn't a name entiti either .
on the other hand if you see a word that is capit , so thi is done by these word shape signatur featur so it's a capit word .
well capit word in english ar just normal proper noun and proper noun ar normal entiti .
so that then ha , thi featur have veri posit weight for both person and locat .
okai so those two featur ar bang against each other and if you add up those term well certainli in thi case just the sum of those weight is approxim zero and so you're not actual get ani particular evid that a capit word after someth of class other , is go to be a name entiti .
but that's just wrong .
and so to get the model to work better , what we have to do is put in an interact term that model the conjunct of the previou state be other and it be a capit word .
and then when we do that , we find that thi interact term vote quit strongli for either the , for the word be either a person or a locat .
when featur overlap it's actual quit subtl the wai the weight of the featur work , but it's someth that's import to get a sens of to understand how maxent model work .
and i hope these exampl have help with that .
the model that we've been look at to motiv maximum entropi model have been joint model .
we've just had some data and we've been put a probabl distribut over that observ data .
so what do these joint model of the probabl of x have to do with the condit model of the full probabl of c given d that we were build beforehand ?
the answer to thi question is that we can think of the c cross d space as a complex x and , in particular , for the applic we're work with , the set of class is gener small ; you know mayb there's someth like two to <num> topic class , part of speech tag , name entiti label .
wherea on the other hand , d is gener huge , so d is the space of all possibl document , which is minim humung and possibl infinit .
in principl we can build model over the joint space of cd .
thi will involv calcul expect of featur over cd , such as is shown in the equat here .
but , in thi equat we're have to sum over the joint space .
and in gener that's impract 'caus we can't enumer all the member of x effect .
it's just far too big a space .
so d mai be huge or infinit , but onli a few d occur in our actual train data .
if at the end of the dai we're train on a million word of train data or a million document in a document classif system , then we have at most on million differ d and often in practic we'll have quit a few less becaus of repeat .
so someth we could try do is ad an extra featur to our model for each d and constrain it expect to match our empir data .
so we're sai that the probabl of each d is it observ probabl .
and what that will end up do is give all of the probabl mass to the observ document , and sai all of the rest of the entri of p c , d will be zero .
and we're show that here .
so now all probabl mass is go to the observ d and all the rest of them ar given probabl zero .
that seem a slightli crude thing to do , but it ha a clear practic benefit becaus now we have a much easier sum , becaus now we onli have to sum over the case which we saw in the data .
and so then we have to iter over the differ possibl class , but we don't have to iter ani more over all possibl data item .
so , if we've constrain the d margin in thi wai then in estim our model the onli thing that can vari is the condit distribut .
so , we're re write it as thi form and then we're sai that the probabl of d ha to be the observ probabl of d .
and so to maxim the likelihood that the model give to the data , the onli degre of freedom we're left with is adjust thi condit probabl distribut here .
thi is the connect between joint and condit maximum entropi or exponenti model .
condit model can be thought of as joint model with margin constraint where we exactli match the distribut of the observ data .
in thi constrict model form , maxim joint likelihood and condit likelihood of the data ar actual equival , becaus the joint likelihood with the constraint of match the margin is the same as maxim the condit likelihood .
okai , so i hope that's enough of an introduct to have made sens of how you can view exponenti or log linear model as model that maxim entropi .
in thi section i'm gonna talk about smooth maximum entropi model .
just like the other model we build in natur languag process , we still have the issu that these model can overfit and that we want to appli smooth techniqu so that the paramet we estim don't lead to too spiki distribut that overfit what wa observ in the train data .
thi topic of smooth maximum entropi model is often also describ as us a prior distribut for the paramet or do regular of the model .
the issu of smooth is veri promin in the model we build , becaus the model we build have lot and lot of featur .
typic , when you do logist regress in a statist class , your model might have onli have four , eight , twelv featur .
and there's enough data that you can suitabl estim the paramet of all those featur .
but typic , the model we'll build in natur languag process will have hundr of thousand , million , even ten of million of featur .
and so on thing to notic right there is simpli store the arrai of paramet valu will have a substanti memori cost , but from a statist estim point of view more importantli that most of the paramet of those featur will be veri poorli estim becaus we'll have veri limit data on which estim the paramet valu , so there ar lot of issu of sparsiti .
overfit to the train data is veri easi and we need smooth to prevent it .
and mani featur that we saw , happen to see at train time for some exampl , we might never see at all again , in further us of the model at test time .
so we want to not give too much weight to featur we happen to see at train time .
there ar other reason why we need to smooth our maxent model .
if we don't , featur weight can be infinit and the iter solver we us to set paramet valu can take a long time to get to those infin .
so , realli , we want to chang the model formul so the optim featur weight ar finit and therefor easili found in , findabl by an optim procedur .
let me motiv thi issu of smooth by look at a realli simpl exampl .
let's assum we're toss a coin and we have a distribut over head and tail .
the natur wai that that's formul with maximum entropi featur of the kind we've talk about , is we have two featur on for is it a head , and on for is it a tail .
and then we'll have the follow model distribut .
so , thi is the probabl of head where we have the featur weight for it be a head .
and the probabl of tail , with the featur weight for it be a tail .
and then , thi is our normal term , which is the same in both case .
that make it look like there ar two paramet .
but , there ar realli onli on degre of freedom here , and thi piec of math here show how thi maxent formul connect to the normal formul you see of two class logist regress in a statist textbook .
so if we instead said , what realli matter is the differ between the weight of the head and tail paramet , then we can sai that the probabl of head can instead be written like thi .
so what we're do here is we've just taken thi equat up here , and we've multipli each term by e to the minu lambda t .
and so then if we simplifi that math down a littl , we get e to the lambda over e to the lambda <num> .
and then the probabl of tail , do the same kind of trickeri , come out as e to the zero over e to the lambda plu zero and again if we simplifi that down , that's then on over e to the lambda plu on .
and so thi is the simpl form that you often see for a logist regress model .
and in the gener that's true , right ?
that what's import is the differ of the weight of oppos paramet when you have a multi , two class or multi class model .
okai so then if we graph lambda we get thi classic logist curv .
so with a weight of zero you get a half each chanc of head and tail and as the weight goe neg the probabl drop down initi sharpli and then veri slowli .
and as the lambda becom posit , the probabl climb initi quickli , and then veri slowli .
well , let's assum that we've seen just a littl bit of data and we estim lambda so as to maxim the likelihood of the observ data .
so then , the probabl of the observ data is the number of head time the log of the probabl of head .
thi is the log likelihood of the observ data .
plu the number of tail time the log likelihood of the probabl of tail .
and us the form on the previou equat , on the previou slide , that come out like thi .
okai , suppos we toss two head and two tail , then not surprisingli the optimum valu of lambda come out as zero , and that correspond to a probabl of head and tail as a half .
if we toss three head and on tail , then the optimum valu for the lambda paramet come out posit 'caus there's about a three quarter chanc of a head , and it come out about there .
the problem is , suppos in our train data we saw four head and no tail .
well , what that mean is we have a categor distribut in the observ data .
there ar no tail , and that mean the bigger lambda get , the higher the condit log likelihood of the observ data is , becaus a lambda valu of posit infin correspond to a categor distribut where the probabl of head is on and the probabl of tail equal zero .
and that's immedi problemat for both of the reason that were mention earlier .
firstli we don't want to estim categor model , where if a certain featur appear we sai that there's a probabl of on of some outcom .
we want to have smooth becaus normal our data is spars .
but also , if we ar ask our optim procedur to optim someth where the optimum valu is a po , is a infinit valu for a paramet , that's gonna be difficult for the optim procedur .
so there ar sever wai that thi problem ha been dealt with for maximum entropi model .
and i'll mention sever of them while concentr on the us of a prior distribut , which is the usual us method these dai .
so in the <num> <num> case there were two problem , so the first problem wa the optim valu of lambda wa infin which is a long trip for an optim procedur to find the optim valu of a paramet .
and inde it kind of can't .
it can just move it out and out to a larg number .
the second problem is that we got no smooth ; our learn distribut is gonna be just as spike as the empir on .
and again thi is gonna commonli happen with our nlp featur .
becaus we're gonna throw a million featur into the model and it's just gonna turn out that we're gonna sai word begin with gho .
well we onli saw two of them in the train data and in both case the word wa a person name .
therefor we're gonna sai anytim we see someth that start with gho it ha to be a person name .
but it might turn out that in futur data that's not necessari the case it might be an organ name , a place name , or mayb even a word that just start with gho like ghoul which isn't a proper name at all .
so how can we solv thi problem ?
on crude wai to solv thi problem is just to stop the optim earli .
so we could run an iter solver and sai , let's just run it for twenti iter and stop where we ar .
so if we do that , the valu of lambda will definit be finit , but will have grown fairli larg .
and the optim won't run an infinit loop .
and so thi wa commonli us in earli maxent work .
so the idea is we , over here , start the optim procedur .
we're track the likelihood up , make lambda bigger but we simpli stop after a few iter and so , for exampl , we might stop when the valu of lambda is five .
and the end result of that is that the probabl distribut for lambda equal five might be someth like a <num> percent chanc of a head , and a on percent chanc of a tail .
and we have satisfi thi goal of smooth the distribut slightli so that it's no longer categor and have our optim procedur run in a finit amount of time .
but here's a better wai of achiev that goal .
and that's to do what's refer to as us a prior distribut .
and so when we do thi , we talk about us map estim , which stand for maximum a postiori estim .
and the idea of that is , we suppos that we had a prior , we have a prior expect that paramet valu shouldn't be veri larg .
more strongli , we can sai that our prior expect is that most featur ar irrelev to the classif , and so , our prior expect is that featur weight ar zero .
and then , if we see evid that thei're us featur we'll gradual increas the weight of the paramet .
and so , we can then balanc the evid from the observ data with our prior belief .
the evid will never total defeat the prior , and so paramet will be smooth and kept finit .
and so we can do thi by explicitli chang optim object to maximum a postiori likelihood .
and so that's what we have down here , so now we're go to have a penal log likelihood of the condit , a penal condit log likelihood of the observ data , which is go to be the sum of the prior probabl of the paramet weight plu the previou condit log likelihood of the observ data , which is the evid .
the most common wai to do thi in practic is to us gaussian which ar also known as quadrat or l2 prior .
so our formal here is to sai that let's assum our prior belief is that each paramet will be distribut accord to a gaussian with mean mu and varianc sigma squar .
and so then the probabl of the paramet have differ valu is be given by thi equat for the normal curv , and the reason , we will soon see why thi is also refer to as quadrat or l2 prior , is thi term end up not realli matter and the import part is thi bit in here where we're get the squar distanc of the paramet valu from the mean .
so paramet ar penal for their valu drift far from the mean .
and in practic usual we take thi mean to be zero , which is sai that the featur is irrelev to the classif .
zero weight featur have no influenc on the classif .
there's thi extra paramet down here , a hyper paramet , as to how big is the varianc .
and thi is go to control how easi it is for the paramet to move awai from zero .
if the varianc is veri small , the optim will keep the paramet valu cluster close to zero .
if the varianc is veri weak , thei'll be allow to walk farther awai .
as a start off point when build model , just take two sigma squar to equal on commonli work rather well .
you can plai with thi valu .
commonli what you'll find for the kind of spars evidenc , mani featur model that we build , that make sigma much smaller than on quickli becom problemat but you can make sigma , two sigma squar , quit a bit larger and commonli you could have sort of somewher in the rang between a half and <num> , <num> and the model will work fairli well .
and so , here's a graph over here that sorta show you how the optim happen onc you've got a regular term .
so we have exactli the same four to zero distribut of head and tail and when , then we set the regular differ .
so if you if you take two sigma squar equal to on which is fairli strong regular , the maximum is posit so it's gonna sai you're more like to get a head than a tail .
but it's still gonna be someth like two third head , on third tail .
if you take two sigma squar equal to ten , then the regular is weaker , and so the valu of lambda is gonna come out at about two .
and so that's gonna sai , about <num> percent chanc of a head , and five percent chanc of a tail .
make the two sigma squar infinit is equival to have no regular at all .
that that's , lead us back to our previou model and then the optim valu of the paramet is infinit .
so if we us gaussian prior we're put in a trade off between expect match versu smaller paramet valu .
what thi mean in practic is that when multipl featur can be recruit to explain a certain observ , it's the more common on that will receiv the most weight .
so the wai to think about thi is to think that in the model formul , that you pai the penalti for have a non zero paramet weight just onc , wherea you get , can gain from have a non zero paramet weight for everi data item for which that paramet , that featur and it paramet weight will be us .
and so therefor , a featur that can be us a lot of time in explain data item will be allow to have a high weight becaus the prior is basic overwhelm .
wherea a featur that can onli help in explain a veri small number of observ will have it weight greatli constrain by the prior .
and the other good thing to know about have gaussian prior is put them in will improv the accuraci of your classifi , as we'll see .
let me just now go , veri concret through what happen , and the equat that you have to us when you have a prior .
so we're now go to us thi penal condit log likelihood , and so as well as thi term from befor , we're ad in thi term for the prior .
and so what is that term for the prior ?
it's the log of the normal distribut over the paramet weight .
so if we then go back and look at thi equat here , we then have to take the log of thi and so we're go to have effect out here , thi is just a constant .
and then we're go to be take the log of an exp and that will go awai and be an ident function .
so what we end up with here is that we're subtract thi term here , wherea the k over here is the log of the constant which we can ignor for the maxim of the penal condit likelihood , and so what we're actual work out here is thi squar term that is how far ar the paramet valu awai from the mean of the distribut .
and that term is then be scale by thi two sigma squar .
so you should be abl to see that how big you make two sigma squar sort of balanc the tradeoff between these two term .
and then , for the deriv of the condit , penal condit log likelihood , we're take the deriv of thi , which is the part that we saw befor , which is the differ between the actual weight and the predict weight of the featur .
and then we're subtract off the deriv of thi , and so that's just an easi quadrat form .
so the deriv of that is just over here .
and so what we're then go to sai is that to make thi zero , that our predict weight , the predict expect of some featur be true is go to be a littl bit less that it actual expect becaus it's be penal by thi amount here .
to simplifi thing down on step further .
in practic we almost alwai take the mean of our prior distribut to be zero , sinc the posit of least commit is to assum that featur ar irrelev .
and so then those equat simplifi down on time further that and we just get the result shown here , where we just have the lambda and so it's just how big the lambda ar , in their distanc awai from zero , that's determin what we're us .
let me go back to the exampl of make a maximum entropi name entiti recognit model that i've shown you befor .
actual when i wa show the slide befor i wa leav out a detail becaus the paramet weight of thi model were actual estim in a model that us gaussian prior smooth .
and now that we understand a littl bit more about that we can see the effect of that in the estim of the paramet of the model .
so here we have two featur and thi featur is a conjunct featur .
the present tag is proper noun and the previou tag is preposit .
so thi featur is necessarili go to be much less evidenc in the train data than thi featur , which is just that the current part of speech is a proper noun .
so the gener expect is that higher weight should be given to the more common featur if thei carri as much predict inform .
and that's what we see here , that you have the high weight given to the more gener featur and much smaller weight given to the more specif featur and that's becaus even though there is some further evid from thi conjunct featur here , the fact of the matter is you're quit like to have a name entiti when the part of speech is a proper noun .
and so most of the weight is go to the more gener featur here .
and we also see the same effect in some of the other featur class that we talk about befor .
so for the two abov it , the current word be grace is a fairli specif and rare evidenc featur , wherea the fact that the word begin with a capit letter g is a much more gener featur .
and so that we're see most of the weight for thi be good evid that it's a person rather a locat go to thi much more gener featur .
and if we look down further below , you can mayb see other case of the same thing happen .
but you should also realiz that when there is a lot of extra inform from a featur conjunct , then that featur conjunct will get a lot of weight still .
and we saw an exampl of that with these two case here .
so that we discuss how , sinc most token should be classifi as not an entiti , that if you just simpli know that the previou word wa not an entiti , most like the next word is not an entiti as well .
and so , you get thi highli neg weight for both entiti class .
but if you know that the previou word wa not an entiti , but you also know that the current word is capit , then in that case there's strong evid that you should classifi thing differ and so thi featur doe get signific weight even though it's a conjunct featur that involv thi other previou featur .
you just get kind of a lot of evid from know that conjunct is true .
okai .
here's an exampl show the effect of us a gaussian smooth for estim a maximum entropi model .
and thi is an exampl from learn a part of speech tagger from the work of christina toutanova and me and other at stanford .
okai .
and so thi exampl show the typic effect of what happen with smooth versu unsmooth maximum entropi model .
so what we find is that , so we've train both model for up to <num> iter up here .
what we , on thing we find that's just good to know is , and thi is look at accuraci on test data , that the model with smooth perform a lot better than the model without smooth .
okai that's about . <num> of a percent better but part of speech tag gener is highli accur and so it's better off think of thi as error reduct , so if you're reduc the error rate from <num> down to <num> that's actual quit a larg error reduct .
okai , but we can see a bit more than just that .
so you should see here that for the model with no smooth it show thi veri typic pattern that you us to observ for the train of maxent model , that as you train the model , initi , the accuraci improv strongli up to a peak and then it start to declin .
so thi is where you see the overfit of the model to the train data in a wai that actual mean it doe wors at test time .
and so common practic wa to stop train of the maxent model around iter <num> .
and so thi is sort of be a littl bit unfair to what normal happen without smooth .
if you did earli stop you'd realli be get an accuraci of someth like <num> . <num> , let's sai .
but nevertheless that's still well less than the accuraci of the smooth model .
on the other hand , if you have a smooth model , that the smooth model's likelihood just nice increas until it converg and then optim stop , give us thi perform here .
there's actual a second effect that's veri interest , is if we're focus on particular , on the perform of the , of the model on unknown word .
now unknown word ar be estim , the part of speech tag distribut is be estim by us special featur that gener over word , such as what letter do thei begin with and end with .
but nevertheless a lot of those featur ar themselv pretti spars .
and so what we find is that the model without smooth especi overfit and tend to do badli on the unknown word , where the model with smooth is do three percent better here on the unknown word .
and so that's a veri signific increas , and veri help to applic , 'caus perform on previou , on word that the model wasn't train on is especi import to the good perform of applic .
okai .
so , smooth is just good .
it soften distribut .
it push the weight onto more explanatori featur .
it allow you to dump more and more featur into the model without a lot of problem .
and at least if you don't do earli stop , it speed up the converg of your model .
let me give just on terminolog note .
talk of prior and maximum a posteriori estim is languag from bayesian statist .
in frequentist statist peopl will instead talk about do regular of maximum entropi model and in particular gaussian prior call l2 regular .
i'm not realli go to get into that .
all you realli need to know is that the math come out the same .
it doesn't matter what name you choos .
let me then just quickli mention two other wai of do smooth .
anoth wai of think about smooth is to smooth the data , not the paramet .
and we saw that earlier when talk about languag model .
so if the distribut we actual saw wa four head and zero tail , we can smooth that by do add on smooth sai , and sai we got five head and on tail .
and well then we've solv our problem , becaus if we then do maximum condit likelihood estim , that we're set the valu of the paramet to someth like <num> and it's got a finit valu and we don't have a problem .
that work fine for a veri simpl exampl like thi .
the reason that that's not practic for model with million of paramet is it becom almost imposs to know how to creat artifici data in such a wai that everi paramet ha a non zero valu without creat enorm amount of artifici data .
so that the amount of artifici data overwhelm the amount of real data .
a final thing that's commonli done in nlp model is to us count cutoff with featur .
so the idea there is you calcul featur for each data item in your model and then you look at the featur and what their empir support in the train data is and you simpli sai someth like , okai for ani featur that wa observ three time or less in the train data , i'm just go to dump it from my model and estim the model over the remain featur .
in the discuss of smooth , thi is a weak and indirect smooth method .
so effect , what it's do is sai we're go to estim the weight of all rare featur as zero , which you can also think of assign them a gaussian prior with zero varianc and mean zero so that their weight can never move awai from zero .
so drop low count doe remov the featur which ar most in need of smooth and it doe speed up estim of the model by reduc the model size but it's a veri crude method of do smooth and so the messag i'd like to give is , count cutoff gener hurt accuraci in the presenc of proper smooth .
a lot of peopl got into the habit of us count cutoff in the dai befor regular model , 'caus in those case , it would usual help to us count cutoff , becaus you got less overfit of your model .
but with proper smooth , you shouldn't need to us count cutoff to get the best possibl model .
that doesn't mean that there's no reason to us count cutoff .
the most common reason to us count cutoff in practic is becaus you want to shrink the size of your model that ten million paramet take a lot of memori to store , and you might prefer to build a model that onli ha <num> , <num> paramet .
and then , obvious what you want to do is keep the most us featur which will be normal basic the on that have signific frequenc of occurr in the data .
okai , so that's the end of the discuss on smooth or prior for maxent model .
in thi section we've onli talk about us of gaussian or l2 prior .
in recent work there ha been quit a bit of discuss of us other prior , in particular there's common us of l1 prior which is a differ wai of cut down the number of featur in your model .
you mai have seen that if you've seen other thing like machin learn but i'm not gonna discuss that in these class .
in thi segment i will give a brief introduct to the task of part of speech tag and then we'll come back and see how discrimin sequenc model can be us as a techniqu for part of speech tag .
there ar mani linguist tradit in differ part of the globe , but in the west the idea of part of speech perhap start with aristotl in the fourth centuri befor the common era , and he suggest the idea that there were part of speech , which ar otherwis known as lexic categori , word class , tag , or po is a common abbrevi .
but what we think of todai as the part of speech ar normal attribut to dionysiu thrax of alexandria who propos that there were eight part of speech .
and that's the number that's still with us todai as the number of part of speech peopl get taught in school sometim .
though actual if you look into the detail a littl the eight part of speech that dionysiu thrax propos aren't the same that ar with us todai .
so the normal list that peopl ar taught in school in english now ar noun , verb , adject , adverb , preposit , conjunct , pronoun , interject .
but for thrax there wa actual some littl differ in the list .
and of cours he wa work with ancient greek , and not with english .
so the begin of the list start out the same , but note that he doesn't recogn adject as a part of speech .
in greek and latin adject and noun behav veri similarli .
but he doe recogn participl as a part of speech .
and then the other differ , is that a separ part of speech is , is given for articl , word like the and a .
wherea in the modern list , you get a class for interject .
the statu of the articl ha alwai been a bit controversi .
in our current standard set of eight part of speech , articl ar seen as a case of adject but actual , as soon as you start look at the grammar of languag like english , that the articl have a veri special behavior which is quit differ from regular adject .
for exampl , adject , you can get multipl of larg green leather couch .
wherea you can't do thing like the a thi book , except as a kind of speech hesit as to which articl you should be us .
when we do part of speech tag in comput linguist , we normal work with rather more than eight part of speech , becaus there ar a lot of distinct which can be us to make that multipli the number of part of speech .
and i , so i start to indic the kind of part of speech you get in thi slide and you can see that there ar variou hierarch group .
so insid noun , we can divid into proper noun and common noun , and we might also want to divid between singular and plural which would happen with both proper and common noun .
verb veri much divid into main verb , see , regist ; versu variou auxiliari and modal verb like can and had .
there ar adject and adverb .
note that adject , at least the short adject in english have sever form the posit , compar , and superl .
and we might want to distinguish those form as well .
neither of the list on the last slide had number but thei seem a fairli obviou and practic us part of speech to recogn .
and then similarli here among the close class , i'll sai a bit more about close class on the next slide , we don't have a categori that's call articl , we have a categori call determin .
so the modern linguist tradit is to group the articl , a and the , togeth with certain other demonstr that behav similarli .
word like some , thi .
and then we have some other categori .
preposit ar familiar but we're distinguish off a class of particl .
so the particl here ar word that ar normal the same as preposit but take part in english phrasal verb construct , thing like made up , mean to invent .
the top level distinct on the last slide wa between open class and close class .
it's veri clear that not all part of speech ar the same .
some of them like determin and pronoun ar occupi by onli a small set of word .
so in english , for pronoun there's he , she , it , him , her and the first and second person pronoun i , you , we .
the third person plural thei , them .
but it's just a veri small set of word and that there aren't new pronoun be invent .
and so thing like that we call close class becaus in gener close class don't get new member .
though you know sometim occasion thei do over long term languag chang .
so , sometim peopl have suggest that we should be invent some new gender neutral pronoun in english , though noth's realli taken off .
but in contrast the open class , noun , verb , adject , adverb , these ar class which have a vast number of member and beyond that it's veri easi for them to gain new member so that when we have a new protocol in comput scienc we creat a new noun for it like ssh and then veri quickli we decid it's us to us that as a verb as well and then we sai i sshed in to my server to fix the configur .
the task of part of speech tag is to sai , for each word , what it part of speech is in the context of some run text .
the reason why that's not trivial is becaus lot of word have more than on part of speech .
so , for exampl , for the word back , it can be an adject when it's modifi a noun , like the back door .
it can be a noun when it's refer to the bodi part , on my back .
it can be an adverb , in a phrase like win the voter back and it can also be then a verb in anoth polit exampl promis to back the bill .
and so what we're want to do is when we see an instanc of the word back assign it to on of these part of speech .
so in the task of part of speech tag , we start with some text , plai well with other .
and then for each word we consid what ar it possibl part of speech , so that plai can be a plural noun or verb .
well can be an interject , an adject , a noun or an adverb .
with can be , is just a preposit here , and other is a plural noun .
and so for the word that ar ambigu , we're then go to choos for them what the correct part of speech is in context .
and so here , plai is a verb , and well is an adverb modifi plai .
and we get disambigu these part of speech .
so what we're find here at the end , when we've got , you know , these choic here which we ar then determin , these ar our part of speech tag .
and depend on the part of speech tag set , there ar commonli in natur languag process applic somewher between twelv and <num> part of speech tag that ar differenti .
and so you can see that thi set of part of speech tag is differenti thing like is it a plural noun or a singular noun , here's a singular noun .
or if it is a verb , what form of the verb it is .
so thi is the third person singular plai as oppos to plai .
so the particular part of speech tag set that we're see here is the penn treebank part of speech tag , which ar the most commonli us part of speech tag for english , especi in the unit state .
i'm embarrass to sai that i've been in thi busi long enough that i can actual tell you off by heart what all the penn part of speech tag ar .
there ar a set of <num> of them , which cover word class like thi , and punctuat .
so , what good is part of speech tag ?
as well as let you answer your high school english problem , it ha a varieti of other us .
it ha some particular us where just part of speech ar suffici so that if you're do text to speech and you want , and you see an instanc of the word l e a d you want to know whether to pronoun it as lead or lead .
and the answer to that is well , what's it part of speech ?
if it's a verb it's lead .
if it's a noun it's lead .
have part of speech let you easili do some surfac linguist analysi .
so for lot of applic it's us to pick out what we call base noun phrase .
so thei're a noun or noun compound , possibl modifi in front by a determin and some adject .
so the nearest school or , an old green couch ar base noun phrase wherea big complex noun phrase with rel claus after them aren't .
if we have part of speech we can easili write regular express that pick out thing like base noun phrase or verb follow by a particl for phrasal verb .
and that can be us for a lot of applic where you want to do shallow process but nevertheless want to be abl to put , to give to the user multiword keyword or someth like that .
part of speech tag is commonli us as input to , or to speed up a parser , so quit a few parser work on alreadi part of speech tag text , and even on that can do their own part of speech tag can often be sped up by first do the process of part of speech tag .
final a lot of model we build for probabl estim ar veri spars if thei try and work at the word level .
and we discuss variou wai of back off , where you might back off from a trigram to a bigram earlier .
but anoth common wai of back off is to back off from a word that you don't know much about to it part of speech categori .
you can predict base on know it's an adject .
part of speech tag perform is normal measur as accuraci .
what percent of tag do you get right ?
and the answer to that , that for the best current tagger , thei get about <num> percent of word right .
that sound realli good .
it is import to realiz that part of speech tag is in some sens an easi task or at least a task where you get high number .
so there's a baselin approach to part of speech tag where you tag everi word with it most common tag , if you know about it in your train data and if you don't know about it in your train data , you tag it as a noun .
and it turn out that that veri fairli simpl baselin method alreadi give you perform of <num> .
and so that's , it turn out that you get thi high baselin for a coupl of reason .
on reason is that mani word ar unambigu , that if you see famou , it's alwai an adject .
and also the wai part of speech tag is normal measur , that you get point for everi token .
so not onli do you get point for everi instanc of unambigu articl , which ar veri common like the and a , you even get point for get the part of speech tag of punctuat right .
but that doesn't mean that all part of speech decis ar easi .
some part of speech decis ar quit difficult , even for human be .
and so here ar some exampl of the word around .
so here we have around act as a particl , got around to .
here we have around again straight after a verb but thi time it's thi preposit phrase , around the corner , and so around is be a preposit .
and here again in thi third exampl , around is again follow straight after a verb but actual thi time it's a modifi of thi noun phrase of around <num> , short for around <num> .
and so around is then an adverb ; it's modifi a number which is kind of like an adject .
it turn out that in an english corpu , around eleven percent of the word type ar ambigu with regard to part of speech .
that doesn't sound like veri mani , but most of them tend to be common word , such as the word that .
so that can be a word that introduc a subordin claus and in the penn treebank those word were tag as in preposit , which might surpris you ; there's some linguist argument about that .
it can be a determin , that plai , or it can be an adverb .
so modifi here far , that far , is an adverb .
and so it turn out that although onli eleven percent of the word type ar ambigu as to part of speech , about <num> percent of the word token ar ambigu , so there actual ar a lot of decis for a part of speech tagger to make .
okai , i hope that's given you a bit of background inform on part of speech tag so you can make sens of the task in term of our method for approach it .
now let me return to part of speech take and sai a littl bit more about how sequenc model perform and what kind of featur end up be us and us .
so what ar the main sourc of inform for part of speech tag ?
on sourc of inform is knowledg of neighbor word and their possibl part of speech tag .
so if our sentenc is , bill saw that man yesterdai , we have a lot of part of speech ambigu .
so , bill can be either a proper noun or a verb .
saw can be either a noun or a verb .
that can be either a determin or what's tag as in when introduc a complement claus .
man can be a noun or a verb but at thi point , we start to notic that there ar some constraint from neighborhood .
so it just can't be the case that , that is a determin but then it's follow by a verb , that thi is a bad sequenc .
and so thi is sequenc inform that give us some idea as to how a part of speech tag thing .
and when peopl start work on part of speech tag , peopl thought of that as the main sourc of evid .
but it turn out that that's not realli the case , that the easiest and biggest sourc of evid is statist , knowledg about how often word have differ part of speech .
so , we can also get a long wai by just sai , man is veri rare us as a verb and therefor just without even look at the context , we probabl should tag , take the part of speech tag noun for man .
and that latter sourc of inform prove most us but that's not to mean that the first sourc of inform isn't so us .
so what we're gonna wanna do is start put more and better featur into our maximum entropi tagger , produc a featur base sequenc model .
and on part of that is just featur that we can defin over the word itself .
so know what the word is is us but it turn out you can get a lot of further valu , especi for rare and unknown word , by also put in featur that pick out properti of word .
so look not just at the word , but it lowercas form .
so mayb there ar mani word that we haven't seen capit at the begin of a sentenc , but if we know what their lowercas form is , and it part of speech ar , then that will help us .
know the begin of a word , so if you know a word start with un , that give you a bit of inform about what it part of speech is .
know the suffix of a word , so if you know a word end in ly , it's almost certainli an adverb in english .
capit is us .
so , in english the capit letter , onc you're awai from the begin of a sentenc , is a veri good clue that you've got a proper noun .
and here's an interest differ kind of featur that we've made a lot of us of in some of our stanford system .
we call these word shape featur , an idea first suggest by michael collin .
it's a differ wai of make featur from word that creat natur equival class that is veri us for gener .
and so the idea here is that you're map a word into on of a small set of equival class that repres it shape .
so here what we ar do , and there ar differ exact scheme that you can us , we can sai thi is a digit charact ; thi is also a digit charact .
let's collaps , for our equival class , sequenc of charact that ar the same .
then there's a hyphen here and then we've got a sequenc of lower case letter charact .
and so we will give that the word shape of d x .
some number of digit follow by a hyphen , follow by some number of lower case letter .
and that kind of featur prove to be a veri us featur in quit a few of our discrimin model .
so here's the headlin good new .
if you get clever us all these kind of featur and just build a model that look at the current word and assign a tag to it , it turn out that without look at the context whatsoev , you can build a part of speech tagger that overal get <num> percent of word right .
and actual doe quit well on unknown word as well .
commonli we pull out separ the accuraci for unknown word becaus it's alwai lower and can be distinct differ from the accuraci for known word .
so here ar some figur over all just to give you a pictur of the kind of accuraci you should be expect from part of speech tagger and how differ featur impact the classif .
so we discuss befor thi idea of a baselin method of just give the most frequent tag to word and call everyth els a noun .
so overal that give about <num> , but onli get about half of the unknown word right .
the next set of model that peopl look at were hmm model , hidden markov model , which i'm not realli gonna cover in thi class but thi wa sort of in the mid 90s the state of art of part of speech tag so that got around <num> percent of all word right and just did a littl bit better on unknown word .
we just saw on the previou slide that just have a straight classifi and no sequenc model but with rich and carefulli chosen featur actual work rather well , it doe much better on unknown word .
and doesn't actual perform badli in , on known word .
but peopl have gone on from there .
the idea that you could us featur to do a better job wa abl to be incorpor into hidden markov model as well .
and so thorsten brant produc thi high perform hidden markov model base tagger which us featur base idea for predict unknown word .
and that perform rather nice .
so it get around <num> percent overal , and veri competit perform on unknown word .
but you can keep on build featur into your maximum entropi markov model includ us context featur that we didn't have in the first model .
so then if we go into a sort of a standard maximum entropi markov model tagger of the kind we've discuss , that that might be get almost <num> on all word and start to do a bit better again on unknown word .
and that work ha been push up further at sever place includ at stanford .
so at stanford we have a model that's like , almost like a maximum entropi model but allow some bi direct depend and so that's then push the overal accuraci up to about <num> percent and push up the unknown word accuraci further to about <num> .
and that's get close to how good you can expect part of speech tagger to perform .
becaus for reason of human themselv not be sure what the right answer is to part of speech tag problem and also just becaus the gold standard have error .
becaus sometim human goof , and put in the wrong part of speech tag , even when it seem like the correct answer is clear .
that it seem like around <num> percent is the upper bound of what you could possibl hope to score on the kind of test set we have for part of speech tag .
and if on want to keep on work on improv thing , normal the wai on goe about that is by stare hard at the output of your part of speech tagger or whatev it is and look at where it make error and think of wai in which you could encod some inform into featur that would let you detect that someth ha gone wrong and get the system to prefer some other configur .
i'm just gonna give an exampl of that now .
so here we have an error of the part of speech tagger , that there's the word as , and the part of speech tagger ha chosen the preposit tag for it .
wherea , what it should have chosen is the adverb tag caus thi is as soon , modifi soon .
well , how could we fix that ?
well , it seem like the inform that we want to us is that , if the next word is soon , that's a realli good clue that as will be function as an adverb .
and so we can fix thi error by ad a featur that look at the next word .
so we're us next word as well as previou word and previou part of speech tag .
so here , the part of speech tagger for intrins ha label it as a proper noun .
that's a common error , becaus if the part of speech tagger see a capit word that it never saw in the train data , normal it first best guess is to sai , that's a proper noun .
but tha t's not necessarili true at the begin of a sentenc , when all word get capit .
but mayb we actual know about the word intrins , and that wa seen sever time in the train data and we know that it's an adject .
so if we could util that knowledg we'd be abl to get thi case right as well .
so if we put in a featur of what's the word lower case , that that featur would argu that thi word should be tag as an adject and so we might be abl to hope to fix that error .
note that both of those featur didn't requir ani sequenc inform .
we were look at featur of word to the right and what's the lower case form of a word , and that's an interest gener observ .
in earli work on sequenc model peopl were veri fixat on sequenc model and us the sort of , sequenc of label , the sequenc of tag to predict other tag .
it turn out for mani of the problem that , that were and ar convention done as sequenc label that the sequenc inform is bare us .
it normal give you a fraction of extra perform , but veri , veri littl and so we alreadi saw in the case of part of speech tag that just us thi model , of us a lot of featur of the current word to predict the tag perform veri nice .
so that gave us thi perform of <num> on all token and <num> on unknown token .
but you can do a lot better than that as wa be suggest on the previou slide by also consid thing like what's the next word , and us that to influenc the tag .
and what's the previou word and us that to influenc the tag .
and so that's what's be refer to here as the three word model .
so you're have featur independ of these three word influenc the tag .
and it's actual quit stun how well that perform , so that give you a total accuraci of about <num> percent and an unknown word accuraci of about <num> percent and the thing to notic is that , that perform is actual quit similar to the perform you get with sequenc model .
so if we go back to the perform here , that you can see that that perform is veri close to the perform that's be list here for the mem tagger , <num> , <num> .
<num> , <num> , fraction below but exceedingli close in perform .
okai .
so what we've learn about part of speech tag is that by itself , chang from gener to discrimin model doesn't give a big boost to part of speech tag perform .
the big boost come from be abl to put in lot of featur of observ , and combin them togeth in a good wai so that thing like suffix analysi , word shape featur , lower case , thing like that .
so thi addit power from have rich featur ha been shown to result in major improv to the accuraci of sequenc model .
there is some cost and the cost as you'll probabl find out in the assign is that train discrimin model is just much slower than train gener model .
if you rememb back to languag model , well you can estim languag model essenti just by count data , wherea for discrimin model we have to do process of numer optim .
and thei just ar much more time intens .
that complet the discuss of part of speech tagger , and so now you should be in a posit not onli that you understand someth about part of speech tag , but in gener how to appli maximum entropi sequenc model to part of speech tag and other similar sequenc problem .
okai , in thi section of the cours , we're gonna start talk about how to pars text us statist model .
but befor do that , let me just introduc two view of syntact structur that we'll explor is our pars model .
so the first view is what's call constitu pars and the idea of constitu pars is that what we do is that we organ word into nest constitu .
let me give you a bit of an exampl of that .
so we had long ago thi sentenc fed rais interest rate .
and so , we had the part of speech that we've seen befor , that thi is a proper noun , and thi is a verb , and thi is , these ar both noun in the sens that it's the correct pars , but then abov that , what we want to sai is these noun go togeth to form a noun phrase .
and then the verb and the noun phrase go togeth to form a verb phrase .
and everyth goe togeth to form a sentenc .
and so , these thing up here , these ar our constitu .
and so the claim is that the part of the sentenc , that's also sometim shown with squar bracket , interest rate is a constitu , that it's a unit of a sentenc .
now , how do we know what a constitu is ?
well , that's a complex issu , but here ar some of the idea that we us .
so , on big idea is , by look at the word that stai togeth and go togeth when phrase move around becaus of variou syntact construct .
so in thi exampl here , we've got these two and three word to the children and about drug .
and , well , someth that you'll notic is we can actual swap them around into the opposit order , and that that's fine .
john talk about drug to the children .
or you could do other thing with them .
so , for exampl , mayb you could take thi on , and front it .
in certain discours context you could sai , to the children john talk about drug .
and so thi is all evid that each of these group of word is a constitu .
on the other hand , if you randomli reorder the word of a sentenc , normal you get someth that just doesn't work at all .
so , here what i've tri to do is take the word drug from the end here and thought , hmm , mayb i can just take it and stick it after talk .
but that doesn't work at all , so you can't sai john talk .
thi is down here .
john talk drug to the children about .
there ar lot of other test .
anoth test is thi idea of phrasal expans and substitut , so for the phrase on the box , you can make it a bigger unit , like right on top of the box and you can make it a smaller unit , like there and in gener you can appli these substitut in ani place where the phrase on the box can appear .
there ar lot other syntact test that peopl us for constitu .
realli if you want to learn a lot about thi you should take a linguist cours and then you'll see thi all in more detail , but i hope that's enough to give a rough idea .
those were simpl sentenc .
thi is the kind of thing that happen when you've got big real sentenc .
so thi sentenc is , analyst said mr .
stronach want to resum a more influenti role in run the compani .
and so the idea of constitu structur or phrase structur is that we have these unit again that ar constitu , so here's a constitu , a more influenti role .
here's anoth constitu which just is the name mr .
stronach .
and then here is some differ constitu down at the bottom .
so run the compani is a constitu , but also thi bigger in run the compani , is also a constitu , so constitu nest togeth .
these ar the kind of pars tree that nlp peopl actual spend their time deal with , and we'll talk more about these .
but note just on attribut of them is , as well as actual pronounc word , thei're postul these unpronounc unit in them as well call empti element .
so the straightforward wai to model phrase structur constitu structur is just as a context free grammar , which we assum you've seen somewher in comput scienc .
that is just the idea that you've got these rule that sai , a rewrit as bcd .
and if you appli these , and let's sai you have b goe to ef .
if you appli these , you get phrase structur rule , where thi is bcd and then thi goe to ef .
that give you the constitu structur .
but thi , these kind of rule you can write for arbitrari symbol but if you actual look at what happen in natur languag , there's alwai a lot more structur and thi is the idea that get call x bar theori .
and what that sai is that in natur languag , by and larg , phrase ar head by particular kind of word which then take modifi and depend around them .
so inform we call thing a verb phrase precis becaus thei have a verb in the middl of them , and we call thing a noun phrase precis becaus thei have a noun that is the central element of them .
so a noun phrase , someth like , the man in the corner , central ha the noun , man and then ha other modifi sit around it .
and so , although accord to a context free grammar , these ar just arbitrari symbol , in practic we give them name that reflect thi head structur or phrase structur .
and the same is true of other categori as well , like adject phrase and adverb phrase .
so thei're a larg part of our categori inventori .
we do have other kind of phrase .
so we have sentenc , and variou kind of invert and question sentenc which basic correspond to a subject noun phrase and a verb phrase .
and then thei can appear in bigger complement claus , these sbar and sbar q which also have someth like a that introduc the sentenc .
and variou other miscellan stuff happen as well .
again , you can learn a lot more about that in a linguist syntax class .
but thi idea of head phrase structur is the connect between phrase structur and the other big approach to syntact structur that i want to show you and that's depend structur .
so in depend structur the idea is we directli show for the word of a sentenc which other word depend on , that is modifi or ar argument of them .
let's look at that with thi exampl the boi put , put the tortois on the rug .
so the head of thi whole sentenc is put .
that's the central word of the sentenc .
and that's sometim repres by have a depend arc , come in from the edg , point to the whole , the head of the whole sentenc .
okai so then put ha two argument itself , so put's argument ar the boi .
i'm sorri it's got three argument itself , the tortois and on the rug where thing ar put , so , who put what ?
and then where ?
okai , and so then , at that point , we then sort of sai , well , boi , doe it have ani modifi or argument ?
it modifi is the .
tortois , doe it have ani modifi or argument ?
it modifi is the .
and then on , as a preposit take a complement .
so it argument is it complement , the rug where the rug is the head and the modifi of rug is the .
and so , thi set of arrow i've now drawn here , is a depend analysi of thi sentenc .
and thi form of depend analysi is the other major syntact represent we'll us .
okai , so i hope that's been enough to give you some idea of the represent we us for syntact structur and how the two represent connect to each other .
in thi segment , i'm go to talk about why nlp research adopt an empir approach us data and statist for make progress on the problem of pars the structur of human languag sentenc .
what wa the situat that exist befor that ?
well the classic wai of pars , by which i mean befor <num> , wa that peopl would write a grammar , which might be a phrase structur grammar or a context free grammar , those two term ar equival or might be of a more complex format and a lexicon .
so someth like thi babi exampl that i've shown here .
and onc you had a grammar of that sort , you can then us a grammar pars system or a proof system to find allow pars for a string of word .
the problem with thi approach is that it scale veri badli to the kind of sentenc that peopl typic write and sai , and didn't give a suitabl solut to that problem .
let me try and give some indic of the problem .
so consid the full version of thi sentenc that we've us as an exampl befor fed rais interest rate <num> percent in effort to control inflat .
if i as a linguist write a grammar that's not complet crazi , but is the smallest possibl grammar that can pars that sentenc , and then sai , parser , pars thi sentenc .
what i find is the grammar alreadi allow <num> pars for that sentenc .
if i write a slightli more gener grammar , which will allow other thing that commonli happen in natur languag sentenc , well then suddenli my grammar allow <num> pars for thi sentenc .
and if we then look at the kind of broad coverag grammar that ar actual us in the statist parser we'll talk about soon , and i try and pars thi sentenc , i find thi sentenc ha million of pars .
so you can see that we've got a problem that's kind of out of control .
so classic pars wa stuck between two problem .
on the on hand you could try and put a lot of constraint on grammar to limit their abil to gener weird and unlik pars for sentenc .
but the problem wa that to the extent that you did that , the grammar becam veri non robust .
so it wa quit common in tradit system that even if you gave them well edit text like new wire articl that , you know , someth like a third of the sentenc would have no pars whatsoev .
but on the other hand if you made the grammar looser so it could pars more sentenc , what you found is that even extrem simpl sentenc start get more and more unlik and weird pars .
and you didn't have ani good wai to choos between them .
so what we need is a system of grammar that's flexibl enough that it can deal with the flexibl wai in which human us languag in their daili life , but is predict enough that will , it will allow us to choos the correct or like pars for a sentenc .
and so that's precis what the statist pars system that we'll look at in these lectur allow us to do .
how can you build a statist pars system ?
that's where the topic of data come in and so the huge enabl factor for be abl to build statist pars system wa the develop of treebank .
and here i'm show an exampl sentenc from the penn treebank which wa the earliest wide avail treebank and still the most famou .
so what , we've shown exampl of treebank sentenc befor but what you actual get in the treebank is these kind of structur .
so the sentenc tree structur is be indic by these parenthes which ar nest to show constitu .
if you're an old time lisp programm , thi should look to you like as , and inde is lisp s express .
so thi is give the structur of a sentenc with noun phrase and verb phrase and variou other thing .
so these mark also will indic empti element of variou kind , and also sort of variou function annot .
so thi sai that thi is the subject of the sentenc .
and so , onc we had thi kind of annot data we're then in a posit in which we can us machin learn techniqu to train parser to sai what is the most like pars for a sentenc .
we're also in a much better posit to build robust parser becaus we have a lot of sentenc that give us some kind of indic of what kind of flexibl of structur we need to admit into our languag to be abl to pars typic languag us .
and so thi wa a revel , that if you're start off , it actual seem like build a tree bank is a lot of work with veri low payoff , caus the whole power of write rule of grammar is you can write on rule and it gener to thousand , million , in fact , an infinit number of possibl sentenc .
wherea , build a tree bank , you're take on sentenc at a time and pars it .
so that seem slower and less us .
but actual a tree bank give us mani , mani us thing .
it give us reusabl .
normal the parser of on person wa complet not us in develop the parser of anoth person .
it wa done complet independ .
but now we've been abl to build mani , mani parser , part of speech tagger , etc .
off penn treebank data .
and it also ha other wider us .
so for exampl mani linguist now us the penn treebank as a sourc for test out and develop hypothes about languag .
the penntre bank give us someth that is , broad coverag .
it give us the statist that'll allow us to choos between pars , and it give us anoth thing that's prove to be incred import .
it give us a wai to evalu parser on a common testb so that we have good data on which parser ar better or worth than other parser .
now of cours these dai the penn treebank isn't the onli parser .
there ar now dozen of treebank .
there ar both treebank for differ languag and treebank for differ genr .
the origin penn treebank wa just newswir .
it's been extend to some other genr .
but now there ar other treebank that cover thing like , well biomed english is on popular domain , but also other thing like look at variou kind of inform text and more special kind of english usag such as treebank of question .
okai , so i hope that's motiv for you why it's been import to take an empir approach in develop pars system for human languag and the great valu that we've been abl to get from have data resourc like treebank .
in thi segment i'm gonna talk about where the huge number of pars for human languag , sentenc come from , and how we might go about solv it in our statist pars system .
so a kei part of do pars for human languag sentenc is thi problem of what's call attach from the psycholog literatur .
it's , for variou kind of phrase , how do we decid what thei modifi ?
so ani time we have preposit phrase , adverbi or participi phrase , infinit , coordin , and thing like that , we have thi attach decis of work out what is , what ar thei modifi ?
let's work through an exampl to get an idea of that problem .
so here we have a sentenc .
the board approv it acquisit by royal trust co .
limit , of toronto for <num> a share at it monthli meet .
and if we look at thi sentenc , well what do we find ?
we find , that well , we have here the subject , the board , then a verb approv and an object it acquisit .
but then if we look after that , what we find is that there ar actual four preposit phrase in a row by royal trust co ltd of toronto for <num> cent a share at it monthli meet .
and for each of those what we need to do is decid , what ar thei modifi .
so in gener a preposit phrase can modifi a verb , it can modifi a preced noun phrase .
so it's got these two target .
but actual each of these preposit phrase actual includ in it a noun phrase .
so onc we get to the later preposit phrase , there'll be more thing that each on can choos to modifi .
okai let's go through each on in turn .
right so for by royal trust co . , that's modifi the acquisit , not the approv .
so , it acquisit by royal trust co .
so that's in depend format we have that relationship .
okai , of toronto , thi is not modifi the approv or the acquisit .
rather it's royal trust co .
limit of toronto .
so thi preposit phrase is modifi royal trust co .
limit .
okai , so then if we go on to the third on , for <num> cent a share , that's not modifi toronto or royal trust co . , it's modifi the acquisit .
so thi is an acquisit for <num> cent a share and then for the last preposit phrase , at it monthli meet , that's presum not refer to an acquisit at it monthli meet or ani of these other thing , but rather it approv at it monthli meet .
so thi preposit phrase modifi the first verb .
okai , that wa kind of complex , though as human be we just read the sentenc and understand what it mean and do these attach decis without ani appar effort most of the time .
how , but the thing to , we want to think about for a moment now is how mani possibl attach there ar .
well if you just sort of think there ar , you know , on , two , three , four thing here and each of them could attach to a bunch of point and each on can be chosen complet freeli , it should be fairli clear that , well we're go to get an exponenti number of possibl , exponenti in the number of preposit phrase .
and it's precis from that caus is where these huge number of pars that we saw previous , like have <num> pars for a sentenc came from .
that realli , you ar make a number of decis for thing like categori and attach , and a lot of them can fairli freeli combin .
so you multipli out these ambigu , and you get a huge number .
howev , it's not the case in exampl like thi that you're complet free to attach anyth anywher .
so , when we did by royal trust co . , it could certainli choos to attach either to the preced noun or the verb .
and then , syntact , when we attach of toronto , it had three choic .
it could attach to royal trust co . , or to the preced noun phrase , or to the preced verb phrase .
and all of those were syntact possibl .
but onc you've made some higher attach , that mean earlier in the sentenc , thing becom more restrict .
so in particular , onc we decid to attach for <num> a share as a modifi of it acquisit , that restrict the attach that ar possibl for thi preposit phrase .
it could still attach to a share or it could attach to an , an acquisit , or it could attach to approv .
but these on here that ar kind of hidden behind the attach of for <num> a share to an acquisit .
these on becom not possibl anymor .
so that leav a kind of a littl puzzl of well , as you add on preposit phrase attach to a sentenc how mani possibl attach ar there that observ thi constraint , which correspond to depend not cross or for there be a tree structur to a sentenc in a phrase structur grammar .
well it turn out that the answer to that question is a well work out piec of math .
the answer can be found in the seri of catalan number which is a sequenc that turn up in a lot of place , wherev you've got a kind of tree like context .
for exampl it even turn up in probabilist graphic model and the import thing to know from our purpos is that the growth of the catalan number is still an exponenti sequenc , we get an exponenti number of pars .
unless you're a math geek , work through the factori there probabl isn't the most us thing to do , but rather it's probabl much more valuabl linguist to actual look at what ar the possibl structur for a simpl sentenc and get a sens of how mani there ar there and so that's what we'll ask you to do in the quiz question that follow thi .
okai .
you hopefulli work out in the quiz that the answer to , onc you have two preposit phrase , the number of possibl structur is five .
but rather than focus on that number now , what i want to start do is focus on the two problem that come from have thi exponenti possibl number of pars for even quit simpl sentenc .
so problem number on is that if we pars naiv , and that mean that we us the simpl kind of top down or bottom up pars that ar normal us in context like , in program languag , where thei're us with less ambigu languag , that that would mean that our pars end up do an exponenti amount of work themselv .
and that's no good if we'd like our pars to pars quickli .
so where doe that exponenti amount of work come from ?
well , the thing to notic is that we're gener thi exponenti number of tree , and that come about from have a small number of basic ambigu .
so , in thi silli exampl we have the two pp's with cat and with claw .
we're choos where to attach each of those .
so there're realli onli two decis , but we're then multipli that out into an exponenti number of pars .
and in the cours of do that what happen is that we do a lot of repeat work .
so if you look at these pars structur you can see that we keep on build the same piec of structur over and over again .
so here's a littl pp of with claw , and we build it in thi pars , we build it in that pars , we build it in that pars , we build it in everi pars .
we're alwai go to have that pp .
but if we just gener all the structur we're build it over and over again .
and the same is true with larger bit of structur .
so here's a bigger pp , with cat with claw .
it also appear in thi pars .
and so we're do , complet duplic work there .
and that goe on .
so here's a verb phrase of scratch peopl , and we gener it in that pars , and we gener it in that pars .
and there ar other place in which we build other piec of structur that get repeat over .
so here's a noun phrase of peopl with cat , and here's the same piec of noun phrase of peopl with cat .
here is a verb phrase of scratch peopl with cat .
and here's again the verb phrase scratch peopl with cat be gener a second time .
so thi is just a lot of repeat work .
and so the secret to build effici parser and get awai from do an exponenti amount of work is to avoid do the same work twice .
we onli want to find possibl piec of structur for a sentenc precis onc .
and then we'll be abl to turn pars from an exponenti problem into a polynomi time problem .
problem is then how to choos the correct pars or sever like pars out of the mani , mani possibl pars for a sentenc .
how can we do that ?
well let's look at thi simpl exampl .
so thi is again the classic , here's a verb with an object noun phrase , and a preposit phrase attach follow it .
so it could be that thi modifi the verb .
so she is look through a telescop , and she saw the man with a telescop .
or it could be that thi modifi the noun .
so thi is a man with a telescop and she saw that person .
well which of those is correct ?
well the answer is that it could be either .
so thi is what's commonli refer to as an ai complet problem , know which is correct could depend on arbitrari knowledg about the world situat that's be observ or the prior discours context in realli complex wai and it can be realli hard to tell .
well that's true , but it turn out that in a lot of case , you can be pretti sure what's correct without understand a lot .
that just look at the word that ar involv , the noun , the verb , and the preposit , you can get a realli good idea of which attach is correct even without have anyth like full understand of the sentenc .
let's look at a coupl of exampl that illustr thi .
okai , so here is now some more real text exampl .
thei have exactli the same structur , so we've got a verb , a noun phrase , and our preposit phrase .
moscow sent more than <num> , <num> soldier into afghanistan .
so on possibl is that the send is into afghanistan , the verb attach , and the other possibl is it's soldier into afghanistan .
now , syntact , either of these is possibl .
so soldier into afghanistan would have the same structur as a phrase like , whatev , student into ladi gaga or someth like that .
it's a possibl syntact structur , but it's just realli , realli unlik .
you're just veri unlik to have soldier into afghanistan and so almost certainli the structur that we want to choos is with into afghanistan modifi sent and that's a veri common idiom that you send someth into some environ .
and so we should choos thi pars .
then let's look at thi next exampl .
so in thi exampl , the same structur again .
and we can have the pp with new south wale head be a depend of breach , breach with new south wale health , or it can be a depend of the agreement ; agreement with new south wale head .
well again , breach with new south wale head just isn't veri like .
you just don't sai i breach with my friend .
doesn't sound veri good .
on the other hand , an agreement with new south wale health , that's realli common and normal sound , 'caus you make agreement with somebodi .
and so thi is the attach that we should choos .
so that then , didn't look that hard .
and that's exactli what we'll do .
now , statist parser will try and exploit such statist about word combin , which kind of preposit is like to with which noun and which verb , to be abl to choos the correct pars of sentenc without have full understand of them .
okai .
i hope that's given a sens of how pars human languag sentenc can just be a realli bad problem , but also some idea of how we can start develop method that will solv both the problem of there be an exponenti number of pars , not caus an exponenti amount of work , and how we choos the most like pars for sentenc .
in thi segment i'm go to introduc context free grammar and their extens with probabl , probabilist context free grammar .
so , thi is what a linguist call a phrase structur grammar , which is also known in comput scienc as context free grammar , same thing .
and , so what we have is these variou rule , where we have a categori which rewrit as a sequenc of other categori .
and then eventu thi write down to what ar call termin symbol which ar word .
and so us thi grammar we can produc sentenc .
so we start with the start symbol s and then we can expand down us ani of the rule of the grammar .
so s goe to np vp and then there's a rule that sai a noun phrase goe to a noun and then a noun can go to sai peopl .
and then a verb phrase can go to a verb and a noun phrase .
and the verb can go to sai fish .
and the noun phrase can go into a noun again , and the noun can go to sai tank .
and so us thi grammar , we can make sentenc of the languag .
so here ar two sentenc of the languag .
we just saw that thi on wa a sentenc of the languag , and thi is anoth sentenc of the languag .
if you look carefulli and plai around with it a bit , you'll see that thi is actual a veri , veri ambigu grammar .
so sentenc like peopl peopl peopl peopl peopl or fish fish fish ar also sentenc of thi languag .
okai , so what is context free grammar formal ?
so a context free grammar formal is a four tupl consist of a set of termin symbol , so thei were our word like fish and peopl , a set of nontermin symbol , those were our on like s and np for noun phrase and vp for verb phrase , a start symbol , which is on of the non termin symbol , and then , we have a set of rule or product of the form a nontermin rewrit as some sequenc of nontermin or termin like vp goe to v np pp .
that's our sequenc .
so thi give us a grammar .
and what we'll sai is that a grammar gener a languag .
so languag l is all the sentenc that can be produc by do that process of rewrit from the start symbol , down to a sequenc of termin .
and in mani case , when there's ani complex in there , the size of that languag will be infinit , but it won't actual includ all possibl string .
so that's the formal definit of context free grammar .
in practic , when we're work in linguist and comput linguist , we alwai make a coupl of refin from that to thi , which aren't super theoret interest , but give us a more natur form for linguist purpos .
so let's just quickli look at thi .
so here's our nlp phrase structur grammar .
so , how is it differ ?
it's differ becaus we've introduc thi set of pre termin symbol .
so in practic , when we ar do linguist grammar , almost alwai we have rule with nontermin , like noun phrase goe to determin noun .
and then we have , oh , and thi , thi part up here we often refer to as our grammar .
and then we have a lexicon , where we have word , termin symbol that belong to categori .
so determin can rewrit as the .
or nn can rewrit as man .
so , in the first definit , pretermin the lexic categori have no special statu , but we'd kind of like them to .
and so that's what we do here .
so we sai that our categori ar our noun , verb , and so on ar the pretermin .
and then we have termin ar the word as befor .
and then the nontermin realli becom our phrasal categori .
thing like noun phrase .
okai , but noth els realli chang .
we have a start symbol , our lexicon is now just rule of the form that pretermin rewrit as a termin .
and then our grammar is a set of rewrit rule where we rewrit the sequenc as befor , but the left hand side is alwai a phrasal nontermin and the right hand side can includ phrasal nontermin and the lexic categori .
so by convent , for exampl often thi start symbol is taken to actual be s , 'caus s can also stand for sentenc .
but let me note that for a lot of nlp purpos includ the kind of tree bank we look at , we alwai have some symbol abov s , 'caus sometim you can find thing in text that aren't full sentenc .
thei might be a fragment like a preposit phrase or thei might just be a whole noun phrase or someth .
so we so alwai have some symbol abov that , that will rewrit as s or pp or fragment or whatev .
commonli that's taken , been name either root or top .
and then on fine point in here , is when you can have the right hand size gamma be ani sequenc of phrasal and lexic categori , that actual includ the empti sequenc .
but often it seem a bit funni just to have noth on the right hand side .
that might be confus to peopl .
so peopl commonli write an ital e to mean the right hand side is actual empti .
let's see all of those convent at work in our phrase structur grammar here .
okai so here we have the noun phrase rewrit as empti .
so linguist often us empti categori in grammar becaus it seem us to describ thing as miss someth .
so , for exampl as well as have the sentenc peopl fish tank you can make the imper fish tank .
or , you can have an unspecifi object , and you can sai peopl fish .
and i don't want to get into the linguist analysi a lot .
and thi certainli isn't a linguist refin grammar .
but the idea is that a wai you might want to think about explain thing like thi , is to sai , well , actual , there is a subject np to thi sentenc , but it's empti .
and there is an object np here that is also empti and unexpress .
and so that's why grammar will often have rule like thi with empti in them .
and we'll come back to them and often talk about wai of get rid of them for nlp a bit later .
okai , so thi is our empti .
and then the other creatur we have , these thing ar call unari rule when on categori rewrit as anoth categori .
then we have a bunch of thing here that ar binari rule .
but then you're also abl to have rule where someth rewrit as three or more thing .
and so here's a case of that here , where a verb phrase rewrit as a sequenc of a verb , noun phrase , and preposit phrase .
and these ar also thing that we often want go get rid of for do our nlp grammar , as i'll explain in a bit .
okai , so then over here is our grammar rule .
and here , now over here is our lexicon .
okai .
so thi ha just given us a phrase structur grammar , context free grammar , but we're want to have probabl in our grammar , so how do we go about do that ?
well , thi is realli actual a veri simpl extens of what we had so far .
so all of thi stuff is just like our context free grammar befor .
but what we do is we add on sai that there's on extra thing , we have a probabl function .
and so the idea of the probabl function is it take each rule and give it a probabl .
it map it to some real number between zero and on .
well you don't just let it map to ani number complet unconstrain .
we add on thi constraint here , that , for ani nontermin , the sum of the probabl of it rewrit add up to on , so that you have a probabl distribut over how , how sai noun phrase rewrit , and if you make just thi condit and actual there ar a coupl of technic assumpt that in practic when estim grammar off tree bank aren't alwai true , so i won't explain here .
the end result that you get is that you get a grammar produc a languag model in the technic sens we introduc beforehand , when talk about languag model .
that is , if you look at the languag l that's gener by our grammar , here i've just express that , you know , consid all sequenc of termin and then work out their probabl here .
if you sum those probabl , those probabl will sum to on .
so , it's a languag model in the same sens as languag model .
here's an exampl of a pcfg .
so it's just like what we had befor , except that now , next to each rule , we're give it a probabl .
so there's onli on rewrit for s .
so it ha a probabl of on to meet the condit we gave befor .
but to take a more interest exampl , there ar three rewrit for np .
and if you sum these three rewrit up , thei sum to on as requir .
now thi is just about the grammar that we show you befor , but i actual made on chang to it here , which is , i delet the noun phrase goe to empti rule .
and the reason i did that is thi is alreadi a realli , an ambigu grammar .
and i , if i left it in , the exampl that i'm about to show you would get wai too ambigu and complex , as you could try and work out for yourselv .
so it's slightli simplifi .
okai so us thi pcfg , let's go through an exampl of work with probabl .
and so there ar two thing that we want to look at .
so first of all , if i draw a tree we can work out the probabl of a tree .
and that's actual pretti easi .
all we do is actual take the probabl in the grammar and lexicon and , that ar us to gener the tree and we multipli them all togeth .
and the slightli trickier thing that we want to do is the languag model question .
we want to know the probabl of a string of word .
and , well , for that we have to consid all possibl tree structur that could have gener the string .
and so the probabl of a sequenc of word is the sum of all possibl , all possibl tree , where the tree is a pars of a sentenc .
but sinc the tree includ the sentenc down the bottom , that's just the sum of all tree with that condit .
let's look at a concret exampl .
so my sentenc here is peopl fish tank with rod .
and here's on pars that the grammar gener for it , and what i've done is write just next to each parent categori , what is the probabl .
so thi <num> sai that the vp goe to v np pp rule had probabl <num> .
but we get a , thi is the pars , where we get the preposit phrase modifi the verb .
but as we've seen befor , we also get the other pars where we have the preposit phrase modifi the noun .
and we can write out it pars as well .
in the less ambigu grammar without the empti , these ar the onli two pars for thi sentenc .
and so , do that we can work out the probabl for each tree .
so the probabl for each tree is just we multipli togeth the probabl of each rule expans , and we get as alwai a veri littl number here .
and then , thi is the probabl for the other tree with , do the noun attach .
so to work out the probabl of the sequenc of word , the sentenc , peopl fish tank with rod , we simpli sum the probabl of all pars and that give us thi probabl here as the languag model score .
and if we look at thi , as well as get the languag model score , we can also see which pars the pcfg would choos as the most like pars for the sentenc .
and the answer is , it's thi on .
it choos , the verb attach , which seem like it is the most natur read of thi sentenc in english .
it might be interest you , for you gui to look and just think for a moment more as to why it choos that pars .
and if you start look at it , what you'll see is that a lot of the probabl in the tree ar exactli the same for both pars .
there's realli onli on area of differ where in the vp attach we us thi rewrit rule here .
or mayb i should draw it , sort of bigger to includ that littl sub tree .
wherea in the np attach we have thi vp rewrit rule , and then we have an extra np rewrit rule , so thi bit is differ .
and so then if you , we'll look a those that if what we do is we found that here we had a <num> that wa uniqu to thi on and here we had .
and sort of thi piec .
a <num> time <num> , which wa uniqu to thi on , so it's <num> versu <num> . <num> .
and so if you compar those , the end result is that the vp , the verb attach is <num> . <num> , three and a third , time more like than the np attach pure account for by that differ .
that could even make you a littl bit suspici of pcfg's , someth we'll come back to later , caus it sort of look like thi on's get lower probabl becaus there's more depth to the tree , more rewrit be done .
and pcfg's tend to have some slightli odd effect like that .
but we'll stop that explor there for now , and we'll just content ourselv by sai the pcfg is choos the right pars for the sentenc in thi case .
and feel that hopefulli now you gui understand both what a context free grammar is , and it probabilist extens .
in thi segment i'm go to tell you about the kind of grammar transform that we us to do effici pars of pcfg .
so the most frequent known grammar transform is the chomski normal form .
so the idea of the chomski normal form is that cfg rule ar restrict to be of two simpl form , that thei're either x goe to yz where all of x , y and z ar non termin , or thei're simpli of the form of a non termin x rewrit as a termin w .
so you can take ani cfg and transform it into it chomski normal form , deriv normal form and produc anoth cfg which recogn the same languag .
that is , it doesn't necessarili give the same tree structur but the same string ar part of and not part of the languag .
and the wai that you do that is by go through a seri of transform that get rid of the empti rule and the unari rule and then divid up rule that have more than two thing on the left side .
let's just quickli go through an exampl of how we do that .
so here's our ugli phrase structur grammar with all the bad case in it .
so first off , let's do epsilon remov .
so there's just on rule with epsilon in it , so we're go to get rid of that on .
and the wai we're go to do that is everi time that there had been a rule that ha an np on the right hand side , we're go to split into two rule , on just as befor , and on that then note that the noun phrase can be empti and so just sai s can go to a vp .
okai , so if we keep on do that through all the other place that np turn up , we then get thi grammar .
so then , at thi point , we see that there ar a lot of unari rule .
there ar more than there were befor .
and so we're gonna want to start get rid of them .
so we , the wai we do that is we pick the first on , sai , and then we work down the consequ of it downward .
so now we're sai that s can go straight to a vp , and so that mean we now look for where there's a vp on the left hand side , and sinc an s can immedi go to a vp , as well as keep thi rule , we're go to have to add anoth on that sai an s can go to a v np .
and so , we make that chang .
so now we have thi grammar , and unfortun , those chang mean that we've introduc a new unari rule with an s on the left hand side .
so you have to do thi unari remov recurs until everi unari ha disappear .
so now we have s goe to v .
and well , where doe v appear on the left hand side ?
that happen in the lexicon over here .
so when we get rid of thi rule , we're go to have to add new lexic entri sai that an s can go to peopl , and so on for the other case .
and so that , then , get us to thi state .
okai , and then at that point we just keep on go .
so we're go to get rid of thi vp goe to v , but that mean that we're go to look again for place where a v appear on the left hand side , and we're go to further , increas the size of our lexicon .
and then there ar just still more unari , and so we keep on go .
so , the first on here is np goe to np .
that's an a over a unari that doesn't add anyth apart from perhap for idea of linguist structur .
so we can just eras that without chang the languag that's recogn .
and then we have here , np goe to n , and so then again , we're gonna have start look where n appear on the left hand side .
and what we'll find in thi case is , now there's nowher in the grammar where n appear on the right hand side of a rule , so we don't actual have to split these lexic entri .
we can actual just renam them to put np in for each on .
and so then we keep on go .
there ar a coupl of other unari rule down here .
so onc we get rid of them all , we get , the grammar look like thi .
and so now , the next step is to sai , well gee , we've still got some rule that have three thing on the left hand side , and we're gonna have to chang those into binari rule .
and the wai that we do that is we introduc extra categori .
so let's take thi on .
so what we're gonna sai , is that a vp goe to the first thing here , a v , and then we're go to introduc a new categori , sai x , and then we're gonna sai x goe to np v , np pp .
so i just call it x here , but to make thi a littl bit simpler we're gonna us systemat and unwieldi name for them .
so we can call thi vp underscor v .
and the wai to think of thi is that thi is just the name of a nontermin , just like ani other nontermin , you just treat as an atom .
but we've given a systemat wai to introduc them .
so , we make those chang and then here we have , thi is our final grammar in chomski normal form .
now if a linguist hand you their grammar like thi and you do these step and hand it back to them and sai , here it is in chomski normal form , thei're not like to like what thei see , becaus it's made a real mess of the structur of the grammar .
but that's not realli someth that you should worri about .
you should regard thi as an intern represent insid your system which will allow effici pars , and isn't realli design to be the structur of the languag as the linguist see it .
so , thi is a system that let us do grammar transform for effici pars .
and we haven't exhibit here , but with some extra bookkeep and symbol name you can do these kind of transform , and still be abl to reconstruct the origin tree that you would have made without do the grammar transform .
nevertheless , in practic , while do full chomski normal form is a pain , you should be abl to see from the wai that we con , deconstruct the rule with more than three thing on the left hand side , that turn them back into nari rule from binari rule , that's gonna be straightforward .
but reconstruct the empti and the unari is trickier .
and so at thi point , there is actual a divid .
the thing that you want to know is binar of grammar rule is absolut essenti to the algorithm that we're go to show that allow cubic time cfg pars , cubic time pars of arbitrari context free grammar .
so ani system for effici polynomi time pars of context free grammar somewher there's binar .
it might be done in advanc like in the exampl we ar go to show with the cky algorithm where we explicitli transform the grammar and then pars .
or for some other form of pars the binar get hidden insid the work of the parser .
but binar is alwai done somewher .
on the other hand , the get rid of the unari and empti is more option .
so , if you want to have the neatest version of the algorithm , you need to , you will want to do the unari and empti as well .
but leav the unari and empti around doesn't chang the asymptot complex of the algorithm .
so commonli it is more conveni to leav some of them in .
and we'll demonstr that with our real exampl .
so we've discuss thi as a grammar transform , but what we normal actual do in statist pars these dai is read tree from a tree bank and then do stuff to them and then count the sub tree and thei becom our grammar .
so what we're gonna do is we're go to sai by and larg where we're read in tree from the tree bank and look thi sub tree is go to be someth for our grammar .
and thi sub tree here is gonna be someth for our grammar .
but well we don't want to have these sub tree with three or more thing beneath them becaus that ruin our effici parsabl properti .
so as we read in the tree we're go to transform them and turn them into binari tree .
and we've done it in the same wai as we discuss befor by introduc thi new non termin and divid up the rule that had three children .
and we mai want to do the same thing with empti and unari , so let's just discuss that a littl bit more .
so here is an actual tree from the penn tree bank , so it wa a headlin of a newspap articl , and the headlin wa just aton .
and so the wai that's been analyz , is by sai that that's a sentenc , an imper sentenc , so here's an imper verb , and the imper verb ha an unexpress np subject , so thi is an empti here .
okai and thi sai that it's a headlin and these ar the function tag sai thi is a subject .
so , normal , when we process tree bank tree the kind of thing we do , is first of all , we strip off the function tag and we just deal with the basic categori which get us to over here .
veri commonli our pars algorithm don't explicitli deal with the empti , so we also just delet the empti node and everyth that's abov an empti node .
and so that then map us onto thi tree here .
and thi tree ha a lot of unari in it .
now it's possibl to also get rid of unari .
now if you us the algorithm that we show befor , you get rid of unari by keep the highest node .
so if you kept the truli highest node , our root node you'd even get rid of thi s , and it would just be root goe to aton .
the start symbol of the grammar would rewrit as a word .
commonli we don't do that , and we'll at least keep the rewrit from our start symbol to the differ type of thing that you can get , a sentenc or a noun phrase .
but then after that if we keep the high node , we'll have s goe straight to a word aton .
normal we don't want to do that becaus we like to keep our lexicon , which ha part of speech rewrit as word .
so it's more usual to keep the low end of a unari chain and get rid of the higher up thing .
but in that case , if you want to have a uniqu start symbol you definit have to keep your start symbol and allow the start symbol onli to rewrit unarili to either a nontermin phrasal categori or to a pretermin .
but it turn out that you don't actual have to do ani of thi stuff , and so it's perfectli okai to leav unari in your grammar and us the algorithm that we're go to show .
it make them a bit more complex and messier in term of a pars algorithm , but it make it much easier to reconstruct the origin pars tree on the wai out .
so the pars algorithm that we're about to show in the next segment actual work over a represent like thi .
so we still have unari rule , but we've delet the function tag and the empti element .
okai , so i hope you have a sens now about what ar in grammar rule , and how we can transform them to get them in framework that allow for more effici and cleaner pars algorithm .
and now we'll go on and look at a particular pars algorithm that work with binari and unari cfg rule .
in thi segment i'm go to introduc a wai of do pars of context free grammar exactli in polynomi time .
in particular , it'll give us mean of find the most probabl pars of a sentenc accord to a pcfg in polynomi time .
and , the method i'm go to introduc is a famou pars method from the 60s call the cky algorithm .
so rememb our goal in constitu pars is we start with a sentenc .
thi is the exampl we're go to us fish peopl fish tank .
we have a grammar with probabl for the rule in the pcfg , and what we want to do is find a sentenc structur that's licens by the grammar .
and in the pcfg case normal the most probabl such sentenc structur .
and rememb crucial what we want to do is do thi without do exponenti amount of work , and if we explor everyth exhaust , sinc there can be an exponenti number of pars we will do an exponenti number of work .
but the cky algorithm give us a cubic time algorithm both in term of the length of the sentenc and the size of the grammar in term of the number of nontermin , that allow us to do thi by compact represent , the us of dynam program again , just like that edit distanc algorithm in the first week .
let's see how it goe about do it .
so the secret of what it doe is it us thi data structur here , which get refer to as a pars triangl , or also quit commonli as a chart .
and so the wai it work is that thi cell here is go to describ thing you can build over fish .
thi squar here is go to describ thing that you can build over peopl .
then thi squar here is go to describ thing that you can build over fish peopl .
and so on up .
and so what we can see is that , sinc thi is basic half a squar , the number of squar that we have is order n squar .
and so what we're gonna show is that we can fill in each squar in order n , and so the result algorithm run in cubic time .
so the secret to the cky algorithm is that we fill up thi chart work in layer accord to the size of the span .
so , the first thing we do is fill in the cell for singl word .
then , we fill in the second row of the chart , and so these cell in the second row of the chart , ar precis the cell that describ two word .
then we fill up the third row of the chart , which ar constitu that ar three word in length .
and final , we fill up the top cell and that will give us categori that pars the enti , that cover the entir sentenc .
let's look in detail how we fill on of the cell of thi chart .
so , here we're assum that we've alreadi fill in these two cell .
now actual accord to the grammar i'm us i haven't fill them exhaust , but i put in enough that we can get the gener idea .
and so what we then want to do is to fill in a cell abov it , so we want to fill in the cell up here that describ the two word constitu peopl fish .
and the wai we do it is , what we're gonna do is build thing in a binari fashion by sai , let's take a constitu from here , a constitu from here , and a grammar rule that's abl to combin them .
and then we can build the thing on the left hand side of the grammar rule by , with the probabl that come from multipli the three probabl .
so what we're go to build , so what we're go to build up here , is that a noun phrase which is built from two noun phrase .
and it probabl is go to be <num> . <num> time <num> . <num> time the probabl of the rule <num> , and if you multipli that all togeth that come out to <num> . <num> .
okai , but that's not the onli thing that we can build in thi cell .
so we can also take thi verb and the same noun phrase , and then we can build a verb phrase out of a verb and a noun phrase .
and so we're gonna write down that possibl as well .
so we're gonna have vp goe to v np .
and so the probabl of that is gonna be <num> time <num> . <num> time <num> which equal <num> . <num> .
okai and then that point we keep on go and see what els we can build .
well anoth thing that we can build is that we can take thi np and the vp , and then we can combin them with thi s rule right at the top .
so we can build here an s out of an np and a vp .
and the probabl of that is <num> . <num> time <num> . <num> time s goe to np vp <num> .
and the product of those is also no , it's a differ number from the on we've seen befor , <num> . <num> .
okai , and we've run everi binari rule that we possibl could .
and on possibl is that there will be collis , that we'll find multipl wai of build an s or a vp .
i don't actual have an exampl of that now but what i am go to do is illustr the same thing onc we extend on to unari rule .
so in my grammar , i also have unari rule .
so i have a rule , like s goe to vp right here .
so that rule can be appli just insid thi cell sinc i can build a vp right here .
so i can also build s goe to vp here .
and the probabl of that will be the probabl i calcul befor , <num> . <num> time the probabl of that rule <num> .
and then , though , that multipli out get to <num> . <num> .
okai , so at thi point though if , to have my cubic time pars algorithm i'm not want to store these two wai of make an s in a cell .
what i want to do is just recogn i can build an s over thi two word span and for a pcfg where i'm try to find the most probabl pars , what i want to record is simpli the best wai of make an s over that span .
becaus precis becaus the independ assumpt of a pcfg , that wai , if that constitu is us in the most probabl pars of the sentenc , what we'll be us is the best wai of make an s over that span .
so in thi case here , what i do is compar the two wai of make an s and here ar their probabl , . <num> versu . <num> , so the s goe to a np vp is the higher probabl wai of make an s , and so i keep that on , and i just don't store thi on .
okai , and so now i fill in the cell , i mean again actual strictli onli partial for thi higher up cell .
and so , repeat that over and over , move up the chart is the heart of the cky algorithm .
now , the origin cky algorithm wa just for chomski normal form grammar , which i show earlier .
but as i indic , you can easili extend the cky algorithm to handl unari rule .
it make the algorithm kind of messier , but it doesn't increas it algorithm complex .
it turn out you can also easili extend the cky algorithm to handl empti .
let's just look quickli at how you do that .
so if befor we had the sentenc , peopl fish tank .
well the wai we did thing wa , we call these word word number on , two and three and we build our cky chart abov them .
and thi cell here is for the stuff abov word on , the <num> , <num> cell , thi is the <num> , <num> cell , and thi is the <num> , <num> cell .
and then we call thi on <num> , <num> , <num> , <num> , and thi on is the <num> , <num> cell .
and so we're measur our constitu span from the first word thei contain to the last word thei contain , inclus on both end .
if instead of that we want to work with empti in our grammar , we us the trick of fencepost that you see at variou place in comput scienc .
so we have the same sentenc , peopl fish tank .
instead , what we do is we put our number between the word , <num> , <num> , <num> , <num> .
and then we build a chart over those number which ha four point .
so , we have drawn like that .
hm , i guess i need to move thi three over a littl .
okai so thi then becom the <num> , <num> cell , which will store ani empti that we put here .
and similarli thi will becom the <num> , <num> cell , the <num> , <num> cell and the <num> , <num> cell .
and so each of these will store all and onli empti constitu .
and so then the actual word now have a span of on becaus it stretch from posit zero to on .
so we're now gonna put the actual word in the second row of the chart and so thei'll be <num> , <num> , <num> , <num> , and <num> , <num> entri .
and at that point we just continu on as we did befor .
so here we then have the <num> , <num> and the <num> , <num> entri , and here we have the <num> , <num> entri .
which is again thing that span the whole sentenc from posit zero to three .
but that could possibl includ empti here , empti here , or empti at ani of these point in between .
i'm not go to go through thi algorithm in detail , but it's someth that you could work out for an exercis and see that it doe work in exactli the same wai .
but what i do want to emphas again is thi idea that binar is vital .
the wai you get cubic time pars algorithm for cfg is by do binar , that that mean , that each rule ha at most two thing on the right hand side , and that allow a cubic time algorithm .
as soon as you allow more than two thing by the right hand side , you're minim n a higher order polynomi algorithm , and if you make no limit as to how long the right hand side can be , well then the algorithm is exponenti .
so if you look at the literatur for cfg and pcfg pars , sometim you see pars algorithm that explicitli work on binar grammar , that's gonna be the case for the cky algorithm that i tell you .
and so we're go to transform our grammar first then feed it to the cky algorithm .
some other grammar aren't like that , so thi is actual a misspel , there's an e in there .
thei do the binar intern to the grammar .
so the earli algorithm look like it work on an arbitrari pcfg , but actual it doe binar intern to the work of the parser .
okai .
so here's the cky algorithm that we're go to us .
in the next segment we'll go through in detail how it work .
but let me just point out a few thing about it .
so we're go to have an arrai of score , where we store the probabl of thing that we can build , build .
so that for each span , so these ar the two span indic from begin to end , and then for each non termin we're go to record the probabl of build a constitu over that span of a certain type .
if you can't build a certain nontermin over a span , we'll sai it probabl is zero .
and secondli here , we're record back pointer which is then sai that for each nontermin over each span , we're record a pointer as to what wa the best wai of build that nontermin over the span , what did you make it out of .
now , you don't actual have to store thi .
thi is on of these time space tradeoff .
you can do pars faster if you do store thi , caus then , onc you get to the end , you know the best wai of build everi constitu over everi span , and so you can easili write out the result pars tree .
but if you would actual prefer to conserv space , you don't have to store it .
and so , for exampl , in the stanford parser , we don't actual store these explicitli .
and we reconstruct the best pars simpli from know the score in the score tabl , and that's actual a fairli good trade off , caus it greatli reduc the space requir and get out the final pars is actual still quit quick .
okai , so thi first part of the algorithm is the part that is the part of the lexicon where we're fill in nontermin that word can rewrit as .
and so it's pretti simpl .
so we're go through each word and then we're sai if we've got a rule for nontermin of a goe to tank or someth , then we're go to put it score into the cell of the chart , so it might have a probabl of <num> .
and that's the main part of the cky algorithm .
and then everyth down here is deal with unari rule .
so deal with unari rule is a bit more complex .
so onc we have some thing in a cell , if we then have other unari rule like b goe to a or then if we've put thi into a cell or then we'll also be abl to build a b over the same span .
but the problem is we might then also have a rule c goe to b , and then we'll be abl to build a c over the span .
and in fact we might even have a rule a goe to c , which mean we've found anoth wai to build an a over a span .
so what we actual have to do is keep on appli unari rule until we stop discov new constitu that we can build over a span with better probabl .
and so , that's what thi doe .
so , there's a littl check as to whether we found a better wai of build a nontermin in an individu iter of thi while loop .
and so , we consid all of our unari rule , work out what probabl thei assign to the categori on the left hand side .
and if it's a better probabl , that's right here , we then store it in the back trace , and sai we've done some work .
and provid we've done some work , well then we're go to do anoth iter right through the loop of check out all the , all of the possibl unari rule again .
okai , but thi is still actual the easi part .
thi is just the lexicon .
then we go on and actual build the rest of the chart .
and so that's thi part .
so , what we do is , rememb we ar order our work by the size of constitu span .
so , we start off with two word span constitu and build up to the length of the sentenc .
and then we go across the cell from left to right .
so , we start with two word sp , constitu start with posit zero , and then head to the right most posit in our pars triangl .
thi work out the end , and then thi is the crucial part of the algorithm .
so , we're then sai , okai , mayb we'll fill in a cell from on to seven .
and so , we're go to build it with binari rule .
and so that mean it ha to be built in some wai .
so it can be built with someth from on to four and four to seven .
or , altern it could be built with someth from on to two , and two to seven .
and so we have to try out these differ possibl .
and that's the choic of the split point .
and then onc we've decid the split point , we're go to consid all grammar rule .
if thei exist , we work out the probabl of build the left hand side , so imagin we have a rule a goe to b c , the probabl of build a over thi span , in term of , if you build a b here and a c there .
and we just work those three probabl and multipli them togeth .
okai and if we found a better wai of build an a over that span , we record it .
and so again , that's the heart of the cky algorithm for binari rule work higher up the chart .
all the rest of thi is then again handl unari which we do in exactli the same wai as befor .
over each span , we then sai well , do we also have a rule that sai d goe to a .
if so we can also build a d , and then we have to repeat thi over and over again , until we've stop be abl to build anyth better than befor .
okai , so then .
that wa for on cell of the chart , and so then we're finish off our for loop for the , for the differ begin point work across row of our chart and then for the differ span work up the pars triangl .
so we go across here then kind of head up like thi .
okai , and so then when we're done , we found the best wai to build everi constitu over everi nontermin , over everi span , and we've in particular found what you can build right in the top cell here .
and , in , and if we have a start categori we know that we want to build that start categori .
so at that point , we can us thi chart to look down at , and find the highest probabl pars for a sentenc , and simpli return that back to the user .
now there's obvious anoth bit of algorithm here , which i'm omit for now but i think it should be fairli clear how to do that .
so there's a lot to grasp here , and i think it will take see the concret exampl in the next segment befor thi make ani sens .
but thi algorithm doe give a veri easi wai to see why thi is an algorithm that is cubic both in the length of the sentenc and in the number of nontermin in the grammar .
so , if you look at what we have here , we have a for loop of the span size , which is of order the length of the sentenc .
you have a for loop of the begin , which is order length of the sentenc .
and then you have a for loop of the split point , which is order length of the sentenc .
so we're o n cube in term of the length of the sentenc .
and then onc we start explor grammar rule we're consid all tripl of nontermin , and so then we're order g cube , for the number of nontermin in the grammar .
so i'm sai that the size of the set of nontermin equal g .
but let me mention for when you gui ar implement your own version , that thi is the simplist wai of do it , which is a polynomi algorithm , the , the right minim order of complex .
but in practic if you want to have a fast pcfg parser , you don't want to naiv do thi .
rather , you want to be do some check and index to make thing go fast .
so the kind of thing that you want to do is not just naiv iter over all tripl of non termin , but instead to sai , well , onc i've got to here , i know that i'm build , sai , from zero to four and four to seven , build up to zero to seven , so i know what constitu i could build over zero to four .
what i actual want to do is sai , for each of these thing that is in thi list , okai , tell me what grammar rule , have thi as the , each on of these as their left corner .
so perhap you would have s goe to np vp , but you might also have np goe to np pp .
so these ar both rule that have np as their left corner .
so these ar precis the rule that ar go to be on that will work down here becaus thei work with thi categori .
and so by do some clever index , we can make thing quit a lot faster .
okai , so that's the cky algorithm .
i'm sure it's kind of confus up until now .
i hope it'll becom a lot clearer onc we work through an exampl in the next segment .
it's hard to realli get a sens of the cky algorithm from stare at pseudo code , but i think if we work through an exampl , it'll actual seem pretti natur and straightforward so let's do that .
so start off , here's the grammar that we're go to us .
it's similar to the on that i've shown befor , in , when we did the grammar transform .
it's a grammar with unari , but with no empti in it .
okai .
so there's onli on conceptu leap that we have to make to get thi to work , and that is , befor i show you pars triangl the natur wai that make sens , but it turn out that it's realli hard to write text in diagon diamond .
and so , by and larg , what everyon ha decid is that it's much , much easier to get human be to tilt their neck <num> degre to the right .
and so , thei can see a pars triangl , rather than to actual print thing that wai .
so conceptu you should think about it just as we've had it befor , that thi is fish peopl fish tank .
and so , thi is the first diagon of the singl word , thi is the second diagon of the pair of word , the tripl of word , and the four word constitu .
but we've actual stuck it to make the box rectangl now , and you have to be hold your head to the side to look at it .
but provid you make it past that obstacl , we're readi to go .
okai .
so what do we need to get go ?
we have our sentenc to pars .
fish peopl fish tank .
the second thing that we need is the grammar .
and so then onc we've done that we can start do thing .
so in particular what we go to do is start fill in the lexic entri .
so we start with the cell here and we sai okai , the word fish that can be a noun or a verb , and so we put in that we can make a noun from fish , with probabl <num> .
and we can make a verb from fish with probabl <num> .
and then we keep on do thi for the other cell along the diagon .
so peopl can be a noun with probabl <num> , and peopl can be a verb with probabl <num> .
and we keep on go .
and thi is what we get .
okai , but now our grammar also ha unari that rewrit to categori .
so we also then have to appli the unari rule .
and so the wai we'll do that is for each cell we'll find unari rule that appli , and if thei creat a categori that isn't there befor , or on with higher probabl than what wa there befor then we'll put it into our chart .
and in thi case , the relev unari rule here , we've got on other unari rule over here actual , ar all creat differ categori , so we'll be ad new thing to the chart .
so , we'll sai , look we can also make a vp here , which ha probabl <num> . <num> .
so , there i'm take the <num> and the <num> and multipli it .
we can make an np , where the probabl of that is <num> time . <num> , <num> . <num> .
and in the first round through that's all we'll do , but after we've been through all of the rule for nontermin , we then sai , well , we have ad some new thing , so ad is true , and so we'll do thi whole while loop onc more .
and we need to do that , so we also discov that we can appli by a second layer of unari rule and put it , build an s over thi span .
so we'll also build an s , and the probabl of that will be <num> time the probabl of the vp , so that'll be <num> . <num> .
okai .
and so , then we're go to appli the unari rule down in the other cell of the lexicon .
and that'll give us thi .
so at thi point , we're readi to start do the heart of the cky algorithm which is build constitu over bigger span , first appli binari rule and then we'll do the unari again on top of them .
okai , so at thi point , the cell that we next want to fill is thi on .
so thi is the cell that's cover from word posit zero to two .
so , thi version of the algorithm i've written us fenc post , even though the particular wai i'm do it , there aren't ani empti element .
okai .
and so , to build a binari constitu from out of thi , we , the onli wai to do it is to build it someth from zero to on and someth from on to two .
well the thing from zero to on ar contain in thi cell , and the thing from on to two ar contain in thi cell , and so what we ar go to do is consid wai of combin on of each of those to make some other constitu , which will be licens provid there's a rule in the grammar over here of x goe to y z , where y is someth here , and z is someth there .
so what of that sort can we do ?
well , there's onli , there ar no binari rule that have an n on the right hand side , onli unari rule .
so we won't be abl to do anyth with the v .
if we then look at thi , sorri , we won't be abl to do anyth with the n .
if we look at thi v here , that could be made with that rule .
so that's someth that we can make .
so look , we've got a vp goe to v np , and so that mean we can make a vp in thi cell , and the probabl of it will be <num> time <num> . <num> time the probabl of thi rule .
and at thi point you're start to stretch my abil to do mental arithmet , so that's . <num> time . <num> so that's five carri the on , or <num> . <num> .
but then there ar also other thing that we can build with binari rule .
so what other binari rule do we have ?
we've got a rule for s goe to np vp .
and , well , we can certainli appli that , becaus we've got an np here and a vp there .
and so we can build an s over thi span .
and similarli for some other on , so np goe to np np , well we've got an np here and np there , so we can build an np over thi span .
i won't try and work out all of the probabl , but if we do that , and appli them , we then end up with all of these probabl here .
now , it turn out that for each of the , the constitu that we built here in the binari phase , there's onli on wai of make them , so there wasn't ani content .
but we'll we abl to see content at the next unari phase .
and our rule for content is alwai , if we discov multipl wai to make a categori over a span , we keep the on of highest probabl .
and so the next phase here we now do , is we do unari for these cell down here .
so , again , we look for unari rule , and if thei give a wai of make someth with higher probabl , we put that into the chart and then we sai we've done some work .
and have done some work will lead us to do the whole while loop again in case we're abl to build up chain of unari rule .
so , in thi particular case , there ar no unari rule that have np on the right hand side , so we can't do anyth there .
but there is a unari rule that ha vp on the right hand side .
and so we discov that there's anoth wai that we can make an s , where we make an s goe to vp .
and well the probabl of that is actual go to be <num><num>5 x <num> , which is <num> . <num> , which is actual a lot more than the probabl of thi rule .
so , we ar go to record a better wai of make a s in our back pointer , and we ar go to record the probabl of that new best wai of make it .
we alwai just keep what constitu we can make , the best probabl wai of make them , and if we're store back pointer , the wai that wa the best wai of make it .
so if we go through all the cell and see if we can make them in better wai , with unari rule , we find that that's possibl in precis two place .
thi on , and also over here we find a better a wai of make an s .
and so we updat those two , and thi is now the second row .
okai , so now we're gonna go on to the third row where we're build three word constitu .
so thi is the fish peopl fish cell and thi is the peopl fish tank cell .
so at thi point , thing get a littl bit more interest for build the binari constitu .
so when we're go to build thing in thi cell , which ar thing from here to here , well we're go to build binari constitu , but there ar two wai that we can do it .
we can either take someth from here , from zero to two , and combin it with someth from two to three .
or els , we can take someth from zero to on , and combin it with someth from on to three .
and that's what we do in thi part of the algorithm here .
we choos a split point for divid the constitu into two .
thi is where we exploit it be onli binari rule to get a cubic time algorithm .
so we iter across split point , which is anoth o n oper .
and then we consid what rule that we can appli for each split point .
and then that part is as we did it befor .
so first off , we're go to start with sai okai , we're go to combin stuff from zero to two , that's thi cell , with someth from two to three , that's thi cell .
and so then we'll see what we can build .
and well , we have an np and an np , and so that will allow us to build a bigger np accord to that rule .
and we have np and a vp .
so that'll allow us to build an s us thi rule .
and we have .
well , it seem like we can't actual build a vp over that span do thing that wai .
okai .
let's then consid the other possibl , where we do thi and thi .
well , thi time we can build an np by combin thi np , and then look for the other on we're look at thi cell .
so thi time we're us these two cell .
so we can us thi np and thi np , which will allow us to build an np .
umm , we can us thi verb and thi np , which will allow us to build a vp , and we can us thi np and thi vp which will allow us to build an s .
and so we can build thing with that span .
so then what we're go to want to do is sai okai .
we can definit build an np over thi span but we want to find out which of these two ha the highest probabl on of , wai of make np and keep onli that .
and similarli , we're go to want to find out which of these two is the highest probabl wai of make an s and keep onli that .
now , actual , if you work through the math in thi case , there's a littl fine point for the np case .
becaus if you look at the np case , for the purpl on , it's <num> . <num> x <num> . <num> , and the probabl of the np rule is a constant <num> .
and if you go to the green on , it's <num> . <num> here , which is the same as there , and <num> . <num> , which is the same as there , reflect the fact that both wai of do it , you ar us the word fish twice and the world peopl onc .
so actual , both wai of make np have ident probabl .
so in practic , when you have ti like thi , these parser just choos on analysi , the first on thei come across .
but sort of to s , you can just sai thei choos on analysi randomli .
okai .
so that then fill in that cell , and we get these number .
and then , at that point , we would again ply , appli unari rule .
so the onli unari rule that's activ at thi level is thi on , caus the other two unari rule onli work on n and v , and so aren't applic in a cell like thi in thi littl grammar .
so we'd also find a new wai of make an s , that , by make it from a vp but it probabl would be <num> time thi number , so it would be <num> . <num> , which is less than the wai we've alreadi discov of make an s .
so we'll keep thi as the highest probabl analysi .
okai , so then we do the , thi cell here .
the on to four cell .
and so again , there ar two split point of wai of make it up .
we could either make someth from on to two , and then three to four .
or els we could make someth from , from on to three , and then from three to four .
and so we explor all those wai of combin thing and fill in the cell .
okai .
now , we're almost at the end .
we just have to fill in the final cell of the chart .
and it's the same thing apart from now when we iter across split point , there ar three wai to do it .
so , you can make thing to put in here either by take thing from zero to on follow by two to four .
or you can have thing that ar from zero to two plu thing that ar from three to four .
or , you can have thing that ar from zero to three , plu thing from three to four .
and so we explor all the wai of combin thing , and again , choos the highest probabl wai of make each of a noun phrase , an s and a vp , which ar the three constitu type that we can build over that span .
now by the time you've gone thi far up the math is much too difficult for me to do in real time in front of you , but i've work it all out in advanc , and if we fill it out , thi is what we get .
so what we get here is , that we can just ask the question of what's the highest probabl pars of thi sentenc regardless of what categori you choos for it ?
and the answer to that is that is pars it as a sentenc .
but , if you have a nomin start categori like s , you can instead sai i just want to find an analysi as an s , regardless .
and then that will also return thi analysi .
okai , i sai we've finish , but actual , well , where is the pars tree ?
and so the answer to the pars tree is we can find it by now call thi build tree routin that trace backward , through the tree .
and so we know that the first thing that we built , at the top just take thi start point is it's an s goe to np vp , and at thi point , i have to confess that i actual left a littl bit out of what's written in the tabl just to keep thing written larger .
but rather than simpli write the rule that you us , you'll actual record which cell you built it from .
so that these ar actual constitu over span , so thi is actual a noun phrase that wa built over zero to two , follow by a verb phrase that wa built over two to four .
okai , so that mean we now look in the zero to two cell to find the noun phrase and we look in the three to four cell to find the verb phrase .
and at that point we'll start to thi is zero to four , zero to two , two to four .
and in thi part we start to recurs downward .
so the highest probabl wai of build a noun phrase wa us the noun phrase goe to noun phrase noun phrase rule .
and sinc that is a binari rule we then necessarili know that it had to be zero on and on to two .
and the highest wai of make a verb phrase is us the verb noun phrase rule , where thi goe from two to three , and thi goe from three to four .
okai , so then , that mean , we're build thi noun phrase as a noun phrase in thi cell .
and we're build thi noun phrase as a noun phrase in that cell .
and then over here , we're build the verb as a verb in thi cell , and we're build a noun phrase as a noun phrase in thi cell .
so at that point we can then sai well , what's the highest wai of build a noun phrase here , oh , it's by make it as a noun from zero to on .
well , what's the highest wai of make a noun over zero to on from the same span becaus that wa a unari rule ?
well it wa by realiz it as the word fish which wa a word in our sentenc .
and now similarli we do the same for the other cell .
okai so from on to two the wai of build a noun phrase with highest probabl wa build it from a noun as a unari rule and the wai of build a noun with highest probabl wa us the word peopl , that's a termin in the sentenc .
the highest probabl wai of build a verb from two to three is directli make it from the word fish , which is a termin in our sentenc .
and then the highest probabl of wai of make a noun phrase over three to four is from make it from a noun from three to four via a unari rule .
and the highest probabl of wai of make a noun over three to four is make it us the word tank , which is a termin .
so , at thi point , we've been abl to back , follow the back trace back out , and if , can show you thi is the highest probabl pars for thi sentenc .
phew , that wa a bit to get through for me as well as you .
but i hope you can see it's actual fairli straightforward as an algorithm .
and it's actual much easier for comput to do it than it is for human be fill in squar of a rectangl .
so i hope you kind of feel like now you understand how you could go off and write a program that did that , thi dure the same kind of for loop for yourself .
in thi segment i briefli discuss how we evalu pars result .
so , how do we go about evalu whether our parser ar do a good job or not ?
on measur would just be to sai , is the pars that we produc exactli the right pars accord to the tree bank .
that's a possibl standard , and actual close to what the probabilist model do , but it's a rather a tough job to get the entir structur of a sentenc right .
and so the most us evalu measur have divid up the piec of the pars so you can get partial credit for be right .
so let's go through how that work .
so the idea is , we start with the gold standard pars tree , thi is the correct pars of the sentenc .
so then we run our parser , and our parser is go to choos some pars tree for the sentenc and return it .
so in the exampl here , the parser is mostli right .
it's got the noun phrase sale execut .
it's got thi verb structur of were examin , and the noun phrase the figur right .
it's actual made on littl mistak at the end , so yesterdai should be thi kind of weird kind of tempor noun phrase that english ha where you have bare noun phrase like yesterdai , or next week , to express a time that can otherwis be express as a preposit phrase like on sundai , in the spring , but the parser ha wrongli just stuck it on as an extra noun at the end of the noun phrase , great care .
and so that's an error .
so , how ar we gonna go about measur that ?
well , the wai that we're gonna do it , is we're go to look at individu constitu claim .
so , here is a noun phrase and we're gonna sai that it goe from posit zero to two in the sentenc , where i'm put these fenc post marker number in between the word of the sentenc .
and so then , we look down at our guess pars , and sai , well , it also ha a noun phrase that goe from zero to two in the sentenc .
and so that particular constitu claim in the gold tree is correctli captur in the pars tree .
and so the wai we do that for all the case is we convert the gold tree into a set of constitu claim , where we leav out the the root node , so i've written claim across the root node .
so thi ar our constitu claim , so we have a sentenc that's go from zero to eleven , and then the variou other constitu claim , then we do exactli the same thing for the candid pars produc , suggest by the parser .
and so it's also suggest that the whole thing is a sentenc from zero to eleven .
it's also suggest that the np from zero to two , and so on .
so it's got a set of constitu claim .
and then what we do is we simpli treat each of these as a unit , as an atom , that you either get right or wrong and then we us exactli the same precis recal f measur that you've alreadi seen sever time befor .
so here ar the gold standard label bracket .
here ar the candid label bracket and i've shown in bold the three that the candid agre with the correct pars ar on .
and so in total there ar eight bracket in the gold pars and seven in the propos pars .
so the label precis is <num> <num> , <num> percent , recal <num> percent and the f1 that combin those in the usual wai come out as <num> .
now , what that partli mean is when befor we had thi , there were these extra node down here for the part of speech that we talk about befor .
we don't , in the pars measur , evalu , evalu the part of speech tag , even though most parser do part of speech tag as part of their work .
so that's report separ .
and in the exampl i've got here the tag is complet correct , and so that's <num> .
someth that you might notic here is that these score ar actual pretti low , and it's worth think a bit about why thei're so low .
and what we find is that even though the candid pars wa mostli correct we actual get a lot of bracket wrong .
so what happen , the onli thing that wa wrong with the candid pars wa that thi noun yesterdai wa wrongli fit into thi noun phrase , rather than be a sententi modifi , as a tempor np .
but as a consequ of make that mistak , the parser is score as do a lot of other thing wrong , so that thi verb phrase is score as wrong , becaus it's wrongli extend to cover yesterdai wherea it shouldn't .
thi verb phrase is wrong for exactli the same reason .
thi preposit phrase is wrong for the same reason and so is thi noun phrase .
so thi label precis recal measur suffer from these cascad error whenev you attach someth veri low that should be high or vice versa , so mani peopl think thi is actual not such a good measur of parser perform .
and peopl , includ me , have argu that instead we should us depend measur of pars perform even for constitu parser .
but that just isn't the current practic of what you see everywher in research .
the measur that you see is thi label precis , label recal , f1 .
and so that's a measur that we've present here .
i'll just briefli mention .
you can also do an unlabel version of these measur , where you simpli look at the constitu claim as a sequenc of word without worri about the label , but that's much less us .
okai .
so now that we know how to evalu pcfg , how well do thei do ?
well , if you just train a pcfg off the penn treebank and run it on some penn treebank test data and evalu with thi measur , the answer is you get about <num> percent f1 measur .
so that's not terribl .
you get quit a lot of the bracket right but rememb it is count each bracket separ so if you're get over a quarter of the bracket wrong , that mean that you're mak , tend to make sever bad attach error in everi sentenc .
so that's not a great result and we'll talk about how to improv it veri soon .
so let's just summar though a littl bit the properti of the pcfg that we've seen up until now .
so the good thing about pcfg is that thei're veri robust .
you , normal when estim a pcfg off a tree bank you get the result that , that with smooth probabl , that everi possibl string of word is includ in the grammar .
so there's no categor grammar constraint whatsoev anymor .
all the action is in the probabl , what's high probabl and low probabl , and that's us becaus the parser is robust .
you can give it anyth at all and it'll do it best job to give you the most like sentenc pars for that .
anoth good thing about pcfg is that thei give us at least part of a solut to grammar ambigu .
so , like in the exampl of peopl fish that i show earlier , the pcfg tell you which pars for a sentenc to choos .
and at least sometim it choos the correct on .
so it give some idea of the plausibl for a pars .
but , as we'll see more later , and you alreadi got a hint of , that becaus of the strong independ assumpt of a pcfg , it's not actual veri good at do that .
a third good properti is that pcfg give us a probabilist languag model , so when you sai how like or unlik sentenc ar in a languag .
but it's import to point out that you shouldn't just now go and run off and think ah ha , i've got a grammar , i'll us a pcfg as a better languag model than the bigram and trigram we saw earlier , becaus actual a pcfg as we've shown it so far doesn't work as well as a bigram or trigram , as a languag model , for task like spell correct or speech recognit .
and the reason for that seem to be , is that pcfg's lack the lexic of a trigram model .
so , what do we mean by that ?
well what we meant by that , is that , if you're look at thi pcfg , most of the pcfg is of the natur vp rewrit as vbd vp , that rule that ar expand without ani refer to what word ar actual us in the sentenc .
in fact the onli rewrit rule that consid word ar the on that rewrit from a pretermin to the word itself .
and so thi limit the abil of a plain pcfg to act as a veri effect languag model .
that's a problem we'll address again veri soon .
but for the moment you now know how to go about evalu constitu parser .
in thi segment i'm go to introduc the idea of lexicl pcfg's .
so let's look at our basic pcfg here and see how the probabl of rule actual work .
so , what we have in the pcfg is local tree correspond to rule such as verb phrase rewrit as vbb pp , and that ha some probabl attach to it .
like , mayb that probabl would be zero three , about three percent of verb phrase expand as a past tens verb and a preposit phrase .
well , similarli , we have other rule so that we have up here a sentenc goe to np vp rule .
and that would have a much higher probabl .
mayb it probabl is <num> or someth like that .
but the realli import thing to notic is that these rule make no refer whatsoev to actual word .
so , thi is sai that overal about three percent of verb phrase consist of a past tens verb and a preposit phrase .
but whether that's like or not depend an aw lot on which verb we're deal with .
so in thi exampl here we have the word , verb walk .
and walk is the kind of motion verb that is realli , realli like to have a pp follow after it .
wherea if we had anoth verb , for exampl if we had the verb saw , then that would be a past tens verb but it would be realli unlik to have a preposit phrase come after it , someth like he saw in the mirror or someth like that , you'd normal get a , a noun phrase object after saw first .
so it seem like we can onli realli come up with good probabl estim if we know more about the word in the sentenc .
and so , that's precis what the idea of lexic is .
the idea of lexic is , let's defin for each categori a wai of find it head .
so , for noun phrase , we'll sai that the last noun in it , whether a proper noun or common noun will be declar it head .
and so , for thi noun phrase we'll sai that it head is sue , and for thi noun phrase here , we'll sai that it head is the word store , becaus thi is the last noun in the noun phrase .
and we'll appli that to other idea .
so for a verb phrase , the head of a verb phrase will be the verb insid it , so the head of the verb phrase is walk , and for a preposit phrase the head will be the preposit insid it , into .
and we'll sai that the head of a sentenc is the head of the verb phrase .
so we put in thi wai lexic item that repres the head of each phrase next to each non termin in the grammar .
let me get my more , neatli print version of that here .
okai , well what happen if we do that ?
well what we then find is that we've now got these categori like s walk , which ar a combin or our old nontermin plu some lexic item .
and so if we do that , we've enorm , enorm expand our effect space of nontermin , becaus if we had someth like twenti nontermin befor , but we had someth like <num> , <num> word in our vocabulari , well now we've got <num> , <num> nontermin in our grammar .
so that suggest we need to do start do some more special engin to be abl to do that .
well let's not worri about that for a moment , and let's just think in term of probabl what thi will allow us to do .
well the neat thing thi will allow us to do is now if we're look at what's the probabl of thi sub tree , we won't just be sai , what's the probabl of a verb expand to a past tens , verb phrase expand into a past tens verb and a pp , we'll be sai what's the probabl of a verb phrase head by walk , take a pp ?
and in particular , a pp that's head by into , and so we're now gonna be captur insid the rule two thing .
we'll be captur both that a vp walk is like to take a pp after it , and we will be captur the relationship between the head here .
so , we will be captur , captur the relationship that it's reason to have someon walk into someth .
and so we have a lot of much richer probabilist condit be captur by our grammar .
thi extra inform will be realli , realli us for resolv variou kind of ambigu .
so a classic exampl of that is preposit phrase attach .
so if we want to work out for a preposit phrase whether it is modifi a noun that preced or modifi a verb that preced .
well , we can captur quit a lot of that insid a pcfg onc we lexic it .
becaus now we'll have a rule where an np rate is take on it right hand side a pp for .
and we can ask whether that is a like thing to happen or not .
or we can sai we have a vp announc is take , expand on it right hand side to a pp in .
is that a reason thing to happen or not ?
and so we can us thi inform to better model pp attach than we could in a vanilla pcfg .
that doesn't mean we can captur everyth about pp attach in thi model , though .
and actual , if you want to , you could think a littl about what other thing that you'd like to know about attach a preposit phrase , which aren't yet captur in thi model , where we have the head word pin on to everi phrase like thi .
but nevertheless , thi kind of head lexic captur most of the addit thing that you'd like to know to make the variou pars decis of a sentenc .
so it's also us for thing like coordin scope , know about the complement pattern of verb , and so on .
inde , it wa the case that do thi lexic of pcfg wa seen as the pars breakthrough of the late 1990s .
so that's a realli us notion to know about , is thi idea of lexic pcfg to captur more of the necessari probabilist condit inform to make pars decis .
let's now look at a particular model for realiz lexic pcfg .
and the model we're go to look at is the model of eugen charniak from charniak <num> .
thi isn't the most recent model , but i'm choos it becaus it's the simplest and most straightforward wai of build a lexic pcfg .
and so hopefulli , it's easi for you gui to get a sens of how it work .
so just to give a bit of context for what i'm explain .
when you're actual pars in charniak's pars model , the wai he's do the pars is bottom up in a wai somewhat similar to the wai that we did cky pars in an earlier segment .
but , just as with the plain vanilla pcfg , the probabilist condit is top down , so you've got the probabl of a right hand side given a left hand side , i . e .
the probabl of stuff below given the stuff abov .
and in thi segment i'm basic just gonna show you what the probabl distribut ar , rather than actual concret go through the pars algorithm , but for the actual pars algorithm you're us these probabl and appli them work upward through the tree .
so thi is the idea of how charniak's algorithm work .
and for thi exampl , what i'm go to assum is that thi is our start point , where we're alreadi partwai through build a tree .
so we have an s node , and we know the headword of the whole sentenc is go to be rose .
and that sentenc is rewrit as a noun phrase and a verb phrase .
well , sinc the verb phrase is the head of the sentenc , we just know automat that it headword is also go to be rose .
and the point at which we ar up to is , we haven't yet decid how to expand thi noun phrase .
and we don't know what the head of thi noun phrase is .
so , in charniak's model , there ar two probabl distribut that ar us to expand out a sentenc and we'll see each of them in turn .
so , the first on is , gee , we have to choos some headword for thi noun phrase .
and for choos that headword , we're go to condit on sever thing .
we're go to condit on the headword of the parent , the categori , which is noun phrase , and the parent categori , which here is s .
and so the probabl distribut we're go to us is thi on .
so we're go to have a probabl over differ headword choic given these three condit thing the categori , the parent categori , and the parent head word .
and so what we ar ask for a noun phrase , which is in thi context of be under an s node and have a head of the whole s be rose , what ar like noun to choos as the head of the noun phrase .
and so you might think of someth like , the balloon rose and sai balloon or temperatur , temperatur rose .
or you might think temper or someth like thi .
but sinc actual here we're deal with the financi newspap , the wall street journal , what it's actual like to be , and is in thi exampl , is that profit rose .
okai so now we have thi noun phrase here , which is a noun phrase , head by profit .
but we don't actual know how it expand and so now we're go to have a second probabl distribut at work and so we ar go to want to have a rewrit rule that's work out how thi expand and what we're go to condit it on is the headword profit , the categori noun phrase , and the parent categori which is s .
and so that's then go to give us thi second probabl distribut here .
what is the probabl of a rule that expand us down on level in the tree , given these three condit thing ?
the categori , the parent categori , and the headword .
okai so we're gonna choos a rule .
and so that rule could be just np goe to a noun , a plural noun and it would have some probabl or it could be someth els .
and so for thi exampl here the actual rule that's chosen is to gener an adject and a plural noun .
and so that's given by thi probabl here .
well , onc we've gener an adject and a plural noun , we then have the notion of a head of a phrase .
so we know determinist that if thi , if thi is a noun phrase head by profit well thi plural noun must be profit becaus it is the head of the noun phrase .
and sinc actual we're down to the level of a part of speech tag here , that mean actual we have the word profit down at the bottom of the tree here .
okai , at thi point we've complet thi element of the expans accord to these two probabl distribut .
and at that point , we're just go to keep on go .
so , we're go to sai , well here is a , is a categori , it's actual a non termin categori .
but we're go to expand that by work out what it headword is .
so , we're go to us thi probabl distribut and we're go to choos a word here as the headword corpor .
and then by that point becaus we're down to a non termin , we'll know that that's the word in the sentenc .
and then over here , the vp , head by rose , it's go to have to be expand by make us of thi probabl distribut so we're go to expand it , and choos some expans phrase .
mayb it'll go to a past tens verb and a preposit phrase .
and then we'll start expand those down by choos headword and how to expand them .
we'll get the rose here automat again of cours , which will then go to the word rose .
so , that then we choos a head word over here and then start expand it .
so , we're pretti much done .
so the onli thing we realli need now is a wai to get start .
and so for that we just need a slightli vari probabl distribut at the begin .
so that we're go to sai that we have a root categori at the top , and it need a headword .
so at that point we're go to have a probabl of a headword given that the categori equal root .
and then the other thing , these don't exist .
so thi is just the chanc of differ word be the head of the whole sentenc .
and so then onc we've done that , we can then start expand downward from there .
so thi will be a , a root head , head by rose , and then we'll look for a rule to expand that .
so we're now look for some expans , which will here just be to a s under thi .
so at thi point we again don't yet have a parent categori but we've now got a categori and a head .
so you need a coupl of special probabl distribut to just get you start at the begin and then you do the basic recurs i've talk of here .
so what we achiev by put these word into the grammar like thi .
so here ar some statist that show thi , that go in a bit more detail what i talk about previous .
so here we have differ expans of the verb phrase rule , and here we have a choic of differ verb for a head .
and thi just illustr more systemat how the chanc of differ expans for the verb phrase vari enorm depend on which verb you're choos so if you have a verb like come , you find out that get a pp after the verb is enorm , enorm common .
that happen about on third of the time and have just a verb in the verb phrase happen about ten percent of the time where mani other kind of complement like s and np complement ar realli , realli rare with the verb come .
but that's then exactli and also tran , with a noun phrase after it .
but for other verb the fact ar veri differ .
so for the verb take , well , take normal take an object , he took a nap , he took a book , he took a ticket , anyth like that .
so about a third of the time , you get just the vp goe to v np , though of cours you can get other thing as well .
if we then move on to think , think is a , sort of sententi complement verb that you're sai what you thought , and in fact nearli alwai with think , almost three quarter of the time , you're get an sbar complement , that's someth like , he think that she is dishonest , he think she is dishonest .
either of those is an sbar complement regardless whether there's an overt that or not .
and in the final exampl illustr here is the verb want , which also take complement but it take infinit s complement .
i want to go to the store , and so about seventi percent of the time , you get that .
and so , essenti you notic that thei're just extrem differ probabl of differ expans to the verb phrase , depend on know what the head verb is .
and thi is precis the kind of inform that can be captur in the charniak pars lexic pars model .
and on other final thing to note here , is that these ar what ar sometim refer to as mono lexic probabl .
so we're look at the expans of categori in term of the categori and know just on lexic item , the head verb .
that's in slight contrast to the other main wai in which lexic pcfg ar us , and that's for predict depend in thing like preposit phrase .
so there we have bilex probabl .
so that wa , when we had exampl like , go into or man with , we're then decid how like the connect is between the preposit and a noun or a verb .
so these then involv two word at a time .
so the charniak model also ha bilex probabl .
so here ar the chanc of choos the head noun of a noun phrase , a plural noun phrase given some amount of inform , which might includ inform about the noun phrase , but also inform about what it parent categori is and what the headword of the , the sentenc , whole sentenc is .
and so what you find in the wall street journal , thi isn't typic of all english , is that if you have a plural noun a bit over on percent of the noun it's price .
but if you know that it's the subject , it's insid a noun phrase under an s node , i . e .
it's the subject of the sentenc , well then the chanc of be price becom about two and a half percent .
but what realli make a big differ is that if you know that the verb is fell .
well .
that's inform that tell you it's the kind of verb that could easili go with price .
and so then the probabl becom far higher again , so now it's up to an almost to fifteen percent chanc , a on in seven chanc , that the head of the noun phrase is gonna be price .
and again we ar captur a lot more of thi probabilist inform .
but you might be wonder now , gee , can we realli estim these probabl ?
and the answer is that in gener , you can't estim these probabl .
that the probabl that you'd like to estim becom far too spars .
and i'll illustr that on the next slide .
but the wai that it's dealt with in charniak's model , is by have a complic scheme of do linear interpol between differ model that ar more or less precis .
so , thi should be reminisc of what you saw for languag model , in the second week of the class .
so we want to estim thi probabl distribut that we've seen befor , choos a headword base on the parent's headword , your current categori and the parent's categori .
and the wai you were do that is by take thi linear interpol of a bunch of differ distribut , that , on of which is just the maximum likelihood estim condit on everyth .
and then there ar further distribut that first of all leav out what the parent headword is , and then also leav out even the parent categori .
so at thi point , you're just choos a headword base on the categori .
so just sai it's a noun phrase , what's the chanc of it have a certain head .
and in charniak's model , these differ distribut ar weight in a determinist wai , depend on how much you'd expect to have seen certain kind of evid .
so , make us of these languag model like techniqu , ar essenti to build these kind of lexic pcfg , becaus the data just is too spars .
and thi next slide show that .
so , here the differ distribut ar be combin togeth in the linear interpol .
so if we do the on on the right first , what we find out is that if you've got a noun phrase head by profit , and insid it there's an adject and you're want to ask what adject it is .
in the wall street journal , about on quarter of the time it's corpor profit .
okai .
so that's a veri precis , fulli condit estim .
wherea when you start eras some of the inform , you get .
coarser , but still non zero estim .
i haven't actual made sure thi second line here , thi wa the method that charniak us to get kind of coars semant class , to try and keep some inform about parent head word befor get rid of it entir .
but if we just look at , don't realli do a lot with that on , and look at these two .
you can see that onc you're onli , condit on know it's a noun phrase under an s or just a noun phrase , the chanc of it be corpor , is then drop by almost two order of magnitud .
so you're down to about half a percent .
okai , so thi is the good case in which you can calcul a probabl from the rich condit , and it help you a lot .
but quit commonli , that just doesn't work for you .
so if you then sai , well , the verb of the sentenc is rose , and i've got a subject noun phrase , a noun phrase under an s , and what , noun should it be ?
well , in the particular sentenc in the data to be pars , the actual noun , wa profit .
but it turn out that in the train data , profit never occur as the noun head a noun phrase that wa the subject of rose , despit the fact that that sound perfectli normal .
profit rose last quarter .
profit rose throughout the economi .
ani sentenc like that .
and so thi , the maximum likelihood estim , the mle here is just zero .
and so the onli wai we're get a non zero probabl estim is by us these probabl where we condit by less stuff .
and even then we're get these low probabl right .
so the probabl of a noun be profit in a noun phrase it's about sort of on 20th of a percent .
but that's the best kind of estim that we have .
and so although you'd like to us rich estim as here most of the time in do lexic pcfg pars , you're actual have to fall back on rather coarser estim becaus you can't get the condit inform you'd like to from the fairli small supervis tree bank that we have to train on .
in particular , for these bilex probabl when you're try to condit on two lexic item , the probabl tend to have to get back off , just becaus the amount of inform to estim thi is extrem , extrem spars .
becaus , you're in thi space where even befor you consid the categori , that you're do someth that's kind of like bigram probabl estim .
and it's hard to estim word bigram probabl on onli about a million word of text .
and that's all the text that we have in the hand construct tree bank .
okai .
there were some detail about , that need smooth at the end there .
but i hope the main thing that you could take awai from thi segment is how there wa a fairli straightforward system of two probabl distribut that charniak wa abl to us to realiz a lexic pcfg model .
in thi segment we're gonna dig it littl deeper into the independ assumpt of pcfg and what that mean in practic .
so the symbol of a pcfg defin the independ assumpt .
so if we have a pair of rule like thi s goe to noun phrase , verb phrase , noun phrase goe to determin noun .
when you're expand here the onli thing you know is that you've got a noun phrase and you've got probabl for how it expand .
so the right wai to think of categori in a pcfg is that thei ar kind of choke point inform flow .
so we have the outsid tree up here and when we've expand thi noun phrase , we know noth about the outsid tree , all we know is that we're expand a noun phrase .
or , look at from revers , when we're work out what pars to choos up here , we don't know anyth about what went on down here .
all we know is that thi is a noun phrase over some number of word , and then we need to work out what happen abov it .
so precis , thi is a point at which there's an independ assumpt .
the assumpt is you can work out the probabl of thing insid here know onli that thi is a noun phrase .
and , you can work out the probabl of thing up here know onli that thi is a noun phrase .
that's a veri strong independ assumpt .
and we can see clearli that it's too strong by look at variou case .
here's on exampl .
so , if we just sai , what ar the chanc of expand a noun phrase overal ?
well , the chanc of a noun phrase expand as a noun phrase , preposit phrase in the penn tree bank , it's about <num> percent , and the chanc of it expand as a pronoun , so thi is someth like a noun phrase goe to the pronoun she .
that's six percent .
and these ar just the overal statist .
and let's suppos we now ask about particular categori .
suppos we know that thi is a noun phrase under an s , so that's the subject noun phrase of a sentenc .
then we find out the statist ar enorm differ .
so the chanc of thi expand as a np pp go down a littl , but what's realli dramat is that the chanc of it becom a pronoun go up enorm , about three and a half time .
in contrast if we know it's a noun phrase after , under a verb phrase , the chanc of it be a pronoun drop .
but the chanc of it be an np pp go up a lot .
thei more than doubl .
now , if you have a bit more background in linguist , you would sai at thi moment , well , of cours .
that's exactli what you'd expect , base on knowledg about inform structur of languag .
so that subject ar normal us for topic , establish discours refer , and therefor it's highli appropri for that to be a pronoun .
wherea , insid the verb phrase , what you're normal do is introduc new inform , which therefor need to be explain a littl bit more .
and therefor , it would be quit common for that to be a noun phrase with a preposit phrase hang off it , becaus that's on of the wai in which you can express more inform , so you can sai someth like , the man by the railwai , railroad track or someth like that , where you're give more descript content .
but just think about it at the moment for our pcfg , well , what we're find is that if we just sai we're expand a noun phrase , we , we're lose thi addit inform which would let us give much better probabl about how to expand it .
anoth wai that you can see that your independ assumpt ar too strong is when you run a basic pcfg and you find that it just doe the wrong thing .
so it's choos rule that look wrong , that it shouldn't have us , but somehow it's choos to us them .
here's an exampl of thi , so for a noun phrase like big board composit train , the noun phrase structur that you're meant to choos in the penn tree bank is just thi flat structur .
in gener the penn tree bank us a lot of flat structur for compound noun like thi .
but if you train up a simpl pcfg and run it on thi sentenc , the structur that you actual get is thi on here where it's put in an extra noun phrase node around big board .
well , it just shouldn't have done that .
and your first reaction is , well , why did it do that ?
becaus all of the compound noun in the penn tree bank have thi kind of flat structur .
why didn't it learn that ?
but the reason that it wa abl to do thi is that there's a differ structur in the penn tree bank where you do get these np node at the start of an np .
and when that happen is with possess .
so here we have thi possess np , and then you do have a noun phrase up here go to noun phrase adject noun .
and so what you'd like to sai is , well thi rule here , it's okai to us it as , for possess , but thi noun phrase isn't a possess .
but then , that's precis where the problem is 'caus both of these node ar just mark noun phrase .
and so we're not record the inform that thi on here is a possess noun phrase .
so somehow , we want to get more inform to thi categori to sai thi is a possess noun phrase , so the parser would know that you can't us that expans rule when you have a non possess noun phrase like big board .
so that's what we want to do .
so we can relax the independ assumpt of a pcfg by encod more inform into the non termin symbol .
a process that's often refer to as state split .
so on of the first propos for state split , and show how success it wa , wa mark johnson notic that you could improv a probabilist context free grammar quit a lot , by encod as part of each non termin , also what wa the parent categori .
and rememb you also saw thi idea in the condit of the charniak parser in a preced segment .
so johnson's observ wa that if you did noth more but chang all of the non termin to also record in them what the parent non termin is , so you then effect have a number of non termin squar space of non termin , that that chang alon , if you just train a pcfg in the regular manner make a big differ , it push up your pars number by a coupl of percent .
so that's a us idea .
but there ar other thing that we might want to do that have a slightli differ charact .
so for the exampl from the previou slide , if we want to rule out the big board bad pars , it seem like what we also want to do is know about possess noun phrase .
so possess noun phrase ar these on like fidel's with an apostroph s at the end and thei have a veri differ natur than regular noun phrase but it wasn't be record in the categori symbol .
thei ar just be call noun phrase .
so why don't we do state split and sai , oh no , we're gonna split out noun phrase that ar possess and call them an np po and then that inform will also be captur in the grammar and again we'll have a more accur pcfg .
of cours there's a danger of do thi too much .
and that is that if we have too much state split , we start get more and more non termin , and our inform about the non termin becom sparser and sparser .
and in particular , in the model we're gonna show in the next segment , there is actual no sm , smooth of the rule rewrit probabl .
and so , if you did too much state split , you'd end up have zero probabl for thing that you haven't seen befor .
but unlik with word probabl where you basic have to smooth them you can do a reason amount of state split of categori in thi sort of a wai befor you have to deal with smooth and so what we're go to look at is well , which wai of split categori ar most us for captur probabilist depend that's need to do a good job in pcfg pars .
okai , i hope that give you a more concret sens about pcfg independ assumpt and how thei can hurt you , but also how to solv them .
now that we have a much better understand of the role of independ assumpt in pcfg's , in thi segment i'm gonna show you how you can make a much , much better pcfg , without do lexic .
and so thi wa work that wa done in the accur unlexic pars paper by me and dan klein in <num> .
so first , what do i mean by an unlexic pcfg ?
what that's gonna mean is that in gener the grammar rule ar not systemat specifi down to the level of lexic item .
so we're not gonna be abl to have lexic categori like in a lexic pcfg , like np stock or vp rose .
on the other hand , we ar go to be allow to do thing like parent annot of categori , or to note other thing about the make up of categori like sai thi is a noun phrase that is a co ordin noun phrase .
to drill down on that just a teeni bit more .
in particular , we wish to make a distinct between close versu open class word .
so there is a long tradit in linguist and syntax to make us of function word as featur or marker for categori of the select .
so in english , the verb have , be , and do act as verbal auxiliari in thing like i am run , he ha eaten , and so it's okai to recogn that those function word have special role and to treat them special .
or if you have a sententi complement , it's okai to know whether it's a that complement versu an if whether complement .
those kind of condit featur ar allow in state split , becaus thei're fundament differ to thi idea of mark the semant head of phrase for work out thing like preposit phrase attach , where realli you're us the lexic head as a kind of proxi for semant .
and so what our thesi wa , is that a larg percent of what you need for accur pars , and inde , much of what wa actual be captur by lexic pcfg wasn't actual anyth to do with probabilist depend be captur by content word .
but it wa actual realli just these basic grammat featur like verb form , finit , presenc of a verbal auxiliari , that were well known from peopl work out tradit grammar .
in particular , peopl who'd work on featur base grammar for languag .
so the wai we went about investig thi , is that with the penn tree bank wall street journal , we chose a small develop set that we could do a whole bunch of experi on .
in the dai when we did it comput were much slower , so do <num> file of the wall street journal seem like it'd take a long time .
so we just us twenti file of the wall street journal develop section that we'd run on again and again .
and we'd make manual state split in the grammar and try and improv perform by break down wrong independ assumpt .
and so we have a coupl of statist .
on is our perform level , which will be our usual f1 of label precis recal .
and then the other on is the size of our grammar .
and as we make more state split , the grammar will be bigger in term of it number of non termin .
and if thi number get too big , that's both dan , danger for two reason .
it'll both slow down the parser , and we'll start to get problem with spars , becaus we're not smooth the rewrit of our pcfg , except down at the lexic level where we're rewrit as word .
and so our goal will be to state split as sparingli as possibl .
so we just want to make a limit number of state split that give the most bang for the buck in term of captur necessari probabilist depend .
so let's just look at a few exampl of how we did that .
our motiv wa in part have look at what had been done in lexic pcfg .
and it turn out that some of the thing that been , been done in the lexic pcfg model the promin on of those dai were eugen charniak's and mike collin's , didn't actual have anyth to do with content word lexic at all .
thei'd been do other thing .
so on idea wa to split up context free grammar rule which have long right hand side .
so , you onli do limit condit on the other thing on the right hand side .
so , if you have a flat wall , of which there ar lot in the penn tree bank , we've alreadi discuss need to binar those .
and there's sort of a straightforward wai of binar where at each point , you preserv your left context , and then you have the probabl of expand further .
so , now we're again preserv our left context and sai what ar the probabl of expand further .
but , as you go on , you're condit on more and more stuff that might not make much of a differ , becaus you're also go to have place in the penn tree bank where a noun phrase expand as five proper noun in a row .
if you have someth like , greater duluth invest advanc committe or someth like that , you're go to get five noun , proper noun in a row .
and it seem like at some point you just wanna know that you're expand a bunch of proper noun .
and how mani there were beforehand doesn't matter .
and so what you can do is get rid of some of the histori in the same wai as we do with languag model where we markov them and sai , let's not us the entir preced context , let's just us a bit of preced context .
but our preced context now is in term of these categori of what we'd seen previous on the right hand side .
so if we onli keep what we're expand and the thing that we saw most recent on the right hand side , we get rid of some of the prior condit context and then these two state becom the same .
our grammar will actual get smaller again .
so from thi perspect , if you do naiv binar of these veri flat rule , you actual get a big grammar and you pars at about thi <num> percent number that we've talk about for a plain vanilla pcfg .
it turn out that if you condit on less context , like onli the two preced tag , or onli the on preced thing on the right hand side , your grammar get enorm smaller .
and actual it perform goe up .
so , in particular , if you condit on just the two previou categori in your expans of the right hand side , your perform goe up a littl bit .
but we have found that we can do a littl bit better than that .
we could condit on sometim on piec of preced context , and sometim two bit of preced context , depend on how often you'd seen expans for that categori , as to how common thi non termin is .
okai , so that's horizont markov .
the idea of us parent categori , you can think of as vertic markov .
so befor , we were sai , okai , we're expand just a verb phrase and then we could sai well , let's also look at the thing abov that in the tree .
and so we're then mark that in our categori .
so thi look at the thing abov you in the tree is kind of look at your histori go upward .
and you can think of have your whole histori go upward and then progress delet out some of it .
and that's a form of vertic markov .
so from thi perspect , a standard pcfg , you're just expand your own categori and you're look at noth abov you .
so you're at vertic markov order on which is again that slightli under <num> percent perform level .
but as johnson notic that by simpli do noth els but also know your parent categori here , that give you a lot of valu .
so that's push up your pars number by that four percent here .
it turn out that if you put in grandpar as well , you can push the number up even a littl bit further .
but the problem is that , as you do that , you start have a bigger and bigger grammar with more non termin .
so the model we us as a basi for do more linguist state split , wa actual , again , to take thi in between parent and noth model , where you were us the parent categori most of the time , but not alwai , for certain veri rare non termin .
so thei're some of the on that we saw mention of occasion .
thei're thing like where you have special non termin for fragment and for reduc rel claus , and thing like that , that occur rare .
okai .
so , onc we took the vertic and horizont markov level 2v between on and two , we then get to a pcfg where our accuraci is <num> percent f1 .
and the number of non termin we have in the grammar is now seven and a half thousand .
and that'll be the basi for introduc some further , more linguist state split .
okai well what other problem do the independ assumpt of pcfg caus ?
an easi wai to find them is to just run your pcfg see where it make pars error on your develop set and then scratch your head and think why is it choos the wrong rule here .
what inform could i encod into the non termin that would caus it to stop do that .
and so here's an exampl of thi , so you find in a basic pcfg that unari rule ar us too often , becaus thei make it easi to chang on categori into anoth , so if you have high categori rule here like a verb phrase go to a verb end in ing and a noun phrase it's gonna want to us it .
and so it find that it can just chang a sentenc into a verb phrase with fairli high probabl that's a high probabl unari rule and so it will us it .
but there's , and do that , but there's a problem here which is that thi unari rewrit isn't appropri for thi higher level rule .
that thi higher level rule is expect here to have a finit sentenc with a s , a subject , and so therefor it shouldn't then expand as a unari rule .
so the wai we can captur that is by sai , well let's mark the unari rewrit in the train data .
whenev there is a unari rule appli , we'll just stick unari on the parent categori , so we know the kind of place that unari rewrit occur .
and then at that point it'll decid on , at pars time , thi isn't a place that unari rewrit occur , and so it'll choos a differ structur for the sentenc .
it will decid to choos the structur where thi is actual a modifi phrase modifi the noun phrase , and that's the right answer .
and so just make thi littl chang here will immedi alreadi move our pars number up by half a percent .
so let's keep go .
anoth problem of the independ assumpt is that in variou place the part of speech tag ar too coars .
on of the worst exampl of that is with the tag in in the penn tree bank tag set .
so that's us for all three of the sententi complement of sentenc complement , word like that , whether , and if ; for subordin conjunct , like while and after ; and for true preposit , word like in , of , and to .
well , and that caus wrong pars .
so here's an exampl of how it caus a wrong pars .
so here we have thi sententi complement , if advertis work .
where we should have thi be a sentenc advertis work .
but instead what it's chosen to do is just give a regular preposit phrase analysi , despit the fact that the preposit here is if , which is the , not realli a true preposit , it's on of the sententi complement .
okai , well the wai to deal with thi is to split thi in tag here and have differ kind of word here .
so if we sai that if you have a noun phrase under your preposit , then what we want is the kind of in that appear with a noun phrase as it complement .
well then , what's go to happen is we're go to learn that if is not an exampl of that .
if is instead an exampl of the kind of in that appear with a sententi complement , becaus it appear in thi sbar construct that we have here .
and so that mean the parser is then go to choos a differ and correct construct here .
it's go to sai that thi part here is an sbar with an s in it .
so split the in tag in thi wai is a big chang .
and by itself , it give you an extra two percent in pars accuraci .
we can keep on and do more of the same .
so let's just look at a coupl of other exampl of that .
so , sometim we also want to refin phrasal categori , so if you have a verb phrase it make a fair bit of differ know what kind of a verb phrase it is .
is it a finit verb , or is it a non finit or infinit verb ?
and on wai that you can captur that is by know what the part of speech tag of the verb is becaus thi here is a finit tag for the verb , and thi here is an infinit non finit tag for the verb .
and so we can repres that inform also on the verb phrase categori .
and if we do that we will find that thi structur is bad becaus , although the verb is can take vp complement , it won't take infinit , bare infinit vp complement like panic .
instead , it's go to take thing like , participi for the progress like , she is run , or thing like that .
and so when we annot that extra inform , the parser will again know that thi isn't a good structur , and it will choos a differ structur .
and so here it's choos to make panic bui a noun phrase correctli .
okai , so that's the idea of split verb phrase categori .
and we mention earlier why we want to split off possess noun phrase categori .
both of those ar worth quit a bit in pars accuraci .
so we're get almost a percent from the possess .
and we're get over two percent by know about what kind of veri phrase we're deal with .
there's on other categori of split that we introduc .
and that is that somehow we want to know someth about attach site for preposit phrase .
even though we're not do lexic condit .
we want to have some kind of an idea as to do preposit phrase tend to attach low or tend to attach high .
and we put in some featur that can , can captur that .
so we mark whether ani phrase contain a verb .
that tend to captur whether it's alreadi someth big and sentenc size .
and also mark , for noun phrase , whether thei've alreadi had thing attach to them or whether thei're still a base noun phrase that hasn't had anyth attach to them .
and these idea also push up the perform number a littl .
inde , on the develop set , we've now gotten up to <num> f1 .
so at the end of the dai , what we've construct here is that we transform the train data with manual rule done as state split to produc thi richer set of non termin here .
so thi is a verb phrase that's under an s categori .
it ha a finit verb in it , and it's a phrase that contain a verb .
and so these ar our categori .
we then build a straightforward pcfg with those categori and pars .
how well do we do ?
well the answer is that thi unlexic pcfg can suddenli actual do rather well .
so these ar the result .
so these were two of the earliest lexic pcfg model and thei got almost <num> and <num> .
and here we have thi hand built unlexic pcfg , and it's actual do a littl bit better than those .
it isn't do quit as well as the model of charniak that i show earlier , or inde , collin's slightli later model .
these on were get , you know , a littl bit more <num> , <num> .
at ani rate .
but that show you that if we're consid .
you know , we start off at about <num> for our plain pcfg .
and then if we look at what gave mileag to get up to here , it then look like , well , you know about thirteen to fourteen percent of the mileag is come from these idea like know more about context , parent annot , refin critic categori .
know that you have possess noun phrase .
know about the type of verb phrase , participi infinit , finit verb phrase .
and the amount that these better model were captur from have true lexic wa realli rather small .
so it's somewher between on and three percent .
and so that chang the orient somewhat as to better understand what is and isn't import in be abl to choos the right pars .
okai .
so that show that although the idea of lexic is clearli import for captur certain kind of pars decis such as preposit phrase attach , you can do a lot with just a littl bit of linguist model , rather than actual go the whole wai of have thi massiv space of lexic pcfg .
and the kind of model that we're do there is actual a kind that wa veri familiar from work on featur base and unif base model that have been explor in linguist in the 1980s and 1990s .
follow on from the work in accur unlexic pars , there wa then an extens which led to latent variabl pcfg , which i'm not gonna go through in detail .
but i just want to briefli introduc the idea of and show you how it work .
so in the model that we were look at previous for accur unlexic pcfg , everyth wa hand done .
someon wa stare at sentenc and how thei pars , and decid how better to split the categori to build a parser that would work better .
so if you're interest in machin learn , you should be think , gee , mayb we could do that automat rather than do it all manual .
and so that's the idea of latent variabl pcfg .
so the start point is , we still have a tree bank to train on .
so the bracket of the sentenc in the train data ar known , and the base categori ar known .
so that we have a noun phrase node that's in some context and we alreadi know that .
but what we're sai is , well mayb there's some wai that we'd like to annot it with more inform to sai what kind of a noun phrase it is so we can pars more accur .
and so concret , the wai that's be done is by split the noun phrase categori into a certain number of subcategori where each on is just be given a number .
so thi is an np17 .
and so then the learn task is to choos both a number of sub categori and then how to group all of the noun phrase into the tra , in the train data into particular sub categori , so that you can have someth in common in all the np17's .
so that you can then have good predict as to how thei expand and which word thei contain .
and i'm not go to go through that algorithm in detail .
if you've seen thing in other class like machin learn or probabilist graphic model , it's a form of the em algorithm like the forward backward algorithm us for hmm , constrain by the preexist tree structur .
so what you're do is you have the preexist tree structur in black , and you're want to have a probabl distribut over differ latent sub state of each categori .
and the simplest wai of do thi would just be to sai okai , each categori ha ten subcategori and then learn a probabl distribut over the choic of differ subcategori for the s state .
that had been tri previous and been found to not work veri well .
so far , slav petrov introduc wa a clever split merg algorithm where for each categori you start with just the singl categori of an s .
and you said , okai let's try and split that into two s categori .
is there a good wai to split it , to captur more condit inform ?
ye .
let's keep that split .
okai .
well , try again .
let's take each of those and split them into two , and so we have four s subcategori .
and then at , mayb at that point you discov that on of those split wa us and the other on wasn't .
so , you just get rid of the other on again and go back to three s subcategori , and you'd repeat that over a number of time .
and so , you'd progress split and merg the subcategori and come out with a good number of set of differ categori .
yeah .
again , i'm not gonna go through the algorithm in detail .
let's just look at how it turn out .
so , thi show some of the subcategori that ar learn for part of speech tag , caus thei're the easiest on to interpret .
so thi is for proper noun .
and so what we find is that the proper noun have been divid into group .
and the wai thei're divid isn't just pure syntact like a featur base grammar anymor .
thei're kind of syntactico semant class base model .
so we have a subcategori of proper noun , which is abbrevi for month .
we have anoth on that's first name .
anoth on that's initi .
anoth on that's last name of peopl .
and then we have two correspond state here , for what ar locat name , which ar multi word , and again we have a first word here and a last word here .
so we have these kind of interest semant sub class of noun .
and someth similar to that is happen also with the person pronoun .
so we're have the on that ar the nomin subject pronoun and we're have the on that ar the accus object pronoun , and then we're also then distinguish between the subject on as to whether thei're capit or not , which is mayb captur someth that we're there at the begin of the sentenc .
and a word isn't restrict to onli appear in on categori .
it can be in the rewrit of multipl categori .
so it appear in both these place , becaus it's us as both a nomin and accus pattern .
look overal at the pattern of state split for phrase thi actual work interestingli and linguist veri effect .
so for the common categori like noun phrase , verb phrase , preposit phrase , but equal the on where the basic categori of the penn tree bank were too crude .
that's what we found out previous with our hand state split .
we found that we want to distinguish possess noun phrase , and we want to distinguish verb phrase depend on whether thei're infinit two verb phrase , whether thei're ing verb phrase , or whether thei're finit verb phrase .
well , in those place the split merg algorithm learn a lot of subcategori .
it's learn <num> , <num> or so subcategori .
on the other hand , for the rare weird categori , the unlik constitu phrase , the right recurs claus , the fragment , it's learn veri few subcategori .
in fact , for these on over here , it's make no split at all and it's just have the singl tree bank categori .
okai so we're then build a grammar that is make split which aren't pure syntact , it's also got semant flavor split , but it's still much coarser than actual do head word lexic .
so in the model that i'm show you here , the maximum possibl number of split that could happen to a categori wa <num> .
and in practic , onli a , at most a reason number of them surviv , so in practic you're alwai get less than <num> as you can see in thi exampl .
so with just that level of state split how well can you make a parser work ?
and the answer turn out to be , you could make a parser that work amazingli well .
so , thi slide show you current pars result for the parser that ar around when i'm sai thi around <num> .
so we start off again with our basic pcfg at around <num> percent f1 .
and we're work up from there .
and i'm show two column of result .
a lot of earli parser result were shown on sentenc up to at most <num> word .
wherea more recent it's been more common to show result on all sentenc of ani length .
okai , so our baselin now is thi unlexic parser of klein and man .
and so , at sentenc of all length , it's get a littl bit under <num> percent here .
matsuzaki et al tri the simpl idea of split each categori with latent state but just assum the same number of latent state for each categori .
and that didn't realli work ani better .
in particular , both of these parser ar still notic below what you could get by do lexic pcfg with head word .
so the best of the charniak collin famili of lexic pcfg wa thi more recent model of eugen charniak's from <num> .
and it wa get <num> and a half percent .
and so it seem like mayb thi three and a half percent gap wa the valu that you were get from lexic , beyond what you got from thi basic state split .
but what the latent variabl pcfg show is , well , actual , you could get all that valu without actual model particular head word , but just us these fairli coars semant class of word .
so the latent variabl pcfg is actual a littl bit better , and it's get thi extra half a percent here .
you kind of can't explain in absolut version , why it's better becaus realli it's us less condit inform than charniak's parser .
so the reason it's better is becaus the probabl estim is better .
and it's better becaus it's actual a lot easier to do here becaus you have a much smaller number of categori in the grammar and you're not have to do complic smooth with head word .
now peopl have gone on and done further work with lexic pcfg and so in particular eugen charniak and mark johnson work togeth to add a discrimin rerank on top of the charniak parser .
so thi discrimin rerank is essenti us a maxent model of the kind that we saw in week <num> , but appli it now to choos between differ pars for a sentenc where the candid pars ar be gener by charniak's gener parser .
so thi is feed into that .
and well , the maxent model wa , again , abl to do thi kind of better forward probabilist condit , which definit help .
so that wa then ad about two percent in perform to the charniak parser and in particular ad a littl bit still abov the level of the petrov and klein parser but still notic know that individu particular head word is still now onli give you about <num> percent in perform .
and final , as alwai , in machin learn context , if you have a bunch of system , a bunch of classifi for do thing , you can alwai get even better result by combin them all togeth .
and peopl have shown that .
so fossum and knight combin variou constitu parser and their perform into an aggreg model , and that wa abl to get an extra percent of perform .
so thi is the state of the art at the moment for perform in probabilist pars , which is actual an extrem high level of accuraci .
probabilist parser actual can give you the right pars for most part of most sentenc quit reliabl .
okai , so that show the idea of have these latent state , when you allow them to be defin to maxim pars perform .
you get these syntactico semant state , which realli allow you to pars rather well , still with a quit compact grammar .
in thi segment i'm go to return to depend pars .
right in the first segment , i introduc the idea of depend syntax , but let's look again at how that work .
so the idea of depend syntax is you connect up the word of a sentenc by put arrow between them that show relationship of be modifi or argument of other word .
so here , in thi exampl , we've got the head of the whole sentenc submit and it's got it depend , so it's got bill submit by somebodi and then also the auxiliari verb were .
now , not necessarili , but quit commonli the arrow ar type by the name of some grammat relat .
and so we can see that here , where we've got the subject of a passiv , the verbal auxiliari , and the preposit relationship .
the other thing that you should know about is just a bit of the terminolog .
so firstli when we have an arrow we alwai have the thing that is the head , or the governor , here submit , and the thing that is the depend , modifi , inferior ; variou word ar us as you can see over here , here bill .
so that thei're the two end , the governor and the govern thing of a depend .
now , beyond that there's actual some inconsist about how thing ar done .
so in these slide and in the origin depend grammar work of tesni re the arrow run from the governor to the depend .
but you can absolut also find other work that point the arrow the other wai .
and actual if like here you're us height in the tree to show what's depend of what , you actual don't need to have ani arrow at all .
you could just draw these on line without ani arrowhead on them .
okai , so , as in thi exampl , normal what you find is that depend form a tree .
so that there's a root node , and then , from there , everyth head down with word have a singl head , and in a nice acycl manner .
i should mention also that it's actual quit common to add sort of on pseudo node at the top , often call root or wall which point at the head of the sentenc .
that actual make thing a lot cleaner , both in term of the pars algorithm but also in term of thing like evalu and represent becaus then you get the properti that everi word of the sentenc includ the root is the depend of on thing .
and so you can think of it as do an assign process of work out what is the governor of each word of the sentenc .
how doe depend grammar relat to the kind of phrase structur grammar that we've concentr on so far ?
well , the central innov is realli that depend grammar is built around the notion of have head and depend , wherea the basic case of a context free grammar , there's no notion of a head whatsoev .
and actual thing have move on from there .
if you look at modern ex , modern linguist theori , that mean thing like x bar grammar to linguist , or our modern statist parser whether the charniak collin or stanford parser .
all of them have a notion of head and us it extens .
so for exampl , in all these parser , there's a notion of head rule where it'll identifi some categori as the head of a larger categori .
and as soon as you have head rule of the kind that we discuss befor , well then you can straightforwardli get the depend out of a phrase structur represent .
so , basic you kind of have a spine of head chain , and then everywher you have someth come off that , that's a depend .
so we have a depend from walk to sue , and a depend from walk into , thi is anoth head chain , and then we've got anoth depend from into store , head chain , and depend , store the .
so , we have the basi for a depend represent insid a phrase structur tree if and onli if , we have head repres .
what about if we go in the opposit direct ?
if you try and go from depend to phrase structur , you can reconstruct a phrase structur tree by take the closur of the depend of a word , and sai that those repres a constitu .
but it slightli chang the represent from what we normal see in phrase structur tree .
in particular , in a situat like thi , you can't have a vp node .
becaus actual , both sue and into ar depend of walk , and therefor all three of those must have a flat phrase structur represent , where you have the three , the head and it two depend sue and into .
how do peopl go about do depend pars ?
a whole varieti of method have been us for depend pars .
on method to do it is with a dynam program algorithm like the cky algorithm that we saw for phrase structur pars .
now if you do thi naiv by ad in head , you end up with someth similar to the lexic probabilist context free grammar we saw earlier , and end up with a big o n to the fifth algorithm .
but there's a clever reformul of what the pars item ar due to jason eisner in <num> which make the complex of do depend pars also n cube .
which is kind of what you'd hope it to be , just think about the natur of the oper .
but there ar a whole bunch of other method .
so peopl have directli us graph algorithm to do depend pars .
so , on idea from the algorithm's literatur is that you can construct a maximum span tree for a sentenc .
becaus , sinc you want all word connect togeth , to be the depend of someth , that mean you have to build a tree that span all the word in the sentenc .
and that's the idea that's us in the well known mst parser .
there ar other idea of constraint satisfact where you start off with a dens set of edg between all word and then elimin on that don't satisfi hard constraint .
but a final train independ pars and actual what we're go to focu on here , is a wai of do depend pars where you head left to right through the sentenc and make greedi decis base on machin learn classifi as to which word to connect to other word as depend .
and so the most well known exampl of thi framework is maltpars , and i'm go to concentr on thi just partli becaus it's veri differ from what we did for our approach to phrase structur pars we look at in depth , but also becaus it's been shown that thi kind of method of do depend pars actual work extrem well .
it can work accur , and exceedingli quickli .
so it's just a good thing to know about as a differ point in the space .
no matter how we do depend pars we need some sourc of inform to let us choos between possibl analys and which word to take as depend of , of other word .
so here's a list of the main sourc of inform peopl us .
so , the most obviou sourc of inform is bilex depend .
so if we have someth like a depend between issu and the .
well we can look at the word that's the head and look at the word of the , that's the depend and sai is that like .
that's similar to the bilex depend of our earlier lexic pcfg .
but we don't want to us that as our onli sourc of inform partli becaus lexic inform is so spars .
and there ar sever other good sourc of inform .
so let's just go through those .
so on is the distanc between the head and the depend , and if you look at thi pictur , what you'll see is that most depend ar short .
thei're with nearbi word .
there ar a coupl of except .
so , thi depend here is a pretti long on .
but most of them ar pretti short .
other sourc of inform ar , what is the interven materi ?
so , in gener , depend don't cross over verb , and commonli thei don't cross over punctuat .
some except , comma ar quit often cross over .
so , look at the word in between can give inform about whether a depend is like or not .
a final sourc of inform is look at the valenc of head .
and that's sai , for a particular word , what kind of depend doe it typic take ?
so a word like the typic take no depend on the left and no depend on the right , as in thi case here .
on the other hand , if you have a word that is , sai , a noun , it will take depend like adject and articl on the left .
but it won't take those kind of depend on the right , though can take other kind of word as depend on the right .
for exampl , take preposit phrase modifi or rel claus as depend on the right .
so you can develop a quit rich typolog of what kind of depend word take .
okai that should give you a better sens of what depend represent look like and it's the big pictur of how we go about pars with them .
in the next segment we'll introduc a concret algorithm for depend pars .
in thi segment i'm go to look at the greedi transit base pars approach to depend pars and in particular i'm go to describ the model that's us by maltpars , the best known exampl of thi framework .
so , the idea of maltpars is that , mayb we could do pars by just make simpl greedi decis , as to how to attach each word as it come along , and in particular to make those decis we'll us a discrimin machin learn classifi .
so the parser doe a sequenc of action work bottom up , kind of , like a shift reduc parser , if you've seen that , in either cfg pars or in the program languag literatur .
the parser ha a stack which will be written with the top to the right and which will start off with a root symbol on it , as i introduc last time and then a buffer , which is the stuff we've yet to look at with the top written to the left and so that will have the input sentenc on it .
and as we go along , we'll build up a set of depend arc which start off empti .
and we'll do thi by do a set of action .
thi is the natur wai to think of a transit base depend parser .
so thi is our start configur with our stack , the buffer and the empti set of depend arc .
and so essenti our move ar that we can shift a word across from the buffer on to the stack which is like the shift oper for cfg pars .
or we can do a reduc oper .
now , in depend pars framework we end up with two reduc oper , depend on we take , whether we're take the word on the left or the word on the right as the head .
so here we have word i and word j , and if we take the word on the right as the head , we get a left arc .
that mean we kind of creat an arc that point like thi which is be ad to our set of arc here or if we do a right arc oper we ar have a depend that goe thi wai .
so the word on the buffer is be a depend of the word on the stack .
now if it's untyp depend pars , there ar just these two oper .
if you want to do type depend pars , you also when you do thi reduct have to sai what label you're us to connect the two word .
and so that mean , if you have twenti label , you actual end up with <num><i><num> <num> , <num> differ action in your depend< i> parser .
and you finish when you've exhaust the buffer .
now someth to note here is that if you have seen shift reduc pars , there's someth that's slightli unusu with thi present here .
so normal , in shift reduc pars for cfg , you first of all put thing on the stack , and the reduct is done fulli on the stack .
but for the model that's be show here , when you ar do the reduct in either direct , you're actual redu , do a reduct with on thing that's on the stack , and on thing that's on the buffer .
that's just a convent that's been adopt and is standard , what you sai the depend pars in literatur .
the claim is that it make it littl bit cleaner to formul these thing that wai .
though i'm not sure that it make a big differ , but i've gone with it caus it's just standardli what you see .
thi is a simpl wai to do thing , and you can do that .
but it's not the standard thing that peopl actual do .
and so i'm gonna move on next to the most common method that you see for transit base depend parser .
and let me just explain why thi simpl wai is a littl bit problemat .
so that if you had a sentenc of the sort , um , 'sue tri to open the door in the cellar' .
well what you're go to have , is that you have a depend from tri to open and as soon as you've gotten up to here in the input at least by then it's obviou that you're go to have a depend between tri to open .
but if you're us the kind of three action model of depend pars i've shown here , you can't construct thi arc immedi .
instead , becaus of the fact that the door is a depend of open and in the cellar is a depend of door , that you have to construct all of thi materi , befor you construct thi arc here .
so the closur of the depend of a depend have to be construct befor you then hook it up to a higher head .
so all of these depend have to be construct first .
well what that mean is that you have to have shift on all of the word in the input befor you can decid anyth about thi depend .
and it's been found that have to do that amount of look ahead actual make it harder for machin learn base greedi classifi to work well .
so instead what peopl have want to do is move to be abl to , in a greedi fashion , hook up local thing on the right even befor you know their depend .
and so that's what's been done in a differ wai of formul the action that's refer to as arc eager depend pars .
for the arc eager depend parser , we have exactli the same start and finish configur .
and the oper ar sort of similar .
there's a shift oper , and there's these left arc and right arc step , though we add in on new oper , a reduc oper .
but there ar a number of differ in the subtleti of what work .
so let's just sort of look at them for a moment .
so , when you have the left arc oper it's do the same thing of construct a left depend between the head that's in the buffer and the top word on the stack .
and the result is the same , that we're construct thi new depend arc .
but we have to add on to it some precondit .
so , we need to have on it a precondit that wi isn't alreadi a depend of some other word .
'caus if we allow that arc to be put in , then we'd get an analysi where two arrow would be point to the same word .
and precis becaus of the arc eager charact , word can still , can alreadi be on the stack even though thei've been made the depend of some other word .
the right arc oper is a bit more differ .
so for the right arc oper our start point is exactli the same that we've now go to want to make an arrow like thi where we have the head on the stack and the depend at the begin of the buffer .
but , and we add thi depend .
but the differ is we then , rather than get rid of wj , that we push it on to the stack .
so that mean that we're keep the word on the right so it can take it own depend later on .
and so that's precis how we get a word on the stack which ha alreadi been declar the depend of some other word .
okai , then it's , the fact that we do that necessit thi extra reduc oper , caus onc we've then found all the depend of that word , we have to eventu get rid of it , so we can go back to find the depend of other word that ar higher up the syntax tree , and that's what thi reduc oper doe .
and , the reduc oper also ha a precondit , and it precondit is , you can onli get rid of a word from the stack if it ha been made the depend of some other word .
so , in other word , if it wa introduc in thi wai as the depend of some other word then at some later point you can reduc it onc it's close .
that's all a bit confus , let's go through a concret exampl and i think it'll make a lot more sens .
okai , so the sentenc i'm go to work with is , happi children like to plai with their friend .
and here's the cheat sheet of the oper that we're go to be abl to perform .
so start off , we start off with here's the stack with just thi root symbol .
the buffer ha our sentenc and we've found no depend .
so , in thi situat , we have these four oper we could appli .
so if we thought that happi wa the head of the whole sentenc we could immedi do a right arc oper .
but that doesn't seem right here .
so what we do is we shift it on to the stack .
and then we're in thi situat .
okai .
well in thi situat we do have that children is go to have as it depend , happi .
so the next thing we want to do is construct a depend of that sort .
and so that mean we're do a left arc oper and , in particular , we're introduc it as an adjectiv modifi depend .
so when we do a left arc oper , we add to the set of depend that we've found , and then we get rid of the depend off of our stack .
so it disappear right here .
okai , at thi point children isn't the head of the sentenc either .
so we do anoth shift oper .
well at thi point we're readi to do anoth left arc oper becaus children is the subject of like , and so we introduc thi noun subject depend and add that to our set of depend .
so a2 is now a set of two depend that we've alreadi built .
okai .
well , at thi point we've actual found the head of the whole sentenc .
so like is the head of the whole sentenc .
so that mean we can connect it to the root by do a right arc oper .
so we now add the root depend for the whole sentenc .
and rememb then for the right arc oper , we haven't yet found the depend of like on the right , and so therefor we're ad it into the stack as someth that still ha to find it own depend on the right .
and inde , at thi point , we can immedi , we're go to be abl to start to do that .
becaus plai is go to be a depend of like .
but first of all , you have to get past to .
so , for to , we shift to onto the stack .
and then to , the infinit marker , is go to be a depend of the verb , plai .
so that's go to be anoth left attach as an auxiliari modifi .
okai .
now at thi point , we we can sai that like can take it first right depend .
like to plai .
and so that's done as a right attach .
and so it's get thi right attach xcomp which is again , ad in to our set of depend .
okai so at that point then were make progress but we still got more to do so thi is what we're up to so far , plai .
and well , plai is also go to take as it argument plai with so we can do anoth right attach of plai and with and so that mean again that with now move on to the stack , that we've attach , we've made thi depend of plai with .
but with hasn't found it argument on the right , so it's place on the stack , and then we're go to be abl to find it argument .
so with's argument is go to be to friend , so befor we can get to that , we have to shift on their , and then we can introduc their as a left arc of friend , and attach it .
and to rememb formal that to appli thi left arc rule each time you have to check the precondit .
and the precondit is that their hasn't been made the depend of ani other word .
and it hadn't been , so that precondit is satisfi .
okai , now we can do anoth right arc oper where we can hook togeth with and friend as the object of the preposit .
okai so we're make good progress .
so at thi point , we've now got thing that we've finish with , friend never had ani right , doesn't have ani right depend at all .
with , we found it onli right depend , that wa friend .
plai , we found it onli right depend with their friend .
and at thi point , we have to start us the reduc oper .
so we reduc off friend .
and again , rememb , that had a precondit .
and the precondit wa check that friend is hook in as a depend of someth , and inde , it is .
we can reduc again to get rid of with .
and we can reduc again to get rid of plai .
okai , at thi point we've now got just the period to deal with , which we sai is the depend of the main verb so we can introduc that also as a right attach oper .
and so now at thi point the buffer is empti , and the wai we defin our finish state is as soon as the buffer is empti we can stop .
you could if you want to sai well geez can't we just sort of reduc , reduc to pop these thing off back to the root and you know you could have defin thing like that and it wouldn't do ani harm , but actual it's unnecessari , becaus onc your buffer is empti you can't construct anymor depend becaus each oper that introduc a depend is take on thing from the stack and on thing from the buffer , and that there's no wai in thi formul for thing to reappear in the buffer onc thei've been move to the stack .
so , we know that we're done and we've found the complet set of depend .
which is you have to here , kind of work back up though my list .
but you make a set of all the depend we've introduc .
okai .
so that's the model of how the parser oper is do these oper step by step .
and at each step there , there ar multipl thing that could have been done .
it could have chosen to shift or it could have chosen to make a left arc or right arc .
and if it choos to make a left arc or a right arc , it ha to label the depend it introduc with on of mani depend label .
so , how do you do that ?
well , the wai that's done is by us some form of discrimin classifi .
support vector machin , svm have been most commonli us in practic .
but it could equal be anoth kind of discrimin classifi such as a maxent classifi .
and so , these classifi ar choos from a set of move , so if it's an untyp depend parser , there ar four move in that arc eager configur .
but if it's type , then there ar twice the number of depend type , plu two , class to choos from .
but it's a finit set of class you're choos from .
so what ar the featur that you us in the depend parser ?
well , you definit us , what's the word on top of the stack ?
what's it part of speech ?
what's the first word in the buffer ?
what's it part of speech ?
that will let you choos oper like shift , or the kind of depend label to choos .
but those aren't the onli oper in good depend parser .
caus rememb we had other thing that we knew could be import like the length of the depend arc that's be propos or know what's interven between them and also you might want to know about , for these word that , at the top of the stack well , what depend do thei alreadi have ?
that will influenc how like you ar to get other depend for them .
so you can put all of these thing as featur into a discrimin classifi , kind of like we saw when we look at maxent classifi befor .
so in the simplest form of transit base depend parser , there is henc absolut no search whatsoev .
that at each point you're make a greedi decis that you've got the stack and buffer in some state .
you run a classifi .
it decid the most like next action and you just take it .
and that's been an approach that's been strongli pursu in the maltpars framework to see how good a job you can do in , in thi manner by have realli good classifi that ar good at choos the next move .
i mean of cours , you don't have to do that .
you could do some kind of beam search to explor differ possibl .
but the rather stun result is that you can do thi form of complet greedi transit base pars and do realli well .
you can build depend parser that work almost as well as the best lexic probabilist context free grammar do the kind of complet cky pars style search over all possibl that we saw previous .
where we're take it that we're convert the lpcfg's into depend represent to evalu them .
i'll sai a littl bit more about evalu in a moment .
well , if it wa just thei were close to the state of the art that would be good .
but the dramat reason why peopl have realli like these parser , and thei've becom extrem wide us , is that becaus thei're these greedi transit base parser , that thei're super , super fast .
so thi style of parser can work wai faster than ani kind of dynam program parser that is pursu everi possibl altern analysi .
how do we evalu depend parser ?
the standard wai to do that is just via accuraci .
but sinc for each word we're go to choos someth as it governor or head , we can just sai how mani of those decis that didn't we get right .
let's look in the exampl of that .
so here's the simpl sentenc we're go to us .
she saw the video lectur .
so commonli we'll number each word of the sentenc includ give a number of zero to thi root that we add to the sentenc .
so in thi analysi , thi is show you the correct depend .
and we can lai them out as a chart where we have for each word , we have it own word number .
and then we sai which word index is the head of that word .
and in particular for the word that is the head of the whole sentenc saw we ar sai the word index of it governor is zero which is thi root symbol .
and then addition if we want to , we can also label the depend and that happen over here .
okai so then we built a parser such as a transit base depend parser and the parser tri to pars the sentenc and it get some result and if you look over here , it start off well .
it sai that saw is the root of thi sentenc and that she is the subject of saw , but it actual make a bit of a booboo after that .
so it sai that , it's that she saw someth lectur , so saw ha as it depend , lectur as a verb and the video is be analyz as the subject of the verb lectur .
well , if we do that and evalu accuraci how doe it work out ?
so for accuraci we're take the number of correct depend over the total number of depend .
and there ar two wai we can do that .
on is ignor the label for the grammat relat .
and so that standardli get refer to as unlabel attach score .
and so , if you look at that we're do pretti well .
so , thi on is correct .
thi on is correct .
thi on is correct .
thi on is correct .
and so the onli place where a differ word is chosen as the governor of a word is right here .
so here , what it should be is that the is the depend of lectur , but in thi wrong analysi we sai that the is a depend of video .
howev , if we look at the label accuraci score , thi wrong pars isn't do so well .
so it get thi on right , and then it get thi on right , but everyth els it get wrong .
it get them wrong for two reason .
so for the case of the , it choos the wrong word , to be , have as it governor .
but for video and lectur , although it choos the right word to be the governor , it choos the wrong function .
so it's becaus it's think that lectur is a verb that is a complement claus of see , that's wrong becaus it should be a direct object .
and then it's analyz video as the subject of lectur , and that's wrong as well , realli we should have thi compound noun structur here .
and so we're onli get two out of five right , or <num> .
here ar a coupl of number just to give you a sens of how well peopl do with depend parser .
so if you want to look more at depend pars evalu , on good sourc of inform wa that in the conll confer , the confer on natur languag learn that wa held in <num> , the share task that wa us wa to do depend pars over a collect of thirteen differ languag .
and mani depend parser took part , and you can see score .
so if you look at the result for maltpars in that competit , which wa evalu primarili by thi label attach score .
the score rang from , for the best languag , it got about <num> percent of the depend right .
and for the worst languag , it got about <num> percent right .
it wa a huge rang , so some languag i think ar intrins harder than other .
but i think there were also issu that the qualiti of some of the tree bank wa much better than other , and that's also reflect in the number .
but to try and connect up with what we saw earlier for constitu pars , let me also give a few number from english .
so these ar all depend number , but thi time we're look at unlabel attach score sinc we can alwai get unlabel attach from a constitu parser that ha a notion of head where it would be produc label .
so if we convert the output of the charniak and collin model of gener constitu pars , both lexic pcfg parser , that their accuraci on depend untyp is about <num> percent .
so , the yamada and matsumoto's parser is anoth transit base parser kind of like the maltpars and so it doe a bit wors than that , about <num> percent .
here's a differ style depend parser , thi is the minimum span tree graph base style depend parser .
it did <num> , almost , almost as well as these .
but part , you know , peopl have gone on do research on thi and partli becaus depend parser ar often much simpler , there's been quit a lot of work in look at combin of depend parser .
and so , for exampl , here's on depend parser from <num> , where it's then get result that ar a littl bit abov the two constitu parser .
now all of these result ar even a few year out of date .
there hasn't been a recent care comparison of constitu and depend parser but i think big pictur what you should take awai is that greedi transit base depend parser , by themselv ar perhap a littl bit wors in perform than dynam program pcfg parser , but if so it's onli a veri littl bit , and their other perform characterist make them extrem , extrem attract .
befor finish thi present of depend pars techniqu , there's just on more issu i should mention , which is the issu of project .
so if you take depend from a cfg tree us head , you have to get a project depend pars and what that mean is that your depend can be regard as someth in which everyth nest togeth just like constitu nest togeth without there be ani cross depend .
but most theori of depend grammar allow cross depend , i . e . , non project structur .
and inde , you can't get the semant of certain construct right without these non project depend .
so let's look at an exampl of that .
so here's thi sentenc , who did bill bui the coffe from yesterdai ?
so , most of the depend nest togeth give us a kind of tree structur , but if we then want to hook up who , then what we want to sai is well , who is the object of thi preposit here from .
and so if we put in it depend we then , thi depend ha to cross these two other depend and so thi is a non project depend structur .
and so the question then is , how do we handl thi kind of non project in pars method .
in particular , the transit base arc eager algorithm i present onli build project depend tree .
and so thi ha been an activ area of investig in the depend literatur .
i'm not gonna cover it in detail here .
but i will just briefli mention the rang of possibl analys that ar be pursu .
so on possibl is just to declar defeat on non project arc , for languag like english , there ar quit few non project arc that basic for the same reason that context free grammar work well for english becaus by and larg everyth is tree structur .
you can work on get almost everyth right with a non project parser .
a second possibl is to us a depend formal which onli ha project represent .
you'll see on of those in the next segment .
and that's kind of analog to what context free grammar actual repres .
becaus a context free grammar also doesn't repres ani connect between word that can't be done insid a tree structur .
and so effect it's someth like the tree bank represent we saw for phrase structur grammar .
it just fail to repres connect between word that aren't within a tree represent .
thei just hook the word in somewher higher up in the structur .
but there ar other method that peopl have pursu .
a , a third method peopl have us is to do project pars , and then run some kind of post processor that perhap pick up on class of label which indic we're hook thi on to our analysi , but realli it should be move somewher els .
and then to work out how to resolv the non project link .
well someth els you can do is you can actual add extra oper to the transit base depend parser model that can handl at least the common case in non project .
that is , thing like question word be move to the front of the sentenc in english .
well final , you can move to a pars method which just doesn't assum project .
so , in particular the minimum span tree base , graph base pars method don't make ani assumpt of project and you can just directli build non project structur .
so , in a wai thi is appeal , but in a wai it seem like it's possibl go to a too gener situat , becaus you're go from the situat of onli allow project structur to make no special requir of project structur .
and if you look at natur languag , what you find is that thei're mostli project .
and thei're onli particular construct like , thing like wh movement that move question word to the begin of a sentenc .
or variou kind of right displac like afterthought that creat non project .
so in the mst parser , all of that , when you should and shouldn't have non project is in the space of the machin learn classifi decid which depend ar like to construct .
okai .
i hope that's given you a concret sens of how transit base depend parser ar implement , and so , mayb you feel like you could go off and implement on of those .
in thi segment i'm go to show you that depend syntax is a veri natur represent for relat extract applic .
on domain in which a lot of work ha been done on relat extract is in the biomed text domain .
so here for exampl , we have the sentenc the result demonstr that kaic interact rhythmic with sasa , kaia , and kaib .
and what we d like to get out of that is a protein interact event .
so here is the interact that indic the relat , and these ar the protein involv .
and there ar a bunch of other protein involv as well .
well , the point we get out of here is that if we can have thi kind of depend syntax , then it's veri easi start from here to follow along the argument of the subject and the preposit with and to easili see the relat that we d like to get out .
and if we're just a littl bit clever , we can then also follow along the conjunct relat and see that kaic is also interact with these other two protein .
and that's someth that a lot of peopl have work on .
in particular , on represent that is be wide us for relat extract applic in biomedicin is the stanford depend represent .
so the basic form of thi represent is as a project depend tree .
and it wa design that wai so it could be easili gener by postprocess of phrase structur tree .
so if you have a notion of headed in the phrase structur tree , the stanford depend softwar provid a set of match pattern rule that will then type the depend relat and give you out a stanford depend tree .
but stanford depend can also be , and now increasingli ar gener directli by depend parser such as the maltpars that we look at recent .
okai , so thi is roughli what the represent look like .
so it's just as we saw befor , with the word connect by type depend arc .
but someth that ha been explor in the stanford depend framework is , start from that basic depend represent , let is make some chang to it to facilit relat extract applic .
and the idea here is to emphas the relationship between content word that ar us for relat extract applic .
let me give a coupl of exampl .
so , on exampl is that commonli you ll have a content word like base and where the compani here is base lo angel and it is separ by thi preposit in , a function word .
and you can think of these function word as realli function like case marker in a lot of other languag .
so it d seem more us if we directli connect base and la , and we introduc the relationship of prep<u>in .
< u> and so that is what we do , and we simplifi the structur .
but there ar some other place , too , in which we can do a better job at repres the semant with some modif of the graph structur .
and so a particular place of that is these coordin relationship .
so we veri directli got here that bell make product .
but we d also like to get out that bell distribut product , and on wai we could do that is by recogn thi and relationship and sai okai , well that mean that bell should also be the subject of distribut and what thei distribut is product . and similarli down here , we can recogn that thei ar comput product as well as electron product .
so we can make those chang to the graph , and get a reduc graph represent .
now , onc you do thi , there ar some thing that ar not as simpl .
in particular , if you look at thi structur , it is no longer a depend tree becaus we have multipl arc point at thi node , and multipl arc point at thi node .
but on the other hand , the relat that we d like to extract ar repres much more directli .
and let me just show you on graph that give an indic of thi .
so , thi wa a graph that wa origin put togeth by jari bj rne et al , who were the team that won the bionlp <num> share task in relat extract us , as the represent substrat , stanford depend .
and what thei want to illustr with thi graph is how much more effect depend structur were at link up the word that you want to extract in a relat , than simpli look for word in the linear context .
so , here what we have is that thi is the distanc which can be measur either by just count word to the left or right , or by count the number of depend arc that you have to follow .
and thi is the percent of time that it occur .
and so what you see is , if you just look at linear distanc , there ar lot of time that there ar argument and relat that you want to connect out that ar four , five , six , seven , eight word awai .
in fact , there is even a pretti larg residu here of well over ten percent where the linear distanc awai in word is greater than ten word .
if on the other hand though , you ar try to identifi , relat the argument of relat by look at the depend distanc , then what you d discov is that the vast major of the argument ar veri close by neighbor in term of depend distanc .
so , about <num> percent of them ar direct depend and anoth <num> percent of distanc too .
so take those togeth and that is greater than three quarter of the depend that you want to find .
and then thi number trail awai quickli .
so there ar virtual no argument of relat that aren t fairli close togeth in depend distanc and it is precis becaus of thi reason that you can get a lot of mileag in do relat extract by have a represent like depend syntax .
okai , i hope that is given you some idea of why know about syntax is us , when you want to do variou semant task in natur languag process .
hello , in thi segment i am go to introduc the task of inform retriev , includ in particular , what's now the domin form web search .
the task of inform retriev can be mayb defin as follow .
that our goal is find materi , which is usual document , of an unstructur natur , usual text , that satisfi an inform need , that's what the person is look for inform on , from within larg collect , usual store on comput .
so there's lot of mention there of prototyp case and other kind of inform retriev do exist .
so there ar thing like music inform retriev where there ar sound , not text document .
but effect , what we're go to talk about here is the case where all of those usual claus hold .
there ar mani scenario for inform retriev .
the on that peopl think of first these dai is almost invari web search , but there ar mani other .
so search your email , search the content of your laptop comput , find stuff in some compani's knowledg base , do legal inform retriev to find relev case for legal context or someth like that .
it's alwai been the case that a larg percentag of human knowledg and inform is store in the form of human languag document .
and yet there's also for a long time been someth of a paradox there .
so thi is just a kind of a not quit real graph to give a flavor of thing .
don't realli believ the number on the left hand side ar exactli what thei mean .
but what we're show in the mid <num>'s it wa alreadi the case that if you look at the volum of data that there wa some data that wa in structur form , by that i mean thing like relat databas and spreadsheet , but there wa alreadi vastli more data in compani , organ , and around peopl's home that wa in unstructur form , the form of human languag text .
howev , despit that , in the mid 1990s structur data manag retriev wa a develop field and there were alreadi larg databas compani , where in the field of unstructur data manag , there wa veri littl .
there were few teeni littl compani that did variou kind of corpor document retriev and thing like that .
that situat just complet chang around the turn of the millennium .
so , if we look todai the situat is like thi so the data volum have gotten larger on both side but in particular thei've gotten larger on the unstructur side with massiv outpour of blog , tweet , forum and all those other place that now store massiv amount of inform .
but there's also then been a turn around on the corpor side .
so now we have huge compani that ar address the problem of unstructur inform retriev , such as the major web search giant .
so let's start and just sai a littl bit about what ar the base , what is the basic framework of do inform retriev ?
so , we start off by assum that we've got a collect of document over which we ar go to do retriev .
and , and at the moment we're go to assum that it's just a static collect .
later on we'll deal with , when document get ad to and delet from that collect , and we can go out and find them in someth like a web search scenario .
then our goal is to retriev document that ar relev to the user's inform need , and help the user complet a task .
let me go through that onc more a littl bit more slowli with thi pictur .
so at the start what we have is the user ha some task that thei want to perform .
let's take a concret exampl .
suppos what i want to do is get rid of the mice that ar in my garag and i'm the kind of person that doesn't realli want to kill them with poison .
so that's my user task that i want to achiev .
and so to achiev that task , i feel what i need , more inform , and so thi is the inform need .
i want to know about get rid of mice without kill them .
thi inform need is the thing with respect to which we assess an inform retriev system .
but we can't just take someon's inform need and stick it straight into a comput .
so what we have to do to be abl to stick it into a comput is to translat the inform need into someth that goe into a search box .
and so that's what's happen here , we now have a queri .
and so here's my attempt at a queri , how trap mice aliv , so that's taken thi inform need and i've made an attempt to realiz it as a particular queri .
and so then it's that queri that goe into the search engin , which lead , which then interrog our document collect , and lead to some result come back .
and that mai be the end of the dai , but sometim if i'm not satisfi with how my inform retriev session is work , i might take the evid i got from there and go back and come up with a better queri .
so i might decid that aliv is a bad word and put in someth like without kill , and see if that work ani better .
okai .
what can go wrong here .
well there ar a coupl of stage of interpret here .
so first of all there wa my initi task and i made some decis as to what kind of inform need i had .
it could be that i got someth wrong there , so i could have misconcept .
so mayb , get rid of mice , the most import issu is not whether or not i kill them , but whether i do it human .
but we're not gonna deal with that issu so much .
what we're more interest in here is the translat between the inform need and the queri .
and there ar lot of wai in which that could go wrong in misformul of the queri .
so i might choos the wrong word to express the queri , i might make us of queri search oper like invert comma , which might have a good or a bad effect on how the queri actual work , and so those ar choic in my queri formul which aren't alter my inform need whatsoev .
that's import when we talk about how to evalu inform retriev system .
that's a topic we'll sai more about later , but let me just give you the first rough sens of thi , caus we'll need it for everyth that we do .
so whenev we make a queri to an inform retriev system , we're go to get some document back and we want to know whether the result ar good .
and the veri basic wai in which we ar gonna see whether thei ar good is as follow .
we ar go to think of two measur , which ar complementari , on of which is precis .
so precis is the fraction of the retriev document of the system that ar relev to the user's inform need .
so that's whether , when you look at the result it seem like on document in ten is relev to the inform need you have or whether seven document in ten ar .
that's assess whether the mix of stuff you're get back ha a lot of bad result in it .
the corr , complementari measur is the on of recal .
recal is measur how much of the good inform that's in the document collect is the system succeed in find for you ?
the fraction of relev doc in the collect that ar retriev .
we'll give more precis definit and measur for inform retriev later , but the on thing i want to express right now is that for these measur to make sens , thei have to be evalu with respect to the user's inform need .
for certain kind of queri , such as the on we'll look at in the follow segment , the document that get return ar determinist in term of what queri is submit .
so if we were go , just go to evalu what get return with respect to the user's queri , then we'd necessarili sai that the precis is <num> .
but that's not what we do .
what we do is think about the user's inform need and the precis of the result is assess rel to that .
so in particular , if we just look back for a moment , if there's been a misformul of the queri by the user or just thei didn't come up with a veri good queri that will be seen as lower the precis of the return result .
okai .
thi is onli the veri begin of our look at inform retriev , but i hope you've alreadi got a sens of how we think of the task of inform retriev and roughli how we conceiv whether a search engin is do a good or a bad job on it .
hello , in thi section , i'm go to introduc the import idea of a term document matrix .
but also , i'm go to explain why it isn't actual a practic data structur for an inform retriev system .
we'll take , as our exampl , do inform retriev over the work of william shakespear .
so let's suppos we have thi concret question .
which plai of shakespear contain the word brutu , and caesar , but not calpurnia ?
well , if you're start from a veri basic level of text search command , the first wai that you'd think about to solv thi problem is by us , search through the text of the document exhaust , what's known in the unix world as grep .
and so we could kind of , first of all , grep for plai that contain brutu and caesar .
and then , if you know your grep command well , you can give a flag for file that do not match .
and you can get out the on that don't contain calpurnia .
now these dai , the work of the size of william shakespear , for thi kind of queri , grep is a perfectli satisfactori solut .
our disk drive and comput suffici fast that you could us thi method and it take no time at all to find the answer .
but nevertheless , that isn't a good answer for the full inform retriev problem .
it fall flat in a number of wai .
onc your corpu becom larg , and so that mean someth like everyth on your hard disk or even more so , the world wide web , we can't afford to will linear scan through all our document everi time we have a queri .
then some part of it , like the not part , becom less trivial to implement than just find thing .
but even more so than the not part , we'll have more complex queri , like find us of the word roman near countrymen , and we can't do that with a grep command .
but even more than that , the big thing that's happen in inform retriev , is the idea of rank , find the best document to return for a queri , and that's someth that we just can't get out of the linear scan model of find thing that match .
and we'll talk about all of these issu and the wai thei're handl in modern inform retriev system in later lectur .
but let's first go to thi idea of a term document matrix .
so what we do in a term document matrix is that we have the row of the matrix ar our word or often thei're also call in inform retriev the term .
and then the column of the matrix ar our document .
and what we're do here is a veri simpl thing , we're simpli sai let's fill in everi cell in thi boolean matrix by whether the word appear in the plai or not .
so , antoni appear in antoni and cleopatra but calpurnia doe not appear in antoni and cleopatra .
so thi matrix repres the appear of word and document , and if we have thi matrix , it's straightforward to then answer boolean queri such as our exampl befor queri for document that contain brutu and caesar but not calpurnia .
let's just go through concret how we do that .
so what we're go to do is we're go to take the vector for the term in the queri and then we're go to put them togeth with boolean oper .
so first of all we can take out the row that is refer to brutu , which goe up here .
then we can take the row for caeser and and it there .
and then final , we can take the row for calpurnia , complement it , and then stick it down here , so calpurnia onli appear in juliu caesar , and so we complement it to a vector where everyth is on apart from juliu caesar .
and at that point , we can just and those three vector togeth , and our answer is .
thi on of <num> , and so we've been abl to do inform retriev successfulli , and can tell that these , thi queri is satisfi by the document , antoni and cleopatra and hamlet .
and inde we can then go off to the document collect and confirm that , that is the case .
so here we ar .
so in antoni and cleopatra , when antoni found juliu caesar dead , he cri almost to roar and he wept when at philippi , he found brutu slain .
and similarli we find both word occur in hamlet .
okai .
so that suggest that we could do inform retriev simpli by work with thi term document matrix .
so an import thing to realiz is that doesn't realli work onc we go to sensibl size collect .
and so let's just go through that for a minut .
let's go through a sensibl size but still small collect .
so suppos that we have <num> , <num> , <num> document , and we'll often us n to refer to the number of document , each of which is on averag a <num> , <num> word long .
okai , so what doe that mean in term of the size of our document collect , and in term of the size of our matrix ?
so , if we have an averag of six byte per word , includ space and punctuat , the amount of data we're talk about here is six gigabyt .
so , that's a teeni fraction of on modern hard disk in your laptop .
but let us then suppos we try and work out how mani distinct term thei ar in our document collect .
and we need to know the number of distinct term becaus that correspond to the number of column in our matrix .
and let's suppos there ar about <num> , <num> that'll be a typic number for a <num> , <num> , <num> document and so i'll often refer to thi number of differ term as m .
well what doe that mean , well , what it mean is that even with that size document collect , we can't build thi term document matrix , becaus you'll have <num> , <num> row and a million column .
and that's half a trillion <num>'s and <num>'s , it's alreadi huge and probabl bigger than we have space to store .
and as the document collect get bigger than a million document , thing ar just gonna get wors .
but thi is a realli import observ , which is although the matrix here had a half a trillion zero and on in it , that actual , almost all of the entri ar zero .
that the document ha at most on billion 1s .
and it would be good for you gui to stop and think for a fraction of a second why is it that there ar at most on billion <num>'s .
and the answer to that is , well if we have on million document and the averag document is a thousand word long , as we said last time , then the actual number of word token is onli <num> , <num> , <num> , <num> .
so even if we assum that everi word in everi document were differ , we could onli have at most <num> , <num> , <num> , <num> entri , and most like we have far less than that , caus we'll have common word like the , a , of , and to occur mani , mani time in each document .
and so therefor , the kei observ is the matrix we're deal with is veri , veri , veri spars , and so the central question in the design of inform retriev data structur is take advantag of that sparciti and come up with a better data represent .
and the secret of do that and have an effici storag mechan is we want to onli record the posit that hold a on and not the posit that hold a zero .
okai , so i hope that's given you an understand of the term document matrix .
it's an import conceptu data structur , that we keep on come back to again and again when we talk about variou kind of algorithm .
we think about them in term of that matrix as you'll see .
but , when we actual come to do storag in comput system , we can also see that we never want to actual store document and their inform retriev represent in that form .
hello again .
in thi segment we're gonna talk about the invert index and how it's construct .
an invert index is the kei data structur that underli all modern inform retriev system , from system run on a singl laptop to those run on the biggest commerci search engin .
an invert index is a data structur that exploit thi sparsiti of the term document matrix that we talk about in the preced segment and allow for veri ineffici .
sorri , and allow for veri effici retriev .
it's essenti without peer as the data structur us in inform retriev system .
so let me go through what's in an invert index .
so for each term , each word , we must store a list of all the document that contain the word .
let's identifi each document by a docid , which is just a document serial number .
so you can think of thi start with the first document call on , then two , three , et cetera .
and then the question is what data structur should we us , i mean on idea might to be us fix arrai like the vector that ar in the term document matrix .
but that's veri ineffici , becaus while for some word thei'll appear in a lot of document , other word will appear in veri few document .
moreov there ar perhap other problem if we thing about a dynam index where some document ar ad later on .
that then we have or if document ar chang .
then we'll have difficult thing in adjust our vector size .
for these reason , on wai or anoth , we need to us variabl size list to store the document in which a word occur .
and in standard inform retriev terminolog , these list ar call post list .
post list tradition were usual store on disk but that mai not be the case for , now for big search engin .
and if you're store post list on disk , the right wai to store them is as on continu run of post , becaus that give the most effici method of them be abl to load off disk back into memori when you're interest in the post for a particular word .
in memori a post list can be repres as a data structur like a link list or variabl length arrai with some obviou trade off in the size versu the eas of insert .
so the data structur that we end up with for an invert index is like the on that i'm show here .
so , we have the term that ar in ani of our document .
and then for each term we've got , then got a pointer to a post list that is then give the differ document which ar describ by their document id in which it occur .
okai , so on occurr of a word document pair is refer to as a post , and the sum of all the post list ar then refer to as the post .
and so overal then , we have the part of on the left hand side we have the dictionari , and then on the right hand side we have the post .
and these , and a properti of the post is that thei're sort by document id .
and veri soon now , we'll explain why that's essenti .
so these two data structur , the dictionari and the post have somewhat differ status .
becaus in global size , the dictionari is rel small , but it's normal essenti that it's in memori .
wherea the post ar larg , but at least for someth like a small scale enterpris search engin , these will normal be store on disc .
let me move now to how an invert index is construct .
so the start off point is , we have a bunch of document to be index .
and each of those document we'll think of as be a sequenc of charact .
we'll assum that we've alreadi dealt with , perhap , by someon els's softwar convers from pdf and microsoft word document , and thing like that .
so then we're go to go through first some pre process step , so we need a token that turn the document into a sequenc of word token , which ar the basic unit of index .
but we often don't index exactli the word that ar contain in the document .
there might be variou linguist modul that in some wai modifi the token to put them into some kind of canon form .
so for instanc here , we're sai that for friend here , it's be both lower case , and it's be stem to remov the s plural end .
okai , so then it's those modifi token which will be fed to the index , which is the thing that build the invert index that i wa just talk about .
so here's the invert index .
and it's thi step here of the index that is the main thing that i want to talk about .
but let me first just briefli mention those initi stage of text process .
so in just a fraction more detail .
the thing that happen in those initi stage is firstli token .
that's just how we decid to cut the charact sequenc into word token .
and there ar variou issu there .
there ar punctuat that come up against word , how to treat possess , hyphen term and all that kind of stuff that we can talk about in more detail .
then normal is thi issu that , while certain thing like usa with and without the dot , you probabl want to treat as the same term and map both the text and the queri term to the same form , so that thei will match .
you might want to do other kind of map , such as stem .
so that author and author ar both be map to the same stem , so thei straightforwardli match in a queri .
and final you mai not want to index at all with most common word .
tradition mani search engin have left out veri common word like the , ar , to , and of from the index .
it's not clear that in the modern world when our amount of storag is so vast that that's such a good idea becaus there ar queri that you'd might like to do such as for the song to be or not to be , where you realli need the stop word .
and it turn out that with modern index it's not that ineffici to store them .
okai , now let's go through in detail how the index goe from the sequenc of perhap normal token to build an invert index .
so for thi exampl we're assum we have two document , doc on and doc two here .
so there ar kei sequenc of step that we go through .
so our input is that we have the sequenc of token of the first document in the order that thei come in the text , and the sequenc of doc , of token of the second document , in the order in which thei come in the text .
so the first step is that we do a sort and we sort as the primari kei by the term put them in alphabet order .
so here we have thi alphabet list of term and if we have the same term appear in multipl document we do a secondari sort by the document id so the word caesar appear twice , onc in document id on and twice in document id two , and we're sort it secondarili by document id .
and so that's a core and expens index step .
onc we've got that far what we then do is essenti a consolid of what we've found over here on the right .
so we take that , here it is again , and multipl entri in a singl document ar merg .
so that's the two instanc of caesar , and thei're just treat as on .
and then we also merg all instanc of a particular term , and so then we re repres that as over here .
so we sai we have the dictionari entri caesar .
we record it total frequenc in the collect i'll come back to that a bit later and then we build for it the post list , which is the list of document in which it occur .
and straightforwardli , becaus of a consequ of our sort in the previou step , that thi post list is itself go to be sort by the document id .
so think about the size of an invert index , we can think for a minut about where do we pai in storag .
so we pai some amount for the list of term and their count .
but the number of term will be rel modest .
in exampl beforehand there were <num> , <num> term .
we pai , for a pointer to that identifi where the post list ar , but again that's of the order of <num> , <num> thing .
and then we pai for the actual post list themselv .
and these post list ar by far , the biggest part .
but even then , thei're bound by the size of the number of token in the collect .
so an examp , in our exampl befor of the million document of averag length , <num> word , we're still less than on billion item there .
and so storag is manag .
so , when we ar actual build an effici i r system implement , we think further about these question .
we think about how can we make the index as effici as possibl for retriev ?
and how can we minim the storag on both side of thi , both on thi side and thi side , in term of variou sort of compress .
we're not gonna get into the detail of that now , but what i hope you can start to see is that the invert index give an effici basi on which to do retriev oper .
and that's someth that we'll talk about in more detail in the next segment .
but at ani rate , now you know the underli data structur , it's realli not that complex , that underli all modern inform retriev system .
in thi segment , we're gonna keep on look at the invert index , and see how it's an effici data structur for do queri oper in an ir system .
and in particular , we'll step through in detail how you can perform a common kind of queri , an and queri for two term .
so , start off now , we'll look at the detail of queri process .
and then we'll have a later segment where we'll talk in even more detail about the kind of queri we can process .
so suppos we want to process a queri .
so suppos our queri is brutu and caesar .
well , let me even do a simpler exampl than that .
suppos the veri first kind of queri we want to look at is just a queri for brutu .
well , how to do that is total straightforward , what we do is locat brutu in the dictionari and then we return it post list that we look up and sai okai thi is the set of document where brutu occur and we don't need to do anyth els .
but now let's go to that fraction more complic case .
well then we're go to locat , for brutu and caesar , we're go to locat both the word in the dictionari , look up their post list , and what we'd like to do is then work out what ar the document that contain both brutu and caesar .
and do the put them togeth is standardli refer to as merg the two post list .
now that term can actual be mislead becaus what we're do for an and queri is we're actual intersect the two set of document to find the document that , in which both word occur .
wherea merg suggest do some kind of put them togeth in a union oper .
but the term merg is us actual in both case .
so the merg algorithm famili refer to a famili of algorithm where you can step through a pair of sort list and do variou boolean oper on that .
let's look in concret detail how that happen .
okai , so the wai we do a merg oper to do brutu and caesar is like thi .
we start with a pointer which point at the head of both list , and what we're go to be want to do is then work out what's in the intersect .
so the wai we do that is we ask ar these two pointer point at the same , an equal doc id .
and the answer is no .
and so what we do is then advanc the pointer that ha the smaller doc id .
so now i have two pointer like thi and we sai doe , is , ar the two pointer point at the same document id ?
and here the answer is ye .
so we put that into our result list and then if we've done that , we can then advanc both pointer .
we now sai , ar these pointer both point at the same doc id ?
no .
is the first list greater , first , the thing point to by the first list pointer greater than the thing point to by the second list pointer ?
no , so we advanc the bottom pointer on .
then we sai , is the doc id point at by the two pointer equal ?
no , and so again , we advanc the smaller on .
equal , no , advanc the smaller on .
at thi point , thei're again both point at the same doc id .
so we add that to our result set , and we advanc both pointer .
ar thei the same ?
no , what we do is advanc the smaller on .
ar thei the same ?
no , we advanc the smaller on .
same , no , advanc the smaller on .
same , no , advanc the smaller on .
same , no .
and at thi point , when we try and advanc the smaller on , on of our list ar , is exhaust .
and so then there can be no other item in the intersect and so we can stop .
and so thi is our return document set .
document two and eight contain both brutu and caesar .
so i hope we went through that carefulli enough that you can see if the list length ar x and y then thi merg algorithm take big o x plu y time .
that it's linear in the sum of the length of the two post list .
and you should also have seen what's crucial to make thi oper linear and what's crucial to make it linear is the fact that these post list were sort in order of document id .
precis becaus of that , we could do a linear scan through the two post list , where if that hadn't been the case , then it would have turn into an n squar algorithm .
okai .
here's the post list intersect algorithm on more time , as a real algorithm , but hopefulli you can see it's do exactli the same as what i wa by hand .
so we start here with the answer set as zero .
and then we're go to be do thi while loop while the post list ar both not equal to nil , caus as soon as on's nil , we can stop .
so that's the and oper .
okai , so then at each step , what we do is ask whether the two , the document id of the two pointer is the same .
if so we add it to our answer .
if not and sorri , and if thei ar the same we can advanc both pointer and if not , we work out which doc id is smaller and then we advanc that pointer , so either thi on or thi on .
and that wa exactli what i wa do and then as soon as on of the document list run out , we can return our answer set .
okai , i hope that made sens and you now feel like you can write your own code to do the intersect of post list us the merg algorithm .
in thi segment i m go to introduc phrase queri , which in practic have been the most import kind of extend boolean queri and then examin how you need to extend our invert index data structur to be abl to handl phrase queri .
so veri often we d like to queri not onli for individu word , but also for multi word unit .
mani us thing that we want to queri on ar multi word unit .
so inform retriev is a multi word unit , or mani mani other thing like organ name stanford univers .
so , what we want to do is have a mechan where we can sai , let's match thi pair of word as a unit .
and the syntax that's standardli us in modern web search engin is to put it in invert quot like that .
and so if we have that kind of phrase as a queri , then what we're go to sai is a document that sai i went to univers at stanford doesn't match that phrase queri .
so thi notion of put thing in invert comma ha proven to be veri easili understood by user .
someth that inform retriev system design gener lament is that when peopl design advanc search function for their system , that almost nobodi us it , just a few peopl like me and other academ and peopl like that .
but regular user just don't us it .
and realli , thi notion of phrase queri is sort of the nearest thing to an except , that it is fairli wide understood and us .
but i'll point out that as well as overt phrase queri like thi , there's actual anoth interest phenomenon is that mani other queri ar implicit phrase queri .
so someon will give as their queri san francisco or someth like that , and well , realli what thei want to do is have it interpret as a phrase , but thei just didn't bother or know to put invert quot around it .
so in modern web search engin , an activ area of research ha been how to identifi these implicit phrase and take advantag of know it's an implicit phrase to alter what document get return .
but we're go to leav that asid for the moment and we're just go to deal with these explicit phrase and work out how we can do a good job at match them with our inform retriev system .
okai , well first what should be clear is it no longer suffic to store just a term and a post list of document for each term , becaus if we have onli that we can easili find document that contain two term , but we have no idea whether the term ar adjac to each other as a phrase in that document .
the onli wai we could do it is by exhaust post process of candid document to see if thei actual did contain the phrase , and that would be extrem slow becaus we'd be back to linear scan of document .
on wai to solv thi problem is the idea of a biword index .
so for a biword index what we do is we index everi consecut pair of term as a phrase .
so for exampl , if we have thi text here , friend , roman , countrymen , we gener the biword of adjac word , of friend and roman , and then we gener the biword of roman countrymen , and for each of these biword is now a dictionari term .
and so what doe that mean ?
well that mean that for each of these we'd sai that friend roman occur in , let's sai thi is document <num> , and mayb it also occur in document <num> , and roman countrymen occur in document <num> and it might occur in some other document .
well , if we build that , <num> word phrase queri is now immedi , becaus if we want to find document that contain friend roman in them , we can just look up thi dictionari entri and then grab the post list that wa return for it .
if we consid more complex case , we can even basic handl those .
so we can handl longer phrase queri by break them down .
so if our phrase queri is stanford univers palo alto we can break it up into an and queri of stanford univers and univers palo and palo alto .
and so we can us our biword index to find document that contain all three of these bigram .
now , that's not go to be perfect .
without do a linear scan through the doc , we can't be sure that the document that we're return actual have thi continu phrase stanford univers palo alto , but it seem highli like that thei will becaus we have check for each of these bigram occur .
so there's a small chanc of fals posit , but it seem like we're in a pretti good state .
so what ar the issu with us a biword index ?
well , as we note befor there can be fals posit in what's return , but mayb that's not such a big problem .
the big problem is that there's thi enorm blowup of the index , becaus our dictionari ha gotten much more massiv .
so that mean that , you know , it's not practic to have triword and quadword indic to actual exactli match long queri .
but even for biword , we're then go to have the , sort of , potenti have the space of word squar possibl dictionari entri .
so becaus of that biword indic ar not the standard solut for handl phrase search .
but as i'll show , it wasn't useless that i explain them to you , becaus as i'll show at the end , thei can be an import compon of a solut .
so what is the standard solut ?
the standard solut is to move to have a posit index .
so the idea of the posit index is that in the post , we store not onli the document that contain a term , but also the posit in each document where the term appear .
so the organ of what we have now is in the dictionari we have a term , and the document frequenc of the term , and then when we point off to the post list , then instead of just have a list of document id , we then have a list of document id where for each document we then have a list of posit where the term occur .
let's look at that concret with an exampl .
so here we have the word be , which occur in almost a million document , and then we have where , so in document <num> it occur at word <num> , <num> , <num> , etc .
document <num> it occur in these two word posit .
it doesn't occur in document <num> .
document <num> it occur in a bunch of place , and so on .
and so , with these we can then be abl to check whether phrase occur by then see whether word occur adjac to each other .
to get the idea of that , you could consid thi littl question here .
so , which of these four document could contain to be or not to be base on those document posit ?
now obvious we haven't seen the post list for the other word , but just look at where be occur , which document is a candid ?
so with thi kind of data structur , we can handl phrase queri us a merg algorithm much as we show befor , it's just a littl bit hairier becaus rather than just do an intersect of the document id , we have to do thi sort of <num> level algorithm , where we both intersect the document id and then also check that there ar compat posit for the word occur in the phrase .
and so that mean that we need to deal with a bit more than just equal .
so for exampl , if we're want to find instanc of the phrase inform retriev , what we want is that if the word inform occur at posit <num> in a certain document , we want retriev appear at posit <num> in the document .
so , go through that in slightli more detail , to process a phrase queri what we do is , so let's assum our phrase is to be or not to be that we want to find , in invert comma .
so we find , we get the post list of each of the individu term , and then what we're go to do is we're go to progress intersect them .
so if we start off with the to be , we're go to start at the begin do post merg , and we're sai , well , document <num> can't be a candid becaus it doesn't appear on the other post list .
document <num> can't be a candid becaus it doesn't appear on the other post list .
and at thi point we've got to document <num> and the doc id match .
but then at thi point with a posit queri we have to recurs and then do anoth merg algorithm for the posit within the post list .
so we start here and here , but thi time rather than an equal check , what we're want to see is can we find a place where be occur with a token number on larger than to .
so we'll again step along , and we'll sai , well here's on candid , <num> and <num> , and here's a second candid , <num> and <num> .
so those will be two candid match insid document <num> , and so we'll be return both of those candid match separ , caus we're actual find the posit in document where our phrase queri is match , and that we'll need to refin with the other queri word come up ahead .
at that point we then revert back , oh wait .
no , sorri i'm wrong .
there's on more candid , sorri , there ar three candid here , right ?
but , onc we've exhaust the posit in on document , we then revert up to the higher level of our post merg , and then we sai <num> is less than <num> , so thi get cross off .
and then we advanc that pointer and proce along .
and i hope you can see that actual thi method work not onli for phrase queri , where the word ar offset by <num> , but you could actual us exactli the same method for the proxim queri that we saw earlier with the westlaw servic , where you could ask for on word be within three word of anoth word , or someth like that .
right , so that wa the exampl like thi .
so here we had limit within <num> word of statut , which is within <num> word of feder , within <num> word of tort .
so we could us the same kind of techniqu of optim the queri but if we start with thi on , we're then have a condit on how close togeth the token posit indic have to be to count as a match within a document .
and so clearli posit indic can be us for these kind of proxim queri , where biword indic do noth for you .
so i've sort of said the algorithm here , but someth that you might want to work through veri concret is how you can chang the linear post merg algorithm to handl proxim queri .
in particular , you can think about how you can get it to work for ani valu of k .
it's actual a littl bit tricki to do thi correctli and effici , becaus you can have the within k word match on either side of the word , and you have have to keep track of thing for the right number of word on each side .
it would be good to try and work out by yourself , but you can also see on exampl of an answer in the figur <num> of our introduct to inform retriev book , which you can find free on the web to look at .
so a posit index is great becaus it allow us to answer phrase queri and proxim queri , but there's a cost to a posit index , which is that our post list just got a lot larger , becaus rather than onli store document id , thei're store document id and the offset of token within the document .
and that's a major factor , even though indic can be compress .
nevertheless , it's now complet standard to us a posit index becaus of the power and flexibl you can get from handl phrase and proxim queri , whether thei're be us explicitli in term of have these kind of phrase queri or within <num> queri , or whether thei're just be exploit to improv the rank of a system when it's look for implicit phrase .
but as i said , the posit index get much larger .
and so when we're estim the size of a posit index , we note that there ha to be an entri for each occurr of a word , not just onc a document .
so what that mean is the size of the index depend on the averag length of the document now .
so if we have fairli short document , it's not such a big deal , caus actual most of the word that occur in a document occur onli onc or twice .
but if we have veri long document , then that blow out the size of the posit index rather more .
so for exampl you can consid a word , a common word with frequenc <num> , so thi word occur <num> word in a thousand on averag .
and so , well if you have a document size of averag length <num> , then we're kind of get no blowout realli from go to a posit index , becaus there'll onli be on posit be record .
but if we have a document , document that ar realli long , then we might be get <num> time blowout in the size of the posit index .
so everyth that depend , but just to give you some veri rough rule of thumb for , you know , what is in some sens typic document like web page , that you might expect a posit index to be somewher around <num> <num> time as larg as a non posit index .
and in particular , the size of a posit index remain smaller than , but start to approach the volum of the size of the origin text , so head to sort of a third or a half the origin size of the text that's be index , which is much larger than in the case of a non posit index where it might be down to someth like just <num> .
so have an ir system doesn't actual take a lot more space than store the text in the first place .
i mention when i mention biword indic , that thei aren't a useless idea even though thei're not normal the solut by themselv .
and so let me just come back to that .
these two approach can be veri profit combin .
so if you look at the queri stream of high queri volum servic like web search engin , there tend to be some queri that keep on turn up again and again and again .
so thing like person name of popular peopl .
well , if we just treat those as , you know , just regular post list intersect , or the posit phrase queri post list intersect , what we'd have to do is keep on do thi intersect over and over again everi time that someon send that question .
and it's bad for case like michael jackson .
it's less bad when you've got rare name like britnei spear , caus presum their post list ar much shorter , and the intersect is roughli the same as each individu post list .
do thi intersect everi time is especi bad when you have common word , like if you're search for the band the who , then what's go to happen is you're go to retriev two enorm post list , do the intersect of them , and end up with a veri short post list for thi phrase queri .
so in a paper from a group in melbourn that's a well known inform retriev group , in <num> thei investig a more sophist mix index system .
so in thi , what happen wa that for common phrase , you know , like these exampl , thei did build biword and index the biword .
where for rarer phrase thei were done by a posit index .
and so what thei were abl to show wa with a typic web queri mixtur , you could execut it in on quarter of the time of the posit index alon by make us of also have index some biword .
but at the cost of onli take <num> more space than have the posit index alon .
so that make that bit look a fairli appeal tradeoff to augment a posit index with common bigram .
and , well on model of do that is to do it as here where you work out in advanc the common bigram and then index those in your standard invert index .
in practic what happen a lot in modern system is that peopl try to do thi a bit more dynam so that thei keep a cach of commonli be queri phrase queri , and what the result of the post list intersect is for each of those .
and so that's like have ad those biword to the invert index , but done a bit more dynam .
okai , so that's introduc what's the most us extens to the classic boolean retriev model which is have support for phrase queri .
and we introduc a method for handl those , or two method , but in particular we look at posit indic , which can handl phrase queri , but also the proxim queri that we saw earlier .
hello , in thi segment , i'm go to start talk about rank retriev .
so , so far , all of our queri have been boolean .
we give a queri , like ocean and liner , and the search engin is gonna return precis the document that contain the word ocean and liner and no other .
document either match or thei don't .
now , thi can be good for expert user with a precis understand of their need and the collect and is can be good when inform retriev in the system is a compon of a larger applic .
the system can consum thousand of result and adjust the queri as need .
but such a system turn out not to actual be good for the vast major of user .
most user ar incap of write good boolean queri , or even if thei can write them thei think it's far too much work to write them .
in particular , boolean system often produc thousand of result , and user don't want to wade through thousand of result .
thi is particularli true of web search applic .
in gener there's thi problem with boolean search , the feast or famin problem .
boolean queri often result in either too few return result , on or two or even zero becaus document don't precis satisfi the search request , or els thei mai result in too mani result , in the order of thousand or more .
so for exampl here's a system give result .
if i give it the result standard user dlink <num> , it return <num> , <num> result .
so i try and make my queri more specif , and ask standard user dlink <num> no card found .
but then i get zero result .
it take a lot of skill to come up with a queri that produc a manag number of hit .
the basic problem is , if you're put a and between word , you get too few result .
and if you put or between word , you get too mani result .
so part of the solut of that ha been the develop of rank retriev model .
and the idea of these is that rather than a set of document satisfi a queri , in rank retriev model the system return an order over the top document in the collect with respect to the queri .
go along with that ha been the adopt of free text queri , rather than explicit queri languag like the boolean retriev model with it and , or , and not .
instead , the user's queri is now just some word in a human languag .
in principl , these ar two separ choic which could be manipul separ .
but in practic , rank retriev model have normal been associ with free text queri , and the opposit for the boolean retriev model .
the , the feast or famin problem doesn't exist in rank retriev .
when a system produc a larg result set , user don't realli notic .
inde the size of the result set basic isn't an issu .
becaus normal the system will start off by just show the us of the top few result and so not overwhelm the user .
and so the total number of result is someth thei probabl won't even know or notic .
thi all depend on have a rank algorithm that work well though , so that the top result ar good result .
so the basi of rank retriev is have a good system of score .
we need to be abl to return the document that ar most like us to the searcher .
and so that rais the question of how can we rank order the document with respect to a queri .
and the method we'll look at to do that is the idea that what we should do is assign a score , sai a number between zero and on , to each document .
thi score measur how well the document and the queri match each other .
so we need a wai of assign a score to a queri document pair .
let's start with a on term queri .
well , if the queri term doesn't occur in the document the score for the document should be zero .
and then beyond that probabl what we want to sai is that the more frequent the queri term appear in the document the higher the score should be .
and after that how exactli we score document isn't quit so clear .
and so , in upcom segment we'll look at a number of altern for thi .
okai .
but i hope that's given you an idea of what rank retriev model ar and how thei differ from the boolean retriev model .
as a first simpl exampl of the rank retriev model , let's consid score with the jaccard coeffici .
so the jaccard coeffici is a commonli us measur for the overlap between two set , a and b .
and what it is , is simpli , you take the number of item in the intersect of a and b , and you divid it by the number of item in the union of a and b .
and so if we take the jaccard coeffici of a set with itself , then the set ha some size .
and then that size will be also the size of the intersect and the union .
and so the ratio will be on , and the jaccard coeffici is on .
if two set ar disjoint and have no member in common then the numer of the jaccard coeffici will be zero and so the jaccard coeffici is zero .
now the two set don't have to be the same size but you should be abl to see that the jaccard coeffici will alwai assign a number between zero and on , caus at most the intersect can be as larg as the union .
so suppos we decid to make the queri document match score the jaccard coeffici comput for the set of word the two document contain .
so the idea is that let's suppos our queri is id of march , which ha these three word , and then we have these two document .
so what we can do is sai , so there ar three differ word here .
and then for document on , caesar doesn't occur in it .
di doesn't occur in it .
in doesn't occur in it .
march doe occur in it .
so the size of the intersect is just on word and the total number of word is six .
if we then do the second document , well , the doesn't occur in the queri , long doesn't occur in the queri , but again march doe occur in the queri .
so thi time , the jaccard coeffici of d1 , d2 , is go to be on divid by the number of word , which is thi time five .
okai , and so thi document is go to win as have the higher jaccard score .
now of cours that differ mai not seem veri signific .
essenti thi document is win here just becaus it's shorter .
but if we imagin a differ exampl where we mayb had the word id in the second document .
then we'd get that the jaccard coeffici for it is now two overlap word over six .
and that mayb make more sens to you that you're get more overlap if the jaccard score is higher .
but thi idea that all els be equal a shorter document should be prefer is a common on that we'll see again in ir model .
okai , so is jaccard score a good idea for a retriev model ?
in gener it's not felt to be .
it ha a coupl of issu .
on is that it doesn't consid term frequenc .
it just us the set of word in the document and ignor how mani time the word occur in a document .
but that's typic not all the inform we want and we'll look at model in a minut that do deal with term frequenc .
there's also a second finer point , which is that rare term in a collect ar more inform than frequent term when evalu the , a queri .
and that's someth that we'll also want to factor into our model .
there's on other aspect in which the jaccard coeffici turn out not to work veri well .
and that is , the wai in which it doe normal by divid through by the union isn't necessarili quit right .
i mean , in particular , later on in these segment we'll introduc the idea of us cosin similar .
and we'll go through the math of that for the more gener case .
and if after you've seen that you want to kind of come back to thi and work out what the cosin similar score is , if you just have a on zero bag of word and work out a cosin score .
it'll turn out that it's thi , which is like the jaccard coeffici except that we've got thi slight differ in the denomin that we're now take the squar root of the union .
and that actual turn out to be a better form of length normal .
okai .
so , i introduc the jaccard coeffici just as a veri simpl exampl of a rank retriev model .
but i think it wa , i hope it wa also a wai to show some of the issu that we need to deal with in a good retriev model .
how to factor in the frequenc of term , the rare of word , and how to normal the score for differ document length .
the next thing i'd like to introduc is term frequenc weight , which is on of the compon of the kind of document score that ar regularli us in inform retriev system .
let's go back to where we began , with the term document incid matrix .
so with thi matrix we record a number , which wa either on or zero , in each cell of the matrix depend on whether the word occur in the document .
if we then think about what the represent of each document is , well what we have is a vector .
it's a binari vector , which , the dimension of the vector is the size of the vocabulari and it's record these 1s or 0s .
but we don't have to limit ourselv to a binari vector like thi .
an obviou altern is instead to move to a count vector .
so now we still have a vector for each document , but rather than simpli put <num>'s and <num>'s in it , we're put in the number of time the word occur in the document .
so we've still got a vector of size of vocabulari , but it's now a vector in the natur number vector space .
previous in the boolean retriev model , we were just look at a set of word that occur in the document and do set oper like and or or .
now with thi count model , we've move to the commonli us bag of word model .
so in the bag of word model , we're not consid the order of the word in the document , but we ar consid how mani time a word occur in a document .
and thi word bag is commonli us for an extens to set , which doe record how often a word is us .
so the bag of word model ha some huge limit .
so , john is quicker than mari , and mari is quicker than john , have exactli the same vector , there's no differenti between them .
and that's obvious ha it limit .
so in a sens , thi is a step back .
earlier on when we introduc posit indic , thei were abl to distinguish these two document by either proxim or phrase queri .
and we'll want to get back to that .
we'll look later at recov posit inform , but for now we're go to develop the bag of word model and how it's us in vector space retriev model .
so we have thi quantiti of the term frequenc of a term in a document , which is just the number of time that it occur .
and so the question then is , how can we us that in a retriev score ?
think about it a littl , i hope you can be convinc that raw term frequenc is perhap not what we realli want .
so the idea underli make us of term frequenc is , if i'm search for someth like squirrel , then i should prefer a document that ha the word squirrel in it three time over on that just ha the word squirrel in it onc .
but on the other hand , if i find a document that ha the word squirrel in it <num> time , it's not clear that i should prefer it <num> time as much as the document that onli mention squirrel onc .
and so the suggest is that relev goe up with number of mention but not linearli .
and so we want to come up with some wai of scale term frequenc that is rel to it frequenc but less than linear .
befor i go on to outlin such measur let me just highlight on last point .
we talk here about term frequenc .
now the word frequenc actual ha two usag .
on is the rate at which someth occur , the frequenc of burglari .
and the other sens of it is the on that's alwai us in inform retriev .
so when we talk about frequenc in inform retriev , frequenc just mean the count , so the count of a word in a document .
okai , so thi now is what is standardli done with the term frequenc .
what we do is we take the log of the term frequenc .
now if the term frequenc is zero , the word doesn't occur in the document , well the log of zero is neg infin .
so that's slightli problemat .
so the standard fix for that is we have thi two case construct where we add on to the term frequenc if the term doe occur in the document .
so if it occur onc , then it valu will becom on .
becaus the log will be zero and then we'll add on to it .
and we return an answer of zero if the word doesn't occur .
so that mean that if , go on a littl , and if we us base ten logarithm as here , you can see how we're get thi less than linear growth .
so if a word occur twice in a document , it get a weight of <num> , a littl more .
if it occur ten time , it get a weight of two , <num> time , a weight of four , and so on .
so in order to score a document queri pair , we're just gonna sum over these term for each word in the queri and the document .
so it's suffici to take the intersect of word that ar in both the queri and the document , caus everyth els will contribut noth to the score .
and then for each of those term , we're gonna calcul thi quantiti and sum them up .
and so note in particular , that the score is inde still zero if none of the queri term is present in the document .
okai , so that's the idea of term frequenc weight and how it can be us to give a score for document for a particular queri , which can be us to rank the document return .
in thi segment , i'm go to introduc anoth score that's us for rank the match of document to a queri .
and that is , to make us of thi notion of document frequenc .
in particular , we alwai us it in revers , so it's normal refer to as invers document frequenc weight .
the idea behind make us of document frequenc is that rare term ar more inform than frequent term .
so if you rememb earlier on we talk about stop word , which wa you know , word like the , and to , and of .
and so the idea that it wa that these word were so common , so semant empti , that we didn't have to includ them in our inform retriev system at all .
thei had no effect on how good a match a document wa to a queri .
well , that's mayb not quit true , but there's some truth in it .
in particular it seem like in gener veri common word aren't veri determin of the match of a document in the queri , wherea rare word ar more import .
so consid a term in the queri that is veri rare in the collect , perhap someth like arachnocentr .
well , if someon had type that word into their queri and we can find a document that contain the word arachnocentr it's veri like to be a document that the user would be interest in see .
so we want to give a high weight in our match score for rare term like arachnocentr .
on the other hand , frequent term ar less inform than rare term .
so consid a term that is frequent in the collect , like high increas line which might occur in lot of document .
well , a document contain such a term is more like to be relev than a document that doesn't , if the queri contain on of those term .
but it's not such a sure indic of relev .
so frequent term we want to give posit weight for a document match a term in the queri , but lower weight than for rare term .
and so the wai we're go to go about do that is by make us of thi notion of document frequenc score .
so what exactli is that ?
well .
the document frequenc of a term is the number of document that contain the term .
so what thi mean is that we're look at the entir collect .
so mayb the collect is a million document and if ten document have thi word , we're sai that the document frequenc is ten .
so that's just count the number of document that occur regardless of the number of time it occur .
that's someth i'll come back to .
so document frequenc is an invers measur of inform that , of inform of the term .
and we also note that the document frequenc ha to be , of all term ha to be smaller than the number of document in the collect .
so put that togeth , thi give us the measur of invers document frequenc where we start with the document frequenc and us it as the denomin .
and the numer n here is the number of document .
so for a , a word that appear in just on document , thi part will be n .
and for a word that appear in everi document , it valu will be on .
so it's some valu between on and n .
and so then what we do after that is we take the log of it .
and the log is us to dampen the effect of invers document frequenc .
the idea again is that if you just us the absolut score , that would be too strong a vector .
now in thi comput you can see i've us log to the base ten and that's veri commonli us .
but actual it turn out that what we us as the base of the log isn't realli import .
okai , let's go through a concret exampl , where again we ar go to suppos that the size of our document collect is <num> , <num> , <num> document .
so if we take an extrem rare word like calpurnia , which let's sai occur in just on document .
well , then what we ar go to be do is we're go to be take , <num> , <num> , <num> , the number of document , divid by on , and then take the log of that , which mean with log to the base ten , then that would be six .
if we take a somewhat more common doc , word that occur in mai <num> document , then we go to get the invers frequenc of that is four .
and so then we can work on down for progress more common word , and the invers document frequenc or countdown .
and in particular in the final case if we assum the word the occur in everi on of our document , well then we've got a million divid by a million , which is on , and if we take the log of that , which is , we get the answer zero .
so the result we actual get is that a word that occur in everi document doe have a weight of zero , accord to an idf score and ha no effect on the order of word in retriev .
and that make sens becaus if it ha , if it occur in everi document it ha no discriminatori valu between document , and get a weight of zero .
and so what you can see with these number overal though is that thi invers document frequenc weight will give a small multipli to pai more attent to word that a rarer word , rather than veri common word .
anoth thing to note here is that idf valu aren't thing that chang for each queri , that there's precis on idf valu for each term in the collect , and that's go to be the same regardless of what queri you're issu of the collect .
okai , here's a , a ye no question for you gui , doe the idf have an effect on rank for on term queri like thi on .
the answer is no , it doesn't .
idf ha no effect on on term queri .
so for a term on queri , you're go to have on of these term of n over the document frequenc , and it'll be work out .
but it's go to be just a scale factor .
which , sinc there's onli on idf valu for each term will be appli to everi document , and therefor , it won't affect the rank in ani wai .
you onli get an effect from idf when you have multipl term in a queri .
so , for exampl , if we have the queri , caprici person , well , now , we're in a situat where caprici is a much rarer word .
and so idf will sai .
pai much more attent to document that contain the word caprici , than to document that contain just the word person in rank your retriev result .
there's anoth measur that reflect the frequenc of a term and inde you might have been wonder why we're not us it .
and that other measur is what inform retriev peopl refer to as the collect frequenc of a term .
so the collect frequenc of a term is just the total number of time it appear in the collect , count multipl occurr .
so that's the measur that we've been us in other place , it's the measur we were us to build unigram languag model or when we're work out spam classifi or someth like that .
but it's not what usual us in inform retriev rank system , and thi next exampl can mayb help explain why .
so here we have two word , insur and try .
and i pick those two word becaus thei have virtual ident collect frequenc , overal thei both occur somewhat more than <num> , <num> time in the collect .
but let's then look at their document frequenc , so the word try occur in <num> , <num> odd document and that stand in contrast to insur , which occur in slightli under <num> document , and so what doe that mean ?
what that mean is that when try occur in a document it tend to occur onli onc , but that try is wide distribut across document .
on the other hand , when insur occur in a document , it tend to occur sever time .
it tend to occur two to three time .
and so what doe that reflect ?
it reflect the fact that there tend to be document about insur , which then mention insur sever time where there don't realli tend to be document about try .
and so what doe that mean in term of come up with a score for retriev system with word match .
what it seem to suggest is that what we should be do is give higher weight to instanc of the word insur appear .
so if we had some kind of , imagin some kind of queri like , try to bui insur .
the most import word to make sure we're find in our document to match the queri is insur .
and probabl the second most import word is bui , and try should be come in third place , befor the near stop word of to .
and so that's an idea that is be correctli captur by look at the document frequenc , but as you can see , it's not captur by the collect frequenc , which would score , try and insur equal .
okai , so i hope now you know what document frequenc weight is , and why peopl usual us that as a retriev rank score rather than collect frequenc .
we've now introduc two weight for term and document to us in our inform retriev rank process term frequenc and invers document frequenc .
in thi segment we're go to put them togeth to get the tf idf weight of term .
the tf idf weight of a term in a document , right here , is simpli the product of it tf weight scale with a log term as we discuss befor , time it invers document frequenc weight .
thi is the best known weight scheme for term in inform retriev .
there's been a lot of research , and there ar mani other .
but if you onli know on , it's the on to know .
note in particular on fine point .
so thi littl dash or a hyphen here in thi tf idf weight , that's a hyphen , it's not a minu sign , that we're take a product .
so sometim peopl indic that more explicitli by us a dot or us a multipl sign .
so , what ar the featur of tf idf weight ?
tf idf weight increas with the number of time a term occur in a document so that the tf idf weight for a queri term depend on the document .
it's not independ of the document .
and then the tf idf weight for a term also goe up with the rariti of the term in the collect .
that's from the idf weight here .
so us thi to find the rank of document for a queri , what we're do to work out the score of the queri in the document is we're take the term that appear in both the queri and the document .
the rest of them have no weight , and we're work out thi td idf weight for each of those term and then we're sum them to get the score of the document with respect to the queri .
so what have we done here ?
what we've done is we've gradual move from first binari vector in the origin model of do boolean inform retriev , to count vector which were us when we just had an unscal term frequenc .
to now we have weight vector for a document and henc a weight matrix between term and document and that's now what we see here .
so each document is now be repres by a real valu vector .
so for exampl , the document juliu caesar is be repres by thi vector .
so that for each document , it's in the vector space of real valu number where the dimension is the number of differ term in our collect again .
okai , and then when we have a bunch of these vector for each document in our collect , we have a term , a term document matrix which is now a real valu matrix .
we'll sai a littl bit more later about what some of the properti of thi is .
but hopefulli , see thi kind of term document matrix of real number is enough to see how we can do a rank of document accord to some queri , accord to these tf idf score that we've assign for each term and each document so that's tf idf weight , on of the most central concept in inform retriev system .
hi again .
okai we've alreadi laid some of the ground work with notion like term frequenc and invers document frequenc .
in thi segment what i want to introduc is probabl the retriev model of the vector space model , which is on of the most commonli us model of inform retriev in real system .
so we saw in the previou segment how we turn document into real valu vector .
and so we now have a v dimension vector space where v is the number of word in our vocabulari .
the term , the word , ar the ax of the space , and document you can think of as either just point in thi space , or vector from the origin point out to those point .
so we now have a veri high dimension space .
ten of million of dimens in a real system when you appli thi such as in a web search engin .
the crucial properti of these vector is that thei're veri spars vector .
most of the entri ar zero , becaus each individu document onli typic ha a few hundr or thousand word in it .
so then if we have thi vector space of document , how do we handl queri it when a queri come in ?
and the kei idea there is that we treat queri in exactli the same wai .
thei're also go to be vector in the same space .
and then if we do that we can rank document accord to their proxim to the queri in thi space .
so proxim correspond to similar of vector , and therefor it's roughli the revers of distanc .
and we're do thi becaus we want to get awai from the , you're either in or out , boolean model , and have a rel score as to how well a document match a queri .
we're go to rank more relev document higher than less relev document .
let's try and make that all a bit more precis .
so how can we formal proxim in a vector space ?
the first attempt is just to take the distanc between two point .
that is the distanc between the end point of their vector .
and the standard wai to do that in a vector space would be euclidean distanc between the point .
but it turn out euclidean distanc by itself isn't actual a good idea .
and that's becaus euclidean distanc is larg for vector of differ length .
let me explain what i mean by that .
let's suppos , here is our vector space .
well , what we're find is the distanc between here and here is larg .
in particular it's larger than either the distanc here or the distanc there .
but if we actual think of thi in term of an inform retriev problem and look at what's in our space , that seem wrong .
so in thi teeni exampl , the two word ax shown ar here for gossip and here for jealou .
and what our queri is , thi is the queri that would come out precis if your queri is gossip and jealou .
so it ha both of those word occur with equal weight .
well if we then look at our document , what we find is document on seem to have a lot to do with gossip and noth to do with jealousi .
and document three ha a lot to do with jealousi and noth to do with gossip .
wherea document two seem just the kind of document we want to get , on that ha a lot to do with both gossip and jealous .
so the term in the document d2 ar veri similar to the on in q , so we want to be sai that that is actual the most similar document .
so thi suggest a wai to solv thi problem and move forward .
and that is , rather than just talk about distanc , what we want to start look at is the angl in the vector space .
so the idea is we can us angl instead of distanc .
so let's in particular motiv that onc more by consid thi thought experi .
suppos that we take a document and append it to itself , give us a document d prime .
so clearli semant , d and d prime have the same content , thei cover the same inform , but if we're just work in a regular vector space with euclidean distanc , the distanc between the two document will be quit larg .
that's becaus if we had a vector , and we had , thi wa the vector for d , then the vector for d prime would be twice as long , point out here , and so that we have a quit larg distanc between these two vector .
so we don't want to do that , instead what we want to notic is that these two vector ar in a line so the angl between two vector is zero , correspond to maxim similar .
and so the idea is , we're go to rank document accord to that angl between the document and the queri .
and so the follow two document ar equival .
rank document in decreas order of the angl between the queri and the document , and rank document in increas order of the cosin of the angl between the queri and the document .
and so i'll go through that in a littl bit more detail .
but you'll often hear the phrase cosin similar , and thi is what we're introduc here .
and the secret here is just to notic that cosin is a monoton decreas function for angl between the interv zero and <num> degre .
so here's the cosin which you should rememb .
so if the angl is zero , the cosin of it is on .
if it's , if it's perpendicular , <num> degre , the , the cosin is zero .
and it can keep on go right up to <num> degre and the cosin is continu to descend to minu on .
so essenti , all we have , need to observ here is that cosin is a monoton decreas function , in the rang of zero to <num> .
and so therefor cosin score serv as a kind of invers of angl .
and , well that might still make it seem a rea , a rather strang thing to us .
i mean , we could have just taken the reciproc of the angl , or the neg of the angl , and that would've also turn thing around .
so we got a measur of close between document as a similar measur .
but it turn out that the cosin measur is actual standard , becaus there's actual a veri effici wai to evalu the cosin of the angl between document us vector arithmet , where we don't actual us ani transcendent function like cosin that would take a long time to comput .
so the start point of go through thi is get an idea of the length of a vector , and how to normal the length of a vector so , for ani vector so if we have a vector x , we can work out the length of the vector by sum up each of it compon squar and then take the squar root around the outsid .
so that , if have someth like a vector that's <num> , <num> , what we're go to do is take three squar nine , four squar sixteen , and then take the , add those give <num> .
take the squar root give five .
and that's the length of the vector just like in the standard pythagorean triangl .
okai so if we then take ani vector and divid it by it length we then get a unit length vector , which will , you can think of as a vector that touch the surfac of a unit hyperspher around the origin .
now if we go back to the exampl that we had earlier of two document , d and d append to itself to give d prime , you can see that these document , if thei're both length normal will both go back to exactli the same posit .
and becaus of that , onc you length normal vector , long and short document have compar weight .
so , the secret of our cosin measur is that we do thi length normal .
so here's the cosin similar between two document , which is the cosin of the angl between the two document .
and the wai we do that is in the numer , we calcul here a dot product .
so we're take the individu compon of the vector here , compon by compon , and multipli them and take their sum .
but then the wai we do that is that we've then got thi denomin , which is consid the length of the vector .
and you can write it like thi .
but actual what it's equival to is take each vector and length normal it .
and then take the dot product of the whole thing , becaus it's these sort of two part .
you can factor apart as you wish .
and so of , fill , written out in full it's over here that we have the length normal on the bottom and then thi sum up dot product on the top .
okai where each of these element qi is a tf idf weight of term i in the queri .
and di is the tf idf weight of the term in the document .
in particular what we might want to do is actual length normal our document vector in advanc .
and length normal our cosin , length normal our queri vector onc the queri come in .
and if we do that , thi cosin similar measur is simpli the dot product of length normal vector .
and , so , we're simpli just take thi sum here in the vector space .
where as we discuss befor , in realiti we won't do it over all element of the vector .
we'll just do it over the term in the vect , the term of the vocabulari that ar in the intersect of the on that appear in q and the document .
so go back , to the kind of pictur we had befor , we now again have our vector space , which again we're show with just two ax here to keep it viewabl , which ar now poor and rich .
and we can take ani document vector and we can map it down to unit length by do by thi length normal .
and when we do that we get all document vector , be vector that touch the surfac of thi unit hyperspher , which is just a circl in two dimens .
and so then when we want to order document by similar to a queri , we take thi queri here and we're work out the angl , or the cosin of the angl to other , to other document .
so in particular the cosin will be highest for small angl .
so we'll be , if we order these document in term of the cosin of the angl , the document that will rank first will be d2 , then it'll be d1 , and then it will be d3 .
okai .
now let's now go through thi concret with an exampl .
so , in thi exampl what we have is three novel of jane austen's and we ar go to repres them in the vector space , length normal .
then we're go to work out the cosin similar between the differ novel .
so , in other word in thi exampl there isn't actual ani queri vector .
we're just work out the similar between the differ novel that ar our document .
so the start off point is start with these term frequenc count vector , for the differ novel .
and so what we can see is , affect is on of jane austin's favorit word that appear frequent in everi novel .
the word wuther onli occur in wuther height .
and then other word like jealou and , jealou and gossip occur occasion .
and so thi is go to be our vocabulari for thi exampl that i give .
and what we're go to want to do is take these term frequenc vector and turn them into length normal vector on the unit hyperspher .
now for thi exampl i'm just go to us term frequenc weight .
we're go to leav out idf weight to keep it a bit simpler .
let's see what happen on the next slide .
okai , so here we've done log frequenc weight of the kind we saw befor .
so what were the zero , stai zero , and then we're have map down , so we're get a weight of three for the number of time that affect appear in sens and sensibl .
but these vector aren't yet of the same length .
thi is clearli the longest of the vector .
so the next step is to length normal them .
so now here ar the length normal vector for the three document , and you can see how thi vector ha gotten much shorter than it wa here by scale it down .
and the properti that we have for each of these vector for their be length normal is that if you took thi quantiti squar , plu thi quantiti squar , plu thi quantiti squar , you would get on .
and therefor the squar root of that sum would also be on .
so thei're length on vector .
so given that thei're length on vector we can then calcul cosin similar as simpli the dot product between the vector .
so let's see what happen when we do that .
okai , so then we have the cosin similar between sens and sensibl and pride and prejudic is take these pair wise product and sum them togeth .
and it give us a cosin similar of <num> . <num> , so thei're veri similar .
and then we can do it for the other case , and what we see that for sens and sensibl and wuther height , it's <num> . <num> .
and for the final pair , these two , it's <num> . <num> .
and the thing that we might wonder is , why do we have that the cosin similar of sens and sensibl and pride and prejudic is higher than that for sens and sensibl and wuther height ?
and so we can try and look at that .
so we're go to be compar thi on with the other two .
and what we can see is that , thi part of the wuther height vector doesn't help at all in produc similar with sens and sensibl .
the biggest compon in the sens and sensibl vector is thi on .
and so that gener a lot of similar with pride and prejudic , which also ha that word veri promin repres , where that word is less repres in wuther height .
and so therefor , thi dot product here , that thi term in the dot product is much larger , and so we get greater similar .
and so you can see there that it's sort of the ratio of occurr of differ word in the document ha a big effect on measur overal similar .
okai , i hope that exampl help to make it more specif , and that you now have a good idea of what the vector space model of inform retriev is .
it's the idea that we can rank document for retriev , base on their similar of angl in a high dimension vector space .
okai .
let me tell you a littl bit more about how tf idf score and the cosin similar measur get us togeth in a rank ir retriev system .
i'm not go to get a lot into the detail of make these system practic and effici , but at least give you a littl bit more of a sens .
so the first thing that you might alreadi start to notic is that tf idf weight isn't realli on thing , there's realli a famili of measur .
let's just look at that in a littl bit more detail .
so first of all , you have the term frequenc and what you do with the term frequenc .
and you could just have the natur term frequenc , but we suggest that that is usual mute by someth like log weight .
and that's inde the most common thing to do .
but it's not the onli method that's be us .
there have been a bunch of other method that have been suggest for normal term frequenc .
and if we move on to the document frequenc , we can not us document frequenc weight at all .
we could us thi kind of log invers document frequenc weight , which is again , extrem common .
but again , there ar other thing that peopl have tri out do .
so if we put these two thing togeth we have tf idf weight , give us a vector .
but , well we mai well want to normal those vector in some wai to have better similar comput .
and so we discuss us the cosin length normal and it turn out that it ha some advantag and some disadvantag , so again there ar other thing peopl have tri includ both of these and other on that have came up more recent so thing like pivot length normal .
so in gener , we have a kind of a broad menu of choic .
and so , at the begin here of each column , i've given some letter name to these choic .
these were choic , these were choic that were develop in the context of the smart inform retriev system which is a veri famou , pioneer inform retriev system that wa develop at cornel by jerri sultan who wa realli the father of a lot of modern inform retriev .
and so thi is , these choic could be given name by give letter from these .
so if you were us the system that we've mainli been talk about , that would be come out as ltc for logarithm , logarithm idf and cosin weight .
and so it , thi kind of weight it then turn out can be us both for queri and document differ , and so let's go through a littl bit how that all come out .
we can have differ weight for queri versu document .
and if we follow thi smart notat , the standard wai that thei repres thing is by these six letter with the dot in the middl , where there is document weight scheme follow by the queri weight scheme .
and there ar variou variant , but on that wa quit standard come out of the smart work in the 1990s wa thi on .
so we'll just mention thi on in a littl bit more detail .
if we do the queri part of it first what we find out is that , so there's log queri normal .
now thi is actual , onli make a differ if you have long queri which might mention word multipl time .
if , realli , you have short queri , and no word is mention more than onc , that you're just go to be get a weight of on for word that appear , and zero for word that don't appear .
there's then idf weight of the queri term , and cosin normal .
the treatment for the document is the same except there's actual no idf normal of the document .
and that's someth that you might want to think about for a moment .
is that a bad idea ?
there's some reason to want to do that .
on of them is well , you've alreadi put in an idf factor for the same word in the queri , caus rememb that you're onli go to get non zero score for word that occur in both the queri and the document .
and that there ar some advantag in term of effici , of compress indic if you're not put idf in there .
let's take thi weight scheme and again go through a concret exampl .
so , we 're just gonna be work out the score for precis on document against on queri us thi weight scheme , but we'll do it in great depth .
okai , so our document is car insur , auto insur .
it's a bit of a fake document but we want someth short , and then the queri is best car insur .
so if we go to the queri first , best car insur , these ar it raw weight , and so then we're gonna scale those with logarithm scale but sinc each word onli occur onc , it stai on .
we then get the document of frequenc of each of those word which we map onto an invers document frequenc .
so , the rarer word like insur ar get the highest weight there .
we then multipli thi column by thi column , which end up look just like the document frequenc score , except for the word that didn't occur .
and then we turn that into a unit vector with cosin normal and so thi our final represent of the queri vector .
we then move to the document .
so the document ha some term frequenc that aren't just zero on .
so , we reduc those with term frequenc weight so that thei look like that .
in thi case there is no idf compon on the document , so the weight go to be exactli the same just come from term frequenc .
and then we again do cosin normal , which give us thi as our final document vector .
okai , so then to work out the score for thi document , for thi queri .
we're then work out the cosin similar , which is simpli the dot product of these two length normal vector .
and so that's then thi vector here .
where onli the bottom two compon ar non zero .
so we add those up and the overal score is <num> .
so the document is a good match for the queri .
though , i mean , do rememb when you're look at cosin similar , that becaus of the fact that the cosin kind of is sort of flat up the top here .
you know , flat descend .
it mean that you tend to get , for fairli similar document , that cosin score ar sort of bias fairli high .
so it's more import to rememb the order than the precis valu .
okai .
but that show you how we evalu that document .
and then we'd want to evalu a bunch of other document and then we'd want to rank accord to their cosin similar score .
a littl exercis you might like to do base on thi exampl is , well if you know what the idf score ar here and the document frequenc you should actual be abl to work out what is the number of document be us as a basi of thi exampl .
okai , now let's go through how we can work out cosin score in a vector space retriev system for a document collect .
thi is the rough kind of algorithm that we're go to want to us .
so what we're gonna assum here is that the queri is a typic short web like queri .
so we're onli go to be think of the word as either occur or not occur in the queri .
and also , we're gonna skip on other step then .
we're not actual go to do ani length normal of the queri .
and part of the reason of that is when you have a situat like thi , length normal of the queri is actual unnecessari becaus the queri vector ha some length and for whatev it is , the effect length normal would just be a rescal that appli to all queri document calcul and wouldn't chang the final result .
okai .
so given that background , what do we do ?
so we start off by have a score arrai for all document , which we set to zero .
and so we're go to accumul in here the score of a document for differ queri term .
and so these score ar often also refer to as accumul .
okai and then we're also go to have anoth arrai for the length of the differ document .
and so then what we do is go through each term in the queri , and we sai , well .
the queri term is actual just go to be on .
and then we fetch the post list for that queri .
so then , for each document in the post list , the term ha a frequenc in that document and we mai then want to scale that by do someth like log weight or someth like that and to give us our document weight for the term .
and then we ar do the compon for the dot product here and sum them into the score arrai .
so in .
essenc we're kind of , the outer iter here is for each queri term , and we're work out the compon of the cosin score for each queri term and accumul it in thi score arrai .
but we haven't actual done ani length normal of the document either yet .
so then , the next step is to work out the length of each document , and then divid these score by the length of the document .
so thi then doe the length normal for differ document size .
so given the assumpt i mention at the begin of , the queri vector is on zero and we don't need to length normal it , we have someth that is now order the same as a length normal cosin similar score for the document .
and so then for our rank , what we just want to return is the some number k of document , their id's or a represent of them , that ha the highest valu for score .
now if you think about it a littl , thi isn't quit yet a practic algorithm .
so that if our document collect is huge , we wouldn't actual want to build an arrai which ha a cell for everi document .
sai that we might have , you know , twenti billion document or someth like that .
and so system us method to work out which ar like document , and onli have accumul for those document .
and similarli at the end it's not a good wai to find the most relev document by simpli do a linear scan of thi score arrai .
and so there ar more effici data structur to do that .
but i hope that that's given you a gener idea of how we can build cosin similar score into a rank retriev engin .
so to summar , the essenc of what we've cover for vector space retriev is the follow step .
that the queri is repres as a tf idf vector .
the document is also weight as a tf idf .
the document is also repres as tf idf vector .
and then , to score a pair of a queri and a document , we're work out cosin similar score , which we straightforwardli us to rank the document with .
and then what we'll do in the first instanc is return some top k's .
for exampl , the top ten document accord to thi score to the user as their initi result .
and if thei ask for more , we can then show them more .
okai , so that's the gener idea of how we can start to build a tf idf rank retriev system .
in thi section i'll tell you a littl bit more about evalu the qualiti of a search engin .
there ar mani measur for the qualiti of a search engin .
there ar technic on such as how fast doe it index , and how fast doe it search .
we can look at thing like the express of the queri languag , whether thei're abl to express complex inform need with thing like phrase queri , negat , disjunct .
peopl have other desir like have an unclutt ui and a system that doesn't cost a lot to us .
all of these ar measur that ar measur .
that we can quantifi them and we can get some , some kind of score of what is their good .
but in practic all of those measur while import tend to be domin by anoth measur of user happi that , is the user happi in us thi search engin ?
and speed of respons and size of the index ar certainli factor .
but by themselv , blindingli fast useless answer won't make a user happi .
so a huge part of user happi is , ar the result return the result that thei want ?
and so that's the metric of relev of result to a user's inform need .
i mention thi right at the begin , but just to reiter onc more , when evalu an ir system that we evalu with respect to an inform need .
so an inform need is translat into a queri , and that's what the ir system actual run .
but relev is assess rel to the inform need , not the queri .
so for exampl , if the inform need is the person's look for inform on whether drink red wine is more effect than white wine for reduc your risk of heart attack , thei'll come up with some queri .
for exampl it might be wine red white heart attack effect .
and that will be submit to a search engin .
and in evalu the effect of the search engin in return relev result , we're not ask ar the result that the search engin return document that simpli have those word on the page .
rather , we're sai do these document address the user's inform need .
okai , well how can we go about do that ?
well , if the search engin return a set of result .
well , then , what we can do for evalu is if we start off with three thing , if we have some benchmark collect of document that we can us for evalu .
and we have some benchmark set of queri , which ar in some sens repres of inform need of interest .
and then we've gather thi third thing which is assessor judgment of whether particular document ar relev to particular queri .
commonli in practic thi can't be a , assembl exhaust , certainli not if the document collect is larg .
but at least for a particular set of document that ar return by particular search engin , we can get the assessor to judg whether those document ar relev to the queri .
well , if we have a result set with just these three thing , we're in busi becaus we can us exactli the same measur that we look at previous precis , recal , and the f measur that combin them .
and these ar suitabl good measur for exactli the same reason that thei ar good measur for when we're talk about thing like name entiti recognit .
that normal onli a few document will be relev to a particular queri , and so we can measur that much better by look at these measur of precis and recal .
but what if we've now move on to a search engin that return rank result .
we can't just , total straightforwardli us these measur of precis , recal , and f measur , becaus the system can return ani number of result .
in fact the number it return normal depend on how often we keep on click ask for more .
but if we sort of look at ani s , initi subset of the result , we can then work out the precis recal for that subset .
and then by put them togeth we can come up with a precis recal curv .
let's look at how that work .
so here ar the first ten result for a search engin , where we've label each result as either relev or not relev accord to an assessor's judgment .
so then we can take ani initi sub sequenc of these document and work out a recal and precis .
so for the first document , the system got it right , it's a relev document .
and let's assum that overal there ar ten relev document in the collect .
so it's gotten on out of the ten relev document , and so it recal is <num> .
and well sinc that document wa relev , the system wa right on the first answer , it precis is on at thi point .
well , the next document wa not relev so the recal of the first two document , we're down to here now , is <num> , and the precis is now <num> .
anoth not relev document so the precis is , i'm sorri the recal wa still <num> , and the precis is now drop to <num> . <num> .
and the , if we look at the set of the top four document we've now found two of the ten relev on , so recal is <num> , and our precis ha gone back up again to <num> .
the fifth on is also relev so now our recal is up to <num> , and our is up to three out of five , <num> , then we can keep on go down .
mayb you gui could work out what some of these entri ar down here .
the other measur i want to mention , on of the most recent , most us recent measur is mean averag precis .
if we look at the rank retriev result order thi wai to give me a bit more room .
and so the first document return is relev .
the second on is not relev .
sai , the third on is not relev .
then a relev on , then anoth relev on , not relev , relev , relev .
let's sai those ar our top eight result .
what you're do for mean averag precis , first of all you're work in averag precis for on queri .
and so the wai you do that is by sai , let's work out the precis at each point that a relev document is return , 'caus that's when you're increas recal .
so here , the precis is on .
here , there ar now four document so the precis is a half .
here there ar five document so the precis is <num> .
here there ar seven document of which , four of them ar relev .
so that's .
thi is around <num> . <num> .
you gui can correct my arithmet .
here we now have eight document , of which five ar relev , and that's <num> . <num> and then what we're do to work out the mean averag precis is we're kind of keep on calcul those number .
in practic , normal , thei aren't calcul exh , exhaust , but thei're calcul up to some point , let's sai <num> .
and then you're calcul an averag function of all those number .
and that's then the averag precis for on queri .
you then calcul the same averag precis for all the other queri in your benchmark queri collect .
and you would again take the averag of that and that then give you mean averag precis .
so , in particular , thi is what's refer to as macro averag .
it's each queri that count equal in the calcul of mean averag precis .
so thi is a good measur that evalu to some extent , precis at differ recal level , while still be weight most to what the precis is like for the top few return document .
then at the level of across differ queri , it's give equal weight to differ queri , which tend to be a us thing to do becaus you ar alwai interest in how system return , work on queri of rare term as well as queri of common term .
so thi is a pretti good measur to think about us for evalu inform retriev system .
okai there's even more method that i could talk about , but that's probabl a good sens of how about , how to go about evalu the perform of a rank retriev system .
in recent these dai there's alwai a lot of talk of probabilist model and machin mine .
but if you actual look at larg system under the hood what you'll almost alwai find is that thei also make quit a bit of us of regular express in variou place .
and for mani task it turn out that regular express ar just a veri practic and capabl wai of specifi variou kind of natur languag pattern .
i'm gonna show you on exampl of thi now by show how we us regular express for the english token insid .
stanford and op tool such as the passiron part of speech tagger or for the coran p suit overal .
okai here we ar with the code for the stanford english so what it is , is it's a larg determin stick .
regular express .
so what is written in is with a tool call j flex .
so j flex belong to a famili of what ar commonli call in comput scienc , lexor , which is just anoth word for token , which take a sequenc of charact and cut piec on token at a time off the front of it .
so that wa the origin lexor part of the unix , and then flex .
and then thi is j flex , which is a java compat version .
let's scroll down to where some of the regular express ar us to defin charact class .
often what you find is that mani of the regular express aren't actual veri complic , that thei're realli noth more than list that ar be put into regular express by put verticl bar in between for altern .
and so , for exampl , we see that in sever place here .
so here we have on for abbrevi month and here we have on for abbrevi dai of the week , and that continu on for some of these other on , like american state and variou other kind of person , name , titl , acronym .
down here .
but let go on a littl bit further to on that's a bit more interest than that .
okai .
so , here's on of phone number .
thi is the kind of ill document regular express that's a littl bit hard to actual get your head around , but ar much us in practic .
so , at the veri top level of thi regular express , thing ar divid up by thi altern right here , and .
the right hand side of the ordin , there's a , where the separ is be us as dot .
and so that on's separ out as consist us of dot , cuz otherwis it's easi for the regular express to go wrong and also recogn variou kind of number and other pattern .
and so that part of it is actual the easier part .
so we can have at the begin , option , the us of plu sign , which ar us in europ and most of the rest of the world as an intern prefix by counti code .
and then we .
have the countri code here which is just number of the rang two to four .
and then all of that is option .
and then after that .
we've got a first set of number , which can be the area code , the dot then the second set of number , which i guess histor is the exchang and then final the third set of number and so these set of number ar then be given a length .
so thi ha to be between three and four number , thi ha to be three and five number and the area code ha to be two and four number and so those rang ar chosen so that thei'll work with the phone number of a bunch of the countri around the world .
but if you know well your intern phone .
you'll realiz there actual ar some case that won't still be recogn by those .
so what then if we go .
to the left hand side of the regular express .
it's effect do the same thing but just more complex .
so that the first part of it is again go to recogn .
thing like option countri code , so you can see the same piec over here .
countri code .
but it's allow in some other possibl .
so here we've escap so you can actual sort of have some number that ar put insid of parenthes and further on we have got thi charact class .
we're allow a varieti of separ apart from period .
so we can have dash which again need to be escap .
there can just be a space or there can be a non break space for a non brake space .
so overal thi .
will allow it to recogn a bunch of format for phone number .
so it'll recogn almost all american phone number , and gener doe pretti well with thing like uk and australian phone number .
if you want an exampl of where it doesn't work , the normal phone number format in franc is you just have pair of digit with space in between them .
and that's not includ here .
and the difficulti .
isn't sort of write a regular express that match that .
it's in thi context of make their express match her of manag to write on , which want to alo wrongli match variou other thing , such as number that just appear as a sequenc of number for some other reason .
well i hope that's given you some idea of the us of regular express and nlp system .
if you polk around anoth nlp system , i'm sure you'll find lot of other exampl .
commonli when peopl want to match particular pattern , whether thei be pattern at the level of word or pattern at the level of part of speech , thei can just be veri conveni and practic method to solv mani practic task .
okai .
let me now talk about how we evalu text categor .
and in do thi , i'm gonna come back to the concept of precis recal that we introduc inform befor .
but defin them formal , and show how thei get put togeth into combin measur , the f measur get appli to text classif .
but it isn't onli appli to text classif .
you'll see these concept come back again and again as wai of evalu for task in natur languag process .
the start point for understand these measur is , the follow two by two conting tabl .
and so for ani particular piec of data that we're evalu , there ar essenti four state .
on on axi we're choos whether thi piec of data correctli belong to a class .
or whether it doesn't correctli belong to a class .
so , for exampl , if we're want to decid whether a piec of email is spam , we're .
it either is spam , which is the correct class , or it's not , which is the not correct class .
and so , on thi axi , we're .
describ the truth , now what we've done is built a system that tri to detect the truth and so secondli we're go to look at the statu of our system , so our system could be sai that thi piec of data is spam , or it could be sai that it's not spam .
and so we then go to take these four thing and look at the four possibl that occur , and here thei ar .
so if we're look for spam on possibl is the truth of it that it is that it is spam , and that we said it wa spam .
and so that's then refer to as a true posit .
anoth possibl is that .
our system thought it wasn't spam even though actual it wa .
and so that's then a fals neg .
it wa treat as neg fals .
on the other side of the dial it's possibl that the piec of email wasn't spam .
in that case there ar two possibl .
our system mistakenli thought it wa spam , so let's sai a fals posit , we're classifi someth posit wrongli , but the other possibl is that wasn't a piec of spam and our system said it wasn't a piec of spam and then we're deal with the true nake okai .
so , if we're do thi kind of classif with two class that ar , perhap , about equal common in your email , spam and non spam .
it seem the reason thing to do is just to look at accuraci .
so for accuraci , the on that count as you're get the answer correct ar these on .
so if you wanna work out the accuraci , the accuraci equal the true posit , plu the true neg over .
all four class .
the true posit plu the fals posit plu the fals neg plu the true neg .
in mani applic , accuraci is a us measur for system .
but there's a particular scenario in which it's not a us measur .
and that scenario is when you're deal with thing that ar uncommon .
so , for exampl , suppos that we're want to detect mention of shoe brand in web page .
so the correct class is that someth is a shoe brand .
and the , the not class is anyth els .
well , if you're just look through random web page or tweet or someth , look for mention of shoe brand , probabl .
i don't know .
<num> percent of the word you encount ar not gonna be the name of shoe brand .
and so what doe that mean ?
what that is like to mean is that , perhap , in total , let's sai there ar .
ten mention of a shoe brand in a <num> , <num> word sampl and <num> .
thousand nine hundr nineti word that ar not shoe brand .
well .
if we build a classifi , presum even if it's a mediocr classifi , we're go to sai most word ar not a shoe brand .
and if we go straight to the limit case , on possibl of our classifi is it could sai that no word is a shoe brand .
so , it doesn't select ani word as shoe brand and it sai that all <num> . <num> word ar not shoe brand .
and so that mean the number of true neg is <num> .
thousand nine hundr nineti , but there ar few .
fals neg wa , were shoe brand .
but the number of them is ten .
so if we then actual work out the accuraci of thi system , you'll see that the accuraci of the system is <num> . <num> .
so , we have a <num> . <num> percent accur system .
but the system ha done precis noth .
thi is veri easi system to write .
you just write on line of code that sai return fals for ani token , and you're done , and yet it accuraci is amaz .
so what's clearli miss here is that we just not deal plai with what we want to do .
so what we want to do wa detect shoe brand .
so what's all import to us is the ten token which ar instanc of shoe brand .
and so what we want to do is come up with an evalu metric that is much more focus on ar we detect these veri few word that ar the name of shoe brand .
and so that's what the matrix of precis and recal do .
tell you of the thing that you ar select , ar thei , thei correct thing .
so precis is sai , of the thing that you ar select , which is that row , what percentag of them ar correct thing , thi column .
so precis is the number of true posit out of all the thing that you select , the true posit plu the fals posit .
and then recal is the opposit measur , so .
for recal .
we'll sai of the thing .
that ar correct , what percentag of them did you find ?
so , for recal , the numer is , again , the true posit .
but thi time , the denomin is the true posit plu the not select fals neg .
and so note how look at these measur solv the problem that we had last time with the shoe brand .
becaus the fact .
that thei .
is <num> , <num> token that ar true neg , is now have no effect on these measur at all , and so what would happen in our previou case where there were ten token here , is , and zero token here , sinc our classifi said wa everi word wasn't a , a shoe brand , that what we'd find is that .
we'd sai that the recal of that system is zero .
it's find none of what we want to find .
and basic , we can sai , a system with zero recal isn't interest .
and so thi suggest we've actual , what to do better by return some stuff .
so , what we'd like to do is label some thing as shoe brand .
so we could revis the system , and when we revis the system , mayb what we'll do is , we'll have it return <num> thing as the name of shoe .
and so it will , perhap , find eight of the shoe brand .
but it'll make some mistak and claim some thing as shoe brand that aren't , so there'll be <num> token over here .
and then there'll be two token down here .
and then the remaind of the token .
<num> will be down here .
so then , at the point , we can sai , well , the recal of our system is pretti good .
so it's now find eight out of the ten instanc of shoe brand .
so it recal is <num> .
but the problem is that recal came at a cost becaus it's now also claim lot of other thing the shoe brand that aren't .
so the precis of the system is now eight .
over eight plu <num> equal eight out of <num> , which equal twenti percent .
on in five thing it return is correct .
and so for a lot of practic applic thi kind of precis is go to be too low .
it depend on whether you're realli interest in find refer to shoe brand , and ar prepar to have a human be go through and check them all individu .
but if you want to do someth more automat , twenti percent is too low of a precis .
but in thi , we see the secret of the tradeoff that balanc between recal and precis .
cuz almost inevit , if you ? re go to increas your recal , and find instanc of someth , you're go to make some mistak .
and so your precis is go to go down .
and the more you try and boost recal , the more your precis is go to be start to drop .
and so peopl ar trade off precis and recal .
so that's a good thing about have these two measur .
becaus .
can discuss the tradeoff and sai how import to someon is the precis of what's return versu how import is it to find all of the stuff .
to have high recal .
and that's a trade off that is plai out differ in differ applic .
so in variou applic , such as for thing like legal applic , where you want to find all of the appropri evid , such as in discoveri procedur .
but what you realli , realli want to be do is have a system that's high recal , that find as much of the relev stuff as possibl .
where in other context , where what you're go to do is just show a sampl of stuff to the user .
you might be more interest in the stuff that's shown to the user be stuff that is correct , that look good .
that your precis is high .
and it doesn't realli matter that you're onli show to the user on tenth or <num> <num> of the thing that do satisfi their queri .
so , sometim that explicit tradeoff is realli us , and it's a realli good concept , to rememb when you're build system .
cuz in practic , whenev you're build on of these system , you're choos some tradeoff point as to how much you're emphas precis or recal .
and differ tradeoff point ar appropri in differ applic .
but sometim , that's a slightli annoi thing , have these two measur .
becaus if peopl ar want to compar system and sai , which on is better , then you need some wai to compar .
find these measur , and so that's been thought about as well .
and so the standard wai that's been propos to combin these measur , is someth that's call the f measur .
in disciplin like inform retriev and name densiti recognit .
so what is the f measur ?
what the f measur is , is thi neither more nor less than a weight harmon mean .
so at thi point we have to kind of revis a littl bit of math on mean .
so you doubtless rememb the arithmet mean , which is just the averag of two thing .
and you perhap also rememb .
a geometr mean , but in addit to both of those , there's a harmon mean .
and in a harmon mean , what you do is you take the reciproc of two quantiti , and then add them .
and then take the reciproc of that .
and if you work through in more detail , what that doe , the harmon mean end up as a veri conserv averag .
so if you put in two number into a harmon mean , it's fairli close to the minimum of the two number .
not complet at the minimum , but it's nearer to the minimum than either the arithmet or geometr mean .
then in particular .
for thi f measur we're allow to put weight on these two term and so thi is thi alpha factor , so the weight is how much to pai more attent to the precis or how much to pai more attent to the recal .
so , if you ar in an applic and you know the precis is much import to you , you can actual express that util tradeoff by put a particular valu of alpha in that express that .
so thi formul here veri clearli show that the f measur is the weight harmon mean .
but actual that's not the formul that you normal see in book .
the formul that you normal see in book is actual thi on , which is i guess just a littl bit tidier to write .
and these two formul ar relat , and thei do work through that in on of the exercis .
you can work out the relationship between alpha and beta .
but effect , thi formula also express a weight harmon mean , and it also .
thi control paramet , beta , as to how much .
emphasi is be put on precis versu recal .
so control paramet is us in the absenc of other evid , the mostli commonli us thing is a balanc f measur .
and so the balanc f measur get refer to as the f1 measur .
and what that's mean is that you're set .
theta equal to on .
that then give you equal balanc between precis and recal , which correspond in turn to alpha be a half .
and if you do the balanc def measur , thi formula here simplifi to just thi formula .
and so thi is realli the thing that you'll see most common .
so if you don't rememb ani of the rest of thi stuff , what you should do is rememb thi littl formula for the f measur , two time the precis time the recal divid the sum of the precis and the recal .
so that should give you a good sens of what these measur of precis record ar , how thei ar combin into the f measur , why thei're us , and how we us them to evalu text classif .
in thi section of the cours , we're go to start look at discrimin model .
we're gonna look at how thei contrast with the gener model that we've look at so far and in particular go through a detail examin of maximum entropi model .
so far we've been look at gener model .
in particular we've look at languag model and na ve bay model .
but in addit to these kind of gener model , there's now much us of condit discrim model in natur languag process .
and also in relat field like speech and inform retriev , or machin learn , or gener .
and that's for a coupl of good reason .
these model tend to have veri high accuraci perform .
and also , thei're linguist interest becaus thei make it easi to includ lot of linguist import featur .
and henc thei easili allow the build of languag independ re target nlp system .
so let me now contrast joint versu discrimin model .
in both case , we're go to assum that we have some data d , c of pair observ of the data d and the hidden class is c .
and so the defin characterist of joint model , is that thei place probabl over both the observ data , and the hidden stuff .
so , we have the probabl of c and d .
and commonli , the wai that that's done , is there's a gener model .
so the gener model gener the observ data from the hidden stuff .
these kind of joint or gener model compris all the classic statist nlp model .
not onli the n gram model and na ve bay classifi that we've seen alreadi , but also hidden markov model , probabilist context free grammar , and the ibm machin translat align model .
in contrast discrimin or condit model more directli target the classif decis that we want to make .
formal thei put probabl distribut that ar condit , the probabl of the class given the data .
discrimin model includ logist regress , or , in gener , the whole area of condit log linear model .
includ , in particular , maximum entropi model , and their gener to sequenc , condit random field .
mani other machin learn method , such as svm , perceptron , ar also discrimin classifi .
but thei're not directli probabilist model .
on wai of show the differ between the two model class , is by mean of the graphic model diagram that ar us for probabilist model in gener .
in these diagram , we draw circl for random variabl , and line for direct depend between them .
and some of the variabl ar normal observ and other ar hidden .
and then for each node is , like , a littl classifi , base on it incom arc .
if we draw these kind of model , what we find is that , for a na ve bay model , the pictur look like thi .
so at classif time , we're observ the variou word of the document , which is our given data .
and base on those , we want to predict the class .
but in term of the probabilist model , their probabl factor ar a prior probabl of the class .
and then the probabl of the word in the document , given the class .
and so , that we have thi gener direct in which the word ar gener from the class , rather than actual predict what we haven't observ .
in a discrimin model such as logist regress the situat is the opposit .
we've again observ the word of the document and want to predict the class .
but thi time we're directli put a probabl over the class given all the data we've observ d1 , d2 , d3 .
so in gener model , we look at the joint probabl over the data and the class .
and what we try and do is attempt to maxim thi joint likelihood .
and as we've alreadi seen , that for categor model , it's trivial to optim the joint likelihood .
all we do is take the rel frequenc of differ event .
that is , we count how often differ thing occur , and then we divid by a normal denomin , and we're done .
so , that's veri practic for estim .
in contrast , the condit model .
we want to work out these probabl of c given d .
and so , in a condit model , what we'll do is attempt to directli maxim the condit likelihood .
the probabl of the class observ given the data .
as we'll see , thi turn out to be much harder to do , but is perhap more us becaus it's more directli relat to classif success or error .
thi slide give some initi motiv for be interest in condit model by show that thei have high perform .
in thi slide i show the result of some experi done by dan klein and me in <num> .
the task here wa word sens disambigu .
but essenti that's the same task of text classif that we've look at earlier .
that you're want to choos on of a number of sens of word .
such as for instanc star , whether it's a rock star in thi astronom aspect object base on the evid of the word around it .
that you see in the document .
and in these experi , what we ar veri care to do is to set up two model which were exactli the same in all respect .
thei had exactli the same featur , exactli the same method of smooth the data , but differ onli in whether we were do condit estim or joint estim .
and what do we see in the result of these experi ?
what we see is , well , if we just skip first of all straight to how it perform on the test data , we see the good new stori , the condit likelihood .
the condit likelihood is give us perform that's <num> percent better then what we got from the joint likelihood naiv bay model , so thi joint likelihood model is a naiv bay model .
and so that's a nice gain and someth that is veri appeal when build practic nlp system .
incident , we can also notic someth interest by look at perform on the train set .
if we look at perform on the train set , the joint model get <num> , which goe up to <num> percent for the condit likelihood model .
you could think that that's good new .
but it's also actual a caus for worri .
what you will find is that condit model can veri easili memor much of a train set , and so thei're veri prone to overfit by memor too much what happen in receiv train data , which mai not reappear in test data and so we'll go on and look at how to control thi kind of overfit later on in thi discuss .
okai , that's a motiv introduct for discrimin model and discrimin estim .
in the next section we'll start look at how we can defin the featur that ar us in these model , and then go from there into the detail of how their estim .
in thi section i'm go to look at how you can get featur from text that you can us in side discrimin model for variou kind of classif task .
so in these slide , and in most max work , by featur what we mean is an elementari piec of evid that link between what we observ , which is the data piec d and a categori c that we want to present .
so a featur .
is a function with a bound real valu , so what we can do is write that we have a featur f , which is map from .
the space of class , and piec of data , onto a real number .
so here there ar a coupl of exampl of the kind of featur that we have .
so let's look at these featur with respect to some particular piec of data .
okai , so the first featur , the purpl featur is sai that the class is locat so in these , so in these exampl piec of data , i'm assum that i'm look at the last word in the sequenc and then the class about that is be shown in orang .
so the featur exampl where the class is locat so that the two piec of data on the left and .
the previou word is in , and the word is capit , so .
all three of those criteria ar true of these two piec of data .
but aren't true of the two piec of data on the right .
realli for multipl reason , both becaus the class is wrong and the proceed word is also wrong .
okai , so then if we go on .
and look at the second featur .
it's again sai the class's locat .
and for the rest of the featur , you can put in just anyth that seem like it will be us .
so we might think it's a us featur to know whether a word ha in it someth like accent and latin charact , so it's not just plain , a to z but ha some other charact in there .
so here , you know , we've sort of notic that some name have accent letter in them .
so thi featur , it will be true , thi piec of data , and .
again , it's not true of ani of the other piec of data .
okai , so then let ? s look at featur three , so featur three set is the class is drug and the word end in c .
so that's a featur that's true of thi piec of data but not ani of the other .
so in gener , that , that's what's happen , we're imagin for our train we'll have data which alreadi give us the right answer .
it , thei'll be some word , and that thei will have a class .
and so , in thi exampl , we're then also assum that we have a particular posit that we're look at , so that thi is the basic word w and then when we want to .
that we can refer to someth like the word minu on here , to ask featur about other nearbi word .
and so then , our featur will then ask some question .
about the class , is it a drug ?
and some question about the word .
when , normal , it won't sort of just ask for the complet sequenc of word .
but ask , might ask for featur of them , like an individu word or someth in between , like in thi exampl we gave , where we're just look at whether the word end in the letter c .
at that point .
the next thing is that the model will assign to each featur , a weight .
where the weight is a real number .
so it might be someth like <num> , or <num> .
so thi isn't the valu of the featur , thi is the weight of the featur .
and so a posit weight vote that thi configur is like correct .
it's the kind of thing that happen in real text .
so that's someth like , for featur on , well , if the preced word is .
in and .
the word is capit .
well , that's inde a good indic for the word is the locat .
so if a featur won that match these two piec of data , we'd expect it to have a posit light .
then the other choic is you can have a neg weight .
so the neg weight vote that a configur is like incorrect .
so , we could have anoth featur , for exampl , which wa kind of like featur on , that might be featur fourteen , which said the class equal and the rest of the condit wa the same , and so that featur would match a differ classif , where we had in arcadia and we were sai arcadia wa a drug .
and featur fourteen would match that configur , but what we'd like to sai , is that's unlik to be correct , and we could express that by give a neg weight like . <num> .
later on when we're work with featur we can crucial make us of two expect .
so an expect is an actual predict count of a featur fire and we have two expect , on is from our supervis data .
we can just actual look how often a particular featur is satisfi by piec of data and so i'll refer to that as the imperi count with the imperi expect of the featur .
so that's thi gui .
we simpli just look through everi piec of .
data that we ar train of and we ask .
is the featur true of that piec of data and we count the number of time it's true and that sound per the expect .
the other expect is the model expect of the featur .
we'll look at thi more in a minut , but what we're go to have is a probabl distribut over pair of a class and a data set .
and so what we're gonna do is us that distribut we're gonna consid all class and piec of data and then sai , well given that what is the expect valu of f base on the probabl of differ configur and what the valu of f is to those configur .
in the particular case of natur languag process applic , what we find is that usual a featur is of a veri particular form .
so the featur consist of firstli an indic function , a ye no boolean match function of properti of the input .
and secondli , it specifi a particular class .
so the arbitrari featur is just .
a featur of a class state of pair and it's return .
some real number .
but in practic , the featur that we defin have thi veri particular form .
so we have a match predic against the data .
so that's someth like ns and the letter c , and there's a capit word .
and that's conjoin togeth with sai , like , particular class match .
and the return valu of the featur can , in gener , be a real number .
but our featur here ar .
logic bullion predic .
and so their return valu is either zero or on .
now , everi featur that we're go to present in thi class is of exactli thi form .
and that's true of <num> percent of how these featur have been us in natur languag process work .
and so we can equal sai that phi d is a featur of the data d , when , for each class , cj , the conjunct phi d , and c equal cj is a featur .
of a class data pair .
and so that's how , fi , featur pair .
so a lot of time , what we'll find is we'll think in term of these phi d , featur .
but you should rememb that the actual math and program we write is go to be work in term of these fi featur , where thi i index is be particular to both match a data predic , and some particular class .
but the , nevertheless a good wai to think about thing is that each fit , featur identifi a subset of the data and suggest a label for it and that's what we saw in the exampl befor .
in featur base model , the decis about a data point is base entir on the featur that ar activ at that point .
so let's look at a coupl of exampl applic , and the kind of featur that get defin for them .
on of the simplest case is text categor .
so we have an articl which ha data , like stock hit .
a yearli low and it's assign some topic class here , busi .
and so typic in these kind of text categor applic the featur ar just the word that occur in the articl .
so each word type that occur in the articl will be a featur .
of a five featur and so then a complet f featur will be , the word stock occur and the class is busi or the word stock occur and the class is sport .
word sens disambigu is the task of determin the sens of the word in context .
so , for exampl , whether usag of the word bank is refer to the bank of a river , or to the financi institut .
in deci , make a classifi that decid sens of word .
in practic , it come out kind of like a text categor exampl , where you regard the context of the word as thi littl mini document around the word .
so , here we have the observ data , the sequenc of word .
here's the word we wanna and here it ha the particular class and the train data .
thi is an instanc of the monei wand .
so , we could just us the same kind of featur as here , a bag of word , set of featur of the word around and the context .
but we can also us more particular featur that look at particular thing in the context .
so we could have a featur that sai , the word befor is restructur the word afterward is dead .
and we can have other kind of , featur if we want to , too .
like we can have a featur that the length of the sentenc is twelv .
you should realli think that you can make featur in ani wai you want that will be good in predict what the class is .
and commonli in practic what you'll find is word sinc disambigu system have both bag of word featur like thi , and thei have featur that ask about particular word that ar adjac on the left and right .
and it's been shown that actual have both of those type is us .
here's on more exampl that's slightli differ , which is when we're work out the part of speech of a word in context whether it's a noun or a verb .
and so , here's fall which can be a verb , to fall down , or a noun .
and in thi particular case it actual class is noun .
so if we're want to do part of speech take we're like to want to know particular thing about a veri narrow content and thing in particular posit .
so we'd want to know what word it is that we're tag .
the current word is fall .
it's like to be .
would indic whether it's a verb or a noun , what the previou word is , and thi exampl .
point in the direct of when we want to do a sequenc of classifi decis , where we actual wanna classifi each word .
and that's a topic that we'll return to later , when we show how to build sequenc model .
but the easiest wai to think about it is that we're just go to give each word a part of speech in turn .
so onc we're decid thi word's part of speech , we can assum that we've alreadi given part of speech to the word that preced it .
so we can also us , as a featur , for decid thi word's part of speech , what the part of speech of the previou word wa .
and so thi is thi featur here .
yeah , that sai the previou tag is adject .
so , look here at a coupl of exampl in more detail .
let's just go through a coupl bit of work on featur base classifi .
so , young and ol is on well known piec of work for do text categor .
and so , the featur that thei us ar precis bag of word featur .
so , the featur ar precis presenc of a word in the document and the document class .
thei actual don't us ever word that occur in the document , document .
and so .
common refin is to do a process of featur select , where you pick out a particular subset of word that ar deem to be reliabl indic .
you might be drop veri rare word , or also drop extrem common word , sort of view to be semant empti .
the test set that thei us is a well known text categor data set , the reuter's new wire data set .
and here ar just some indic result of what thei find .
so thei build a naiv base model , which get <num> percent f <num> across the differ class .
and then thei compar sever discrimin classifi .
thei've got a linear aggress model .
now , despit the fact that .
linear regress model shouldn't be right for do these kind of categor text predi predict task becaus of variou reason such as the number go out of bound and not be probabl .
actual , what you can see is that the linear regress work alreadi much better than the naiv bay model .
in fact , what thei find is that it work almost as well as the logist regress model .
the logist regress model is a fraction better , but actual not veri much .
thei also train a support vector machin model which is anoth veri common popular discrimin classifi .
us wide the text base model like text categor .
it perform is almost distinguish differ from the linguist regress model .
and .
so that's someth .
interest that come out of a lot of recent work .
so you can see that the big differ here is between the naiv bay model and all of the discrimin model .
all the discrimin model do much better and in turn the differ between the discrimin model ar veri small , and so a lot of the time , ye , you want to us a appropri model , but it doesn't actual matter veri much which model you choos .
what's go to matter veri much more is the qualiti of the featur that you defin .
anoth thing that the paper emphas that we'll come back to is the import of regular or smooth for success us of discrimin model .
that wa someth that wasn't us in much of the veri earli work in discrimin model that turn out to be veri import .
there ar mani place where you can us a max m classifi .
it's whenev you have some data point that you want to assign to on of the number of class .
so let's just look thru a few more exampl .
so here is a fairli straightforward on , which is where what you want to do is decid whether a period is the end of sentenc or an abbrevi .
so , you might have someth like i . e . , u2s .
and we have thi period here .
and what we'd like to sai is that thi period isn't the end of sentenc .
and we can kind of think that there ar some wai that we could tell that , by see thi abbrevi beforehand .
and so we'll be want to make featur that look at the stuff to the left of the period .
and perhap also thing to the right of the period , thi word isn't capit .
we've alreadi talk a bit about sentiment analysi , as to whether someon is give a posit or neg review .
and we can make a discrimin classifi for that .
by put in featur for the word and perhap not the kind of featur like word pair featur , differ part of speech and thing like that .
preposit phrase attach is the task we're work out with the preposit phrase .
like with a beard , is modifi a particular noun or verb .
and that's get us into the realm of syntact decis .
or more in gener , on wai , in gener , to make pars decis for the structur of the sentenc , is to build classifi for the particular decis that ar involv in make the classifi .
okai , i hope that's given you a good sens of what the featur ar , thei're us modern discrimin nop system and some idea of the kind of applic to which thei have been appli .
okai , now let's look at how these featur ar us insid a classifi .
and in particular i want to introduc the notion of linear classifi .
our featur base maxent classifi ar linear classifi .
and so , what that mean is that , at the end of the dai , what thei're go to do is go to have a set of featur .
and then it's gonna calcul a linear function of those featur , which will end up as a score for a particular class .
and the wai that we do that , is that for each featur fi , we assign it a weight lambda i .
and , so what we're gonna do is then consid for an observ datum , everi particular class that we can give it , and then we're gonna work out what featur match against that datum and therefor what the weight of the vote for the class ar , in term of thi land dry weight .
so let's look through a concret exampl .
so thi is again the exampl for where we're gonna be want to determin the class of thi word , we have our choic here , a person , locat or drug and so what we're gonna do is consid which of the class get the most vote in term of the three featur that we defin befor ?
so the vote for class is just go to be the sum of the weight that ar assign to each of the featur .
so if you rememb the three featur from befor , the first featur , look whether the preced word wa in .
whether the word is capit , and then whether the class is locat .
so it's not gonna match here , but it is go to match over here .
and so that's a featur that would tend to pick out locat .
so we'll assum that it ha a posit weight of <num> .
the second featur wa then also featur that match on locat and look for an accent latin charact here .
so .
it will match against thi datum , but not against the other two datum , and again , and so in gener , i'm assum that , that featur , at least for american english , will have a neg weight , becaus you're more like to see accent in thing like person name , so it might have a weight of <num> .
okai .
then the third featur that we had defin wa that .
the class wa drug and the word end in c .
so of these three datum the on which will match is thi on here .
and it's not true in thi case .
but in gener i think that's like be true becaus quit a few drug name like zantac , intimaci , or not that mani regular name do .
so , mayb we'll give that a weak posit vote and sai that , that featur ha <num> .
and so then what we're go to be do is that we're go to be choos the class that ha the highest total vote .
and so , there three class choic here .
so thi .
class choic of person actual match no featur , so it total vote is zero , which is that kind of neither a posit nor a neg prefer .
thi class choic of locat , in aggreg , had a vote for it of <num> and thi class choic of drug , in aggreg , had a vote for it of <num> .
and so what we're go to choos is the class which maxim thi vote quantiti is go to be locat .
there mani wai to chose the weight of the featur in our classifi .
so , for exampl algorithm what thei do is look for a current misclassifi exampl .
and then thei nudg the weight in a direct that will correct the classif of that on exampl .
anoth popular form of discrimin classifi support vector machin , which we're not gonna cover in these class , but what thei're try to do is creat a lot of distanc between exampl of differ class by adjust the featur weight to achiev that direct .
so thei're call max margin classifi .
but the classifi that we're gonna look at here max m classifi .
which ar in a famili which ar refer to as exponenti or log linear classifi .
in gener thi famili ha lot and lot and lot of name .
so , there's thing in the class also refer to logist classifi and gib model which ar veri close relat to max m model .
and so , the idea .
of thi class is that what we're gonna do is we're gonna make a probabilist model out of the linear combin that we alreadi saw .
the sum over the dot product .
so it's a dot product , the lambda f dot product , which is the sum of lambda ifi of c and d .
so how do we do it ?
well what we're go to imagin do it is kind of in the simplest possibl wai .
so here's the sum of lambda ifi .
of , which is a function of c and d .
now the problem with thi linear combin is that it might come out as either posit or neg .
and that's bad if you're a probabl .
and so what we're gonna want to do is we're gonna make that alwai posit .
and the wai we're gonna do that is take an exponenti of it .
so we're gonna take e to thi power .
so thi is someth that will alwai make the vote posit .
and thi form i'm show here , so it's e , that's , you know , the <num> . <num> etc .
and then you're rais the featur dot product to that , so you can all associ as e to the lambda dot f , where these ar all vector of featur .
okai , so now we've got someth that's a posit quantiti but it's not necessarili .
a probabl .
so we're go to make it a probabl in the simplest wai possibl too , which is we're go to normal it .
so thi is some kind of score for on class .
and so then what we can do is work out the score for everi class and divid through by it .
, okai , and then we're gonna sai that thi quantiti here is the probabl that we assign to a class .
probabl of a particular class for a datum given a certain set of paramet , where thi is our vector of all the paramet .
and so we can work out a concret exampl so that for our previou exampl where we had the probabl of choos locat given in quebec .
well , what we're go to work out is for our numer we've got the featur that match .
so we're work out e to the <num> . <num> .
and then we're go to divid that through by the total vote for all of the class .
so , on class had no featur match .
the second class had the singl vote of <num> .
and then , here's the on for locat , which is e to the <num> <num> .
and you can work that out on your calcul and it come out to be equal to about <num> . <num> .
and so these kinda there .
you can also simplifi them .
so have expon is just the same as multipli .
so you can also think of thi as on point eight time e to the minu zero point six .
and so that's the sens in which each of these featur give them extra multipl term that chang the probabl .
that when you add a featur you're ad an extra multipl term that chang the probabl .
so if you work through all of the exampl of the differ class , i'll just skip and show the math , that what you get is , that these ar differ probabl and so note that we're give a probabl of each of our three class choic and togeth that thei do add up to on , and that's precis by the normal that we've defin here .
so these weight lambda ar now the perimet of a probabl distribut and we're combin them .
like thi function here .
thi function is also refer to as the soft max function and the reason for that is becaus when you're us thi expon function to make thing posit , thing get bigger .
so , if you take someth like eight to three , that's approxim <num> or someth .
and so , the effect of that is the featur with the strongest vote domin more strongli in thi form here .
and , so sort of like a soft maximum , so it will give you a valu .
if it's much strongli determin by the biggest vote .
so exponenti model .
what we're want to do is sai , given thi model form , what we're gonna do is try and choos paramet in lambda i that maxim the condit likelihood of the data accord to thi model .
so we've defin probabl over data and class .
and we're go to maxim that choic of class , accord to that probabl distribut .
so a featur of these model is that we construct not onli a classif function , but also probabl distribut over classif .
so there ar mani good wai of discrimin class .
so , svm boost percept , mani machin learn algorithm will give you a wai to just distinguish between class .
but these other on aren't as natur and inher thought of as distribut over class .
their peopl have thought of wai to extend them in thi direct .
you mai have seen logist regress or multiclass logist regress in our statist for our machin load model .
so , that mayb divid peopl .
if you haven't seen thi befor , don't worri about it . 'caus the present and deriv that we show here ar complet self contain .
but on the other hand , if you have seen these model befor .
that someth you might want to do is just think about how , what is present here is a littl bit differ .
and it's effect differ in two wai .
on is the choic of parameter .
so the parameter is slightli differ than the parameter that's normal shown for multi class logist regress model and statist .
and there ar kind of pluse and minus to the present here .
the minu is it's slightli statist ineleg , becaus the model's over parameter .
but the plu is that thi style of parameter is actual veri take thi to nop model , which have the properti that thei're alwai a veri larg number of featur most of which ar inact for ani particular data .
but secondli the thing to observ is that the wai thing ar formul here is in term of featur function , featur function of both .
the observ data and of the class , thi both show how you can put text into the model most simpli , but it's actual also a more gener formul than you normal see from multi class logist regress model .
we actual won't us the more gener formul in the materi that is present here .
but it actual ha been shown to be us onc you go onto more complex natur languag process structur predict task .
here's a quiz question for you gui to work through .
so we're still in the situat that we're want to choos some class final word , which is now and our choic of class ar the same class as befor , person locat or drug .
and we have exactli the same three featur that we've been us throughout these exampl .
and so , what we'd like you to do is just concret work through what the probabl that ar assign to each of the three class thi time .
so let me now just go through quit concret how you will go about build a maxent model and thi is the kind of stuff that you will be do in the assign for thi week .
so the first step is that we defin featur .
so our featur ar .
boolean function , binari indic function over the data point .
and what we're think about is what ar thing that ar , we'll pick out set of data point which sai someth distinct about our classif task .
so for someth like text classif or sentiment we're definit go to want to includ individu word as featur , but sometim those aren't the best or the onli good featur .
for exampl , there ar infinit space of number like <num> . <num> or <num> . <num> and it's go to be hard to usefulli us these number just as word .
becaus most like the number that turn up at test time weren't in your train data .
so you can get a lot of valu from defin more gener featur , which might be just simpli word contain a digit , or it might be someth more specif , like is float point number .
but there ar also mani other case of us kind of featur .
so in mani languag , includ english , the end of a word give you inform about it class .
so if you have word like , help , make , dry that you can tell that these word ar participi form of verb by look at the ing end .
of cours thi isn't alwai true , you'll get a word like sting .
but nevertheless it's a good indic and so a featur like thi is someth that you can be expect to get a posit weight in a model if you're want to have a model that classifi word for someth like part of speech .
so what we will do in class , and it's commonli done in practic is that for each phi featur that pick out a data context that we will just repres as a string .
so the string could just be the word , um , comput or whatev .
but sinc we're also go to have other featur like word contain number commonli what we wanna do is kind of have some inform name space .
so thi might be the word equal comput and thi on might be the num equal decim or the featur here might be end equal ing .
so we've got thi inform name space of featur .
and the reason that we want to do that is , we want to repres each featur as a uniqu string so we can us that in someth like a hash map , where we can then look up the weight for each featur .
now of cours you don't have to encod the featur as string , you could encod them as some more gener structur object , but in practic a lot of the time string ar us , 'caus thei're just a fairli flexibl inform wai to encod featur .
okai .
and then rememb that thi part give us our phi featur and then from the phi featur we're gonna construct the f featur , where the f featur ar go to be a phi featur and a particular class .
so for a particular phi featur , we're then go to build a set of f featur , on for each choic of class cj .
and then it's the particular f featur like f1 , f2 , f3 , that ar then go to be given weight like <num> , <num> , <num> or whatev .
we concentr on defin the phi featur , in term of what ar us featur for the problem domain , but you should rememb that in the present that follow , and in the code , that the code is work in term of indic i of the individu f featur , where each phi featur is then be turn into a set of f featur , on for each class .
how do you go about build a maxent model ?
well , normal to start off with you defin some featur that you're just basic sure ar go to be us , so that might be thing like word in the document , or the word befor the word you want to classifi , or someth like that .
but typic , we go through an iter develop process where we initi build on model with some featur , we test it on some develop data , not our final test data , and we see how it doe .
and it get some perform level .
like , it might be <num> .
and we'd like to do a bit better .
and so what we're then gonna do is come up with a refin second model , and then repeat over .
and we'll commonli do thi a number of time until we've tri to come up with the best model that we think we can .
and so the question , then , is , well , what do we do for thi next model ?
and , in gener you can look at the featur and see which on ar us with good featur and bad featur .
often the easiest wai to make progress in practic is to try and defin featur that mark bad situat .
so if you look through your classifi develop data you'll see some piec of data , and it's classifi someth as a drug , and it just shouldn't have been a drug .
and so then mayb you can look at the , actual observ data , and see someth about it .
so mayb there's a smilei face here , or someth like that .
and you can sai , well , it just isn't like that a piec of text which ha a smilei face near it , is go to be talk about a drug .
and so you can defin a featur for thi combin .
and so thi kind of style of featur , where you target error and find some wai to explain to the model why that's a bad configur , is often on of the most effect wai to add featur .
then for ani particular set of featur and their weight , what we're gonna want to be abl to do is calcul the data condit likelihood , the probabl of a class given the data item .
but we alreadi know how to do that .
that's what we did in the preced slide with the work exampl .
but then secondli , we're also gonna want to work out the deriv .
the deriv of the condit likelihood with respect to each featur weight .
so that's so we're gonna want a partial deriv of our overal probabl accord to a particular featur like lambda i .
thi is what we're gonna work it through the math of in the next section , but crucial to do that , what we're gonna end up do is us those two featur expect that i defin earlier .
and so us those two thing we'll then be abl to work out these optimum featur weight .
okai , so i hope that's given you a good sens of what featur ar in discrimin maxent model , and at least some idea of how to go about defin them to build a classifi in a practic context .
though that question of what kind of featur ar good is someth that we'll come back to later , when look at particular problem .
after we work through more of the math and properti of maximum entropi model .
hello in thi section i want to point out a problem with gener model such as naiv bay model which is how thei can overcount evid when it is correl and at least hint how maxent model can solv thi problem .
i'm gonna us thi exampl .
so in thi exampl , here's our train data and in our train data we have eight document .
and so four of those document ar about europ and four of those document ar in the class asia , and we're want to do a two class classif problem between document about europ and asia .
now to keep thi exampl small , i'm build a text classifi with a veri teeni vocabulari .
you can alwai truncat the vocabulari us in a naiv bay text classifi and peopl often do to make the model smaller , more compact .
in thi exampl i've done that to an extrem extent and so i onli have three word left in my vocabulari , and so in the document we're just look at instanc of those word .
so in the first document , there's two instanc of monaco and no instanc of the other two word in my vocabulari which ar hong and kong .
okai , so if you look at the overal statist , what ar we start to find ?
so , we've got four document from each class and there ar eight document in total , so the prior probabl of asia and europ ar half each .
if in total in europ there ar eight word .
and six of them ar monaco .
so the probabl of a word be monaco , given that it's in the europ categori is three quarter , <num> <num> .
okai , look at the asia categori , there ar , again , eight word in our train data .
and two of those ar monaco .
so the probabl of a word be monaco in the asia categori is on quarter .
now , let's suppos we've now built our naiv bay model , and we're go to classifi a document .
so here's our pictur of the naiv bay model .
and for thi particular document , the onli word from our vocabulari that appear in it is monaco , that appear onc .
so what doe our model predict ?
so , for the joint probabl of asia and monaco , we've got the prior probabl of asia , that's a half , time the probabl of monaco , given asia , which is a quarter .
then , the joint probabl of europ and monaco is the prior probabl of a half time three quarter .
and so , to work out the posterior probabl , we're go to take each of these term over , divid by the sum of the two to normal it .
and so , what we have here is , on eighth and three eighth , which is go to give us , <num> <num> .
then for europ we're go to take three eighth over three eighth plu on eighth which simplifi down to three quarter .
so thi isn't surpris , thi give us exactli what we'd expect .
so what we saw in our train data that monaco appear six time over here and twice over here .
the two class ar equal like so if you've got a document with just the word monica appear in it , well , we shall expect to sai there's a three quarter chanc it's a document about europ .
that's a naiv bay model work correctli .
but now let's look at anoth exampl .
in thi exampl we have exactli the same train data so the prior probabl of each class is a half but thi time we're go to be work with document with hong and kong in them .
so the probabl of a word be hong given it's about asia so that's three out of eight .
and the probabl of kong be it's about asia is again three out of eight .
so , that's three eighth .
then over here for europ , there's on instanc each of hong and kong and there ar eight word in total so we've got probabl of on eighth .
okai , now let's again move to test time .
so , here's our naiv bay model , and our document that we ar test on ha the word hong and kong appear onc in each and noth els from our vocabulari .
okai , so if we work out the same kind of probabl as befor , we get on half time three eighth .
time three eighth .
and here we get a half time on eighth time on eighth .
the denomin ar alwai the same so we can just look at the numer for work it out .
so thi is proport to nine and thi is proport to on .
so then when we work out these posterior probabl for asia , we get <num> <num> plu on equal nine tenth and for europ we get on over nine plu on equal on tenth .
so look at what's happen .
the classifi's now give <num> percent probabl that it's an asia document , rather than just three quarter .
and intuit , that doesn't make sens , that the answer should be exactli the same as from the document with just monaco .
'caus look , hong kong appear onc in a europ document , and it occur three time in asia document .
so the probabl should be three quarter .
but why did that not happen ?
well , that didn't happen , becaus , although i'm suggest now that .
hong kong is on word , the name of on place , in our model it's be treat as two complet independ word that have noth to do with each other .
and so , it count the evid for each on separ and so that's precis where you're get these three eighth time three eighth , and on eighth time on eighth .
that end up with you have thi odd ratio of nine to on in favor of asia , wherea previous when you had the document with just monaco you had a ratio of three to on .
so , you start multipli in an odd factor of three for each repeat instanc of the word .
so , what's go on here is that , realli we have , on piec of evid .
appear of the word , the place name hong kong .
but becaus it's two token , we're treat it as two separ piec of evid and so we doubl count that evid and we're fals confid that the document is about asia .
doe that creat problem for classif ?
it turn out it doe .
let's look at on more exampl .
so in thi exampl , everyth in the naiv bay factor is just the same as befor .
we have exactli the same train data that we're build a model from .
but thi time at test time we've got exactli the same naiv bay model but our document of interest ha hong kong monaco on time each .
so what doe that mean we get when we work through our naiv bay model predict .
so the joint probabl of asia and all the word is a half time hong given asia three eighth time three eighth for kong time on quarter for monaco , the probabl of thi joint probabl is a half time on eighth time on eighth time three quarter .
again the denomin ar the same and can be ignor , so the posterior probabl of asia is then go to be three time three , nine , over nine plu three .
so that's then , three quarter .
and the posterior probabl of europ is go to be <num> over <num> plu <num> equal on quarter .
and , so , look at thi .
what we've gotten out is that there's a <num> percent chanc that thi document here is an asia document .
and intuit that doesn't make sens .
inform , in the train data there ar two document that look just like thi , and on wa about europ , on wa about asia , but more precis , we have these statist that the word . . .
the place monaco appear three quarter of the time in europ document , and on quarter of the time here .
and the place hong kong occur three quarter of the time here and on quarter of the time about europ .
so those factor should just complet cancel each other out and we should sai that thi document is equal like to be a document about europ or asia .
but again , we don't get that effect becaus the two token here ar wrongli be treat as independ sourc of evid and so , therefor we think we still have a three to on odd ratio in favor of the document be about asia .
okai , so what we've seen is that naiv bay model will multi count evid that when it treat evid as independ even when it's partli or total correl with other piec of evid .
each time you see a featur it's be multipli in .
and what we're gonna show in the upcom part is that maximum entropi model pretti much solv thi problem .
thei complet solv it when two piec of evid ar complet correl .
and as we shall see , thi is done by weight featur by a more complex algorithm that end up mean that the model expect of featur match their observ empir expect .
now obvious there ar name of place like hong kong , or saudi arabia that , ar veri . . .
provid two token that ar veri correl .
but you might still be think , that thi doesn't come up veri much , but the truth is that when you're build featur rich classifi defin featur as we discuss befor , thi happen a ton .
let me just quickli give you on more exampl .
so , suppos we're do some medic document and a word we might have in the document collect is xanax , and we'll have that as a word featur , but we discuss how commonli you'll also want to have substr featur for prefix and suffix .
and so , that mean we might have a featur for the first four letter of the word as xana with capit x .
the first three letter of the word as xan and the first two letter , and the first letter .
but the problem is it's quit like in our train data that the onli word that we ever see that start with capit xan is xanax a veri rare begin of word and therefor , thi featur will fire onli when the word is xanax and thi featur will fire onli when the word is xanax .
and so , these three featur , their occurr will be complet correl with each other .
and so , if we build a naiv bay model with these three featur , we'll tripl count on piec , what's realli on piec of evid .
wherea the maximum entropi model won't do that .
okai , i hope that's given you a sens of the problem and motiv you to stick with the math that come up ahead in explain how maximum entropi model work .
in the last segment we saw how to defin a maximum entropi or exponenti model which had paramet which were the weight of the variou featur .
in thi segment , we're go to look at how we can set those paramet so as to maxim the likelihood of observ data .
the log condit likelihood of a maxent model is a function of the observ data , the actual datum and their class , d and c which we assum to be independ and ident distribut .
and the paramet lambda of the model .
and it ha the form that we see here .
so , thi here wa the same form for the model that we saw in the previou section .
and what we can see is that in principl it's straightforward to work out the log likelihood of some data but it's onli gonna be easi to do in practic if the number of class is reason modest becaus we're sum over the class here .
we'll actual come back to that issu later in thi segment .
we can take thi log likelihood and separ it out into two compon , so here we have what us to be the numer and here we have what us to be the denomin .
so we can sai that the log likelihood of the entir model is a differ between the log likelihood of the numer , and the log likelihood of the denomin .
the deriv of the numer is realli easi to work out .
we start here with work out the partial deriv of the numer with respect to each paramet of the model .
and so thi is the numer over here .
and well , first of all we see that the log exp just cancel out becaus thei're invers of each other , which give us the form over here .
and then we simpli move the partial deriv insid the sum , which we can do twice .
and then move it in here .
and then we ask , what ar the partial deriv with respect to these lambda i , fi term ?
well , the partial deriv with respect to lambda i is go to be zero , except for the term that involv lambda i .
and so the partial deriv for that term is simpli f .
fi of c , d .
so , by sum that up , we give the , get the numer .
and the thing to notic here , is that the numer actual ha a veri simpl and intuit form .
so , what thi is calcul , is precis the empir count of featur fi .
deriv of the numer is just thi empir count .
work out the deriv of the denomin is a fraction more complex .
you actual need to rememb a littl bit of calculu for thi on .
but it's not that hard .
so , here we have taken the denomin term from befor and we're take the deriv of it with respect to each paramet weight as a partial deriv .
so now .
first of all .
we can move the partial deriv insid the sum , but then we need to take the deriv of the log of someth and so to do that we have to then us the chain rule .
so the chain rule is that you take the deriv of the outsid function time the origin function , time the deriv of the insid function .
so the deriv of log is the on on x function .
so we get on on x of the origin function here .
and then we take the deriv of the insid function right here .
work then on that right hand side , we now can move the deriv insid the sum again , and then we're get the deriv of the exp function , so at that point we have to invok the chain rule a second time .
so , the deriv of exp is itself , so then we have exp time the insid function here .
and then we take the deriv of the insid function over here .
okai .
so , at thi point to go down then to the next line , what we're do is we're regroup these two term , and that give thi part over here .
and then on the right hand side we're still work on the deriv of thi function .
and well thi time thi function ha the same form we saw in the numer .
sinc we've taken the deriv of thi sum .
here , that the onli term that is go to be non zero is the on that involv lambda i .
and so then that term in the deriv is just fi of c prime d and so what we end up with here is thi term .
it look exactli like our model .
we've got the exp , the same exp for a particular class over the sum of the exp of all the differ class .
so what we have here is the model probabl of class c prime and then here we have the function , the featur valu on the valu c prime .
and so what we're get at here is the model expect , i . e .
the predict count of fi given the paramet weight lambda .
so , put those two part togeth , what we have is the deriv of the log likelihood with respect to a particular paramet lambda i is simpli the differ between the actual count of that featur minu the predict count of that featur , accord to our current paramet weight .
and well how do we maxim the function ?
we maxim the function in the standard case , at the point at which thi deriv is zero .
so we want thi differ to be zero .
and so that in other word is sai that the optimum paramet in the model ar found when the model's predict expect equal it empir expect .
so thi optimum is alwai uniqu .
the actual paramet valu that give it to you mai not be uniqu .
but the valu that is the maximum of the function is uniqu , becaus we have a convex function .
and provid you estim your model from real data it alwai exist .
thi is someth i will come back to later , but these model ar also call maximum entropi model becaus , we find that what we ar actual do is find the model that ha maximum entropi while satisfi these constraint on the expect .
okai .
now we know about work out the partial deriv for the condit log likelihood function .
so , to recap , what we then want to do is choos valu for the paramet , lambda on , lambda two , et cetera , that maxim the condit log likelihood of the train data .
and what we find is , the wai we do that is to make us of these partial deriv .
in particular , if we take the vector of all partial deriv , that give us the gradient of the function .
let's see that in a pictur .
so here .
we ar imagin that we have two paramet , lambda on and lambda two , and depend on the set of those paramet , we get differ valu for the condit log likelihood and if we map them out we get thi kind of likelihood surfac .
so the idea of what we want to do , is we're go to start with the lambda on and lambda two set to some valu , and we're go to calcul the deriv , the partial deriv at thi point and those partial deriv give us the gradient and direct of steepest ascent on thi likelihood surfac .
and so we're gonna walk in that direct a littl , calcul the gradient again .
walk in the direct of the gradient a bit , calcul the gradient again , walk .
and we're gonna head off and come to the maximum valu of the condit log likelihood function .
so , for maxent model , thi log likelihood surfac is alwai convex , and ha a maximum .
but while it look fairli easi to maxim in thi exampl , in the real problem we deal with , there might be hundr of thousand or million of paramet .
and so it's consider more difficult .
so how we solv that problem is that we find a good numer optim packag and get it to find good paramet valu .
now in particular , let me just note that commonli these packag , includ the on that we us for the program assign actual minim , so instead you minim the neg of the condit log likelihood , which is equival .
now there ar mani numer optim techniqu .
the , the simplest is gradient descent .
which just sai that you walk alwai in the direct of the gradient .
a var , a variant of it is a stochast version , stochast gradient descent thi on is actual quit effect and is still often us for big problem .
, in the earli work on maxent model in statist and nlp , peopl commonli us iter proport fit method , like gener iter scale or improv iter scale .
these aren't us much anymor though .
other standard numer optim method like conjug gradient ar quit effect .
but the method of choic that's usual us , these dai is quasi newton method , in particular on well known algorithm is thi l bfg which is the on that we us in our assign code .
and that's a good method to us in gener .
okai .
so i hope that mean you've now got a good sens of how to calcul the partial deriv of the log likelihood function , and understand how that can be us to find the optim paramet for the model .
hi , in thi segment i'm go to introduc the two task that name entiti recognit and the inform extract , method for get simpl .
structur inform out of text document .
let's start with inform extract .
the goal of inform extract system is to find and understand limit relev part of a text and normal we're go to run it in the context where the system gather inform across mani piec of text and the goal of do thi inform gather is to be abl to produc some sort of structur represent of the relev inform .
so you can think of it as .
instanc of a databas relat aros in the databas tabl , or ha a , or ha a knowledg base .
and when we do inform extract we can have on of two goal .
on goal is to directli help peopl to organ inform so there us to peopl .
and the other possibl is to put the inform in semant precis form .
so , commonli comput for other downstream inform process task won't be abl to perform them if the input is just natur languag text .
but if we can import out variou kind of relat such as a databas tabl .
then it's veri easi to do subsequ data mine and process .
so the kind of inform that ia system extract is clear actual inform .
you know , kind of who did what to whom , when , kind of inform .
so , let's take a look at the exampl of that .
so , here ar a coupl of exampl .
on's gather earn , profit , board member , headquart , ani of thi kind of factual inform about compani from compani report .
so the idea is that the system can look through the text and then it see , okai , here's the name of the compani .
and then it's talk about it headquart and it sai that the headquart ar locat in , so that's kind of an indic of the headquart locat , melbourn , australia .
and what we're go to get out of that then is thi relat which you can think of as an databas tabl .
that's onli on exampl .
there ar lot of other exampl and anoth exampl is thi on that i show here .
there ar lot of case in .
bio text mine .
so learn drug gene product interact , learn sub cellular locu , local .
lot of relat of interest ar by a scientist and variou wai have been work on in inform extract system .
in the more consum space inform extract is now avail .
and i think popular in variou applic like either appl or googl mail or is us in web index .
so here's a littl exampl , how from appl mail and so .
yeah , there's thi date and so i , appl mail recogn that there wa a date there and put in that littl arrow box and when i hold that down it's then offer to creat a calendar event for me .
and so that's ad a littl help and from that allow me to do that kind of task faster .
thi kind of inform extract is often fairli simpl and seem to be mainli base on us the kind of regular express that we saw earlier , includ name list .
here's on more exampl about how compani ar start to surfac inform extract and search applic .
so if i stick in to googl the hp bulletin headquart as well as regular web search result those actual give me write here best guess for headquart is melbourn , london .
and of cours we can .
argu a littl about , about whether that's the perfect answer .
thei're realli kind of two separ answer here .
php is actual kind of a complex case becaus thei have thi complic jewel compani structur and there is a offic and a london offic .
so , we'll call that just about correct .
okai , that wa inform extract .
let me now go on and look at a particular sub task that's becom veri import , name entiti recognit .
so the idea of name entiti recognit is that we're go to in particular just look for and classifi name in a text .
so the first step is we find name .
so , here , we then found a name .
and in the particular of what that mean is we're find the limit of the name .
so thi on is these two token .
thi name is thi on token .
and then , onc we've done that , we're go to classifi the name .
so we're go to sai , that thi name here is a person name .
thi here is the name of an organ .
thi is the date .
and then we have a sequenc of further person name .
fine .
thi is on of and goe across the line break .
so , to sai a littl bit more precis of what the task is , we have thi sort of phrase of .
name entiti recognit .
and so a littl bit of a codeword these dai that reflect the histori .
the idea of it wa that entiti were made to refer to discret thing in the world .
so i am an entiti .
stanford univers is an entiti .
but someth like sand , or , air isn't an entiti .
that there's not a specif delin physic thing .
and then , the idea of name were , that some .
entiti have name , so i christoph mane have a name , but you know , the chair i'm sit in as i record thi , it is also an entiti it's a discreet physic thing , but it doesn't actual have a name attach to it , so , it's not a name ident .
that's the kind of philosoph histori , in practic the wood is us in name ident recognit , that's not quit what it mean .
cuz , effect what we're do is us name entiti recognit to mean easili distinguish name or other .
thing in text that we can pick out .
so , in particular , <num> .
so thing like date , time , quantiti ar normal regard as name entiti , although accord to , you know , the origin start point of the name , someth like a date or a chemic or a protein isn't actual an entiti at all , but we alwai includ those kind of thing when we're build name entiti recognit system .
okai , so what is the us of name ident recognit system ?
well name ident .
if you identifi them in a text , there's someth that can be index or link off .
so as i note down below , mani , compani make us of variou techniqu for take entiti on web page .
and then thei can provid link off of those entiti to biopag or topic page , or other thing of that sort .
so there ar a whole bunch of commerci product that run as web servic that allow you to do that kind of thing .
if , in gener , you want to be crawl the web , and pick up sentiment , we discuss earlier detect .
the sentiment as to whether it's posit or neg .
but you will then also need to work out who the sentiment is about .
or what the sentiment is about .
and so , at that point you need to be pick up compani or product name in the text .
and that's a task of name entiti recognit .
but there ar a lot of other us when name entiti recognit is a sub compon of a larger task .
and so , commonli when you're do inform extract a lot of what you're do is actual identifi name entiti and then work out the relationship between them .
so , we'll talk later .
about question answer .
and in question answer for variou kind of question often the answer ar name entiti .
when did someth happen or who did someth .
and so it help a lot for question answer if you have good name entiti extract .
anoth exampl that we also saw of how peopl us name recognit that plai off thi have semant interpret inform , is to us it in , in variou kind of thing , like calendar applic .
where you're make us of the semant interpret .
okai , i hope that's enough to have given you gui a good sens of these two task , name densiti recognit of simpli pick out concret name of object , peopl , organ , etcetera , or quantiti , date , time , and thing like that .
and then the bigger task of inform extract where the goal wa to pick out particular relat from a databas tabl from piec of unstructur natur languag text .
okai .
let me now introduc how we evalu name entiti recognit .
in the name entiti recognit task , we have a sequenc of word token .
and what we're go to want to do is predict entiti .
so we're go to want to predict that thi is an organ , these two word .
these two word ar a person .
thi on word is an organ .
so , in gener , we can have entiti name that ar sever token long .
and we want to identifi both the boundari of the entiti .
and then also it class , that thi is a person .
now .
you can think of that as make a classif that each token in sequenc .
in a wai that doesn't terribl make sens , cuz realli , our unit of interest is these whole entiti , the person and the organ .
and so the standard and better task motiv evalu is us for name entiti recognit is to evalu per entiti not per token .
and so , when we're work out our two by two conting tabl of true posit .
and so on and here's our system guess .
what we're gonna do is do it at the level of entiti .
so in thi data there ar three entiti and we could imagin perhap that our system identifi thi on as a person name , and identifi thi on as an organ name but miss thi on .
so what we'll be do is sai that there ar two true posit and on fals neg out of the three token .
and so the precis of our system here is <num> percent , everi it sai is right and it recal is two third .
okai .
so that look okai .
but when we get into the detail , it get a littl bit trickier than that .
so the problem is that recal and precis ar straightforward for task like web search , inform retriev or text categor .
but there's onli on grain size that you're put a classif on a document .
but in thi case , what we're do , is put classif on subsequ of word , and the precis , recal and f measur actual behav a bit funnili when that happen .
so here's an exampl to give you a good sens of the problem , which actual occur commonli in system .
so here's the piec of text , first bank of chicago announc earn .
and the correct entiti is right here .
first bank of chicago , which is a singl organ name .
howev , our system made a littl bit of a booboo .
our system ha said bank of chicago is the name of an organ and so that mean it's made a boundari error .
it's got the right boundari of the entiti correct but it's got the left boundari of the entiti wrong , and thi is the kind of error that ner system make a lot , and it's veri easi to see in thi case why it's made the error , becaus first is also a common noun and at the start of a sentenc it's perfectli reason to have the common noun of first appl announc thi and then microsoft announc thi .
so intuit you might feel like realli , in thi case , the name entiti recogn should be count as mostli correct .
it identifi that there wa an organ name here , and it label three of the four token .
but that's not how thing work us the set base measur of fals posit , fals neg , true posit and true neg when you're work on sequenc .
becaus what we sai is that the true annot is that there's an organ that span from token on through token four of the text .
wherea what our system guess is that there is an organ that span from token two through token four of the text .
and each of these claim is taken as a unit and is put into a set of claim , and then we count the number of match claim that's the true posit , and then we count the set differ in both direct and that give us the fals posit and the fals neg .
so what we end up with in our classif in thi case , is that thi is a fals neg and that thi on here is a fals posit , and so actual our system will be score as have made two error if it doe thi .
and so , actual , the system would have score better in an f1 evalu of name entiti recognit , by have label noth .
now , that can easili seem kind of wrong to you , and it ha seem wrong to other peopl .
so there have been variou suggest to provid measur for evalu name entiti recognit system where you get partial credit for do thing like thi , for get entiti almost right .
so , for exampl , the muc score that wa us in some of the promin earli evalu of name entiti recognit .
it had an algorithm that gave partial credit for case like thi .
but onc you do that , then there ar these complic question of how much partial credit to give in which case and it's not exactli clear and you have variou arbitrari paramet .
so realli most of the rest of the field hasn't gone there and ha end up us the straightforward f1 measur for name ident recognit , despit the complex with boundari error that i've just tri to illustr .
okai , so that should give you a good sens of what these measur of precis , recal and f measur ar .
and , why thei're us .
how we us them , for name ident recognit .
but also a slight sens of how you have to be a littl bit care interpret the number in that case .
hi .
in thi segment i'm gonna introduc the machin learn sequenc model approach to name entiti recognit and other kind of inform extract task .
so i'm gonna sai a littl bit about the structur of how you approach thing and the featur that you us for that task and then in the next segment i'm gonna talk about the detail of us maximum entropi model as sequenc classifi .
so , if we're go to us a sequenc model for name entiti recognit , we need supervis train data .
what that mean is we have exampl of train document where the word ar label for what their entiti class is .
so the step that we're go to have to go through is , first of all collect a repres set of train document that contain entiti that we're interest in and the context we're interest in them and then we're go to go though each word and label each token for it entiti class or if it's not in ani class , it'll be label other , which is normal denot o .
then .
on the machin learn classifi side , we're go to design appropri featur extractor , for identifi word of the class .
and then we're go to train a sequenc classifi , whose job is to do the best job possibl it can of label each token with it entiti class or other .
and so thi is the part we'll talk about , in the next chunk .
when we then want to run the classifi on actual document to do stuff that's often refer to as test but mayb we should just call it classifi .
we than have the train model so we get a set of test document we have model .
and we just run the sequenc model infer each document and it will be abl to tell us the highest probabl label for each token .
and we us those label to output recogn entiti .
thi all probabl becom more concret if i show you an exampl .
okai so here is our document which is a sequenc of word and so the label which is done by hand for the train document and automat by the train model at classif time is put on each word a label which is repres either it entiti type or it's an other .
so , in thi column , i am show what get call io encod , which is insid , short for insid outsid .
and it's the most obviou and natur thing for you to , for someon to come up with for do name entiti recognit and sequenc label .
so we ar take fred and label him as a person .
we're then take show and label it as an other .
then sue is a person name , mengqiu is a person name , huang is part of a person name , and then these next three token ar all other , other , other .
but there's a catch in that label scheme , which is actual , thi is on person's name , and then thi is anoth person's name .
and we can't repres that in io encod .
we can onli sai that we'll take maxim sequenc of entiti of the same class , and call them the name of an entiti .
so to recogn two entiti here , wherea , realli , there ar three .
and so there's a technic wai to fix that problem and that's known as iob encod .
and the wai that you do iob encod is you're prefix each class with a b if it's the begin of an entiti of that class , or an i if it's a continu of an entiti of that class .
so , then we can see here's on person name , here's a second person name , and we know it stop here becaus we have anoth b right there .
and then we have a third person name which is two token long .
so , the iob encod isn't defici and solv thi problem .
it come at a bit of a cost becaus if we suppos that we have c entiti class , for io encod you need to have c plu on label .
wherea for iob encod you have to have two c <num> label and the <num>'s come from the other for which you don't need to distinguish the bi , even in iob encod .
well that seem a fairli small differ , but as we'll see when we look at sequenc model .
if you have ani sequenc inform you're rais thi to the order of the sequenc model .
so you're at a minimum squar thi .
and so that mean that you ar end up with thing have a consider slower runtim with the iob encod .
and so , while in some sens thi is clearli the right thing to do sinc it's not defici represent , and a lot of peopl actual do , do thi .
i'll reveal a littl secret here which is in the stanford name entiti recogn we actual us ion code .
and the reason why we do that is it run a lot faster and it turn out that the slight limit in the represent aren't realli a problem .
in practic .
and there ar two reason that .
it's not a problem in practic .
on is , situat like thi veri , veri rare occur .
while you quit often get entiti next to each other , thei're most commonli entiti of differ class .
but the io encod had no problem if it's a person follow by an organ , then it can see the boundari perfectli well .
you onli have a problem when you have two entiti of the same class , and that happen pretti rare .
now it doe happen occasion , but it turn out that in practic system train with iob encod veri rare get thi right even though thei ar capabl of repres it .
that , becaus of the fact that have more class worsen the spars and becaus of the fact that it's simpli hard to tell where on name end and the next on begin , what you find is , in practic iob train system , given data like thi to tag , will nearli alwai tag them as on person name of three token .
which is exactli the same classif we extract in practic with the io classif .
and so , in practic , us thi work fine , despit it slight ugli and so , we us it .
okai , let me now just sai a moment about what featur we put into sequenc label for inform extract or name entiti recognit problem .
the obviou start point , is we put in featur for the word .
so we put in a featur for the current word in each class .
so that essenti work like a learn dictionari of word in each class , and then we also put in featur for the previou and next word , which give us some context featur .
we know that for thing like word after at or to might be more like to be locat .
if we have us other kind of linguist process and we know part of speech tag , thei'll often also be us featur , and we might also throw them in for the current word's part of speech tag , the next word's part of speech tag , and the previou word's part of speech tag .
now all of these featur ar just look at the observ data , and thei could be just done in a straight classifi built for each word .
you onli have a sequenc model when you also put in the label context .
and so that's when you ar sai that .
john smith .
if you think that john smith .
if you think that john is a person name , then it's quit like that the next token is also a person name .
becaus person name ar commonli more than on token long .
and so you can have featur that model thi label sequenc .
and it's have featur of thi type that ar definit for make someth a sequenc model .
but befor we get into the detail of sequenc model , i'd just like to mention a coupl of other kind of featur that ar back at thi level that ar a littl bit more interest than just us the word as thei ar .
and these featur ar realli us for have the model gener better and work better on rare and unseen word .
the on of those kind of featur is charact sub sequenc .
so charact sub sequenc of a word can be veri us classificatori featur .
so i am gonna show a neat exampl of thi that wa done by a student of mine joseph smarr year ago .
so he wa classifi entiti as on of these five class .
drug , compani , movi , place , person .
and what he ask wa , how indic ar particular charact subsequ ?
and here ar just a coupl of exampl from hi data .
so here ar some of the word he wa try to classifi , just to give you an idea .
well , thei weren't onli word , actual .
thei could be , multiword sequenc , like thi on here .
but now let's ask some question about particular charact subsequ .
suppos you know that the term to be classifi ha oxa in it .
well it turn out thi x letter is a realli strong marker of drug name , right ?
that's all of the drug name like xanax and thing like that , that peopl have these kind of particular semant sound pattern that thei name drug after and in thi data , at least , if you saw the letter oxa in a term , <num> percent of the time it wa a drug name .
so that's why it's all purpl here , so that wa a categor indic featur .
so that's an extrem case and most of them aren't like that , but there ar lot of other good featur .
here's anoth veri good featur from hi data , for these term , if you saw a colon in the term , that wa pretti much a giveawai that it wa a movi name .
there were onli a few except up here , so that on's an almost categor featur .
here's perhap a more typic exampl , but a place where charact subsequ ar still veri us .
so that if a word end in field , i mean , it could be anyth just about .
it couldn't be a drug name with thi data , but it could be a place , it could be a person , sai david copperfield .
it could be the name of a movi caus there's a movi david copperfield .
it could be the name of a compani .
but becaus of the semant origin of thi field end , if it end in field it's overwhelmingli a locat over two third of the time .
so , end in field is still a veri good indic featur .
and so in thi wai , charact substr featur ar veri us .
here's on other kind of featur that turn out to be quit complementari to charact subsequ .
so thi is what i call word shape sequenc , and it's an idea that wa first suggest by michael collin , as far as i'm awar , and the idea is that you map word onto equival class , which ar a simplifi represent , that encod attribut such as .
someth about the length of the word , someth about it capit , us of numer , intern punctuat and thing like that .
there ar veri mani particular wai that you can do it but thi give you an exampl of the gener idea of how to do it done on biolog entiti .
so precis in thi system the wai it wa defin wa that ani capit letter , a , b .
etc , were be map to a capit x .
ani littl latin letter wa be map on littl x .
ani number wa be map to a littl d and symbol like a hyphen or colon or period were be map to themselv .
and then there's on more trick that wa be us here , which wa a wai of shorten adjac letter .
the idea wa that begin and end of word ar import , but mayb we can pai less attent to stuff in the middl .
so , how these were work out is the first two letter and the last two letter ar be encod accord to thi system that i've drawn over here .
and so that mean , if the word is four charact or less .
so that's pick up thi length idea .
you can see it length in the encod .
so if we had anoth exampl , and someth wa just xp , that would be encod as xx .
but if a word is longer than four charact you ar not encod everyth about the remain charact .
for all the charact in the middl you ar just sai what is the set .
all of these charact type that occur , so in thi set here .
there's a lower case letter and there's a dash symbol and then that set is be written out in a canonic order which is alwai the same so actual dash come befor the x and that's give thi form .
lot of detail there , lot of differ wai you can imagin do it but the import part is that in thi wai we're defin thi word shape equival class for each word and so these ar much denser than the rare individu word but can be kind of good predictor of their behavior becaus thei're record import attribut like is there a digit , is it capit , is it all cap or have funni capit at the end of it .
and so those can be us featur for classif .
okai , so i hope that introduc the problem of sequenc model and some of the featur we us for ner and now we'll get back into a bit of the detail of build a maximum entri sequenc model .
we've now look in detail , how to make maximum entropi classifi .
and what i want to do in thi segment is extend from there , to show how we can build sequenc model out of classifi .
in particular , we ar go to make maximum entropi sequenc model .
mani problem in nlp have data which is a sequenc of someth .
it might be a sequenc of charact , a sequenc of word , a sequenc of phrase , a sequenc of line , paragraph , sentenc .
and we can think of the task , perhap with a littl code as on of label each item .
so a veri straightforward exampl is part of speech tag that we just look at .
so here we have the word and for each word we're go to label it with a particular part of speech .
other exampl ar fairli straightforward as well .
so for name entiti recognit here we have the word and we're go to label it with an entiti if it is on like organ or if it's not an entiti we're go to label it with an o .
some of the case of thing that can be done as sequenc problem , it's a littl bit subtl to think about the encod that is actual quit straightforward onc you've work through the idea .
so here we have a piec of chines text and what we want to do is word segment .
so thi is a word .
thi is a second word .
thi is a third word .
thi is a fourth word .
thi is a singl charact word , and so on .
and what we can do , is we can us a label to captur that .
so here we have what's call bi label where we're distinguish just two state , begin and insid , and that suffici to repres task like word segment .
so thi sai thi is the begin of a new word , but then the next token sai it's also the begin of a new word .
so we start a word here but then we've got thi i class so we continu , anoth i class , we continu .
so that's a three charact word .
and then we go back to a b , and we start a new word , which is a two charact word .
so although , realli we're segment the charact into sub sequenc , we can encod the decis we have to make by regard the problem as a sequenc label task .
here's on more slightli differ exampl , so what we have here is a stretch of text , and we can think of the text as line of an old fashion file with hard line break or sentenc .
and so what thi is , is a faq , as you find commonli on websit .
and what we want to do is to automat process it and work out where ar the question and where ar the answer .
and so we're gonna regard that as a sequenc label task , where each of our item is repres a line or a sentenc .
and so then we're encod our decis us exactli the same kind of two class classif over here , so each line ha been classifi as either a question line or an answer line .
and then we have the answer line group togeth for a particular answer .
so what we're gonna do with our maximum entropi model is we're go to put them into a sequenc model and these ar usual refer to as maximum entropi markov model , memm or condit markov model .
so what the classifi doe is it make a singl decis at a time , but it's abl to be condit on evid , both from observ and from previou decis .
so here we're show a pictur , we're in the middl of do part of speech tag and we've alreadi given part of speech tag to the first three word and so we're proceed left to right and that what we're up to is give a part of speech tag for thi word here and so the idea is for featur for classif we're go to be abl to us featur of the current word .
we're go to be abl to us featur of other word if we wish .
but we're also go to be allow to us featur of the previous assign part of speech tag .
perhap the part of speech tag two backward .
and all of these can influenc the classif , and so that's what's be shown over here for our featur .
so we have current word , next word , previou word , previou tag , previou two tag taken jointli as a featur .
and then we can also defin other featur of the kind that we've discuss befor .
so here we have a ha digit featur of the current word which is be us to gener over differ number caus mayb we don't know veri much or anyth about the number <num> in particular .
so gener a littl , thi is the pictur of how we move from our basic maxent classifi to a sequenc model .
so overal we have sequenc data here and what we want to do is classif at the sequenc level .
so we have individu word or charact and we want to assign to them their class .
but the wai we're gonna do that is we're go to look at each decis individu .
so we're go to sai , okai , there's a particular classif of interest , thi on here , and how ar we go to make that classif .
well what we're gonna do is sai for that particular classif , there's data local of interest to it .
so there's the current word , the previou word , the previou class .
and so what we're go to do , is we're go to do featur extract over that data .
and so we have the label that we're try to predict , or which we can see in supervis train data .
we've got featur of the observ data in previou classif , and then we're build a maximum entropi model over that and so at that point we're do all the stuff that we've talk about .
we're do optim of a model , we're do smooth , and at the end of the dai we build a littl local classifi that make the individu decis .
so what we'll do is we'll do it at thi posit , but then we'll repeat the same thing over at the next posit and we'll go along tag our sequenc .
it's extrem easi to see how to do that in on wai , where we first of all decid thi label , then we go on to decid thi label and then thi label , thi label and at each point we can us preced label to help determin the next on .
and if we do that we have a greedi sequenc model that's just decid on sequenc at a time and move on .
that in mani applic actual work quit well .
but commonli peopl want to explor the search space a littl bit better becaus sometim although here you might decid on part of speech tag is best , that later on onc you've look over here , that that would give some reason to think that mayb you should have chosen differ back over here .
and so there ar coupl of method that ar commonli us to do that .
on method is beam infer .
so a beam infer , at each posit rather than just decid the most like label , we can keep sever possibl .
so we might keep the top k complet subsequ up to the point where we're at so far and so then at each stage we'll consid each partial subsequ and extend it to on further posit .
now beam infer is also veri fast and easi to implement .
and it turn out that a lot of the time that beam size of <num> or <num> give you enough mainten of a few possibl and work almost as well as do exact infer to find the best possibl state sequenc .
and it's easi to implement .
of cours that's not necessarili true .
you get no guarante that you've found the global best part of speech tag sequenc , or whatev your sequenc problem is .
and it can certainli be the case , that possibl that would later have been shown to be good might fall of the beam befor thei get to be shown to be good .
we can do better than that .
we can actual find the best sequenc of state that ha the global highest score in the model .
and do that is refer to in nlp as do viterbi infer , sinc andrew viterbi invent a lot of these algorithm for find the best wai of do thing .
viterbi infer is a form of dynam program , or you can also think of it as an memoiz .
and you can do thi kind of dynam program , provid you have a small window of state influenc .
so , for exampl , when you're try to decid thi label , if it depend on word howev it want to , but onli depend on , sai , the previou label and the on befor that , then you end up with thi sort of fix small window of stuff you need to know about .
to make decis .
and noth back here is affect what decis you make .
provid that's true , you can write dynam program algorithm to find the optim state sequenc .
and so that ha the obviou advantag that you're guarante to find the best state sequenc .
but it ha some disadvantag .
it's , requir a bit more work to implement .
and it forc you to restrict your us of previou label in your infer process to a fix small window .
that is a restrict , but a lot of the time it's not such a bad restrict becaus in practic , it's hard to get long distanc interact to be work effect anywai , even in someth like a b model , which doe allow long distanc interact .
okai .
i've introduc maximum entropi markov model and i hope that you now feel that you understand them and will be abl to build them as an assign .
befor i end i should just veri briefli mention condit random field .
so , condit random field ar anoth probabilist sequenc model .
and if you just sort of look big pictur in the math of a , a condit random field , boi , that equat look exactli like the equat that we've been stare at in all of the recent slide .
the differ with condit random field is , that these probabl ar in term of the entir sequenc .
so thi is the sequenc of class and the sequenc of observ data valu , that it's not in term of particular point in that space .
and so we get thi whole sequenc condit model rather than a chain of local model .
that look as if it would be veri difficult to deal with caus the space of a sequenc of c's is exponenti in it length .
and the space of a sequenc of data item repres as featur is at a minimum huge and perhap even infinit .
but it turn out that provid the fi featur remain local permit dynam program that the condit sequenc likelihood can be calcul exactli .
train is somewhat slower but crf's have theoret advantag of avoid certain causal competit bias that can occur with maximum entropi markov model .
howev , to explain the detail of these model we'd have to go quit a wai afield look at in gener how to do markov random field infer , which i feel a better topic for other cours .
so , let me just mention these and sai that these dai us crf or variant of them that us a max margin criterion come out of svm , ar seen as the state of the art method for do sequenc model .
and there ar variou bit of softwar , includ the stanford softwar for name entiti recognit you can download that implement crf .
but a thing to know is that although crf ar theoret cleaner and can avoid some problem of memm , that in practic , when you're build model with rich featur which condit on observ data , both befor and after you , that in practic thei tend to have perform that can't realli be distinguish from maximum entropi markov model .
and so there's realli no problem with us maximum entropi markov model to do the job of sequenc classif , and that's what we'll us in the assign .
okai , and so i hope that you gui now have a concret idea of how you can build a maxent classifi , and then incorpor it into a system for do sequenc infer .
