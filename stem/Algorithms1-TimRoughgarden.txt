hi , my name's tim roughgarden .
i'm a professor here at stanford univers .
and i'd like to welcom you to thi first cours on the design and analysi of algorithm .
now , i imagin mani of you ar alreadi clear on your reason for take thi cours .
but let me begin by justifi thi cours's exist .
and give you some reason why you should be highli motiv to learn about algorithm .
so what is an algorithm anywai ?
basic it's a set of well defin rule , a recip in effect for solv some comput problem .
mayb you have a bunch of number and you want to rearrang them so that thei're in sort order .
mayb you have a roadmap and an origin and a destin and you want to comput the shortest path from that origin to that destin .
mai be you face a number of differ task that need to be complet by certain deadlin and you ant to know in what order you should accomplish the task .
so that you complet them all by their respect deadlin .
so why studi algorithm ?
well first of all , understand the basic of algorithm and the relat field of data structur is essenti for do seriou work in pretti much ani branch of comput scienc .
thi is the reason why here at stanford , thi cours is requir for everi singl degre that the depart offer .
the bachelor degre the master degre and also the phd .
to give you a few exampl rout and commun network piggyback on classic shortest path algorithm .
the effect of public kei cryptographi reli on that of number theoret algorithm .
comput graphic need the comput primit suppli by geometr algorithm .
databas indic reli on balanc search tree data structur .
comput biologi us dynam program algorithm to measur genom similar .
and the list goe on .
second , algorithm plai a kei role in modern technolog innov .
to give just on obviou exampl , search engin us a tapestri of algorithm to effici comput the relev of variou webpag to it's given search queri .
the most famou such algorithm is the page rank algorithm current in us by googl .
inde in a decemb <num> report to the unit state white hous , the presid's counsel of advis on scienc and technolog argu that in mani area perform gain due to improv in algorithm have vastli exceed event he dramat perform gain due to increas processor speed .
third , although thi is outsid of the score , the scope of thi cours .
algorithm ar increasingli be us to provid a novel len on process outsid of comput scienc and technolog .
for exampl , the studi of quantum comput ha provid a new comput viewpoint on quantum mechan .
price fluctuat in econom market can be fruitfulli view as an algorthm process and even evolut can be usefulli thought of as a surprisingli effect search algorthim .
the last two reason for studi algorthim might sound flippant but both have more than a grain of truth to them .
i don't know about you , but back when i wa a student , my favorit class were alwai the challeng on that , after i struggl through them , left me feel a few iq point smarter than when i start .
i hope thi cours provid a similar experi for mani of you .
final , i hope that by the end of the cours i'll have convert some of you to agre with me that the design and analysi of algorithm is simpli fun .
it's an endeavor that requir a rare blend of precis and creativ .
it can certainli be frustrat at time , but it's also highli addict .
so let's descend from these lofti gener and get much more concret .
and let's rememb that we've all been learn about and us algorithim sinc we were littl kid .
sometim when you were a kid , mayb sai third grade or so , you learn an algorithm for multipli two number .
mayb your third grade teacher didn't call it that , mayb that's not how you thought about it .
but you learn a well defin set of rule for transform input , name two number into an output , name their product .
so , that is an algorithm for solv a comput problem .
let's paus and be precis about it .
mani of the lectur in thi cours will follow a pattern .
we'll defin a comput problem .
we'll sai what the input is , and then we'll sai what the desir output is .
then we will proce to give a solut , to give an algorithm that transform the input to the output .
when the integ multipl problem , the input is just two , n digit number .
so the length , n , of the two input integ x and y could be anyth , but for motiv you might want to think of n as larg , in the thousand or even more , perhap we're implement some kind of cryptograph applic which ha to manipul veri larg number .
we also need to explain what is desir output in thi simpl problem it's simpli the product x time y .
so a quick digress so back in 3rd grade around the same i wa learn the integ multipl algorithm .
i got a c in penmanship and i don't think my handwrit ha improv much sinc .
mani peopl tell me by the end of the cours .
thei think of it fondli as a sort of acquir tast , but if you're feel impati , pleas note there ar type version of these slide .
which i encourag you to us as you go through the lectur , if you don't want to take the time deciph the handwrit .
return to the integ multipl problem , have now specifi the problem precis , the input , the desir output .
we'll move on to discuss an algorithm that solv it , name , the same algorithm you learn in third grade .
the wai we will assess the perform of thi algorithm is through the number of basic oper that it perform .
and for the moment , let's think of a basic oper as simpli ad two singl digit number togeth or multipli two singl digit number .
we're go to then move on to count the number of these basic oper perform by the third grade algorithm .
as a function of the number n of digit in the input .
here's the integ multipl algorithm that you learn back in third grade illustr on a concret exampl .
let's take sai the number <num> , <num> , <num> , <num> and <num> , <num> , <num> , <num> .
as we go through thi algorithm quickli , let me remind you that our focu should be on the number of basic oper thi algorithm perform .
as a function of the length of the input number .
which , in thi particular exampl , is four digit long .
so as you'll recal , we just comput on partial product for each digit of the second number .
so we start by just multipli <num> time the upper number <num> , <num> , <num> , <num> .
so , you know , <num> time <num> is <num> , <num> carri to <num> , <num> time <num> is <num> , with the <num> that's <num> , write down the <num> , carri the <num> , and so on .
when we do the next partial product , we do a shift effect , we add a <num> at the end , and then we just do exactli the same thing .
and so on for the final two partial product .
and final , we just add everyth up .
, what you probabl realiz back in third grade , is that thi algorithm is what we would call correct .
that is , no matter what integ x and y you start with if you carri out thi procedur , thi algorithm .
and all of your intermedi comput ar done properli .
then the algorithm will eventu termin with the product , x time y , of the two input number .
you're never go to get a wrong answer .
you're alwai go to get the actual product .
well , you probabl didn't think about wa the amount of time need to carri out thi algorithm out to it conclus to termin .
that is the number of basic oper , addit or multipl of singl digit number need befor finish .
so let's now quickli give an inform analys of the number of oper requir as a function of the input length n .
let's begin with the first partial product , the top row .
how did we comput thi number <num> , <num> ?
well we multipli <num> time each of the number <num> , <num> , <num> and <num> .
so that wa for basic oper .
on for each digit at the top number , plu we had to do these carri .
so those were some extra addit .
but in ani case , thi is at most twice time the number of digit in the first number .
at most two end basic oper to form thi first partial product .
and if you think about it there's noth special about the first partial product .
the same argument sai that we need at most <num> n oper to form each of the partial product of which there ar again n , on for each digit of the second number .
well if we need at most two n oper to comput each partial product and we have n partial product .
that's a total of at most two n squar oper to form all of these blue number , all of the partial product .
now we're not done at that point .
we still have to add all of those up to get the final answer , in thi case <num> , <num> , <num> .
and that's final addit requir a compar number of oper .
roughli , anoth sai two n squar , at most oper .
so , the upshot , the high level point that i want you to focu on , is that as we think about the input number get bigger and bigger .
that is as a function of n the number of digit in the input number .
the number of oper that the grade school multipl algorithm perform , grow like some constant .
roughli <num> sai time n squar .
that is it's quadrat in the input length n .
for exampl , if you doubl the size of the input , if you doubl the number of digit in each of the two integ that you're given .
then the number of oper you will have to perform us thi algorithm ha to go up by a factor of four .
similarli , if you quadrupl the input length , the number of oper go , is go to go up by a factor of <num> , and so on .
now , depend on what type of third grader you were .
you might well of accept thi procedur as the uniqu or at least the optim wai of multipli two number togeth to form their product .
now if you want to be a seriou algorithm design .
that kind of obedi tumid is a qualiti you're go to have to grow out of .
and earli and extrem import textbook on the design and analysi of algorithm wa by aho , hopcroft , and ullman .
it's about <num> year old now .
and there's the follow quot , which i absolut ador .
so after iter through a number of the algorithm design paradigm cover in the textbook .
thei sai the follow , perhap the most import principl of all , for the good algorithm design is to refus to be content .
and i think thi is a spot on comment .
i might summar it a littl bit more succinctli .
as , as an algorithm design you should adopt as your mantra the question , can we do better ?
thi question is particularli apropo when your'e face with a naiv or straight forward solut to a comput problem .
like for exampl , the third grade algorithm for integ multipl .
the question you perhap did not ask yourself in third grade wa , can we do better than the straight forward multipl algorithm ?
and now is the time for an answer .
if you want to multipli two integ , is there a better method than the on we learn back in third grade ?
to give you the final answer to thi question , you'll have to wait until i provid you with a toolbox for analyz divid and conquer algorithm a few lectur henc .
what i want to do in thi lectur is convinc you that the algorithm design space is surprisingli rich .
there ar certainli other interest method of multipli two integ beyond what we learn in third grade .
and the highlight of thi lectur will be someth call karatsuba multipl .
let me introduc you to karatsuba multipl through a concret exampl .
i am go to take the same pair of integ we studi last lectur , <num> , <num> , <num> , <num> , <num> , <num> , <num> , <num> .
i am go to execut a sequenc of step result in their product .
but , that sequenc of step is go to look veri differ than the on we undertook dure the grade school algorithm , yet we'll arriv at exactli the same answer .
the sequenc of step will strike you as veri mysteri .
it'll seem like i'm pull a rabbit out of the hat , and the rest of thi video will develop more systemat what exactli thi karatsuba multipl method is , and why it work .
but what i want you to appreci alreadi on thi slide is that the algorithm design space is far richer than you might expect .
there's thi dazzl arrai of option for how to actual solv problem like integ multipl .
let me begin by introduc some notat for the first and second halv of the input number x and y .
so the first half of x , that is <num> we're go to regard as a number in it own right call a .
similarli b will be <num> , c will be <num> , and d will be <num> .
i'm go to do a sequenc of oper involv onli these doubl digit number a b c and d .
and then after a few such oper i will collect all of the term togeth in a magic wai result in the product of x and y .
first let me comput the product of a time c and also the product of b time d .
i'm go to skip the elementari calcul , and just tell you the answer .
so you can verifi that a time c is <num> , where as b time d is <num> .
next i'm go to do someth even still more inscrut .
i'm go to take the sum of a and b .
i'm go to take the sum of c and d .
and then i'm go to comput the product of those two sum .
that boil down to comput the product of <num> and <num> .
mainli at <num> .
now , i'm go to subtract our first two product from the result of thi comput .
that is , i'm go to take <num> .
subtract <num> , and subtract <num> .
you should check that if you subtract the result of the first <num> step from the result of the 3rd step , you get <num> .
now , i claim that i can take the result of step <num> , <num> and <num> and combin them into super simpl wai to produc the product of x and y .
here's how i do it .
i start with the first product , ac .
and i pad it with four zero .
i take the result of the second step , and i don't pad it with ani zero at all .
and i take the result of the fourth step , and i pad it with two zero .
if we add up these three quantiti , from right to left .
we get two , five , six .
six , zero , zero , seven .
if you go back to the previou lectur you'll note that thi is exactli the same output as the great school algorithm , that thi is in fact the product of on , two , the , three , four and five , six , seven , eight .
so let me reiter that you should not have ani intuit for the comput i just did , you should not understand what just went down on thi slide .
rather i hope you feel some mixtur of bafflement and intrigu but , more the point i hope you appreci that the third grade algorithm is not the onli game in town .
there's fundament differ algorithm for multipli integ than what you learn as a kid .
onc you realiz that , onc you realiz how rich the space of algorithm is , you have to wonder can we do better than that third grade algorithm ?
in fact , doe thi algorithm alreadi do better that the third grade algorithm ?
befor i explain full blown karatsuba multipl , let me begin by explain a simpler , more straightforward recurs approach .
to integ multipl .
now , i am assum you have a bit of program background .
in particular , that you know what recurs algorithm ar .
that is , algorithm which invok themselv as a subroutin with a smaller input .
so , how might you approach the integ multipl problem recurs ?
well the input ar two digit .
each two number .
each ha two digit .
so to call the algorithm recurs you need to perform input that have smaller size , less digit .
well , we alreadi were do that in the comput on the previou slide .
for exampl the number <num> we treat the first half of digit as <num> as a number in it own right and similarli <num> .
in gener , given a number x with n digit .
in can be express decompos , in term of two , n over two digit number .
name as a , the first half of the digit shift appropri .
that is multipli by ten rais to the power , n over two .
plu the second half of the digit b .
in our exampl , we had a equal to <num> , <num> wa b .
n wa <num> , so <num> to the n over <num> wa <num> , and then c and d were <num> and <num> .
what i want to do next is illumin the relev recurs call .
to do that , let's look at the product , x time y .
express it in term of these smaller number , a , b , c , and d , and do an elementari comput .
multipli the expand version of x and y , we get an express with three term .
on shift by n , <num> rais to the power n , and the coeffici there is a time c .
we have a term that's shift by <num> to the n over <num> , and that ha a coeffici of ad and also plu bc .
and bring up the rear , we have the term b time d .
we're go to be refer to thi express a number of time , so let me both circl it and just give it a shorthand .
we're go to call thi express star .
on detail i'm gloss over for simplic , is that i've assum that n is an even integ .
now , if n is an odd integ , you can appli thi exact same recurs approach to integ multipl .
in the straightforward wai , so if n wa <num> then you would decompos on of these input number into sai the first five digit and the later four digit and you would proce in exactli the same wai .
now the point of the express star is if we look at it despit be the product of just elementari algebra , it suggest a recurs approach to multipli two number .
if we care about the product of x and y , why not , instead , comput thi express star , which involv onli the product of smaller number , a , b , c and d .
you'll notic , stare at the express star , there ar <num> relev product , each involv a pair of these smaller number .
name ac , ad , bc , and bd .
so why not comput each of those four product recurs .
after all , the input will be smaller .
and then onc our four recurs call come back to us with the answer , we can formul the rest of express star in the obviou wai .
we just pad a time c with n zero at the end .
we add up a , d , and bc , us the grade school algorithm , and pad the result with n over two zero , and then we just sum up these return , again us the grade school addit , and algorithm .
so the on detail miss , that i've gloss over , requir to turn thi idea into a bonafid recurs algorithm , would be to specifi a base case .
as i hope you all know , recurs algorithm need a base case .
if the input is suffici small , then you just immedi comput the answer rather than recurs further .
of cours , recurs algorithm need a base case so thei don't keep call themselv til the rest of time .
so for integ multipl , which the base case , well , if you're given two number that have the just on digit each .
then you just multipli them in on basic oper and return the result .
so , what i hope is clear at the moment is that there is inde a recurs approach to solv the integ multipl algorithm result in an algorithm which look quit differ than the on you learn in third grade , but which nevertheless you could code up quit easili in your favorit program languag .
now , what you shouldn't have ani intuit about is whether or not thi is a good idea or a complet crackpot idea .
is thi algorithm faster or slower than the grade school algorithm ?
you'll just have to wait to find out the answer to that question .
let's now refin thi recurs algorithm , result in the full blown karatsuba multipl algorithm .
to explain the optim behind karatsuba multipl , let's recal the express we were call star on the previou slide .
so , thi just express the product of x and y in term of the smaller number a , b , c , and d .
in thi straight forward recurs algorithm we made four recurs call to comput the four product which seem necessari to valu , to comput the express star .
but if you think about it , there's realli onli three quantiti in star that we care about , the three relev coeffici .
we care about the number ad and bc .
not per se , but onli in as much as we care about their sum , ad plu bc .
so thi motiv the question , if there's onli <num> quantiti that we care about , can we get awai with onli <num> rather than <num> recurs call .
it turn out that we can and here's how we do it .
the first coeffici a c and the third coeffici b d , we comput exactli as befor , recurs .
next , rather than recurs comput a d or b c , we're go to recurs comput the product of a plu b and c plu d .
if we expand thi out , thi is the same thing as comput ac plu ad plu bc plu bd .
now , here is the kei observ in karatsuba multipl , and it's realli a trick that goe back to the earli 19th centuri mathematician , gauss .
let's look at the quantiti we comput in step <num> and subtract from it .
the two quantiti that we alreadi comput in step on and two .
subtract out the result of step on cancel the a c term .
subtract out the result of step two , cancel out the bd term , leav us with exactli what we want all along , the middl coeffici a d plu b c .
and now in the same that on the previou slide we have a straightforward recurs algorithm make four recurs call , and then combin them in the obviou wai .
here we have a straightforward recurs algorithm that make onli three recurs call .
and on top of the recurs call doe just great school addit and subtract .
so you do thi particular differ between the three recurs comput product and then you do the shift , the pad by zero , and the final sum as befor .
so that's pretti cool , and thi kind of showcas the ingenu which bear fruit even in the simplest imag comput problem .
now you should still be ask the question yeah is crazi algorthim realli faster than the grade school algorithm we learn in 3rd grade ?
total not obviou , we will answer that question a few lectur hens and we'll answer it in a special case of an entir toolbox i'll provid you with to analyz the run time of so call divid and conquer algorithm like karatsuba multipl , so stai tune .
in thi video i'll talk about variou aspect of the cours , the topic that we'll cover , the kind of skill you can expect to acquir , the kind of background that i expect , the support materi and the avail tool for self assess .
let's start with the specif topic that thi cours is go to cover .
the cours materi correspond to the first half of the ten week stanford cours .
it's taken by all comput scienc undergradu , as well as mani of our graduat student .
there will be five high level topic , and at time these will overlap .
the five topic ar first of all , the vocabulari for reason about algorithm perform , the design and conquer algorithm design paradigm , random and algorithm design , primit for reason about graph , and the us and implement of basic data structur .
the goal is to provid an introduct to and basic literaci in each of these topic .
much , much more could be said about each of them , than we'll have time for here .
the first topic is the shortest , and probabl also the driest .
but it's a prerequisit for think serious about the design and analysi of algorithm .
the kei concept here is big o notat , which , conceptu , is a model choic about the granular with which we measur a perform metric like the run time of an algorithm .
it turn out that the sweet spot for clear high level think about algorithm design , is to ignor constant factor and lower order term .
and to concentr on how well algorithm perform scale with larg input size .
big o notat is the wai to mathemat thi sweet spot .
now , there's no on silver bullet in algorithm design .
no singl problem solv method that's guarante to unlock all of the comput problem that you're like to face .
that said , there ar a few gener algorithm design techniqu .
high level approach to algorithm design that find success applic across a rang of differ domain .
these rel wide applic techniqu ar the backbon of a gener algorithm cours like thi on .
in thi cours , we'll onli have time to deepli explor on such algorithm design paradigm , name that of the divid and conquer algorithm .
in the sequel cours as we'll discuss , there's two other major algorithm on paradigm to get cover .
but for now , divid and conquer algorithm , the idea is to first break the problem into smaller problem which then get solv recurs , and then to somehow quickli combin the solut to the sub problem into on for the origin problem that you actual care about .
so for exampl , in the last video .
we saw two algorithm of thi sort , two divid and conquer algorithm from multipli two larg integ .
in later video we will see a number of differ applic .
we'll see how to design fast divid and conquer algorithm for problem rang from sort to matrix multipl to nearest neighbor type problem and comput of geometri .
in addit , we'll cover some power method for reason about the run time of recurs algorithm like these .
as for the third topic .
a random algorithm is on that , in some sens , flip coin while it execut .
that is , a random algorithm will actual have differ execut if you run it over and over again on a fix input .
it turn out , and thi is definit not intuit , that allow random intern to an algorithm , often lead to simpl , eleg , and practic solut to variou comput problem .
the canon exampl is random quick sort , and that algorithm and analysi we will cover in detail in a few lectur .
random primal test is anoth killer applic that we'll touch on .
and we'll also discuss a random approach to graph partit .
and final we'll discuss how random is us to reason about hash function and hash map .
on of the theme of thi cours , and on of the concret skill that i hope you take awai from the cours , is , literaci with a number of comput primit for oper on data , that ar so fast , that thei're , in some sens , essenti free .
that is , the amount of time it take to invok on of these comput primit is bare more than the amount of time you're alreadi spend just examin or read the input .
when you have a primit which is so fast , that the run time is bare more than what it take to read the input , you should be readi to appli it .
for exampl , in a preprocess step , whenev it seem like it might be help .
it should just be there on the shelf wait to be appli at will .
sort is on canon exampl of a veri fast , almost for free primit of thi form .
but there ar on that oper on more complex data as well .
so recal that a graph is a data structur that ha , on the on hand , vertic , and on the other hand , edg .
which connect pair of vertic .
graph model , among ani other thing , differ type of network .
so even though graph ar much more complic than mere arrai , there's still a number of blazingli fast primit for reason about their structur .
in thi class we'll focu on primit for compet connect inform and also shortest path .
we'll also touch on how some primit have been us to investig the structur of inform in social network .
final , data structur ar often a crucial ingredi in the design of fast algorithm .
a data structur's respons for organ data in a wai that support fast queri .
differ data structur support differ type of queri .
i'll assum that you're familiar with the structur that you typic encount in a basic program class includ arrai and vector .
list , stack , and queue .
hopefulli , you've seen at some point both tree and heap , or you're will to read a bit about them outsid of the cours , but we'll also includ a brief review of each of those data structur as we go along .
there's two extrem us data structur that we'll discuss in detail .
the first is balanc binari search tree .
these data structur dynam maintain an order on a set of element , while support a larg number of queri that run in time logarithm in the size of the set .
the second data structur we'll talk a fair bit about is hash tabl or hash map , which keep track of a dynam set , while support extrem fast insert and lookup queri .
we'll talk about some canon us of such data structur , as well as what's go on under the hood in a typic implement of such a data structur .
there's a number of import concept in the design and analysi of algorithm that we won't have time to cover in thi five week cours .
some of these will be cover in the sequel cours , design and analysi of algorithm ii , which correspond to the second half of stanford's ten week cours on thi topic .
the first part of thi sequel cours focus on two more algorithm design paradigm .
first of all , the design analysi of greedi algorithm with applic to minimum span tree , schedul , and inform theoret code .
and secondli , the design analysi of dynam program algorithm with exampl applic be in genom sequenc align and the shortest path protocol in commun network .
the second part of the sequel cours concern np complet problem , and what to do about them .
now , np complet problem ar problem that , assum a famou mathemat conjectur you might have heard of , which is call the p not equal to np conjectur , ar problem that cannot be solv under thi conjectur by ani computation effici algorithm .
we'll discuss the theori of np complet , and , with a focu on what it mean for you as an algorithm design .
we'll also talk about sever wai to approach np complet problem , includ fast algorithm that correctli solv special case ; fast heurist with provabl perform guarante ; and exponenti time algorithm that ar qualit faster than brute forc search .
of cours there ar plenti of import topic that can't be fit into either of these two five week cours .
depend on the demand , there might well be further cours on more advanc topic .
follow thi cours is go to involv a fair amount of time and effort on your part .
so it's onli reason to ask what can you hope to get out of it ?
what skill will you learn ?
well .
primarili , you know , even though thi isn't a program class per se , it should make you a better programm .
you'll get lot of practic describ and reason about algorithm , you'll learn algorithm design paradigm , so realli high level problem solv strategi that ar relev for mani differ problem across differ domain , and tool for predict the perform of such algorithm .
you'll learn sever extrem fast subroutin for process data and sever us data structur for organ data that can be deploi directli in your own program .
second , while thi is not a math class per se , we'll wind up do a fair amount of mathemat analysi .
and thi in turn will sharpen your mathemat analyt skill .
you might ask , why is mathemat relev for a class in the design and analysi of algorithm , seemingli more of a program class .
well let me be clear .
i am total uninterest in mere tell you fact or regurgit code that you can alreadi find on the web or in ani number of good program book .
my goal here in thi class , and the wai i think i can best supplement the resourc that you probabl alreadi have access to is to explain why thing ar the wai thei ar .
why we analyz the algorithm in the wai that we do , why variou super fast algorithm ar in fact super fast , and so on .
and it turn out that good algorithm idea usual requir nontrivi mathemat analysi to understand properli .
you'll acquir fundament insight into the specif algorithm and data structur that we discuss in the cours .
and hopefulli , mani of these insight will prove us , more gener , in your other work .
third , and perhap the most relev for those of you who work in some other disciplin thi cours should help you learn how to think algorithm .
inde after studi algorithm it's hard enough not to see them pretti much everywher , whether you ar ride an elev , watch a flock of bird , bui and sell stock out of your portfolio , even watch an infant learn .
as i said in the previou video algorithm think is becom increasingli us and preval if you ar outsid of comput scienc and technolog like in biologi , statist and econom .
fourth , if you're interest in feel like a card carri comput scientist , in some sens , then you'll definit want basic literaci in all of the topic that we'll be cover .
inde , on of the thing that make studi algorithm so fun , is , it realli feel like you're studi a lot of the greatest hit from the last <num> year of comput scienc .
so , after thi class , no longer will you feel exclud at that comput scienc cocktail parti when someon crack a joke about dijkstra's algorithm .
now you'll know exactli what thei mean .
final , there's no question that studi thi materi is help for technic interview question .
to be clear , my sole goal here is to teach you algorithm , not to prepar you for interview , per se .
but over the year , countless student of mine have regal me with stori about how master the concept in thi class enabl them to ac everi technic question thei were ever ask .
i told you , thi is fundament stuff .
so , what do i expect from you ?
well , honestli , the answer is noth .
after all isn't the whole point of a free onlin class like thi on that anyon can take it and devot as much effort to it as thei like .
so that said , as a teacher it's still us to have on or more canon student in mind .
and i thought i'd go ahead and be transpar with you about how i'm think about these lectur .
who i have in mind that i'm teach to .
so again , pleas don't feel discourag if you don't conform to thi canon student templat .
i'm happi to have the opportun to teach you about algorithm no matter who you ar .
so first , i have in mind someon who know at least some program .
for exampl , consid the previou lectur .
we talk about a recurs approach to multipli two number and i mention how in certain mathemat express , back then we label it star and circl it in green .
how that express natur translat into a recurs algorithm .
in particular , i wa certainli assum that you had some familiar with recurs program .
if you feel comfort with my statement in that lectur , if you feel like you could code up a recurs integ multipl algorithm base on the high level outlin that i gave you , then you should be in good shape for thi cours .
you should be good to go .
if you weren't comfort with that statement , well , you might not be comfort with the rel high conceptu level at which we discuss program in thi cours .
but i encourag to watch the next sever video anywai , to see if you get enough out of them to make it worth your while .
now , while i'm aim these lectur at peopl who know some program , i'm not make ani assumpt whatsoev about exactli which program languag you know .
ani standard imper languag you know , someth like c , java or python , is total fine for thi cours .
now , to make these lectur access to as mani programm as possibl , and to be honest , you know , also to promot think about program at a rel abstract conceptu level , i won't be describ algorithm in ani particular program languag .
rather , when i discuss the algorithm , i'll us onli high level pseudo code , or often simpli english .
my induct hypothesi is that you ar capabl of translat such a high level descript into a work program in your favorit program languag .
in fact , i strongli encourag everyon watch these lectur to do such a translat of all of the algorithm that we discuss .
thi will ensur your comprehens , and appreci of them .
inde , mani profession comput scientist and programm don't feel that thei realli understand an algorithm until thei've code it up .
mani of the cours's assign will have a problem in which we ask you to do precis thi .
put anoth wai , if you're look for a sort of code cookbook , code that you can copi and past directli into your own program .
without necessarili understand how it work , then thi is definit not the cours for you .
there ar sever book out there that cater to programm look for such code cook book .
second , for these lectur i have in mind someon who ha at least a modest amount of mathemat experi though perhap with a fair bit of accumul rust .
concret i expect you to be abl to recogn a logic argument that is a proof .
in addit , two method of proof that i hope you've seen befor ar proof by induct and proof by contradict .
i also need you to be familiar with basic mathemat notat , like the standard quantifi and summat symbol .
a few of the lectur on random algorithm and hash will go down much easier for you if you've seen discret probabl at some point in your life .
but beyond these basic , the lectur will be self contain .
you don't even need to know ani calculu , save for a singl simpl integr that magic pop up in the anali of the random quick sort algorithm .
i imagin that mani of you have studi math in the past , but you could us a refresh , you're a bit rusti .
and there's plenti of free resourc out there on the web , and i encourag you to explor and find some that you like .
but on that i want to particularli recommend is a great set of free lectur note .
it's call mathemat for comput scienc .
it's author by eric lehman and tom layden , and it's quit easi to find on the web if you just do a web search .
and those note cover all of the prerequisit that we'll need , in addit to ton of other stuff .
in the spirit of keep thi cours as wide access as possibl , we're keep the requir support materi to an absolut minimum .
lectur ar meant to be self contain and we'll alwai provid you with the lectur note in powerpoint and pdf format .
onc in a while , we'll also provid some addit lectur note .
no textbook is requir for thi class .
but that said , most of the materi that we'll studi is well cover in a number of excel algorithm book that ar out there .
so i'll singl out four such book here .
the first three i mention becaus thei all had a signific influenc on the wai that i both think about and teach algorithm .
so it's natur to acknowledg that debt here .
on veri cool thing about the second book , the on by dasgupta , papadimitri and vazirani , is that the author have made a version of it avail onlin for free .
and again , if you search on the author' name and the textbook titl , you should have no troubl come up with it with a web search .
similarli , that's the reason i've list the fourth book becaus those author have likewis made essenti a complet version of that book avail onlin and it's a good match for the materi that we're go to cover here .
if you're look for more detail about someth cover in thi class , or simpli a differ explan than the on that i give you , all of these book ar gonna be good resourc for you .
there ar also a number of excel algorithm textbook that i haven't put on thi list .
i encourag to explor and find you own favorit .
in our assign , we'll sometim ask you to code up an algorithm and us it to solv a concret problem that is too larg to solv by hand .
now , we don't care what program and languag and develop environ you us to do thi as we're onli go to be ask you for the final answer .
thu , we're not requir anyth specif , just that you ar abl to write and execut program .
if you need help or advic about how to get set up with a suitabl code environ , we suggest that you ask other student for help via the cours discuss forum .
final , let's talk a bit more about assess .
now thi cours doesn't have offici grade per se , but we will be assign weekli homework .
now we're go to assign homework for three differ reason .
the first is just for self assess .
it's to give you the opportun to test your understand of the materi so that you can figur out which topic you've master and which on that you haven't .
the second reason we do it is to impos some structur on the cours , includ deadlin , to provid you with some addit motiv to work through all the topic .
deadlin also have a veri import side effect that synchron a lot of the student in the class .
and thi of cours make the cours discuss forum a far more effect tool for student to seek and provid help in understand the cours materi .
the final reason that we give homework is to satisfi those of you who , on top of learn the cours materi , ar look to challeng yourself intellectu .
now , thi class ha ten of thousand of student .
so it's obvious essenti that the assign can be grade automat .
now , we're current onli in the <num> gener of free onlin cours such as thi on .
so the avail tool for auto grade assess ar current rather primit .
so , we'll do the best we can , but i have to be honest with you .
it's difficult , or mayb even imposs to test deep understand of the design and analysi of algorithm , us the current set of tool .
thu , while the lectur content in thi onlin cours is in no wai water down from the origin stanford version .
the requir assign and exam we'll give you , ar not as demand as those that ar given in the on campu version of the cours .
to make up for thi fact , we'll occasion propos option algorithm design problem , either in a video or via supplementari assign .
we don't have the abil to grade these , but we hope that you'll find them interest and challeng , and that you'll discuss possibl solut with other student via the cours discuss forum .
so i hope thi discuss answer most of the question you have about the cours .
let move on to the real reason that we're all here , to learn more about algorithm .
okai .
so in thi video , we'll get our first sens of what it's actual like to analyz an algorithm .
and we'll do that by first of all review a famou sort algorithm , name the merg sort algorithm .
and then give a realli fairli mathemat precis upper bound on exactli how mani oper the merg sort algorithm requir to correctli sort an input arrai .
so i feel like i should begin with a bit of an apolog .
here we ar in <num> , a veri futurist sound date .
and yet i'm begin with a realli quit ancient algorithm .
so for exampl , merg sort wa certainli known , to john von neumann all the wai back in <num> .
so , what justif do i have for begin , you know , a modern class in algorithm with such an old exampl ?
well , there's a bunch of reason .
on , i haven't even put down on the slide , which is like a number of the algorithm we'll see , merg sort as an oldi but a goodi .
so it's over <num> , or mayb even <num> year old .
but it's still us all the time in practic , becaus thi realli is on of the method of choic for sort .
the standard sort algorithm in the number of program librari .
so that's the first reason .
but there's a number of other as well that i want to be explicit about .
so first of all , throughout these onlin cours , we'll see a number of gener algorithm design paradigm wai of solv problem that cut across differ applic domain .
and the first on we're go to focu on is call the divid and conquer algorithm design paradigm .
so in divid and conquer , the idea is , you take a problem , and break it down into smaller sub problem which you then solv recurs , . . .
. . .
and then you somehow combin the result of the smaller sub problem to get a solut to the origin problem that you actual care about .
and merg sort is still todai's the , perhap the , most transpar applic of the divid and conquer paradigm , . . .
. . .
that will exhibit veri clear what the paradigm is , what analysi and challeng it present , and what kind of benefit you might deriv .
as for it benefit , so for exampl , you're probabl all awar of the sort problem .
probabl you know some number of sort algorithm perhap includ merg sort itself .
and merg sort is better than a lot of thi sort of simpler , i would sai obviou , sort algorithm , . . .
. . .
so for exampl , three other sort algorithm that you mai know about , but that i'm not go to discuss here .
if you don't know them , i encourag you to look them up in a text book or look them up on the web .
let's start with three sort algorithm which ar perhap simpler , first of all is select sort .
so thi is where you do a number of pass through the wai repeatedli , identifi the minimum of the element that you haven't look at yet , . . .
. . .
so you're basic a linear number of pass each time do a minimum comput .
there's insert sort , which is still us in certain case in practic as we will discuss , but again it's gener not as good as merg sort , . . .
. . .
where you will repeatedli maintain the invari that prefix view of arrai , which is sort version of those element .
so after ten loop of insert sort , you'll have the invari that whatev the first ten element of the arrai ar go to be in sort order , . . .
. . .
and then when insert sort complet , you'll have an entir sort arrai .
final , some of you mai know about bubbl sort , which is where you identifi adjac pair of element which ar out of order , . . .
and then you do repeat swap until in the end the arrai is complet sort .
again i just sai thi to jog your memori , these ar simpler sort than merg sort , . . .
. . .
but all of them ar wors in the sens that thei're lack in perform in gener , which scale with n <num> , . . .
. . .
and the input arrai ha n element , so thei all have , in some sens , quadrat run time .
but if we us thi non trivial divid and conquer approach , or non obviou approach , we'll get a , as we'll see , a much better run time than thi quadrat depend on the input .
okai ?
so we'll get a win , first sort in divid and conquer , and merg sort is the algorithm that realiz that benefit .
so the second reason that i wanna start out by talk about the merg sort algorithm , is to help you calibr your prepar .
i think the discuss we're about to have will give you a good signal for whether you're background's at about the right level , of the audienc that i'm think about for thi cours .
so in particular , when i describ the merg sort algorithm , you'll notic that i'm not go to describ in a level of detail that you can just translat it line by line into a work program in some program languag .
my assumpt again is that you're a sort of the programm , and you can take the high level idea of the algorithm , how it work , . . .
. . .
and you're perfectli capabl of turn that into a work program in whatev languag you see fit .
so hopefulli , i don't know , it mai not be easi the analysi of merg sort discuss .
but i hope that you find it at least rel straight forward , . .
. .
becaus as the cours move on , we're go to be discuss algorithm and analysi which ar a bit more complic than the on we're about to do with merg sort .
so in other word , i think that thi would be a good warm up for what's to come .
now anoth reason i want to discuss merg sort is that our analysi of it will natur segment discuss of how we analyz the algorithm in thi cours and in gener .
so we're go to expos a coupl of assumpt in our analysi , we're focu on worst case behavior , . . .
. . .
or we'll look for guarante on perform on run time that hold for everi possibl input on a given size , . . .
and then we'll also expos our focu on so call asymptot analysi , which mean will be much more concern with the rate of growth on an algorithm perform than on thing like low order term or on small chang in the constant factor .
final , we'll do the analysi of merg sort us what's call as recurs tree method .
so thi is a wai of ty up the total number of oper that ar execut by an algorithm .
and as we'll see a littl bit later , thi recurs tree method gener greatli .
and it will allow us to analyz lot of differ recurs algorithm , lot of differ divid and conquer algorithm , includ the integ multipl algorithm that we discuss in an earlier segment .
so those ar the reason to start out with merg sort .
so what is the comput problem that merg sort is meant to solv ?
well , presum , you all know about the sort problem .
but let me tell you a littl bit about it anywai , just so that we're all on the same page .
so , we're given as input .
an arrai of n number in arbitrari order , and the goal of cours is to produc output arrai where the number ar in sort order , let's sai , from smallest to largest .
okai so , for exampl , we could consid the follow input arrai , and then the goal would be to produc the follow output arrai .
now on quick comment .
you'll notic that here in input arrai , it had eight element , all of them were distinct , it wa the differ integ , between <num> and <num> .
now the sort problem realli isn't ani harder if you have duplic , in fact it can even be easier , . . .
. . .
but to keep the discuss as simpl as possibl let's just , among friend , go ahead and assum that thei're distinct , for the purpos of thi lectur .
and i'll leav it as an exercis which i encourag you to do , which is to think about how the merg sort algorithm implement and analysi would be differ , if at all , if there were ti , okai ?
go ahead and make the distinct assumpt for simplic from here on out .
okai , so befor i write down ani pseudo code for merg sort , let me just show you how the algorithm work us a pictur , . . .
. . .
and i think it'll be pretti clear what the code would be , even just given a singl exampl .
so let's go ahead and consid the same unsort input arrai that we had on the previou slide .
so the merg sort algorithm is a recurs algorithm , and again , that mean that a program which call itself and it call itself on smaller sub problem of the same form , okai ?
so the merg sort is it purpos in life is to sort the given input arrai .
so it's go to spawn , or call itself on smaller arrai .
and thi is gonna be a canon divid and conquer applic , where we simpli take the input arrai , we split it in half , we solv the left half recurs , we solv the right half recurs , and then we combin the result .
so let's look at that in the pictur .
so the first recurs call get the first four element , the left half of the arrai , name <num> , <num> , <num> , <num> .
and , of cours , the other recurs call is gonna get the rest of the element , <num> , <num> , <num> , <num> .
you can imagin these ha been copi into new arrai befor thei're given to the recurs call .
now , by the magic of recurs , or by induct if you like , the recurs call will do their task .
thei will correctli sort each of these arrai of four element , and we'll get back sort version of them .
so from our first recurs call , we receiv the output , <num> , <num> , <num> , <num> , and from the second recurs call , we receiv the sort output , <num> , <num> , <num> , <num> .
so now , all the remain to complet the merg sort is to take the two result of our recurs call , these two sort element of length <num> , and combin them to produc the final output , name the sort arrai of all eight of the input number .
and thi is the step which is call merg .
and hopefulli you ar alreadi ar think about how you might actual implement thi merg in a computation effici wai .
but i do ow you some more detail .
and i will tell you exactli how the merg is done .
in effect , you just walk pointer down each of the two sort of sub arrai , copi over , popul the output arrai in the sort order .
but i will give you some more detail in just a slide or two .
so that's merg sort in a pictur .
split it in half , solv recurs , and then have some slick merg procedur to combin the two result into a sort output .
okai , so let's move on , and actual discuss the pseudo code for the merg sort algorithm .
first , let me just tell you the pseudo code , leav asid exactli how the merg subroutin is implement .
and thu , high level should be veri simpl and clear at thi point .
so there's gonna be two recurs call , and then there's gonna be a merg step .
now , i ow you a few comment , 'caus i'm be a littl sloppi .
again , as i promis , thi isn't someth you would directli translat into code , although it's pretti close .
but so what ar the coupl of the wai that i'm be sloppi ?
well , first of all , there's , , you know , in ani recurs algorithm , you gotta have some base case .
you gotta have thi idea that when the input's suffici .
realli small you don't do ani recurs , you just return some trivial answer .
so in the sort problem the base case would be if your hand an arrai that ha either zero or an element , well it's alreadi sort , there's noth to do , so you just return it without ani recurs .
okai , so to be clear , i haven't written down the base case .
although of cours you would if you were actual implement , a merg short .
some of you , make a note of that .
a coupl of other thing i'm ignor .
i'm ignor what the , what to do if the arrai ha odd length , so if it ha sai nine element , obvious you have to somehow break that into five and four or four and five , so you would do that just in either wai and that would fine .
and then secondli , i'm ignor the detail or what it realli mean to sort of recurs sort , so for exampl , i'm not discuss exactli how you would pass these subarrai onto the recurs call .
that's someth that would realli depend somewhat on what , on the program languag , so that's exactli what i want to avoid .
i realli want to talk about the concept which transcend ani particular program languag implement .
so that's why i'm go to describ algorithm at thi level okai .
all right , so the hard part rel speak , that is .
how do you implement the merg depth ?
the recurs call have done their work .
we have these two sort of separ half the number .
the left half and the right half .
how do we combin them into on ?
and in english , i alreadi told you on the last slide .
the idea is you just popul the output arrai in a sort order , by travers pointer or just travers through the two , sort sub arrai in parallel .
so let's look at that in some more detail .
okai , so here is the pseudo code for the merg step .
so let me begin by , introduc some name for the , charact in the , what we're about to discuss .
so let's us c .
to denot the output arrai .
so thi is what we're suppos to spit out with the number in sort order .
and then , i'm gonna us a and b to denot the result of the two recurs call , okai ?
so , the first recurs call ha given us arrai a , which contain the left half of the input arrai in sort order .
similarli , b contain the right half of the input arrai , again , in sort order .
so , as i said , we're gonna need to travers the two , sort sub arrai , a and b , in parallel .
so , i'm gonna introduc a counter , i , to travers through a , j to travers through b .
i and j will both be initi to on , to be at the begin of their respect arrai .
and now we're gonna do .
we're go to do a singl pass of the output arrai copi it in an increas order .
alwai take the smallest from the union of the two sort sub arrai .
and if you , if there's on idea in thi merg step it's just the realiz that .
the minimum element that you haven't yet look at in a and b ha to be at the front of on or the two list right so for exampl at the veri begin of the algorithm where is the minimum element over all .
well , which ever of the two arrai it land in a or b it ha to be the smallest on there okai .
so the smallest element over all is either the smallest element a or it's the smallest element b .
so you just check both place , the smaller on is the smallest you copi it over and you repeat .
that's it .
so the purpos of k is just to travers the output arrai from left to right .
that's the order we're gonna popul it .
current look at posit i , and the first arrai of posit j and the second arrai .
so that's how far we've gotten , how deepli we've probe in the both of those two arrai .
we look at which on ha the current smallest , and we copi the smallest on over .
okai ?
so if the , if , the entri in the i posit of a is smaller , we copi that on over .
of cours , we have to increment i .
we probe on deeper into the list a , and symmeter for the case where the current posit in b ha the smaller element .
now again , i'm be a littl bit sloppi , so that we can focu on the forest , and not sort of , and not get bog down with the tree .
i'm ignor some end case , so if you realli want to implement thi , you'd have to add a littl bit , to keep track of when you fall off , either , either a or b .
becaus you have addit check for when i or j reach the end of the arrai , at which point you copi over all the remain element into c .
all right , so i'm gonna give you a clean up version , of , that pseudo code so that you don't have to toler my question handwrit ani longer than is absolut necessari .
thi again , is just the same thing that we wrote on the last slide , okai ?
the pseudo code for the merg step .
now , so that's the merg sort algorithm .
now let's get to the meati part of thi lectur , which is , okai , so merg sort produc a sort arrai .
what make it , if anyth , better than much simpler non divid and conquer algorithm , like sai , insert sort ?
other word , what is the run time of the merg sort algorithm ?
now i'm not gonna give you a complet precis definit , definit of what i mean by run time and there's good reason for that , as we'll discuss shortli .
but intuit , you should think of the run time of an algorithm , you should imagin that you're just run the algorithm in a debugg .
then , everi time you press enter , you advanc with on line of the program through the debugg .
and then basic , the run time is just a number of oper execut , the number of line of code execut .
so the question is , how mani time you have to hit enter on the debugg befor the , program final termin .
so we're interest in how mani such , line of code get execut for merg short when an input arrai ha n number .
okai , so that's a fairli complic question .
so let's start with a more modest school .
rather than think about the number of oper execut by merg sort , which is thi crazi recurs algorithm , which is call itself over and over and over again .
let's just think about how mani oper ar gonna get execut when we do a singl merg of two sort sub arrai .
that seem like it should be an easier place to start .
so let me remind you , the pseudo code of the merg subroutin , here it is .
so let's just go and count up how mani oper that ar gonna get us .
so there's the initi step .
so let's sai that i'm gonna charg us on oper for each of these two initi .
so let's call thi two oper , just set i equal to on and j equal to on then we have thi four loop execut a total number of end time so each of these in iter of thi four loop how mani instruct get execut , well we have on here we have a comparison so we compar a i to b j and either wai the comparison come up we then do two more oper , we do an assign .
here or here .
and then we do an increment of the relev variabl either here or here .
so that's gonna be three oper per iter .
and then mayb i'll also sai that in order to increment k we're gonna call it a fourth iter .
okai ?
so for each of these n iter of the four loop we're gonna do four oper .
all right ?
so put it all togeth , what do we have is the run time for merg .
so let's see the upshot .
so the upshot is that the run time of the merg subroutin , given an arrai of m number , is at most four m plu two .
so a coupl of comment .
first of all , i've chang a letter on you so don't get confus .
in the previou slide we were think about an input size of n .
here i've just made it .
see i've chang the name of the variabl to m .
that's gonna be conveni onc we think about merg sort , which is recurs on smaller sub problem .
but it's exactli the same thing and , and whatev .
so an arrai of m entri doe as most four m plu two .
line of code .
the second thing is , there's some ambigu in exactli how we count line of code on the previou slide .
so mayb you might argu that , you know , realli , each loop iter should count as two oper , not just on . 'caus you don't just have to increment k , but you also have to compar it to the , upper bound of n .
eh , mayb .
would have been 5m <num> instead of 4m <num> .
so it turn out these small differ in how you count up .
the number of line of code execut ar not gonna matter , and we'll see why shortli .
so , amongst friend , let's just agre , let's call it 4m plu two oper from merg , to execut on arrai on exactli m entri .
so , let me abus our friendship now a littl bit further with an , an inequ which is true , but extrem sloppi .
but i promis it'll make our live just easier in some futur calcul .
so rather than 4m <num> , 'caus <num>'s sorta get on my nerv .
let's just call thi .
utmost six n .
becaus m is at least on .
okai , you have to admit it's true , 6mo is at least 4m plu two .
it's veri sloppi , these number ar not anyth closer to each other for m larg but , let's just go ahead and be sloppi in the interest of futur simplic .
okai .
now i don't expect anyon to be impress with thi rather crude upper bound , the number of line of code that the merg subroutin need to finish , to execut .
the kei question you recal wa how mani line of code doe merg sort requir to correctli sort the input arrai , not just thi subroutin .
and in fact , analyz merg sort seem a lot more intimid , becaus if it keep spawn off these recurs version of itself .
so the number of recurs call , the number of thing we have to analyz , is blow up exponenti as we think about variou level of the recurs .
now , if there's on thing we have go for us , it's that everi time we make a recurs call .
it's on a quit a bit smaller input then what we start with , it's on an arrai onli half the size of the input arrai .
so there's some kind of tension between on the on hand explos of sub problem , a prolifer of sub problem and the fact that success subproblem onli have to solv smaller and smaller subproblem .
and resolut resolv these two forc is what's go to drive our analysi of merg short .
so , the good new is , is i'll be abl to show you a complet analysi of exactli how mani line of code merg sort take .
and i'll be abl to give you , and , in fact , a veri precis upper bound .
and so here's gonna be the claim that we're gonna prove in the remaind of thi lectur .
so the claim is that merg short never need than more than six time n .
time the logarithm of n log base two if you're keep track plu an extra six n oper to correctli sort an input arrai of n number , okai so let discuss for a second is thi good is thi a win , know that thi is an upper bound of the number of line of code the merger take well ye it is and it show the benefit of the divid and conquer paradigm .
recal .
in the simpler sort method that we briefli discuss like insert sort , select sort , and bubbl sort , i claim that their perform wa govern by the quadrat function of the input size .
that is thei need a constant time in the squar number of oper to sort an input arrai of length n .
merg sort by contrast need at most a constant time n time log n , not n squar but n time log n line of code to correctli sort an input arrai .
so to get a feel for what kind of win thi is let me just remind you for those of you who ar rusti , or for whatev reason have live in fear of a logarithm , just exactli what the logarithm is .
okai ?
so .
the wai to think about the logarithm is as follow .
so you have the x axi , where you have n , which is go from on up to infin .
and for comparison let's think about just the ident function , okai ?
so , the function which is just .
f n n .
okai , and let's contrast thi with a logarithm .
so what is the logorithm ?
well , for our purpos , we can just think of a logorithm as follow , okai ?
so the log of n , log base <num> of n is , you type the number n into your calcul , okai ?
then you hit divid by two .
and then you keep repeat divid by two and you count how mani time you divid by two until you get a number that drop below on okai .
so if you plug in <num> you got to divid five time by two to get down to on .
log base two of <num> is five .
you put in <num> you have to divid by two , ten time till you get down to on .
so log base two of <num> is ten and so on , okai .
so the point is you alreadi see thi if a log of a <num> roughli is someth like ten then the logarithm is much , much smaller than the input .
so graphic , what the logarithm is go to look like is it's go to look like .
a curv becom veri flat veri quickli , as n grow larg , okai ?
so f n be log base <num> of n .
and i encourag you to do thi , perhap a littl bit more precis on the comput or a graph calcul , at home .
but log is run much , much , much slower than the ident function .
and as a result , sort algorithm which run in time proport to n time log n is much , much faster , especi as n grow larg , than a sort algorithm with a run time that's a constant time n squar .
in thi video , we'll be give a run time analysi of the merg sort algorithm .
in particular , we'll be substanti the claim that the recurs divid and conquer merg sort algorithm is better , ha better perform than simpler sort algorithm that you might know , like insert sort , select sort , and bubbl sort .
so , in particular , the goal of thi lectur will be to mathemat argu the follow claim from an earlier video , that , in order to sort an arrai in number , the merg sort algorithm need no more than a constant time n log n oper .
that's the maximum number of line of execut code that will ever execut specif six time n log n plu six n oper .
so , how ar we go to prove thi claim ?
we're go to us what is call a recurs tree method .
the idea of the recurs tree method is to write out all of the work done by the recurs merg sort algorithm in a tree structur , with the children of a given node correspond to the recurs call made by that node .
the point of thi tree structur is it will facilit , interest wai to count up the overal work done by the algorithm , and will greatli facilit , the analysi .
so specif , what is thi tree ?
so at level zero , we have a root .
and thi correspond to the outer call of merg sort , okai ?
so i'm gonna call thi level zero .
now thi tree is go to be binari in recognit of the fact that each indic of merg sort make two recurs call .
so the two children will correspond to the two recurs call of merg sort .
so at the rout , we oper on the entir input arrai , so let me draw a big arrai indic that .
and at level on , we have on sub problem for the left half , and anoth sub problem for the right half of the input arrai .
and i'll call these first two recurs call , level on .
now of cours each of these two level on recurs call will themselv make two recurs call .
each oper on then a quarter of the origin input arrai .
so those ar the level two recurs call , of which there ar four , and thi process will continu until eventu the recurs bottom out , in base case when thei're onli an arrai size zero or on .
so now i have a question for you which i'll , i'll give you in the form of a quiz which is , at the bottom of thi recurs tree correspond to the base case , what is the level number at the bottom ?
so , at what level do the leav in thi tree resid ?
okai , so hopefulli you guess , correctli guess , that the answer is the second on , so name that the number of level of the recurs tree is essenti logarithm in the size of the input arrai .
the reason is basic that the input size is be decreas by a factor two with each level of the recurs .
if you have an input size of n at the outer level , then each of the first set of recurs call oper on arrai of size n over two , at level two , each arrai ha size n over four , and so on .
where doe the recurs bottom out ?
well , down at the base case where there's no more recurs , which is where the input arrai ha size on or less .
so in other word , the number of level of recurs is exactli the number of time you need to divid n by two , until you get down to a number that's most on .
recal that's exactli the definit of a logarithm base two of n .
so sinc the first level is level zero and last level is level log base two of n .
the total number of level is actual log base two of n plu on .
and when i write down thi express , i'm here assum that n is a , is a power of two .
which is not a big deal .
i mean the analysi is easili extend to the case where n is not a power of two .
and thi wai , we don't have to think about fraction .
log base two of n then is an integ .
okai so let's return to the recurs tree .
let me just redraw it realli quick .
so again , down here at the bottom of the tree we have the leav , i . e .
the base case where there's no more recurs .
which when n is the power of two correspond exactli to singl element arrai .
so that's the recurs tree correspond to an indic of merg sort .
and the motiv for write down , for organ the work perform by merg sort in thi wai , is it allow us to count up the work , level by level .
and we'll see that that's a particularli conveni wai to account for all of the differ line of code that get execut .
now , to see that in more detail , i need to ask you to identifi a particular pattern .
so , first of all , the first question is , at a given level , j , of thi recurs , exactli how mani distinct sub problem ar there , as a function of the level j ?
that's the first question .
the second question is , for each of those distinct sub problem at level j , what is the input size ?
so , what is the size of the a , of the arrai , which is pass to a sub problem resid at level j of thi recurs tree .
so , the correct answer is the third on .
so , first of all , at a given level , j , there's precis two to the j distinct sub problem .
there's on outermost sub problem at level zero , it ha two recurs call , those ar the two , sub problem at level on , and so on .
in gener , sinc merg short call itself twice , the number of sub problem is doubl at each level , so that give us the express , two to the j , for the number of sub problem at level j .
on the other hand , by a similar argument , the input size is halv each time .
with each recurs call you pass it half of the input .
that you were given .
so at each level of the recurs tree we're see half of the input size of the previou level .
so after j level , sinc we start with an input size of n , after j level , each sub problem will be oper on an arrai of length n over two to the j .
okai , so now let's put thi pattern to us , and actual count up all of the line of code that merg sort execut .
and as i said befor , the kei , the kei idea is to count up the work level by level .
now , to be clear , when i talk about the amount of work done at level j , what i'm talk about is the work done by those <num> to the j invoc of merg sort , not count their respect recurs call .
not count work which is gonna get done in the recurs , lower in the tree .
now , recal , merg sort is a veri simpl algorithm .
it just ha three line of code .
first there is a recurs call so we're not count that , second there is anoth recurs call again we're not count that a littl j and then third we just invok the merg subroutin .
so realli outsid of the recurs call all that merg sort doe is a singl invoc of merg .
further recal we alreadi have a good understand of the number of line of code that merg need .
on an input of size m , it's gonna us , at most , 6m line of code .
that's an analysi that we did in the previou video .
so , let's fix a level j .
we know how mani sub problem there ar , two to the j .
we know the size of each sub problem , n over two to the j , and we know how much work merg need on such an input , we just multipli it by six , and then we just multipli it out , and we get the amount of work done at a level j .
'kai ?
and all of the littl adjac problem .
so here it is in more detail .
all right .
so .
we start with just the number of differ sub problem at level j and we just notic that , that wa at most two to the j .
we also observ that each level j sub problem is pass an , an arrai as input which ha length n over two to the j .
and we know that the merg subroutin , when given an input , given an arrai of size , n over two to the j , will execut almost six time that mani number of line of code .
so to comput the total amount of work done at level j , we just multipli the number of problem time the work done sub problem , per sub problem .
and then someth sort of remark happen , where you get thi cancel of the two , two to the js .
and we get an upper bound 6n .
which is independ of the level j .
so we do at most six end oper at the root , we do at most six end oper at level on , at level two , and so on , okai ?
it's independ of the level .
moral , the reason thi is happen is becaus of a perfect equilibrium between two compet forc .
first of all , the number of subproblem is doubl with each .
level of the recurs tree but 2ndly the amount of work that we do per sub problem is halv with each level of the recurs tree's , onc those two cancel out .
we get an upper bound 6n , which is independ of the level j .
now , here's why that's so cool , right ?
we don't realli care about the amount of work just at a given level .
we care about the amount of work that merg sort doe ever , at ani level .
but , if we have a bound on the amount of work at a level which is independ of the level , then our overal bound is realli easi .
what do we do ?
we just take the number of level , and we know what that is .
it's exactli log base two of n plu on .
rememb the level ar zero through log base two of n inclus .
and then we have an upper bound 6n for each of those log n plu on level .
so if we expand out thi quantiti , we get exactli the upper bound that wa claim earlier name the number of oper merg sort execut is at most 6n time log base <num> of n plu 6n .
so that my friend , is a run time analysi of the merg sort algorithm .
that's why it's run time is bound by a constant time n log n which , especi as n grow larg , it is far superior to the more simpl iter algorithm like insert or select sort .
have complet our first analysi of an algorithm , name an upper bound on the run time of the merg short algorithm .
what i wanna do next is take a step back , and be explicit about three assumpt , three bias that we made when we did thi analysi of merg short , and interpret the result .
these three assumpt we will adopt as guid principl for how to reason about algorithm , and how to defin a so call fast algorithm for the rest of the cours .
so , the first guid principl is that we us what's often call worst case analysi .
by worst case .
analysi , i simpli mean that our upper bound of six n log n plu six n .
appli to the number of line of execut for everi singl input arrai of length end .
we made absolut no assumpt about the input , where it come from , what it look like beyond what the input length n wa .
put differ , if , hypothet , we had some adversari whose sole purpos in life wa to concoct some malevol input design to make our algorithm run as slow as possibl .
the worst thi adversari could do , is .
upper bound by thi same number , 6n log n 6n .
now , thi , so , sort of worst case guarante pop out so natur from our analysi of merg short , you might well be wonder , what els could you do ?
well , two other method of analysi , which do have their place , although we won't realli dicuss them in thi cours , ar quot unquot , averag case analysi .
and also the us of a set of prespecifi benchmark .
by averag case analysi , i mean , you analyz the averag run time of an algorithm under some assumpt about the rel frequenc of differ input .
so , for exampl , in the sort problem , on thing you could do , although it's not what we do here .
you could assum that everi possibl input arrai is equal unlik , and then analyz the averag run time of an algorithm .
by benchmark , i just mean that on agre up front about some set , sai ten or twenti , benchmark input , which ar thought to repres practic or typic input for the algorithm .
now , both averag case analysi and benchmark ar us in certain set , but for them to make sens , you realli have to have domain knowledg about your problem .
you need to have some understand of what input ar more common than other , what input better repres typic input than other .
by contrast , in worst case analysi , by definit you're make absolut no assumpt about where the input come from .
so , as a result , worst case analysi is particularli appropri for gener purpos sub routin .
sub routin that you design .
find without have ani knowledg of how thei will be us or what kind of input thei will be us on .
and happili , anoth bonu of do worst case analysi , as we will in thi cours , it's usual mathemat much more attract than try to analyz the averag perform of an algorithm under some distribut over input .
or to understand the detail behavior of an algorithm on a particular set of benchmark input .
thi mathemet tractabilti wa reflect in our merg sort analysi , where we had no a priori goal of analyz the worst case , per se .
but it's natur what pop out of our reason about the algorithm's run time .
the second and third guid principl ar close relat .
the second on is that , in thi cours , when we analyz algorithm , we won't worri unduli about small constant factor or lower order term .
we saw thi philosophi at work veri earli on in our analysi of merg sort .
when we discuss the number of line of code that the merg subroutin requir .
we first upper bound it by 4m plu two , for an arrai of length m , and then we said , eh , let's just think about it as 6m instead .
let's have a simpler , sloppi upper bound and work with that .
so , that wa alreadi an exampl of not worri about small chang in the constant factor .
now , the question you should be wonder about is , why do we do thi , and can we realli get awai with it ?
so let me tell you about the justif for thi guid principl .
so the first motiv is clear , and we us it alreadi in our merg short analysi .
which is simpli wai easier mathemat , if we don't have to , precis pin down what the constant factor and lower order term ar .
the second justif is a littl less obviou , but is extrem import .
so , i claim that , given the level at which we're describ and analyz algorithm in thi cours , it would be total inappropri to obsess unduli about exactli what the constant factor ar .
recal our discuss of the merg subroutin .
so , we wrote that subroutin down in pseudocod , and we gave it analysi of 4m plu two on the number of line of code execut , given an input of length m .
we also note that , it wa somewhat ambigu exactli how mani line of code we should count it as , depend on how you count loop increment and so on .
so even there it's small constant factor could creep in given the under specif of the pseudo code .
depend on how that pseudo code get translat into an actual program languag like c or java .
you'll see the number of line of code deviat even further , not but a lot but again by small constant factor .
when such a program is then compil down into machin code , you'll see even greater varianc depend on the exact processor , the compil , the compil optim , the program implement , and so on .
so to summar , becaus we're go to describ algorithm at a level .
that transcend ani particular program languag .
it would be inappropri to specifi precis constant .
the precis constant were ultim determin by more machin depend aspect like who the programm is , what the compil is , what the processor is , and so on .
and now the third justif is frankli , we're just go to be abl to get awai with it .
that is , on might be concern that ignor thing like small constant factor lead us astrai .
that we wind up deriv result which suggest that an algorithm is fast when it's realli slow in practic , or vice versa .
and for the problem we discuss in thi cours we'll get extrem accur predict power .
even though we won't be keep track of lower term and constant factor .
when the mathemat analysi we do suggest that an algorithm is fast , inde it will be .
when it suggest that it's not fast , inde that will be the case .
so we lose a littl bit of granular of inform .
but we don't lose in what we realli care about , which is accur guidanc about what algorithm ar gonna be faster than other .
so the first two justif , i think , ar pretti self evid .
thi third justif is more of an assert , but it's on we'll be bake up over and over again as we proce through thi cours .
now , don't get me wrong .
i'm not sai constant factor aren't import in practic .
obvious , for crucial program the constant factor ar huge import .
if you're run the sorta crucial loop , you know , your start up's surviv depend on , by all mean optim the constant like crazi .
the point is just that understand tini constant factor in the analysi is an inappropri level of granular for the kind of algorithm analysi we're go to be do in thi cours .
okai , let move on the , the third and final guid principl .
so the third principl is that we're go to us what's call asymptot analysi , by which i mean we will focu on the case of a larg input size .
the perform of an algorithm as the size n of the input grow larg , that is , tend to infin .
now thi focu on larg input size is it wa alreadi evid when we interpret our bound on merg sort .
so , how did we describ the bound on merg sort ?
we said , oh , well , it need a number of oper proport , a constant fact or time in login .
and we veri cavalierli declar that thi wa better than ani algorithm which ha quadrat depend of it's run time on the number of oper .
so for exampl , we argu that merg sort is a better , faster algorithm than someth like insert sort , without actual discuss the constant factor at all .
so mathemat .
we were sai the run time of merg short , which we know , which we can repres as the function .
six n log base two of n 6n is better than ani function which ha a quadrat depend on n .
even on with a small constant like let sai <num> <num> n squar which might roughli be the run time of insert sort .
and thi is a mathemat statement that is true if and onli if n is suffici larg onc n grow larg it is certainli true that the express on the left is smaller than the express on the right but for small n the express on the right is actual go to be smaller becaus of the smaller lead term so in sai that merg sort is superior to insert sort the bia is that we're focus on problem with a larg n so the question you should have is that reason is that a justifi assumpt to focu on larg input size and the answer is certainli ye .
so the reason we focu on larg input size is becaus , frankli , those ar the onli problem which ar even , which ar at all interest .
if all you need to do is sort <num> number , us whatev method you want , and it's gonna happen instantan on modern comput .
you don't need to know sai , the divid and conquer paradigm , if all you need to do is sort <num> number .
so on thing you might be wonder is if , with comput get faster all the time accord to moor's law , if realli , it doesn't even matter to think about algorithm analysi , if eventu all problem size will just be trivial solvabl on super fast comput .
but , in fact , the opposit is true .
moor's law , with comput get faster , actual sai that our comput ambit will natur grow .
we natur focu on ever larger problem size .
and the gulf between an n squar algorithm and an m log n algorithm will becom ever wider .
a differ wai to think about it is in term of how much bigger a problem size you can solv .
as comput get faster .
if you ar us an algorithm with a run time which is proport to the input size then the comput get faster by a factor of four then you can solv problem that ar factor of four or larger .
wherea if you ar us an algorithm whose run time is proport to the squar of the input size then a comput get faster by a factor of four , you can onli solv doubl the problem size and we'll see even starker exampl of thi gulf between differ algorithm approach as the time goe on .
so to drive thi point home .
let me show you a coupl of graph .
so what we're look at here , is we're look at a graph , of two function .
so the solid function .
is the upper bound that we prove on merg sort .
so thi is gonna be 6nlog base2 n plu 6n .
and the dot line is an estim .
a rather gener estim about the run time of , sort .
name on half time n .
squar .
and we see here in the graph exactli the behavior that we discuss earlier , which is that the small n .
down here .
in fact becaus on half n .
squar ha a smaller lead constant it's actual a smaller function .
and thi is true up to thi cross point of mayb <num> or so .
again , beyond n <num> .
the quadrat growth in the n squar term .
overwhelm the fact that it had a smaller constant and it start be bigger than thi other function six of n six n so in the regim below <num> it's predict that the insert store will be better and in the regim abov <num> it's predict that merg sort will be faster .
now here's what's interest let's scale the x axi let's look well beyond thi cross point of <num> let's just increas it in order of magnitud up to a rais in size <num> .
and i want to emphas these ar still veri small problem size .
if all you need to do is sort arrai of size <num> you realli don't need to know divid and conquer or anyth els we'll talk about that's a pretti trivial problem on modern comput .
so what we're see is , that even for veri modest problem size here , arrai of , of , size , sai <num> .
the quadrat depend in the insert sort bound is more than dwarf the fact , that it had a lower constant factor .
so in thi larg regim , the gulf between the two algorithm is grow .
and of cours , if i increas it anoth 10x or 100x or 1000x to get to genuin interest problem size , the gap between these two algorithm would be even bigger , it would be huge .
that said , i'm not sai you should be complet ignor of constant factor when you implement algorithm .
it's still good to have a gener sens of what these constant factor ar so for exampl in highli tune version of merg sort which you'll find in mny program librari .
in fact , becaus of the differ in constant factor , the algorithm will actual switch from merg sort over to insert sort , onc the problem size drop below some particular threshold , sai seven element , or someth like that .
so for small problem size , you us the algorithm with smaller constant factor , and the insert sort for larger problem size , you us the algorithm and better rate of growth , mainli merg short .
so , to review our first guid princip is that we're go to pursu wors case analysi .
we're go to look to bound on the perform , on the run time of an algorithm which make no domain assumpt , which make no assumpt about which input of a given length the algorithm is provid .
the second guid princip is we're not go to focu on constant factor or lower return , that would be inappropri , given the level of granular at which we're describ algorithm and third is where go to focu on the rate of growth of algorithm for larg problem size .
put these three principl togeth , we get a mathemat definit of a fast algorithm .
name , we're gonna pursu algorithm whose worst case run time grow slowli as a function of the input size .
so let me tell you how you should interpret what i just wrote down in thi box .
so on the left hand side is clearli what we want .
okai , we want algorithm which run quickli if we implement them .
and on the right hand side is a propos mathemat surrog of a fast algorithm .
right , the left hand side is not a mathemat definit .
the right hand side is , as well becom clear in the next set of lectur .
so we're identifi fast algorithm , which , those that have good asymptot run time which grow slowli with the input size .
now , what would we want from a mathemat definit ?
we'd want a sweet spot .
on on hand we want someth we can actual reason about .
thi is why we zoom out and squint and ignor thing like constant factor and lower term .
we can't keep track of everyth .
otherwis we'd never be abl to analyz stuff .
on the other hand we don't want to throw out the babi with the bath water , we want to retain predict power and thi turn out thi definit turn out for the problem we're go to talk about in thi cours , to be the sweet spot for reason about algorithm okai worst case analysi us the asymptot run time .
we'll be abl to prove lot of theorem .
we'll be abl to establish a lot of perform guarante for fundament algorithm but at the same time we'll have good predict power what the theori advoc will in fact be algorithm that ar known to be best in practic .
so , the final explan i ow you , is , what do i mean by , the run time grow slowli , with respect to the input size ?
well , the answer depend a littl bit on the context , but , for almost all of the problem we're go to discuss , the holi grail will be to have what's call a linear time algorithm , an algorithm whose number of instruct grow proport to the input size .
so , we won't alwai be abl to achiev linear time , but that's , in some sens , the best case scenario .
notic , linear time is even better than what we achiev with our merg short algorithm for sort .
merg short run a littl bit superlinear , it's n time log n , run as the input size .
if possibl , we .
to be linear time .
it's not alwai gonna be possibl , but that is what we will aspir toward .
for most of the problem we'll discuss in thi cours .
look ahead , the next seri of video is go to have two goal .
first of all , on the analysi side , i'll describ formal what i mean by asymptot run time .
i'll introduc big oh notat and it variant , explain it mathemat definit , and give a number of exampl .
on the design side , we'll get more experi appli the divid and conquer paradigm to further problem .
see you then .
so let's talk about the absolut fundament problem of search a graph , and the veri relat problem of find path through graph .
so why would on be interest in search a graph , or figur out if there's a path from point a to point b ?
well there's mani , mani reason .
i'm go to give you a highli non exhaust list on thi slide .
so let me begin with a veri sorta obviou and liter exampl , which is if you have a physic network , then often you want to make sure that the network is fulli connect in the sens that you can get from ani start point to ani other point .
so , for exampl , think back to the phone network .
it would've been a disast if caller from california could onli reach caller in nevada , but not their famili member in utah .
so obvious a minim condit for function in someth like a phone network is that you can get from ani on place to ani other place , similarli for road network within a given countri , and so on .
it can also be fun to think about other non physic network and ask if thei're connect .
so on network that's fun to plai around with is the movi network .
so thi is the graph where the node correspond to actor and actress , and you have an edg between two node , if thei plai a role in a common movi .
so thi is go to be an undirect graph , where the edg correspond to , not necessarili co star , but both the actor appear at least some point in the same movi .
so version of thi movi network you should be abl to find publicli avail on the web , and there's lot of fun question you can ask about the movi network .
like , for exampl , what's the minimum number of hop , where a hop here again is the movi that two peopl both plai a role in ?
the minimum number of hop or edg from on actor to anoth actor , so perhap the most famou statist that's been thought about with the movi is the bacon number .
so thi refer to the fairli ubiquit actor kevin bacon , and the question the bacon number of an actor is defin as the minimum number of hop you need in thi movi graph to get to kevin bacon .
so , for exampl , you could ask about jon hamm , also known as don draper from mad men .
and you could ask how mani edg do you need on a path through the movi graph to get to kevin bacon ?
and it turn out that the answer is <num> , excus me , <num> edg .
you need on intermedi point , name colin firth .
and that's becam , that's becaus colin firth and kevin bacon both star in atom egoyan's movi , where the truth li , and jon hamm and colin firth were both in the movi a singl man .
so that would give jon hamm a bacon number of <num> .
so , these ar the kind of question you're gonna ask about connect .
not just physic network , like telephon and telecommun network , but also logic network about parallel relationship between object in gener .
so the bacon number is fundament not just about ani path , but actual shortest path , the minimum number of edg you need to travers to get from on actor to kevin bacon .
and shortest path ar also have a veri practic us , that you might us yourself in the drive direct .
so when you us a websit or a phone app and you ask for the best wai to get from where you ar now to sai some restaur where you're gonna have dinner , obvious you're try to find some kind of path through a network through a graph , and inde often you want the , the shortest path , perhap in mileag or perhap in anticip travel time .
now i realiz that when you ar think about path and graph , it's natur to focu on sort of veri liter path and quit liter physic network .
thing like rout through a road network or path through the internet and so on .
you should realli think more abstractli as a path as just a sequenc of decis , take you from some initi state to some final state .
and it's thi abstract mental which is what make graph search so ubiquit , it feel like artifici intellig , where you want to formul a plan of how to get from an initi state to some goal state .
so , to give a simpl recreat exampl , you can imagin just try to understand how to comput automat a wai to fill in a sudoku puzzl so that you get to , so that you solv the puzzl correctli .
so you might ask , you know , what is the graph that we're talk about , when we wanna solv a sudoku puzzl .
well thi is gonna be a direct graph , where here the node correspond to partial complet puzzl .
so , for exampl , at on node of thi extrem larg graph , perhap <num> out of the <num> cell ar fill in with some kind of number , and now , again , rememb a path is suppos to correspond to a sequenc of decis .
so , what ar the action that you take in solv sudoku ?
well , you fill in a number into a squar .
so , an edg which here is go to be direct , is go to move you from on partial complet puzzl to anoth , where on previous empti squar get fill in with on number .
and of cours then the path is that you're interest in comput , or what your search for when you search thi graph .
you begin with the initi state of the sudoku puzzl and you want to reach some goal state where the sudoku puzzl is complet fill in without ani violat of the rule of sudoku .
and of cours it's easi to imagin million of other situat where you wanna formul some kind of plan like thi , for exampl if you have a robot hand and you wanna grasp some object , you need to think about exactli how to approach the object with thi robot hand , so that you can grab it without , for exampl , first knock it over , and you can think of million of other exampl .
anoth thing which turn out to be close relat to graph search , as we'll see , it ha mani applic in it own right , is that of comput connect inform about graph , in particular the connect compon .
so thi , especi for undirect graph , correspond to the piec of the graph .
we'll talk about these topic in their own right , and i'll give you applic for them later .
so for undirect graph i'll briefli mention an easi cluster heurist you can deriv out of comput connect compon .
for direct graph where the veri definit of comput compon is a bit more subtl , i'll show you applic to understand the structur of the web .
so these ar a few of the reason why it's import for you understand how to effici search graph .
it's a , a fundament and wide applic graph primit .
and i'm happi to report that in thi section of the cours , pretti much anyth , ani question we wanna answer about graph search , comput connect compon , and so on , there's gonna be realli fast algorithm to do it .
so , thi will be the part of the cours where there's lot of what i call for free primit , process step , subroutin you can run without even think about it .
all of these algorithm we're gonna discuss in the next sever lectur , ar gonna run in linear time , and thei're gonna have quit reason constant .
so , thei're realli bare slower than read the input .
so , if you have a graph and you're try to reason about it and you're try to make sens about it , you should in some sens feel free to appli ani of these subroutin we're gonna discuss to try and glean some more inform about what thei look like , how you might us the network data .
there's a lot of differ approach to systemat search a graph .
so , there's mani method .
in thi class we're gonna focu on two veri import on , mainli breadth first search and depth first search .
but all of the graph search method share some thing in common .
so , in thi slide let me just tell you the high order bit of realli ani graph search algorithm .
so graph search subroutin gener ar pass as input a start search vertex from which the search origin .
so that's often call sourc vertex .
and your goal then is to find everyth findabl from the search vertex and obvious you're not gonna find anyth that you can't find that is not findabl .
what i mean by findabl , i mean , there's a path from the start point to thi other node .
so ani other node to which you can get along on a path from the start point you should discov .
so , for exampl , if you're given an undirect graph that ha three differ piec , like thi on i'm draw on the right , then perhap s is thi left most node here , then the findabl vertic start from s , i . e .
the on which you can reach from a path to s , is clearli precis these four vertic .
so , you would want graph search to automat discov and effici discov these four vertic if you start at s .
you can also think about a direct version of the exact same graph , where i'm gonna direct the vertic like so .
so now the definit of the findabl node is a littl bit differ .
we're onli expect to follow arc forward , along the forward direct .
so we should onli expect at best to find all of the node that you can reach , by follow a success of forward arc , that is , ani node that there's a path to from s .
so in thi case , these three node would be the on we'd be hope to find .
thi blue node to the right , we would no longer expect to find , becaus the onli wai to get there from s , is by go backward along arc .
and that's not what we're go to be think about in our graph search .
so we want to find everyth findabl , i . e .
that we can get to along path , and we want to do it effici .
effici mean we don't want to explor anyth twice .
right , so the graph ha m arc , m edg and n node or n vertic and realli we wanna just look at either each piec of the graph onli onc for a small cost number of time .
so look for run time which is linear on the size of the graph that is big o of m plu n .
now when we were talk about repres graph , i said that in mani applic , it's natur to focu on connect graph , in which case m is gonna domin n , and you're gonna have at least as mani edg as node , essenti .
but connect is the classic case where you might have the number of edg of be much smaller than the number of node .
there might be mani piec of the whole point of what you're try to do is discov them .
so , for thi sequenc of lectur where we talk about graph search and connect , we will usual write m plu n .
we'll think that either on can be bigger or smaller than the other .
so let me now give you a gener approach to graph search .
it's gonna be under specifi , there'll be mani differ wai to instanti it .
two particular instanti will give us breadth first search and depth first search but here is just a gener systemat method to find everyth findabl without explor anyth more than onc .
so motiv by the second goal , the fact that we don't want to explor anyth twice , with each node , with each vertex , we're gonna rememb whether or not we explor it befor .
so we just need on boolean per node and we will initi it by have everyth unexplor except s , our start point we'll have it start off as explor .
and it's us to think of the node thu far as be in some sens territori conquer by the algorithm .
and then there's go to be a frontier in between the conquer and unconqu territori .
and the goal of the gener outcom is that each step we supplement the conquer territori by on new node , assum that there is on adjac to the territori you've alreadi conquer .
so for exampl in thi top exampl with the undirect network , initi the onli thing we've explor is the start point s .
so that's sort of our home base .
that's all that we have conquer so far .
and then in our main while loop , which we iter as mani time as we can until we don't have ani edg meet the follow criterion , we look for an edg with on end point that we've alreadi explor .
on end point insid the conquer territori and then the other end point outsid .
so thi is how we can in on hop supplement the number of node we've seen by on new on .
if we can't find such an edg then thi is where the search stop .
if we can find such an edg , well then we suck v into the conquer territori .
we think of it be explor .
and we return to the main while loop .
so , for exampl , in thi exampl on the right , we start with the onli explor node be s .
now , there's actual two edg that cross the frontier , in the sens on of the endpoint is explor , name on of the endpoint is s , and the other on is some other vertex .
right ?
there's thi there's these two vert , two edg to the left , two vertic adjac to s .
so , in thi algorithm we pick either on .
it's un , under specifi which on we pick .
mayb we pick the top on .
and then all of the sudden , thi second top vertex is also explor so the conquer territori is a union of them , and so now we have a new frontier .
so now again we have two edg that cross from the explor node to the unexplor node .
these ar the edg that ar in some sens go from northwest to southeast .
again , we pick on of them .
it's not clear how .
the algorithm doesn't tell us , we just pick ani of them .
so , mayb for exampl we pick thi right most edg cross the frontier .
now the right most edg of these right most vertex of these four is explor so our conquer territori is the top three vertic .
and now again we have two edg cross the frontier .
the two edg that ar incid to the bottom node , we pick on of them , not clear which on , mayb thi on .
and now the bottom node is also explor .
and now there ar no edg cross the frontier .
so there ar no edg who , on the on hand , have on end point be explor , and the other end point be unexplor .
so these will be the four vertic , as on would hope , that the search will explor start from s .
well gener the claim is that thi gener graph search algorithm doe what we want .
it find everyth findabl from the start point and moreov it doesn't explor anyth twice .
i think that's fairli clear that it doesn't explor anyth twice .
right ?
as soon as you look at a node for the first time , you suck it into the conquer territori never to look at it again .
similarli as soon as you look at an edg , you suck them in .
but when we explor breadth and depth first search , i'll be more precis about the run time and exactli what i mean by you don't explor someth twice .
so , at thi level of gener , i just wanna focu on the first point , that ani wai you instanti thi search algorithm , it's go to find everyth findabl .
so , what do i realli mean by that ?
the formal claim is that at the termin of thi algorithm , the node that we've mark exp , explor , ar precis the on that can be reach via a path from s .
that's the sens in which the algorithm explor everyth that could potenti be findabl from the start point s .
and on thing i wanna mention is that thi claim and the proof i'm go to give of it , it hold whether or not g is an undirect graph or a direct graph .
in fact , almost all of the thing that i'm gonna sai about graph search , and i'm talk about breadth first search and depth first search , work in essenti the same wai , either in undirect graph or direct graph .
the obviou differ be in an undirect graph you can travers an edg in either direct .
in a direct graph , we're onli suppos to travers it in a forward direct from the tail to the head .
the on big differ between undirect and direct graph is when we connect comput and i'll remind you when we get to that point which on we're talk about .
okai ?
but for the most part , when we just talk about basic graph search it work essenti the same wai whether it's undirect or direct .
so keep that in mind .
all right , so why is thi true ?
why ar the node that get explor precis the node for which there's a path to them from s ?
well , on direct is easi .
which is , you can't find anyth which is not findabl , that is , if you wind up explor a node , the onli reason that can happen is becaus you travers a sequenc of edg that got you there .
and that sequenc of edg obvious defin a path from s to v .
if you realli want to be pedant about the forward direct that explor node have to have path from s .
then you can just do an easi induct .
and i'll leav thi for you to check , if you want , in the privaci of your own home .
so the import direct of thi claim is realli the opposit .
why is it that no matter how we instanti thi gener graph search procedur , it's imposs for us to miss anyth .
that's the crucial point , we don't miss anyth that we could , in principl , find via a path .
but we're gonna proce by contradict .
so , what doe that mean , we're go to assum that , the statement that we want to prove is true , is not true .
which mean that , it's possibl that , g ha a path from s to v and yet , somehow our algorithm miss it , doesn't mark it as explor .
all right , that's the thing we're realli hope doesn't happen .
so let's suppos it doe happen and then deriv a contradict .
so suppos g doe have a path from s to some vertex v .
call the path p .
i'm gonna draw the pictur for an undirect graph but the situat would be same in the , in the direct case .
so there's a bunch of hop , there's a bunch of edg and then eventu thi path end at v .
now the bad situat , the situat from which we want to deriv a contradict is that v is unexplor at the end of thi algorithm .
so let's take stock of what we know .
s for sure is explor , right .
we initi thi search procedur so that s is mark as explor .
v by hypothesi in thi proof by contradict is unexplor .
so s is explor , v is unexplor .
so now imagin we , just in our head as a thought experi which travers thi path p .
we start at s and we know it's explor .
we go the next vertex , it mai or mai not have been explor , we're not sure .
we go to the third vertex , again who know .
might be explor , might be unexplor and so on , but by time we get to v , we know it's unexplor .
so we start at s , it's been explor , we get to v it's been unexplor .
so at some point there's some hop , along thi path p , from which we move from an explor vertex , to an unexplor vertex .
there ha to be a switch , at some point , cuz the end of the dai at the end of the path we're at an unexplor node .
so consid the first edg , and there must be on that we switch from be at an explor node to be at an unexplor node .
so , i'm go to call the end point of thi purport edg u and w .
where u is the explor on and w is the unexplor on .
now , for all we know u could be the same as s , that's a possibl , or for all we know , w could be same as v .
that's also a possibl .
in the pictur , i'll draw it as if thi edg ux wa somewher in the middl of thi path .
but , again it mai be at on of the end .
that's total fine .
but now in thi case , there's someth i need you to explain to me .
how is it possibl that , on the on hand , our algorithm termin .
and on the other hand , there's thi edg u comma x .
where u ha been explor and x ha not been explor .
that , my friend , is imposs .
our gener search algorithm doe not give up .
it doe not termin , unless there ar no edg where the on end point is explor and the other end point is unexplor .
as long as there's such an edg , it ha , is gonna suck in that unexplor vertex into the conquer territori , it's gonna keep go .
so the upshot is there's no wai that our algorithm termin with thi pictur .
with there be an edg u x , u explor , x unexplor .
so , that's the contradict .
thi contradict the fact that our algorithm termin with v unexplor .
so that is a gener approach to graph search .
so that i hope give you the flavor of how thi is go to work .
but now there's two particular instanti of thi gener method that ar realli import and have their own suit of applic .
so we're gonna focu on breadth first search and depth first search .
we'll cover them in detail in the next coupl of video .
i wanna give you the highlight to conclud thi video .
now let me just make sure it's clear where the ambigu in our gener method is .
why we can have differ instanti of it that potenti have differ properti and differ applic .
the question is at a given iter of thi while loop , what do you got ?
you've got your node that you've alreadi explor , so that includ s plu probabl some other stuff , and then you've got your node that ar unexplor , and then you have your cross edg .
right ?
so , there ar edg with on point in each side .
and for an undirect graph , there's no orient to worri about .
these cross edg just have on endpoint on the explor side , on endpoint on the unexplor side .
in the direct case , you focu on edg where the tail of the edg is on the explor side and the head of the edg is on the unexplor side .
so , thei go from the explor side to the unexplor side .
and the question is , in gener , in an iter of thi while loop there's gonna be mani such cross edg .
there ar mani differ unexplor node we could go to next , and differ strategi for pick the unexplor node to explor next lead us to differ graph search algorithm with differ properti .
so the first specif search strategi we're gonna studi is breadth first search , colloqui known as bf .
so let me tell you sort of the high level idea and applic of bread first search .
so , the goal is go to be to explor the node in what i call , layer .
so , the start point s will be in it own layer , layer <num> .
the neighbor of s will constitut layer <num> , and then layer <num> will be the node that ar neighbor of layer <num> but that ar not alreadi in layer zero or layer on , and so on .
so layer i plu on , is the stuff next to layer i that you haven't alreadi seen yet .
you can think of thi as a fairli cautiou and tent explor of the graph .
and it's gonna turn out that there's a close correspond between these layer and shortest path distanc .
so if you wanna know the minim number of hop , the minim number of edg you need in a path to get from point a to point b in a graph .
the wai we want to know the fewest number of edg in the movi graph necessari to connect to john hamm to kevin bacon .
that correspond directli to these layer .
so if a node is in layer i , then you need i edg to get from s to i in the graph .
onc we discuss breadth first search , we'll also discuss how to comput the connect compon , or the differ piec , of an undirect graph .
turn out thi isn't that special to breadth first search , you can us ani number of graph search strategi to comput connect compon in undirect graph .
but i'll show you how to do it us a simpl loop version of breadth first search .
and we'll be abl to do thi stuff in the linear time that we want .
the veri ambiti goal of get linear time .
to get the linear time implement , you do wanna us the right data structur , but it's a simpl , simpl data structur , someth probabl you've seen in the past .
name a queue .
so , someth's that first in and first out .
so , the second search strategi that's super import to know is depth first search , also known as df to it friend .
depth first search ha a rather differ feel than breadth first search .
it's a much more aggress search where you immedi try to plung as deep as you can .
it's veri much in the spirit of how you might explor a maze , where you go as deepli as you can onli backtrack when absolut necessari .
depth first search will also have it own set of applic .
it's not , for exampl , veri us for comput shortest path inform , but especi in direct graph it's go to do some remark thing for us .
so , in direct acycl graph , so a direct graph with no direct cycl it will give us what's call the topolog order .
so it'll sequenc the node in a linear order from the first to the last , so that all of the arc of the direct graph go forward .
so thi is us for exampl if you have a number of task that need to get complet with certain preced constraint .
like for exampl you have to take all of the class in your undergradu major , and there wa certain prerequisit , topolog order will give you a wai in which to do it , respect all of the prerequisit .
and final where for undirect graph it doesn't realli matter whether you us bf or df to connect the compon , in the direct graph where even defin connect compon is a littl tricki it turn out depth first search is exactli what you want .
that's what you're go to get a linear time implement for comput the right notion of connect compon in the direct graph case .
time wise , both of these ar superb strategi for explor a graph .
thei're both linear time with veri good constant .
so depth first search again , we're gonna get o of m plu n time in a graph with m edg and n vertic .
you do wanna us a differ data structur reflect the differ search strategi .
so , here becaus you're explor aggress , as soon as you get to a node you'll meet and you start explor it neighbor , you wanna last in first out data structur , also known as a stack .
depth first search also admit a veri eleg recurs formul , and in that formul , you don't even need to maintain a stack data structur explicitli , the stack is implicitli taken care of in the recurs .
so that conclud thi overview of graph search .
both what it is , what our goal ar , what kind of applic thei have and two of the most common strategi .
the next coupl video ar go to explor these search strategi , as well as a coupl of these applic in greater depth .
so in thi lectur we're go to drill down into our first specif , search strategi for graph and also explor some applic .
name , breadth first search .
so let me remind you the intuit and applic of breath first search .
the plan is to systemat explor the node of thi graph begin with the given start vertex in layer .
so let's think about the follow exampl graph .
where s is the start point for our breadth first search .
so to start vertex s will constitut the first layer .
so we'll call that l zero .
and then the neighbor of s ar go to be the first layer .
and so those ar the vertic that we explor just after s .
so those ar l on .
now the second layer is go to be the vertic that ar neighbor vertic of l on but ar not themselv in l on or for that matter l zero .
so that's go to be c and d .
that's go to be the second layer .
now you'll notic for exampl s is itself a neighbor of these node in layer on , but we've alreadi count that in a previou layer so we don't count s toward l two .
and then final the neighbor of l two , which ar not alreadi put in some layer is e .
that will be layer three .
again notic c and d ar neighbor of each other but , thei've alreadi been classifi in layer two .
so , that's where thei belong , not in layer three .
so that's the high level pictur of breadth first search you should have .
we'll talk about how to actual precis implement it on the next slide .
again just a coupl other thing that you can do with breadth first search which we'll explor in thi video is comput shortest path .
so it turn out shortest path distanc correspond precis to these layer .
so , for exampl if you had that as s , you had that as the kevin bacon node in the movi graph , then jon hamm would pop up in the second layer from the breadth first search from kevin bacon .
i'm also go to show you how to comput the connect compon of an undirect graph .
that is to comput it piec .
we'll do that in linear time .
and for thi entir sequenc of video on graph primit , we will be satisfi with noth less than the holi grail of linear time .
and again , rememb in a graph you have two differ size paramet , the number of edg m and the number of node n .
for these video i'm not go to assum ani relationship between m and n .
either on could be bigger .
so linear time's gonna mean o of m plu n .
so let's talk about how you'd actual implement breadth first search in linear time .
so the sub routin is given as input both the graph g .
i'm gonna explain thi as if it's undirect , but thi entir procedur will work in exactli the same wai for a direct graph .
again , obvious in an undirect graph you can travers an edg in either direct .
for a direct graph , you have to be care onli to travers arc in the intend direct from the tail to the head , that is travers them forward .
so as we discuss when we talk about just gener strategi for graph search , we don't want to explor anyth twice , that would certainli be ineffici .
so we're go to keep a boolean at each node , mark whether we've alreadi explor it or not .
and by default , i'm just , we're just go to assum that node ar unexplor .
thei're onli explor if we explicitli mark them as such .
so we're go to initi the search with the start vertex s .
so we mark s as explor and then we're gonna put that in what i wa previous call conquer territori the node we have alreadi start to explor .
so to get linear time we ar gonna have to manag those in a slightli non naiv but , but pretti straightforward wai name via a queue , which is a first in first out data structur that i assum you have seen .
if you have never seen a queue befor , pleas look it up in a program textbook or on the web .
basic a queue is just someth where you can add stuff to the back in constant time and you can take stuff from the front in constant time .
you can implement these , for exampl , us a doubli link list .
now recal that in the gener systemat approach to graph search , the trick wa to , in each iter of some while loop , to add on new vertex to the conquer territori .
to identifi on unexplor node that is now go to be explor .
so that while loop's gonna translat into on in which we just check if the queue is non empti .
so we're assum that the queue data structur support that queri in constant time which is easi to implement .
and if the queue is not empti we remov a node from it .
and becaus it's a queue , remov node from the front is what you can do in constant time .
so call the node that you get out of the queue v .
so , now we're go to look at v's neighbor , vertic with which it share edg , and we're gonna see if ani of them have not alreadi been explor .
so , if w's someth we haven't seen befor , that's unexplor , that mean it's in the unconqu territori , which is great .
so , we have a new victim .
we can mark w as explor .
we can put it in our queue and we've advanc the frontier and now we have on more explor node than we did previous .
and again , a queue by construct , it support ad constant time addit at the end of the queue , so it's where we put w .
so , let's see how thi code actual execut in thi same graph that we were look at in the previou slide .
and what i'm gonna do is i'm gonna number the node in the order in which thei ar explor .
so , obvious the first node to get explor is s .
that's where the queue start .
so now , when we follow the code , what happen ?
well in the first iter of the while loop we ask is the queue empti ?
no it's not , becaus s is in it .
so we remov in thi case the onli node of the queue .
it's s .
and then we iter over the edg incid to s .
now there ar two of them .
there's the edg between s and a and there's the edg between s and b .
and again thi is still a littl under specifi .
in the sens that the algorithm doesn't tell us which of those two edg we should look at .
turn out it doesn't matter .
each of those is a valid execut of breadth first search .
but for concret , let's suppos that of the two possibl edg , we look at the edg s comma a .
so , then we ask , ha a alreadi been explor ?
no , it hasn't .
thi is the first time we've seen it , so we sai , oh goodi .
thi is sort of new grist for the mill .
so , we can add a to the queue at the end and we mark w as , sorri mark a as explor .
so , a is gonna be the second vertex that we mark .
so , after mark a as explor and ad it to the queue , so now we go back to the for loop , and so now we move on to the second edg .
it's into s , that's the edg between s and b .
so , we ask , have we alreadi explor b ?
nope , thi is the first time we've seen it .
so , now we have the same thing with b .
so , b get mark as explor and get ad to the queue at the end .
so the queue at thi junctur ha first a record for a , caus that wa the first on we put in it after we took s out .
and then b follow a in the queue .
again , depend on the execut thi could go either wai .
but for concret , i've done it so that a got ad befor b .
so at thi point , thi is what the queue look like .
so now we go back up to the while loop , we sai is the queue empti ?
certainli not .
there's actual two element .
now we remov the first node from queue , in thi case , that's the node a that wa the on we put in befor the node b .
and so now we sai , well , let's look at all the edg incid to a .
and in thi case a ha two two incid edg .
it ha on that it share with s and it ha on that it share with c .
and so , if we look at the edg between a and s , then we'd be ask an if statement .
ha s alreadi been explor ?
ye it ha , that's where we start .
so , there's no reason to do anyth with s .
that's alreadi been taken out of the queue .
so , in thi for loop for a , there's two iter .
on involv the edg with s , and that on we complet ignor .
but then there's the other edg that a share with c , and c we haven't seen yet .
so , at that part of the for loop , we sai ahah .
c is a new thing , new node we can mark as explor and put in the queue .
so , that's gonna be our number four .
so now how ha the queue chang .
well , we got rid of a .
and so now b is in the front and we ad c at the end .
and so now the same thing happen .
we go back to the while loop , the queue is not empti , we take off the first vertex , in thi case that's gonna be b .
b ha three incid edg , it ha on incid s but that's irrelev , we've alreadi seen s .
it ha on incid to c , that's also irrelev , that's also irrelev , becaus we've alreadi seen c .
true , we just saw it veri recent , but we've alreadi seen it .
but the edg between b and d is new , and so that mean we can take the node d , mark it as explor and add it to the queue .
so d is go to be the fifth on that we see .
and now the queue ha the element c follow by d .
so now we go back to the while loop and we take c off of the queue .
it again ha four now edg .
the on with a is irrelev , we've alreadi seen a .
the on with b is irrelev , we've alreadi seen b .
the on with d is irrelev , we've alreadi seen d .
but we haven't seen e yet .
so , when we get to the part of the for loop , or the edg between c and e , we sai , aha , e is new .
so e will be the sixth and final vertex to be mark as explor .
and that will get ad at the end of the queue .
so then in the final two iter of the while loop the d is go to be remov , we'll iter through it three edg , none of those will be relev becaus we've seen the three endpoint .
and then we'll go back to the while loop and we'll get rid of the e .
e is irrelev caus it ha two edg we've alreadi seen the other endpoint .
now we go back to the while loop .
the queue is empti .
and we stop .
that is breadth first search .
and to see how thi simul the notion of the layer that we were discuss in the previou slide notic that the node ar number accord to the layer that thei're in , so s wa layer zero .
and then the two node that s caus to get ad to the queue , the a and the b , ar number two and three , and the edg of layer three ar precis the on , sorri the edg of layer two ar precis the on that got ad to the queue , while we were process the node from layer on .
that is , c and d ar precis the node that got ad to the queue while we were process a and b .
so , thi is level zero , level on , and level two .
e is the onli node that got ad to the queue while we were process level , layer two .
the vertic c and d .
so e will be the third layer .
so , in that sens , by us a first in first out data structur , thi queue , we do wind up kinda process the node accord to the layer that we discuss earlier .
so , the claim that breadth first search is a good wai to explor a graph , in the sens that it meet the two high level goal that we delin in the previou video .
first of all it find everyth findabl , and obvious noth els , and second of all , it doe it without redund .
it doe it without explor anyth twice , which is the kei to it linear time implement .
so a littl bit more formal , claim number on .
at the end of the algorithm , the vertic that we've explor ar precis the on such that there wa a path from s to that vertex .
again thi claim is equal valid , whether you're run bf in an undirect graph or a direct graph .
of cours in an undirect graph , mean an undirect path from s to v , wherea a direct graph in a direct path from s to v .
that mean a path where everi arc in the path get travers in the forward direct .
so , why is thi true ?
well , thi is true , we basic prove thi more gener for ani graph search strategi of a certain form of which breadth first search is on .
if it's hard for you to see the right wai to interpret breadth first search as a special case of our gener search algorithm , you can also just look at our proof for the gener search algorithm and copi it down for breadth first search .
so it's clear that you're onli gonna , again , the forward direct of thi claim is clear .
if you actual find someth , if someth's mark as explor , it's onli becaus you found a sequenc of edg that led you there .
so the onli wai you mark someth as explor is if there's a path from s to v .
convers , to prove that anyth with an s to v , for with a path from v will be found , you can proce by contradict you can look at the part of the path from s to v that , that bf doe successfulli explor , and then you gotta ask , why didn't it go on more hop ?
it never would've termin befor reach all the wai to v .
so , you can also just copi that same proof that we had for the gener search strategi in the previou video .
okai ?
so , again , the upshot .
breadth first search find everyth you'd wanna find .
okai ?
so , it onli travers path , so you're not gonna find anyth where there isn't a path to it .
but it never miss out .
okai ?
anyth where there's a path , bf , guarante to find it .
no problem .
claim number two is that the run time is exactli what we want and i am gonna state it in a form that will be us later when we talk about connect compon .
so the run time of the main while loop , ignor ani kind of pre process or initi is proport to what i am gonna call ns and ms which is the number of node that can be reach from s and number of edg that can be reach from s .
and the reason for thi claim it just becom clear if you inspect the code which we'll do in a second .
so let's return to the code and just talli up all the work that get done .
so i'm gonna ignor thi initi .
i'm just gonna focu on the main while loop .
so we can summar the total work done in thi while loop as follow .
first we just think about the vertic so in thi search we're onli gonna deal , ever deal with the vertic that ar findabl from s , so that's ns .
and what do we do for the given node , well we insert it into the queue and we delet it from the queue .
all right ?
so we're never gonna deal with a singl node more than onc .
so that's constant time overhead per vertex that we ever see , so that's the proport of the ns part .
now , a given edg , we might look at it twice .
so , for an edg v w , we might consid it onc when we first look at the vertex v , and we might consid it again when we look at the vertex w .
each time we look at an edg we do constant work .
so that mean we're onli gonna do constant work per edg .
okai .
so we look at each vertex at most onc .
we look at each edg findabl from s at most twice .
we do constant time , constant work when we look at someth .
so the overal run time is go to be proport to the number of vertic findabl from s plu the number of edg findabl from s .
so , that's realli cool .
we have a linear time of implement of a realli nice graph search strategi .
moreov we just need veri basic data structur , a queue , to make it run fast with small constant .
but it get even better .
we can us breadth first search as a work hors for some interest applic .
so , that's what we'll talk about in the rest of thi video .
and let's begin with the idea of shortest path .
so , again i'll give you the movi graph .
i'll give you kevin bacon as a start point .
what's the fewest number of hop , the fewest number of edg on a path that lead to , sai , jon hamm ?
so some notat , i'm go to us dist of v , to denot thi shortest path distanc .
so with respect to a start node s , the fewest number of hop or the fewest number of edg on a path that start at s , and goe to v .
and again you can defin thi in the same wai for undirect graph or direct graph .
in a direct graph , you alwai want to travers arc in the forward direct , in the correct direct .
and to do thi we just have to add a veri small amount of extra code to the bf code that i show you earlier .
it's just gonna be a veri small constant overhead , and basic it just keep track of what layer each node belong to , and the layer ar exactli track shortest path distanc awai from the start point s .
so what's the extra code .
well first in the initi step , you set your preliminari estim of the distanc , the number of the shortest path distanc from s to vertex v as well if v equal s , you know you can get from s to s on a path of length zero , the empti path .
and if it's ani other vertex all bet ar off , you have no idea if there's a path to v at all .
so let's just initi put plu infin for all vertic other than the start point .
thi is someth we will of cours revis onc we actual discov a path to vertex v .
and the onli other extra code you have to add is , when you're consid , so when you take a vertex off of the front of the queue and then you iter through it edg and you're consid on of those edg v , w , so your v would be the vertex that you just remov from the front of the queue .
and as usual if the other end of the edg w ha alreadi been dealt with then , you know , you just throw it out .
that would be redund work to look at it again .
but if thi is the first time you've seen the vertex w .
then , in addit to what we did previous , in addit to mark it as explor and put it in the queue at the back , we also comput it distanc , and it distanc is just go to be on more than the distanc of the vertex v , respons for w's addit to the queue , respons for first discov thi vertex w .
so , return to our run exampl of breadth first search , let's see what happen .
so , again , rememb the wai thi work is we start out with from the vertex s , and we set the distanc , you know in our initi equal to zero .
we don't know what the distanc is of anyth els .
so , then how did breadth first search work ?
so , we , in the initi step we put s in the queue .
we go to the main while loop , and then the queue's not empti .
we extract s from the queue .
we look at it neighbor .
those neighbor ar a and b .
we handl them in some order .
let's again think of that we first handl the edg between s and a .
so , then what do we do ?
we sai we haven't seen a yet .
so we mark a as explor .
we put a in the queue at the front , and now we have thi extra step .
it's the first time we're see a , so we wanna comput it distanc .
and we comput it distanc as on more than the vertex respons for discov a .
and so in thi case s wa the vertex whose explor unveil the exist of the vertex a to us .
s's distanc is zero so we set a's distanc to on .
and that's tantamount to be a member of the ith layer .
so what happen in the next iter of the while loop .
so now the queue contain sorri , the next iter of the for loop , excus me .
so after we've handl the edg s comma a , we're still deal with s's edg , now we handl the edg s comma b .
we put , it is the first time we've seen b .
we put b at the end of the queue , we mark it as explor , and then we also execut thi new step .
we set b's distanc to on more than the vertex respons for discov it .
that would again be the vertex s .
s led to b's discoveri .
and so we set b's distanc to be on more than s's distanc , also known as on .
and that correspond to be the other node in layer on .
now have handl all of s's adjac arc we go back to the while loop .
we ask if the queue is empti .
certainli not .
it take two vertic , first a then b .
we extract the first vertex cuz it's fifo , that would be the vertex a .
now we look at a's incid edg .
there's s comma a , which we ignor .
there's a comma c .
thi is the first time we've seen c .
so as befor we mark c as explor .
we add c to the end of the queue and now again we have thi addit line .
we set c's distanc to be on more than the vertex respons for it discoveri .
in thi case it's a .
that first discov c .
so we're gonna set c's distanc to be on more than a's distanc also known as two .
so then have handl a we move on to the next vertex in the queue , which in thi case is b .
again we can forget about the edg between s and v .
we've alreadi seen s , we can forget about the edg between b and c .
we've alreadi seen c but d is now discov for the first time via b .
it get more as explor , it goe to the end of the queue and it distanc is set equal to on more than b's distanc which is two .
so , then we deal with c .
again it ha four arc , four edg , three of them ar irrelev .
the on to e is not irrelev , caus thi is the first time we've seen e .
so , e's distanc is comput as on more than c , caus c wa the on who first found e , and so e get a distanc of three , and then the rest of the algorithm proce as befor .
and you will notic that the label , the shortest path label , ar exactli the layer as promis .
i hope you find it veri easi to believ at thi point that , that claim is true in gener .
that the distanc comput by breadth first search for an arbitrari vertex v , that's reachabl from s is , that's gonna be equal to i if and onli if v is in the ith layer as we've been defin it previous .
and what doe it realli mean to be in the ith layer ?
it mean that the shortest path distanc between v and s ha i hop , i edg .
so i don't wanna spend time give a super rigor proof of thi claim but let me just give you the gist , the basic idea , and i encourag you to produc some formal proof at home if that is someth that interest you .
so on wai to do it is you can do it by induct on the layer i .
and so what you want to prove is that all of the node that belong to a given layer i do inde , breadth first search doe inde comput the distanc of i for them .
so what doe it mean to be a node in layer i ?
well , first of all , you can't have been seen in either of the , ani of the previou layer ; you weren't a member of layer zero through i minu on .
and furthermor , you're a , a neighbor of somebodi who's in layer i minu on .
right ?
you're seen for the first time onc all of the layer i minu on node ar process .
so the induct hypothesi tell you that distanc were correctli comput for everybodi from the lower l , from the lower layer .
so in particular , whoever thi node v wa from layer i minu on wa respons for discov u , in layer i .
it ha a distanc comput as i minu on .
your is assign to be on more than it , name i .
so that push through the induct step everyth in layer i inde get the correct label of a shortest path distanc i awai from s .
so befor we wrap up with thi applic , i do wanna emphas , it is onli breadth first search that give us thi guarante of shortest path .
so , we have a wide famili of graph search strategi , all of which find everyth findabl .
breadth first search is on of those , but thi is a special addit properti that breadth first search ha you get shortest path distanc from it .
so in particular depth first search doe not in gener comput shortest path distanc .
thi is realli a special properti of breadth first search .
by contrast in thi next applic , which is go to be comput the connect compon of an undirect graph , thi is not realli fundament to breadth first search .
for exampl , you could us depth first search instead and that would work just as well .
so , what's the problem ?
well , so i did sai most of thi stuff about graph search it realli doesn't matter undirect or direct .
it's pretti much cosmet chang , but the big except is when you're comput connect .
when you're comput the piec of the graph .
so , right now i'm onli gonna talk about undirect graph .
in the direct case , we can again get a veri effici algorithm for it , but it's quit a bit harder work .
so , that's gonna be cover in detail in a separ video .
so , for now focu just on a undirect graph , g .
and we're certainli not go to assum that g is connect .
inde part of the point here is to figur out whether or not it's connect , i . e .
in on piec .
so , mayb the graph look like thi .
so , for exampl mayb the graph ha ten vertic and look like thi , on the right .
and intuit , especi given that i've drawn it in such a clean wai , it's clear that thi graph ha three piec , and those ar the thing that we wanna call the connect compon .
but we do want a somewhat more formal definit .
someth which is actual you know , in math , that we can sai is true of fals about a given graph .
and roughli we defin the connect compon of an undirect graph as the maxim region that ar connect .
in the sens you can get from ani vertex in the region from ani other vertex in the region us a path .
so maxim connect region in that sens .
now the slick wai to do thi is us an equival relat .
and i'm gonna thi here in part becaus it's realli the right wai to think about the direct graph case , which we'll talk about in some detail later .
so for undirect graph .
so thi isn't super import but let me go ahead and state the formal definit just for complet about what is a connect compon .
what do i mean by a maxim region that's mutual connect .
so a good formal definit is as the equival class of the relat on vertic where which we defin by u be relat to v if and onli if there's a path between u and v in the graph g .
so i'll leav it for you to do the simpl check that the squiggl is inde an equival relat .
i'm gonna remind you what equival relat ar .
thi is someth you gener learn in your first class on proof or your first class on discret math .
so it's just someth which mai or mai not be true about pair of object .
and to be an equival relat , you have to satisfi three properti .
so , first you have to be reflex , mean a , everyth ha to be relat to itself , and inde , in a graph , there is a path from ani node to itself , name the empti path .
so , also equival relat have to be symmetr , mean if u and v ar relat then v and u ar relat .
becaus thi is an undirect graph , it's clear that thi is symmetr .
if there's a path from u to v in the graph , there's also a path from v to u , so no problem there .
final equival class ha got to be transit .
so that mean if u and v ar relat and so ar v and w , then so ar u and w .
that is if u and v have a path , v and w have a path , then so doe u and w .
and you can prove transit just by past the two path togeth .
and so the upshot is when you want to sai someth like the maxim subset of someth where everyth is the same , the right wai to make that mathemat is us equival relat .
so over in thi blue graph , we want to sai on , three , five , seven and nine , ar sort of all the same in the sens that thei ar mutual connect and so that's exactli what thi relat is make precis .
all five of those node ar relat to each other .
two and four ar relat to each other .
six , eight , and ten all pair of them ar relat to each other .
so the equival , so equival relat alwai have equival class of the maxim mutual relat stuff , and in thi graph context that's exactli thi connect compon .
that's exactli what you want .
so , what i wanna show you is that you can us breadth first search wrap in an outer for loop over all of the vertic to comput , to identifi , all the connect compon of the graph in time linear in the graph .
in o of n plu m time .
now you might be wonder , you know , why do you wanna do that .
well there's a lot of reason .
so , an obviou on which is relev for physic network is to check if a network ha broken into two piec .
so , certainli if you're an internet servic provid , you wanna make sure that from ani point in your network you can reach ani other point in the network .
and that boil down to just understand whether the graph that repres your network is a connect graph , that is , if it's in on piec or if it's not in on piec .
so obvious you can ask thi same question about recreat exampl so if you return to the movi graph , mayb you're wonder , can you get from everi singl actor , in the imdb databas , to kevin bacon , or ar there actor for which you cannot reach kevin bacon , via a sequenc of edg , the sequenc of movi in which two actor have both plai a role .
so that's someth that boil down to a connect comput .
if you have network data and you want to displai it , you want to visual it and show it to a group of peopl so that thei can interpret it .
obvious on thing you wanna do is you wanna know if there's multipl piec and then you want to displai the piec separ .
so let me mention on probabl a littl less obviou applic of undirect connect , which is it give a nice quick and dirti heurist for do cluster , if you have pairwis inform about object .
so let me be a littl , let me be a littl more concret .
suppos you have a set of object that you realli care about .
so thi could be a set of document , mayb web page that you crawl , someth like that .
it could be a set of imag , either your own or drawn from some databas , or it could be for exampl a set of genom .
suppos further that you have a pairwis function .
okai ?
which for each pair of object tell you whether thei ar veri much like each other or veri much differ .
and so let's suppos that if two object ar veri similar to each other , like thei're two web page that ar almost the same , or thei're two genom where you can get from on to the other with a small number of mutat , then thei have a low score .
so low number close to zero indic that the object ar veri similar to each other .
high number , let's sai thei could go up to even a thousand or someth , indic that thei ar veri differ object .
two web page that have noth to do with each other .
two genom for total unrel part or two imag that seem to be of complet differ peopl or even complet differ object .
now here's a graph you can construct us these object and the similar data that you have about them .
so you can have a graph where the node ar the object .
so for each pixel , for each imag , for each document , whatev , you have a singl node .
and then for a given pair of node , you put in an edg if , and onli if , the two object ar veri similar .
so , for exampl , you could put in an edg between two object if , and onli if , the score is at most ten .
so rememb , the more similar two object ar , the closer their score is to zero .
so you're gonna get an edg between veri similar document , veri similar genom , veri similar imag .
now in thi graph you've construct , you can find the connect compon .
so , each of these connect compon will be a group of object , which more or less ar all veri similar to each other .
so , thi will be a cluster of close relat object in your databas .
and you can imagin a lot of reason why , given a larg set of unstructur data , just a bunch of pictur , a bunch of document , or whatev , you might wanna find cluster of highli relat object .
so we'll probabl see more sophist heurist for cluster in some sequel cours .
but alreadi undirect connect give you a super fast , linear time , quick and dirti heurist for identifi cluster of similar object given pairwis data about similar .
so that's some reason you might wanna do it .
now let's actual talk about how to comput the connect compon in linear time us just a simpl for loop and breadth first search as it inner work hors .
so here's the code to comput all the connect compon of an undirect graph .
so first we initi all node as be unexplor .
i'm also go to assum that the node have name .
let's sai their name ar from on to n .
so , these name could just be the posit in the node arrai that these node occupi .
so there's gonna be an outer for loop , which walk through the node in an arbitrari order , let's sai from on to n .
thi outer for loop is to ensur that everi singl node of the graph will be inspect for sure at some point in the algorithm .
and again , on of our maxim is that we should never do redund work , so befor we start explor from some node , we check if we've alreadi been there .
and if we haven't seen i befor , then we invok the breadth first search subroutin we were talk about previous in the lectur in the graph g start from the node i .
so to make sure thi is clear , let's just run thi algorithm on thi blue graph to the right .
so we start in the outer for loop and we set i equal to on , and we sai have we explor node number on yet ?
and of cours not , we haven't explor anyth yet .
so the first thing we're gonna do is we're gonna , we're gonna invok bf on node number on here .
so now we start run the usual breadth first search subroutin start from thi node on .
and so we explor , you know , layer on here is gonna be node three and five and so we explor them in some order .
for exampl , mayb , node number three is what we explor second and then node number five is what we explor , third .
and then the second layer in thi compon is go to be the node seven and nine .
so , we'll explor them in some order as well .
let's sai seven first , follow by nine .
so , after thi bf initi from node number on complet of cours it will have found everyth that it could possibl find , name the five node in the same connect compon as node number on .
and of cours all of the five of these node will be mark as explor .
so , now we return onc that exit .
we return to the other for loop we increment i , we go to i equal two , where we'll sai , oh , have we alreadi explor node number two ?
no we have not .
and so now we invok bf again from node number two .
so that'll be the sixth node we explor .
there's not much to do from two .
all we can do is go to node number four .
so , that's the seventh node we explor .
that bf termin find the node in thi connect compon .
then we go back to the outer for loop .
we increment i to three .
we sai ah , have we alreadi seen node number three ?
ye we have .
we saw that in the first breadth first search .
so we certainli don't bother to bf from thi node three .
then we increment i to four , have we seen four ?
ye we have , in the second call to bf .
have we seen node five ?
ye we have , in the first call to bf .
have we seen node six ?
no , we have not .
so the final invoc of breadth first search begin from node number six .
that's gonna be the eighth node overal that we see .
and then we're gonna see the node eight and ten in some order .
so for exampl , mayb we first explor node number eight .
that's a n , that's on of the first layer in thi compon , and then node number ten is the other node of the first layer in thi compon .
so , in gener what's go on , well , what we know about , what we know what will happen when we invok breadth first search from a given node i , we're go to discov exactli the node in i's connect compon .
right ?
anyth where there's a path from i to that node , we'll find it .
that's the bf guarante .
that's also the definit of a connect compon .
all the other node , which have a path to , to i .
anoth thing that i hope is clear from the exampl , but you know , just to reiter it , is everi breadth first search call when you explor a node , you rememb that through the entir for loop .
right ?
so when we invok breadth first search from node number on , we explor node on , three , five , seven , and nine , and we keep those mark as explor for the rest of thi , for the rest of thi algorithm .
right ?
so and that's so that we don't do redund work when we get to later stage of the for loop .
so , as far as what doe thi algorithm accomplish ?
well , it certainli find everi connect compon .
all right , there is absolut no wai it can miss a node becaus thi for loop liter walk through the node , all of them , on at a time .
and so you're not gonna miss a node .
moreov , we know that , you know , as soon as you f , hit a connect compon for the first time , and you do breadth first search from that node , you're gonna find the whole thing .
that's the breadth first search guarante .
as far as what's the run time ?
well it's go to be exactli what we want .
it's go to be linear time which again mean proport to the number of edg plu the number of vertic .
and again , depend on the graph , on of these might be bigger than the other .
so why is it o of m plu n ?
well as far as the node we have to do thi initi there where we mark them all as unexplor so that take constant time per node .
we have just the basic overhead of a for loop so that's constant time per node and then we have thi check .
constant time per node so that's o of n .
and then recal we prove that within breadth first search you do amount of work proport , you do constant time for each node in that connect compon .
now each of the node of the graph is in exactli on of the connect compon , so you'll do constant time for each node in the bf in which you discov that node .
so , that's again o of n over all of the connect compon .
and as far as the edg , no , we don't even bother to look at edg until we're insid on of these breadth first search call .
thei plai no role in the outer for loop or in the preprocess , and rememb what we prove about an invoc of breadth first search .
the run time , you onli do constant amount of work per edg in the connect compon that you're explor .
in the worst case you look at the edg onc from either end point and each of that trigger a constant amount of work .
so when you discov a given connect compon the edg work is proport to the number of edg in that connect compon .
each edg of the graph is onli , is in exactli on of the connect compon .
so over thi entir for loop over all of these bf call for each edg of the graph you'll onli be respons for a constant amount of work to the algorithm .
so summar , becaus breadth first search from a given start node doe , work in time proport to the size of that compon .
piggi back on that sub routin , and loop over all of the node of the graph we find all of the connect compon , in time proport to the number of edg and node in the entir graph .
let's explor our second strategi for graph search , name depth first search .
and again , like with breadth first search , i'll open by just remind you what the depth first search is good for .
and we'll trace through it in a particular exampl and then we'll tell you what the actual code is .
so if breath first search is the cautiou and tent explor strategi , then depth first search or df for short is it more aggress cousin .
so the plan is to explor aggress and onli backtrack when necessari .
and thi is veri much the strategi on often us when try to solv a maze .
to explain what i mean , let me show you how thi would work in the same run exampl we us when we discuss breath first search .
so here if we invok depth first search from the node number s .
here's what's gonna happen , so obvious we start at s and obvious there's two place where we can go next .
we can go to a or , or to b , and depth first search is underdetermin like breath first search , we can pick either on .
so like with the breath first search exampl , let's go to a first .
so a will be the second on that we explor .
but now unlik breadth first search where we automat went to node b next , sinc that wa the other layer on node , here the onli rule is that we have to go next to on of a's immedi neighbor .
so we might go to b but we're not go to b becaus it's on of the neighbor of s , we go becaus it's on of the neighbor of a , and actual to make sure the differ is clear , let's assum that we aggress pursu deeper and we go from a to c .
and now the depth first search strategi is , again , just to pursu deeper .
so , you go to on of c's immedi neighbor .
so , mayb we go to e next .
so , e is go to be the fourth on visit .
now , from e there's onli on neighbor , not count the on that we came in on .
so , from e we go to d .
and d is the fifth on we see , now from d we have a choic , we can either go to b or we can go to c .
so let's suppos we go to c from d .
well then we get to a node number three , where we've been befor , okai ?
and as usual we're go to keep track of where we've alreadi been .
so at thi point we have to backtrack from c back to d , we retreat to d .
now there's still anoth outgo edg from d to explor , name the on to b .
and so what happen is , we actual wind up wrap all the wai around thi outer cycl .
and we hit b sixth .
and now , of cours , anywher we try to explor , we see somewher we've alreadi been , so from b we try to go to s , but we've been there , so we retreat to b , we try , we can try to go to a , but we've been there , so we retreat to b .
now we've explor all of the option out of b .
so we have to retreat from b , we have to go back to d .
now from d we've explor both b and c , so we have to retreat back to e .
e we've explor the onli outgo mark d , so we have to retreat to c .
c we retreat to a .
from a , we actual haven't yet look along thi arc .
but that just send to b where we've been befor .
so then we retreat back to a .
final , we retreat back to s and s , even at s there's still an extra edg to explor .
at s we sai , oh we haven't tri thi s b edg yet but , of cours , when we look across , we get to b where we've been befor and then we backtrack to s .
then we've look at everi edg onc and so we stop .
that's how depth first search work .
you just pursu your path .
you go to an immedi neighbor as long as you can until you hit somewher you've been befor and then you retreat .
so you might be wonder you know why bother with anoth graph search strategi .
after all we have breadth first search which seem pretti awesom right ?
it run in linear time .
it's guarante to find everyth you might wanna find .
it comput shortest path .
it comput connect compon if you imb it in a for loop .
kinda seem like what els would you want ?
well , it turn out depth first search is gonna have it own impress catalog of applic which you can't necessarili replic with breadth first search .
and i'm gonna focu on applic in direct graph .
so there's gonna be a simpl on that we discuss in thi video .
and then there's gonna be a more complic on that ha a separ video devot to it .
so in thi video , we're gonna be discuss comput topolog order of direct acycl graph .
that is , direct graph that have no direct cycl .
the more complic applic is comput strongli connect compon in direct graph .
the run time will be essenti the same as it wa for breadth first search and the best we could hope for which is linear time .
and again , we're not assum that there's necessarili that mani edg .
there mai be much fewer edg than vertic .
so , linear time in these connect applic mean o m n .
so let's now talk about the actual code of depth first search .
there's a coupl wai to do it .
on wai to do it is to just make some minor modif to the code for breadth first search .
the primari differ be instead of us a queue at it first in first out behavior , you swap in a stack where it's last in first out behavior .
again , if you don't know what a stack is you should read about that in the program textbook or on the web .
it's someth that support constant time insert to the front and constant time delet from the front , unlik a queue which is meant to support constant time delet to the back .
okai , so stack .
it oper just like those cafeteria trai that you know where you put in a trai , and the last on that got push in , when you take the first on out , that's the last on that got put in .
so these ar call push and pop .
in a stack context , both ar constant time .
so if you swap out the queue , you swap in the stack , make a coupl other minor modif , breadth first search turn into depth first search .
for the sake of both varieti and eleg , i'm , instead , gonna show you a recurs version .
so depth first search is veri natur phrase as a recurs algorithm , and that's why we'll discuss here .
so depth first search , of cours , take as input a graph g .
and , again , it could be undirect or direct , it doesn't matter .
just , with the direct graph , be sure that you onli follow arc in the appropri direct , which should be automat handl in the adjac list of your graph data structur anywai .
so , as alwai , we keep a boolean local to each vertex of the graph , rememb whether we've , start , we've been there befor or not .
and of cours as soon as we start explor for s , we'd better make a note that , now we have been there .
we better plant a flag , as it were .
and rememb , that depth first search is an aggress search .
so we immedi try to recurs search from ani of s's neighbor that we haven't alreadi been to .
and if we find such a vertex , if we find , somewher we've never been , we recurs call depth first search , from that node .
the basic guarante of depth first search ar exactli the same as thei were for breadth first search .
we find everyth we could possibl hope to find and we do it in linear time .
and onc again the reason is thi is simpli a special case of the gener search of procedur that we start thi sequenc of video about .
so it just correspond to a particular wai of choos amongst multipl cross edg between the region of explor node and the region of unexplor node , essenti alwai be bias toward the most recent discov explor node .
and just like breadth first search , the run time is go to be proport to the size of the compon that you're discov .
and the basic reason is that each node is look at onli onc , right ?
thi boolean make sure that we don't ever explor a node more than onc and then for each edg , we look at it at most twice , on from each end point .
and given that these exact same two claim hold for depth first search as for breadth first search , that mean if we want to comput connect compon in an undirect graph , we could equal well us an outer for loop with depth first search as our work hors in the inner loop .
it wouldn't matter .
either of those for undirect graph , depth first search , breadth first search is gonna find all the connect compon in o m n time , in linear time .
so , instead , i wanna look at an applic particular to depth first search , and thi is about find a topolog order of a direct acycl graph .
so let me begin by tell you what a topolog order of a direct graph is .
essenti , it's an order of the vertic of the graph so that all of the arc , the direct edg of the graph , onli go forward in the order .
so , let me encod an order by a label of the vertic with the number on through n .
thi is just to encod the posit of each vertex in thi order .
so formal there's gonna be a function which take vertic of g and map thing to integ between on and n .
each of the number on through n should be taken on by exactli on vertex .
here n is the number of vertic of g .
so that's just the wai to encod an order and then here's realli the import properti that everi direct edg of g goe forward in the order .
that is , if uv is a direct edg of the direct graph g , then it should be that .
the f valu of the tail is less than the f valu of the head .
that is , thi direct edg ha a higher f valu as you , as you travers it in the correct direct .
let me give you an exampl just to make thi more clear .
so suppos we have thi veri simpl direct graph with four vertic .
let me show you two differ , total legitim topolog order of thi graph .
so the first thing you could do is you could label s1 , v2 , w3 and t4 .
anoth option would be to label them the same wai except you can swap the label of , v and w .
so if you want you can label v three and w two .
so again what these label meant to encod is an order of the vertic .
so the blue label you can think of as encod the order in which we put s first , then v , then w and then t .
wherea the green label can be thought of as the same order of the node , except with w come befor v .
what's import is that the pattern of the edg is exactli the same in both case , and in particular , all of the edg go forward in thi order .
so , in either case , we have s with edg from s to v and s to w .
so that look the same wai pictori .
whichev order v and w ar in .
and then symmetr , there ar edg from v and w to t .
so you'll notic that no matter which order we put v and w in , all four of these edg go forward in each of these order .
now , if you're try to put .
v befor s , it wouldn't work becaus the edg from s to v would be go backward , if v preced s .
similarli , if you put t anywher other than the final posit , you would not have a topolog order .
so in fact , these ar the onli two topolog order of thi direct graph .
i encourag you to convinc yourself of that .
now who care about topolog order ?
well thi is actual a , a veri us subroutin thi ha been , come up in all kind of applic .
realli whenev you want a sequenc , a bunch of task , when there's preced constraint amongst them .
by preced constraint i mean on task ha to finish befor anoth .
you can think , for exampl , about the cours in some kind of undergradu major , like comput scienc major .
here the vertic ar go to correspond to all of the cours and there's a direct edg from cours a to cours b , if cours a's a prerequisit for cours b , if you have to take it first .
so then , of cours , you'd like to know a sequenc in which you can take these cours so that you alwai take a cours after you've taken it prerequisit .
and that's exactli what a topolog order will accomplish .
so , it's reason to ask the question , when doe the direct graph have a topolog order and when a graph doe have such an order , how do we get our grubbi littl hand on it ?
well there's a veri clear , necessari condit for a graph to have a topolog order , which is , it had better be acycl .
put differ , if a direct graph ha a direct cycl then there's certainli no wai there's go to be a topolog order .
so , i hope the reason for thi is fairli clear .
consid ani direct graph which doe have a direct cycl and consid ani purport wai of order the vertic .
well now just travers the edg of the cycl on by on .
so you start somewher on the cycl .
and if the first edg goe backward , well you're alreadi screw .
you alreadi know that thi order is not topolog ; no wai it just can go backward .
so evid the first edg of thi cycl ha to go forward , but now you have to travers the rest of the edg on thi cycl , and eventu you come back to where you start .
so if you start out by go forward , at some point you have to go backward .
so that edg goe backward in the order , violat the properti of the topolog order .
that's true for everi order , so direct cycl exclud the possibl of topolog order .
now the question is , well what if you don't have a cycl ?
is that a strong enough condit that you're guarante to have a topolog order .
is the onli obstruct to sequenc job without conflict , the obviou on of have circular preced constraint ?
so it turn out that onli is the answer ye as long as you don't have ani direct cycl , you're guarante a topolog order , but we can even comput on in linear time no less , via depth first search .
so befor i show you thi super slick and super effici reduct of comput topolog order to depth first search , let me first go over a pretti good but slightli less slick and slightli less effici solut to help build up your intuit about direct acycl graph and their topolog order .
so for the straightforward solut we're go to begin with a simpl observ .
everi direct acycl graph ha what i'm gonna call a sink vertex .
that is a vertex without ani outgo arc .
so in the four node , direct acycl graph we were explor on the last slide , there is exactli on sourc vertex and that's , excus me , sink vertex , that's thi right most vertex here .
right ?
that ha no outgo arc ; the other three vertic all have at least on outgo arc .
now , why is it the case that a direct acycl graph ha to have a sink vertex ?
well suppos it didn't .
suppos it had no sink vertex .
that would mean everi singl vertex ha at least on outgo arc .
so what can we do if everi vertex ha on outgo arc ?
well we can start in an arbitrari node .
we know it's not a sink vertex caus we're assum there aren't ani .
so there's an outgo arc so let's follow it .
we get to some other node , by assumpt there's no sink vertex so thi isn't a sink vertex .
so there's an outgo arc .
so let's follow it , we get to some other node .
that also ha an outgo arc , let's follow that , and so on .
so we just keep follow outgo arc .
and we do thi as long as we want , becaus everi vertex ha at least on outgo arc .
well , there's a finit number of vertic .
right ?
thi graph ha , sai , n vertic .
so , if we follow n arc , we're gonna see n plu on vertic .
so , by the pigeon hold principl , we're gonna have to see a repeat .
right ?
so , if n plu on vertic ha onli n distinct vertic , we're gonna see some vertex twice .
so , for exampl , mayb after i take the outgo arc from thi vertex .
i get back to thi on that i saw previous .
well , what have we done ?
what happen when we get a repeat vertex ?
by trace these out go arc and repeat a vertex we have exhibit a direct cycl .
and that's exactli what we're assum doesn't exist .
we're talk about direct acycl graph .
so , put differ , we just prove that a vertex with no sink vertex ha to have a direct cycl .
so a direct acycl graph therefor ha to have at least on sink vertex .
so , here's how we us thi veri simpl observ now to comput a topolog order of a direct acycl graph .
well let's do a littl thought experi .
suppos in fact thi graph did have a topolog order .
let's think about the vertex that goe last in thi topolog order .
rememb , ani arc which goe backward in the order is a violat .
so we have to avoid that .
we have to make sure that everi arc goe forward in the order .
now , ani vertex which ha an outgo arc , we better put somewher other than in the final posit , all right ?
so the node that we put in the final posit , all of it arc ar gonna wind up to all of it outgo arc ar gonna wind up go backward in the topolog order .
there's nowher els thei can go .
thi vertex is last .
so in other word , if we're plan thi to successfulli comput topolog order , the onli candid vertic for that final posit in the order ar the sink vertic , that's all that's gonna work .
if we put anoth non sink vertex there we're toast , that's not gonna happen .
fortun , if it's direct acycl we know there is a sink vertex , so at v , be a sink vertex of g , if there's mani sink vertic we pick on arbitrarili , we set v's label to be the maximum possibl .
so there's n vertic , we're gonna put that in the nth posit .
and now we just recurs on the rest of the graph , which ha onli n minu on vertic .
so how would thi work in the exampl on the right ?
well in the first iter , or the first outermost recurs call , the onli kei , the onli sink vertex is thi right's most circl in green .
so there's four vertic , we're gonna give that the label four .
so then have label that four , we delet that vertex and all the edg incid to it , and we recurs on what's left of the graph .
so that will be the left most three vertic plu the left most two edg .
now , thi graph ha two sink vertic after we've delet four and everyth from it .
so both thi top vertex and thi bottom vertex ar sink in the residu graph .
so now in the next recurs call , we can choos either of those as our sink vertex .
becaus we have two choic , that gener two topolog order .
those ar exactli the on we saw in the exampl .
but if , for exampl , we choos thi on to be our sink vertex , then that get the label three .
then we recurs just on the northwestern most two edg .
thi vertex is the uniqu sink in that graph .
that get the label two .
and then we recurs on the on node graph and that get the , the label on .
so why doe thi algorithm work ?
well there's , there's two quick observ we need .
so first of all , we need to argu that it make sens , that in everi iter or in everi recurs call , we can inde find a sink vertex that we can assign in the final posit that's still unfil .
and the reason for that , is just , if you take a direct acycl graph , and you delet on or more vertic from it , you're still gonna have a direct acycl graph .
all right , you can't creat cycl by just get rid of stuff .
you can onli destroi cycl .
and we start with no cycl .
so through all the intermedi recurs call , we have no cycl .
by our first observ , there's alwai a sink .
so the second thing we have to argu is that we realli do produc a topolog order .
so rememb what that mean .
that mean that for everi edg of the graph , it goe forward in the order .
that is , the head of the arc is given a posit later than the tail of the arc .
and thi simpli follow becaus we alwai us sink vertic .
so consid the vertex v , which is assign to the posit i , thi mean then when we're down to a graph that ha onli i vertic remain , v is a sink vertex .
so if i is , if v is a sink vertex for , when onli the first i vertic remain , what properti doe it have in the origin graph ?
well it mean all of the outgo arc that it ha ; have to go to vertic that were alreadi delet and assign higher posit .
so for everi vertex , by the time it actual get assign a posit , it's a sink .
and it onli ha incom arc from the as yet unassign vertic .
it outgo arc all go forward to vertic that were alreadi assign higher posit , and got delet previous from the graph .
so now we have under our belt a pretti reason solut for comput a topolog order of a direct acycl graph .
in particular , rememb we observ that if a graph doe have a direct cycl , then of cours there is no wai there's a topolog order .
howev you order the vertic , some edg of the cycl is go to have to go backward .
and the solut on the previou slide show that as long as you don't have a cycl , it guarante a topolog order doe inde exist and in fact it's a construct proof , a construct argument that give an algorithm .
what you do is you just keep pluck off sink , sink vertic on at a time and popul the order from right to left as you keep peel off these sink .
so that's a pretti good algorithm , it's not too slow .
and actual , if you implement it just so , you can even get it to run in linear time .
but i wanna conclud thi video with an applic of depth first search , which is a veri slick , veri effici comput of a topolog order of a direct acycl graph .
so we're just go to make two realli quit minor modif to our previou depth first search subroutin .
the first thing is we have to emb it in a for loop just like we did with breadth first search when we were comput the connect compon of an undirect graph .
that's becaus in comput a topolog order , we'd better give everi singl vertex a label , we better look at everi vertex at least onc .
so to do that , we'll just make sure there's an outer for loop and then if we have multipl compon , we'll just make sure to invok df as often as we need to .
the second thing we'll do is we'll add a littl bit of bookkeep and thi will make sure that everi node get a label .
and in fact , these label will defin the topolog order .
so let's not forget the code for depth first search .
thi is where you're given a graph g ; in thi case we're assum a direct acycl graph and you're given a start vertex s .
and what you do is you , as soon as you get to s , you veri aggress start try to explor it neighbor .
of cours , you don't visit ani vertex you've alreadi been to ; you keep track of who you've visit .
and if you find ani vertex that you haven't seen befor , you immedi start recurs on that node .
so i said the first modif we need is to emb thi into an outer for loop to ensur that everi singl node get label so i'm gonna call that subroutin df loop .
it doe not take a start vertex .
initi all node start out unexplor of cours .
and we're also go to keep track of a global variabl which i'll call current label .
thi is go to be initi to n and we're gonna count down each time we finish explor a new node .
and these will be precis the f valu .
these will be exactli the posit of the vertic in the topolog order that we output .
in the main , in the main loop we're gonna iter over all of the node of the graph .
so , for exampl , we just do a scan through the node arrai .
as usual , we don't wanna do ani work twice , so for a , if a vertex ha alreadi been explor in some previou indic of df , we don't , we don't search from it .
thi should all be familiar from our embed a breadth first search in a for loop when we comput the connect compon of an undirect graph , and if we get to a vertex v of the graph that we haven't explor yet , then we just invok df in the graph with that vertex as the start point .
so , the final thing i need to add is i need to tell you what the f valu ar with the actual assign of vertic to posit ar .
and as i foreshadow we're go to us thi global current label variabl and that will have us assign vertic to posit from right to the left .
veri much mimick what wa go on our recurs solut where we pluck off sink vertic on at a time .
so when's the right time to assign a vertex it posit ?
well , it turn out the right time is when we've complet finish with that vertex .
so we're about to pop the recurs call from the stack correspond to that vertex .
so after we've gone through the for loop of all the edg outgo from a given vertex , we set f of s equal to whatev the current label is and then we decrement the current label .
and that's it .
that is the entir algorithm .
so , the claim is go to be that the f valu produc , which you'll notic , ar go to be the integ between n through on , becaus df will be call eventu onc on everi vertex and it will get some integ assign at the end .
and everybodi's gonna get a distinct valu and the largest on is n and the smallest on is on .
the claim is that is a topolog order .
clearli thi algorithm is just as blazingli fast as df itself , with just a trivial amount of extra bookkeep .
let's see how it work on our run exampl .
so let's just sai we have thi four node direct graph , which we're get quit us to .
so thi ha four vertic .
so we initi the current label variabl to be equal to four .
so let's sai that , in the outer df loop , let's stai we start somewher like the vertex v .
so notic , in thi outer for loop , we wind up consid the vertic in a total arbitrari order .
so let's sai we first call df from thi vertex v .
so what happen ?
well , the onli place you can go from v is to t and then at t there's nowher to go .
so we recurs call df of t .
there's no edg to go through .
we finish the for loop and so t is go to be assign an f valu equal to the current label , which is n and here n is the number of vertic which is four .
so f of t is go to get .
sorri t is go to get the assign the label four .
so then now we're done with t we back track back to v .
we decrement the current label as we finish up with t .
we get to v , and now there's no more outgo arc to explor , so it for loop is finish .
so we're done with it , with depth first search .
so it get what's the new current label , which is now three .
and again have finish with v we decrement the current label which is now down to two .
so now we go back to the outer for loop .
mayb the next vertex we consid is the vertex t .
but we've alreadi been there so don't bother to df on t .
then mayb after that we tri it on s .
so , mayb s is the third vertex that the for loop consid .
we haven't seen s yet so invok dsf start from vertex s .
from s there's two arc to explor the on with v .
v we've alreadi seen noth is gonna happen with arc sv .
but on the other hand the arc sw will caus us to recurs call df on w .
from w we try to look at the arc from w to t , but we've alreadi been to t so we don't do anyth , that finish up with w so depth first search then finish up at the vertex w , w get the assign of the current label , so f of w equal two .
we decrement current label , now it valu is on , now we backtrack to s , we've alreadi consid all of s's outgo arc so we're done with s , it get the current label which is on .
and thi is inde on of the two topolog order of thi graph that we exhibit , a coupl slide ago .
so that's the full descript of the algorithm and how it work on a concret exampl .
let's just discuss what ar it kei properti , it run time and it correct .
so as far as the run time of thi algorithm , the run time is linear ; it's exactli what you'd want it to be .
and the reason the run time is linear is for the usual reason that these graph search algorithm have run a linear time , you're explicitli keep track of which node you've been to so that you don't visit them twice .
so you onli do a constant amount of work for each of the n node and each edg in a direct graph , you actual onli look at each edg onc when you visit the tail of that edg .
so you onli do a constant amount of work per edg as well .
of cours the other kei properti is correct .
that is we need to show that you all ar guarante to get a topolog order .
so what doe that mean ?
that mean everi edg .
everi arc travel forward in the order .
so if uv is in edg , then f of u , the label assign to u , in thi algorithm is less than the label assign to v .
the proof of correct split into two case depend on which of the vertic u or v is visit first by depth first search .
becaus of our for loop , which iter over all of the vertic of the graph g , depth first search is go to be invok exactli onc from each of the vertic .
either u or v could be first , both ar possibl .
so , first , let's assum that u is visit by df befor v .
so then what happen ?
well , rememb what depth first search doe when you invok it from a node , it's go to find everyth findabl from that node .
so , if u is visit befor v , that mean v doesn't get explor so it's a candid for be discov .
moreov , there's an arc straight from u to v .
so , certainli , df invok at u is go to discov v .
furthermor , the recurs call correspond to the node v , is go to finish , is go to get pop off the program stack befor that of u .
the easiest wai to see thi is just to think about the recurs structur of depth first search .
so when you call depth first search from u , that recurs call , that's gonna make further recurs call to all of the relev neighbor includ v and u's call is not gonna get pop off the stack until v's doe beforehand , that's becaus of the last in first out natur of a stack or of a recurs algorithm .
so , becaus v's recurs call finish befor that of u , that mean it will be assign a larger label than u .
rememb , the label keep decreas as more and more recurs call get pop off the stack .
so that's exactli what we want .
now , what's up in the second case , case two ?
so thi is where v is visit befor u .
and here's where we us the fact that the graph ha no cycl .
so there's a direct arc from u to v .
that mean there cannot be ani direct path from v all the wai back to u .
that would creat a direct cycl .
therefor , df , invok from v , is not go to discov u .
there's no direct path from v to u , again , if there wa , there would be a direct cycl , so it doesn't find u at all .
so the recurs call of v , again , is go to get pop befor u's is even push onto the stack .
so we're total done with v befor we even start to consid u .
so therefor for the same reason , sinc v's recurs call finish first , it label is go to be larger , which is exactli what we want to prove .
so that conclud the first quit interest applic of depth first search .
in the next video we'll look at an even more interest on , which comput the strongli connect compon of a direct graph .
thi time we can't do it in on depth for search , we'll need two .
have master comput the connect compon of undirect graph in linear time , let's now turn our attent to direct graph which also aris , in all kind of differ applic .
now the good new is , is we'll be abl to get just a blazingli fast primit for comput connect inform for direct graph .
the bad new if you wanna call it that , is we'll have to think a littl bit harder .
so it won't be as obviou how to do it , but by the end of thi lectur you will know a linear time algorithm with veri good constant , realli just base on depth first search for comput all of the piec of a direct graph .
in fact it's not even so obviou how to defin piec , how to defin connect compon in a direct graph , certainli not as obviou as it wa with undirect graph .
so to see what i mean , let's consid the follow four node direct graph .
so on the on hand , thi graph is in some sens in on piec .
if thi wa an actual physic object made , sai , of a bunch of string connect to each other and we'd pick it up with our hand , it wouldn't fall apart into two piec .
it would hang togeth in on piec .
on the other hand , when you think about move around thi network , it's not connect in the sens that we might think about .
you cannot get from ani on point a to ani other point b .
for exampl , if you start at the right most node in thi graph , certainli there's no direct path that will get you to the left most node .
so what's typic studi and most us for direct graph , is what you call strong connect .
the graph is strongli connect if you can get from ani on point to ani other point , and vice versa .
and the strongli connect compon then inform , ar the maxim portion of thi graph , the maxim region which ar intern , strongli connect .
so , the maxim region from within which you can get from ani on point a to ani other point b along a direct graph .
for the record , let me go ahead and give you a , a formal definit although , you know , thi intuit is perfectli valid just region where you can get from anywher to anywher els .
so we sai that the strongli connect compon of the direct graph or the scc for short .
and as in the undirect case , we're go to give a somewhat slick definit , rather than talk about maxim region satisfi some properti , we're go to talk about the equival class of a particular equival relat .
but realli it mean the same thing .
thi is just sort of the more mathemat matur wai of write it down .
so the equival relat we're go to us it on node of the graph and we'll sai that u , a node , is relat to a node v if you can get from u to v via a direct path and also from v to u on some other direct path .
i'm not gonna bore you with the verif that thi is an equival relat .
that's someth that you can work out in the privaci of your own home .
just rememb what it mean to be in an equival relat .
it's reflex .
that is , everybodi's relat to itself but , of cours , there is a path from everi node to itself .
it also mean that it's symmetr .
so if u is relat to v , then v is relat to u .
well , again , by definit , we're sai that the path ar mutual , the vertic ar mutual reachabl from each other .
so that's clearli symmetr by definit and then it ha to be transit .
and the wai you prove it's transit , is you just past path togeth and it just work the wai you'd expect it to .
so let's illustr thi concret with a , a somewhat more complic direct graph .
so here's a direct graph and i claim that it ha exactli four strongli connect compon .
there's a triangl on the left , a triangl on the right .
ha thi singl node on top and then it ha thi direct four cycl with a diagon at the bottom .
so what i hope is pretti clear is that each of these circl region is inde strongli connect that is if you start from on node in on of these circl region you can have a direct path to ani other node , so that's certainli true caus on a direct cycl you can get from anyon start point and the other place and all of these have direct cycl and there's the on strong compon that just ha on node which is obvious strongli connect .
what i also claim is true is that all of these region ar maxim subject to be strongli connect .
that's why thei're the strongli connect compon .
that's why thei're the equival class of thi equival relat we just defin .
so , if you take ani two pair of node which lie in two differ circl you either won't have a path from on . . .
from the first on to the second on or you won't have a direct path from the second on back to the first on .
in fact , the structur of the strong compon in thi black graph exactli mirror , the direct acycl graph that we start with in red .
so in the same wai , in the red four node graph , you can't move from right to left .
here in thi bigger graph , you can't move from ani of the circl scc's to the right to ani of the circl scc to the left .
so , for exampl , from the rightmost node , there ar no direct path to the leftmost node .
so that's a recap of the definit of the strongli connect compon , i've motiv in a separ video some reason why you might care about comput strongli connect compon .
in particular , on extrem larg graph , which motiv the need for blazingli fast subroutin so for free primit that will let you comput connect inform .
so you'd love to be abl to know the piec of a direct graph , mayb you don't even know why thei're go to be us , but you just comput them becaus why not ?
it's a for free primit .
so that's what i'm gonna give you in thi lectur .
so the algorithm we're gonna present is base on depth first search , and that's gonna be the main reason why it's go to be so blazingli fast becaus depth first search is blazingli fast .
now you might be wonder what on earth doe graph search have to do with comput compon ?
thei don't seem obvious relat , so let's return to the same direct graph that i show you on the previou slide .
and to see why someth like depth first search might conceiv have some us for comput strong compon .
suppos we call depth first search , start from thi red node as a start point .
what would it explor ?
so rememb what the guarante of someth like depth first search or breadth first search , for that matter , is .
you find everyth that's findabl , but natur , noth els .
so what is findabl from thi red node ?
where , by findabl , i mean is you can reach it from a direct path eman from thi red node .
well , there's not much you can do , so , from here , you can explor thi arc , and you can explor thi arc , and then you can go backward .
and so if you do df or bf from thi node you're go to find precis the node in thi triangl .
all of the other arc involv go the wrong direct and thei won't be travers by sai a depth first search call .
so , why is that interest ?
what's interest is that if we invok df from thi red node or ani of the three node from thi triangl then it's go to discov precis thi strongli connect compon .
precis the three node in thi circl scc .
so that seem realli cool .
it seem like mayb we just do a df , and boom , we get an scc .
so mayb if we can do that over and over again , we'll get all the scc .
so that's a good initi intuit .
but someth can go wrong .
suppos that instead of , an , initi df from on of these three node on the triangl , we , sai , initi it from thi bottom most node in green .
so rememb , what is the guarante of a graph search subroutin like df ?
it will find everyth findabl but of cours noth more .
so what's findabl from thi green node ?
well natur , everyth in it own scc .
right , so the four node here , it will certainli find those four node .
on the other hand , if we start from thi green node , sinc there ar arc that go from thi bottom most scc to the right most scc , not onli will thi df call find the four node in the green node's strong compon , but it will also travers these blue arc and discov the three node in the red triangl .
so , if we call df from thi green node , we'll captur all seven of these .
so the point is if we call df , it look like we're go to get a union of possibl multipl scc .
in fact , in the wors case , if we invok df from the left most node , what's it go to discov ?
it's go to discov the entir graph , and that didn't give us ani insight into the strong compon structur at all .
so , what's the take awai point is , the take awai point is if you call df from just the right place , you'll actual uncov an scc .
if you call it from the wrong place , it will give you no inform at all , so the magic of the algorithm that we're gonna discuss next is we'll show how in a super slick pre process step which iron is itself a call to depth first search , we can in linear time comput exactli where we want to start the subsequ depth first search from , so that each invoc get us exactli on strongli connect compon , and noth more .
so the algorithm that , that i'm go to show you is due to kosaraju .
and it will show the follow theorem .
that the strongli connect compon of a direct graph can be comput in linear time .
and as we'll see , the constant ar also veri small .
it's realli just gonna be two pass of depth first search .
and again , let me remind you that for mani problem it's natur to assum that the number of edg is at least the number of node becaus , if you're onli interest in case where the graph is connect .
of cours , when you're comput connect compon , that's on of the most natur case where you might have a super spars , broken up graph .
so we're not go to assum m is at least n .
so that's why linear time is go to be m n , becaus that's the size of the input .
and we don't know , either m could be bigger than n , or n could be bigger than m .
we have no idea .
kosaraju's algorithm is shock in it simplic .
it ha three step , let me tell you what thei ar .
first , veri mysteri , we're go to revers all of the arc of the given graph .
total not clear why that would be an interest thing to do yet .
then , were go to do our first pass , our first depth first search , and we're go to do it on the revers graph .
now , the naiv wai to implement thi would be you liter construct a new copi of the input graph , with all of the arc in the revers direct .
and then just run depth first search on it .
of cours , the sophisti , the sort of obviou optim would be to just run df on the origin graph , but go across arc backward .
so i'll let you think through the detail of how you'd do that , but that just work .
you run df , and instead of go forward along edg , you go backward along edg .
that simul depth first search on the revers graph .
now , i've written here , df loop .
and that just mean the usual trick where , to make sure that you see all of the node of , of the graph , even if it's disconnect , you have an outer loop , where you just try each start point separ .
if you haven't alreadi seen it , then you run df from that given node .
i'll be more detail on the next slide .
and the third step , it's just you run depth first search again , but thi time on the origin graph .
now at thi point , you should be think that i'm total crazi , right ?
so what ar we try to do ?
we're try to comput these strongli connect compon .
we're try to actual comput real object , these maxim strongli connect region .
and all i'm do is search the graph .
and i do it onc forward , i do it onc backward .
i mean , i'm not , it doesn't realli seem like i'm comput anyth .
so , here's the catch , and it's a veri minor catch .
so we're gonna need , to do a littl bit of bookkeep , it's gonna be veri littl overhead , so we'll still have a blazingli fast algorithm .
so , but with a littl bit of bookkeep , here's what's gonna happen .
the second depth first search , which search the graph , will , in it search process , discov the strongli connect compon on at a time in a veri natur wai , and that'll be realli obviou when we do an exampl , which we'll do in just a second .
now , for the second depth first search to work in thi magic wai , where it just discov the strongli connect compon on at a time , it's realli import that it execut the depth first search in a particular order , that it goe through the node of the graph in a particular order .
and that is exactli the job of the first pass .
the depth first search on the revers graph is go to comput an order of the node , which , when the second depth first search goe through them in that order , it will just discov the scc on at a time in linear time .
so let me sai a littl bit more about the form of the bookkeep and then i'll show you how that bookkeep is kept in as we do depth first search .
so we're gonna have a notion of a finish time of a vertex and that's gonna be comput in the first pass when we do depth first search in the revers graph .
and we're go to make us of thi data in the second pass .
so , rather than just go through the node of the graph in an arbitrari order , like we usual do when we sort of have a loop depth first or breadth first search , we're go to make sure that we go through the vertic in decreas order of these finish time .
now there's still the question in what sens doe thi second depth first search discov and report the strongli connect compon that it find ?
so we're go to label each node in thi second pass with what we call a leader and the idea is that the node in the same strongli connect compon will be label with exactli the same leader node .
and again all of thi will be much more clear onc we do a concret exampl .
but i want to have it down for the record right now .
so that's the algorithm at a high level it's realli just two pass of df with some bookkeep .
but thi is under specifi .
you realli , shouldn't , understand how to implement the algorithm just yet .
so what do i ow you ?
i ow you exactli what i mean by df loop although thi you've seen more or less in the past .
it's just a loop over all the vertic the graph and then when if you haven't seen someth yet you df from that start point .
i need to tell you what finish time ar and how thei get comput .
thei're just gonna be integ on to n .
which is basic when depth first search get finish with on of the node .
and then i need to tell you how to comput these leader .
so let me show you all three of those thing on the next slide .
so the workhors for kosaraju's strongli connect compon algorithm is thi df loop subroutin and it take as input in graph .
okai , so it doe not take as input a start node , it's gonna loop over possibl start node .
now for the bookkeep , to comput finish node , we're go to keep track of a global variabl that i'll call t , which we initi to zero .
the point of t is to count how mani node we have total finish explor at thi point .
so thi is the variabl we us to comput those finish time in the first pass .
that magic order that i wa talk about .
now we're also gonna have a second global variabl to comput these thing i wa call leader , and these ar onli relev for the second pass .
so what s is go to keep track of is the most recent vertex from which a df wa initi .
so to keep the code simpl i'm just go to do all of the bookkeep in df loop , but realli df loop get call twice , onc in the revers graph , onc in the forward graph and we onli need to comput these finish time in the first pass , on the revers graph .
and we onli need to comput these leader of the second pass for the forward graph .
but let's just keep them both in there , just for kick .
now , we're go to need to loop through the vertic .
and so the question is , in what order ar we go to loop through the vertic ?
and that's gonna happen differ in the two differ pass .
but let me just us some common notat .
let's just assum in thi subroutin , that the node ar somehow label from on to n .
in our first depth first search it's gonna be label total arbitrari .
so these ar basic just the name of the node , or their posit in the node arrai , whatev , you just do it in some arbitrari order .
now the second time we run df loop , as indic on the previou slide , we're gonna us the finish time as the label .
and as we'll see the finish time ar inde number between on and n .
so now what we do is we just iter through the node in decreas order .
and if we haven't alreadi seen node i , then we initi a df from it .
so , as usual , we're gonna be maintain a local boolean to keep track of whether we've alreadi seen a node yet in on of the df pass .
now rememb , the global variabl s is respons for keep track of the most recent node from which depth first search had been initi .
so if i is not explor and we initi a depth first search from it , we'd better reset s .
and then we do the usual df in g , start from the sourc node i .
so for complet let me just remind you what the depth first search subroutin look like .
so now we're given a graph and a start node .
so the first time we see a node , we mark it as explor .
and just a side note that onc a node is mark explor it's explor for thi entir invoc of df loop .
okai , so even if thi df from a given node i finish and then the outer for loop march on , and encount i again it's still gonna be mark as explor .
now on of our bookkeep job is to keep track of , from which vertex did the df that discov i get call , so when i is first encount , we rememb that s wa the node from which thi df origin , and that by definit is the leader of i .
and then we do what we alwai do with depth first search .
we immedi look at the arc go out of i , and we try to recurs df on ani of those .
although , of cours , we don't bother to do it if we've alreadi seen those node .
now onc thi for loop ha complet , onc we've examin everi outgo arc from i and for each node j either we alreadi saw in the past or we've recurs explor from j and have return , at that point we call ourselv done with node i .
there's no more outgo arc to explor , we think of it be finish .
rememb t is the global variabl that's keep track of how mani node we're done with .
so we increment t becaus now we're done with i and we also rememb that i wa the tth vertex with which we finish .
and that is , we set i's finish time to be t .
becaus depth first search first is guarante to visit everi node exactli onc , and therefor finish with everi node exactli onc .
thi global counter t , well , when the first node is finish , it'll be valu on .
then when the next node get finish , it'll have valu two , then it'll have valu three and so on .
when the final node get finish with , it'll have valu n .
so the finish time of the node ar gonna be exactli the integ from on to n .
let's make thi much more concret by go through a care exampl .
in fact , i think it will be better for everybodi if you yourself trace through part of thi algorithm on a concret exampl .
so let me draw a nine node graph for you .
so to be clear let's assum that we've alreadi execut step on of the algorithm .
that is , we've alreadi revers the graph .
so that is .
thi blue graph that i've drawn on the slide .
thi is the revers .
we've alreadi revers the arc .
moreov the node ar label in some arbitrari wai from on to nine .
just assum these ar how thei show up in the node arrai for exampl .
and rememb in the df loop routin we're suppos to process the node , from the , from top to bottom .
from n down to <num> .
so my question for you then is , in the second step of the algorithm when we run df loop and we process the node from the highest name <num> in order down to the lowest name <num> .
what ar the finish time that we're go to comput as we run df loop ?
now , it is true that you can get differ finish time depend on the differ choic that the df loop ha to make about which outgo arc to explor next .
but i've given you four option for what the finish time of the node on , two , three , all the wai up to nine respect might be , and onli on of these four could conceiv be an output of the finish time of df loop on thi graph .
so , which on is it ?
all right .
so the answer is the fourth option .
that is the onli on of these four set of finish time that you might see be comput by df loop on thi blue graph .
so let's go ahead and trace through df loop and see how we might get thi set of finish time .
so rememb in the main loop we start from the highest node , nine , and then we descend down to the , to the lowest node , on .
so we start by invok df from the node nine .
so now , from here there's onli on outgo arc we have to go to , so we mark nine as explor .
and then there's onli on place we can go .
we can go to six so we mark six as explor .
now there's two place we can go next .
we can either go to three or we can go to eight .
and you know , in gener df could do either on .
now to gener thi fourth set of finish time , i'm go to need to assum that i go to three first .
okai , so again , what df doe , what we're assum it doe is it start at nine , then it ha to go to six .
it mark those as explor .
then it goe to three .
it doe not go to eight first .
it goe to three first .
now from three there's onli on outgo arc which goe to nine but nine you've alreadi mark as explor .
so it's not go to re explor nine , it's go to skip that arc .
sinc that , that's <num>'s onli outgo arc , then that for loop complet .
and so three is the first node to finish .
so when we finish with three we increment t .
it start at zero , now it's on .
and we set the finish time of three to be on .
just like we said it wa in the exampl .
so now we go back .
now we backtrack to six .
now we have anoth outgo arc from six to explor , so now we go to eight .
from eight , we have to go to two .
from two , we have to go to five .
from five , we have to go to eight .
eight , we've alreadi seen , so then we're gonna be done with five , becaus that wa it onli outgo arc .
so then we increment t .
now it's two , and the finish time of five is go to be two .
as promis .
so now we backtrack to two .
there's no more outgo mark from two .
so <num>'s gonna be third on that we finish , as promis .
then we finish with eight .
so the finish time for eight is gonna be the fourth node to be done , as promis .
now we backtrack to six , we're done with six .
that's the fifth node to be complet , as promis .
and final we got all the wai back to where we start at nine and nine is the sixth node to be complet , as promis .
now if we were comput those leader , all of these node would get the leader nine .
but again the leader ar onli relev for the second pass , so we're just go to ignor the leader as we're do thi trace .
we're just go to keep track of the finish time .
so now we're not done , so all we did is we finish with the df that is invok from the node nine and we found six of the node total in that depth first search .
so now we return to the outer for loop and we decrement i , so it start at nine , we're done with that , now we go down to eight , we sai that we've alreadi seen eight .
ye , eight's alreadi explor .
so we skip it .
we go , we decrement i down to seven , we sai have we alreadi seen node seven .
no we have not .
okai , seven is not yet explor , so we invok df now from node seven .
seven ha two outgo arc .
it can either go to four or it can go to nine .
let's sai it check the outgo arc to nine first .
now nine we alreadi explor .
grant that wa in an earlier part of the for loop , but we rememb that .
we , we're go to keep track of who got explor on previou iter of the for loop .
so we don't bother to re explor nine .
so we skip that .
so now from seven we have to go to four .
from four we have to go to on .
from on we have to go back to seven .
<num>'s alreadi been explor , so we backtrack , now we're down to , we're done with on .
so on is the next on we're complet with .
and the finish time of on is go to be seven , as promis .
we backtrack to four .
there's no more outgo arc from four to explor , so that's go to be the eighth on to finish as promis .
and then the last on to finish is poor node seven .
it is last .
so that would be an exampl of how the df loop subroutin comput finish time on a revers graph .
so now let's work through the second pass on the forward version of the graph us the same exampl .
now rememb the point of the first pass is to comput a magic order .
and the magic order is these finish time .
so now we're go to throw out the origin node name and we're go to replac the node name in blue by the finish time in red .
we're also go to work with the origin graph which mean we have to revers the arc back where thei were origin .
so , those ar the two chang that you're go to see when i redraw thi graph .
first of all , all the arc will revers orient .
second of all , all the node will chang name from their origin on to the finish time that we just comput .
so here's our new graph with the new node name and all the arc with their orient revers .
and now we run df again on thi graph .
and again we're go to process the node in order from the highest label nine down to the lowest label on .
moreov we don't need to comput finish time in the second pass .
we onli need to do that in the first pass .
in the second pass we have to keep track of the leader .
and rememb the leader of a vertex is the vertex from which df wa call .
that first , that first discov that node .
all right , so what's go to happen .
well , in the outer for loop , we're go to start with i equal to nine .
and we invok df from the node nine .
so that's gonna be the current leader .
caus that's , where the current df got initi .
now from nine there's onli on choic .
we have to go to seven .
from seven there's onli on choic .
we have to go to eight .
from eight there's onli on choic .
we have to go back to nine .
and then nine's alreadi been seen so we backtrack .
we go back to eight .
we go back to seven .
we go back to nine and that's it .
so when we invok df from node nine the onli thing that we encount ar the node seven , eight and nine .
and these ar all go to be given the leader vertex nine .
you will notic that thi is inde on of the strongli connect compon of the graph .
we just sort of found it with thi invoc of df from the node nine .
so now we go back to the outer for loop .
and we sai , okai , let's go to node eight .
have you alreadi seen eight ?
ye .
what about seven ?
have you alreadi seen seven ?
ye .
what about six , have you alreadi seen six ?
we have not , we have not yet discov six .
so we invok df from node six , we reset the global sourc vertex s to six .
from six we can go to nine or we can go to on .
so let's sai we explor nine first .
well we alreadi saw nine in the earlier iter of the for loop so we don't explor it again .
so we don't discov nine now .
so we backtrack to six .
we go to on .
from on we have to go to five .
from five we have to go to six and then we start backtrack again .
so the onli new node that we encount when we invok df from the node six ar the vertic six , on , and five .
and all of these will have a leader vertex of six .
becaus that's where we call df from , when we first discov these three node .
and you'll notic thi is anoth scc of thi direct graph .
so we invok df again now from a new node , the new node six , and what it discov , the new node discov , is exactli an scc of the graph ; noth more , noth less .
so now we return to the outer for loop .
we go to net five .
have we alreadi seen five ?
ye .
have we alreadi seen four ?
no .
we haven't seen four yet .
so now we invok df from four .
again , we could try to explor five , but we've alreadi seen that befor , we're not go to explor it again .
so from four then , we have to go to two .
from two , we have to go to three .
from three , we have to go back to four , and then after all the back track , we're done .
so the final call to df will be from the node four .
and that df will discov precis , newli discov precis the node <num> , <num> and <num> .
thei will all have the leader vertex four becaus that wa where thi df wa call from .
it's true we will go back to the for loop and we will check , have we seen three yet , ye , have we seen two yet , ye , have we seen on yet , ye , and then the whole thing complet .
and what we see is that us the finish time comput from that first depth first search pass , somehow the strongli connect compon of thi graph just show up and present themselv to us , on at a time , on a silver platter .
everi time we invok df , the node we discov newli were precis on of the scc .
noth more , noth less .
and that's realli what's go on in thi algorithm .
it turn out thi is true in gener .
the first pass , df on the revers graph , comput finish time so that if you then process node accord to decreas order of finish time in the second pass , each invoc to df will discov on new scc and exactli on scc .
so thei'll just present themselv to you on per df call in that second pass's for loop .
thi is , of cours , mere an exampl .
you should not just take a singl exampl as proof that thi algorithm alwai work .
i will give you a gener argument in the next video .
but hopefulli there's at least a plausibl argument .
no longer doe thi three step algorithm seem total insan , and mayb you could imagin perhap it work .
at least there's some principl go on where you first comput the right order to process the node , and , and then the second pass peel off scc on at a time like layer from an onion .
on thing that i hope is pretti clear is that thi algorithm correct or not , is blazingli fast .
pretti much all you do is two depth first search .
and sinc depth first search , as we've seen in the past run in time linear in the size of the graph so doe kosaraju's two pass algorithm .
there ar a coupl of subtleti and i encourag you to think about thi so you'll be forc to think about thi in the program project for week four .
so for exampl , in the second pass how do you process the node in decreas order of finish time ?
you don't want to sort the note by their finish time caus that would take n log n time , so you need to make sure that you rememb in the first pass that you sort of rememb the node in a wai that you can just do a linear scan through them in the second pass .
so there ar some detail but if your intuit is that thi is realli just doubl df properli implement that's pretti much exactli right .
so have spell out the full implement , argu that it's definit a linear time algorithm , and given at least a plausibl argument via an exampl , that it might conceiv be correct , let's now turn to the gener argument .
so the goal of thi video is to prove the correct of kasaraju two pass , depth first search base , linear time algorithm that comput the strongli connect compon of a direct graph .
so i've given you the full specif of the algorithm .
i've also given you a plausibl argument of why it might work , in that at least it doe someth sensibl on an exampl .
name , it first doe a pass of depth first search on the revers graph .
it comput thi magic order .
and what's so special about thi order is then when we do a depth first search us thi order on the forward graph ; it seem to do exactli what we want .
everi indic of depth first search to some new node discov exactli the node of the strong compon and no extra stuff .
rememb that wa our first observ , but that wa unclear whether depth for search would be us or not for comput strong compon .
if you call depth first search from just the right place , you're gonna get exactli the node of an scc and noth more .
if you call it from the wrong place , you might get all of the node of the graph , and get no inform at all about the structur of the strong compon and at least in thi exampl thi first pass with the finish time seem to be accomplish seem to be lead us to invok df from exactli the right place .
so rememb how thi work in the exampl so in the top graph , i have shown you the graph with the arch revers .
thi is where we first invok df loop with the loop over the node go from the highest node name nine all the wai down to the node name on .
and here we comput finish time that's the bookkeep that we do in the first pass so we just keep the run count of how mani node we've finish process .
that is how mani we've both explor that node as well as explor all of the outgo arch and so that give us these number in the red , these finish time between on and nine for the variou node .
those becam the new node name in the second graph and then we revers the arch again and get the origin graph back and then we saw that everi time we invok df in our second pass we uncov exactli the node of an scc .
so when we invok it from the node <num> we discov that <num> , <num> and <num> those have a leader vortex <num> .
then when we next invok df from <num> , we discov <num> , <num> , and <num> , and noth els .
and then final we invok it from <num> , and we discov <num> , <num> , and <num> , and noth els .
and those ar exactli the three , scc of thi graph .
so let's now understand why thi work in ani direct graph , not just thi , in thi on exampl .
so let's begin with a simpl observ about direct graph , which is actual interest in it own right .
the claim is that everi direct graph ha two level of granular .
if you squint , if you sort of zoom out , then what you see is a direct acycl graph , of cours compris it strongli connect compon .
and if you want , you can zoom in and focu on the fine grain structur with on scc .
a littl bit more precis .
the claim is of the strongli connect compon of a direct graph induc in a natur wai an acycl metagraph .
so what is thi metagraph ?
what ar the node and what ar the arc , what ar the edg ?
well , the metanod ar just the scc , so we think of everi strong connect compon as be a singl node in thi metagraph .
so call them sai 'c1' up to 'ck' .
so what ar the arc in thi metagraph ?
well , thei're basic just the on correspond to the arc between scc in the origin graph .
that is , we includ in the meta graph an arc from the strong compon 'c' to 'c hat' if and onli if there's an arc from a node in 'c' to a node in 'c hat' in the origin graph 'g' .
so for exampl if thi is your 'c' and so the triangl is your 'c hat' and you have on or mayb multipl edg go from 'c' to 'c hat' , then in the correspond metagraph your just gonna have a node for 'c' , a node for 'c hat' and the direct arch from 'c' to 'c hat' .
so if we go back to some of the direct graph that we've us as run exampl so we go back to the begin of the previou video and it's look mayb someth like thi the correspond direct acycl graph ha four node and four arch .
and for the run exampl we us to illustr kosaraju's algorithm with the three triangl , the correspond metagraph would just be a path with three node .
so why is thi meta graph guarante to be acycl ?
well , rememb metanod correspond to strong compon , and in a strong compon you can get from anywher to anywher els .
so , if you had a cycl that involv two differ metanod , that is two differ strong connect compon , rememb on a direct cycl you can also get from anywher to anywher els .
so if you had two supposedli distinct scc , that you could get from the on to the other and vice versa , thei would collaps into a singl scc .
you can get from anywher to anywher in on , anywher from anywher in the other on , and you can also go between them at will , so you can get from anywher in thi union to anywher in the union .
so not just in thi context of compet strong compon but also just more gener , thi is a us fact to know about direct graph .
on the on hand , thei can have veri complex structur within a strong compon .
you have path go from everywher to everywher els , and it mai be sort of complic look .
but at a higher level , if you abstract out to the level of scc , you ar guarante to have thi simpl dag , thi simpl direct acycl graph structur .
so , to reinforc these concept , and also segu into think about kosaraju's algorithm in particular , let me ask you a question about how revers arc affect the strong compon of a direct graph .
so the correct answer to thi quiz is the fourth on .
the strong compon ar exactli the same as thei were befor , in fact the relat that we describ is exactli the same as it wa befor so therefor the equival class of the strong compon is exactli the same .
so if two node were relat in the origin graph , that is a path from u to v and a path from v to u , that's still true after you revers all the arc , you just us the revers of the two path that you had befor .
similarli if the two node weren't relat befor , for exampl becaus you could not get from u to v , well that after you revers everyth , then you can't get from v to u , so again you don't have thi relat hold , so the scc ar exactli the same in the forward or the backward graph .
so in particular in kazarogi's algorithm , the strong compon structur is exactli the same in the first pass of df and in the second pass of df .
so now that we understand how everi direct graph ha a meta graph with the node correspond to a strong connect compon , and you have an arch from on scc to anoth if there's ani arch from ani node in that scc to the other scc in the origin graph , i'm in a posit to state what's the kei lemma .
that drive the correct of kosaraju's two pass algorithm for comput the strong connect compon of a direct graph .
so here's the lemma statement .
it consid two strongli connect compon that ar adjac , in the sens that there's an arc from on node in on of them to on node in the other on .
so let's sai we have on scc 'c1' , with a node i , and anoth , scc 'c2' with a node j , and that in g , in the graph , there's an arc directli from i to j .
so in thi sens we sai that these scc ar adjac , with the second on be in some sens after the first on .
now let's suppos we've alreadi run the first pass of the df loop subroutin .
and rememb that work on the revers graph .
so we've invok it on the revers graph .
we've comput these finish time .
as usual we'll let f v denot the finish time comput in that depth first search subroutin on the revers graph .
the lemma then assert the follow .
it sai first , amongst all the node in 'c1' look at the on with the largest finish time .
similarli amongst all node in 'c2' look at the on with the biggest finish time .
amongst all of these the claim is that the biggest finish time will be in 'c2' not in 'c1' .
so what i wanna do next is i wanna assum that thi lemma is true temporarili .
i wanna explor the consequ of that assumpt and in particular what i wanna show you is that if thi lemma hold , then we can complet the proof of correct of kosaraju's two pass scc comput algorithm .
okai , so if the lemma is true then after . . .
i'll give you the argument about why we're done .
about why we just peel off the scc on at a time with the second pass of depth first search .
now of cours a proof with a hole in it , isn't a proof .
so at the end of the lectur i'm gonna fill in the hole .
that is , i'm gonna suppli a proof of thi kei lemma .
but for now , as a work hypothesi , let's assum that it's true .
let's begin with a corollari , that is a statement which follow essenti immedi , from the statement of a lema .
so for the corollari , let's forget about just try to find the maximum maximum finish time in a singl scc .
let's think about the maximum finish time in the entir graph .
now , why do we care about the maximum finish time in the entir graph ?
well , notic that's exactli where the second pass of df is go to begin .
right , so it process node in order from largest finish time to smallest finish time .
so equival , let's think about the node at which the second pass of depth first search is go to begin , i . e . , the node with the maximum finish time .
where could it be ?
well , the corollari is that it ha to be in what i'm gonna call a sink , a strongli connect compon , that is a strongli connect compon without ani outgo arc .
so for exampl let's go back to the , meta graph of scc for the veri first direct graph we look at .
you recal in the veri first direc ted graph we look at in when we start talk about thi algorithm there were four scc .
so there wa a 'c1' , a 'c2' , a 'c3' , and a 'c4' .
and of cours within each of these compon , there could be multipl node but thei ar all strongli connect to each other .
now , let's us f1 , f2 , f3 and f4 to denot the maximum finish time in each of these scc .
so we have f1 , f2 , f3 and f4 .
so , now we have four differ opportun to appli thi lemma .
right ?
those four differ pair of adjac scc .
and so , what do we find ?
we find that well , compar f1 and f2 , becaus c2 come after c1 , that is there's an arc from c1 to c2 , the max finish time in c2 ha to be bigger than that in c1 .
that is f2 is bigger than f1 .
for the same reason f3 ha to be bigger than f1 .
symmetr we can appli the limit to the pair c2 , c4 and c3 , c4 and we get that f4 ha to domin both of them .
now notic we actual have no idea whether f2 or f3 is bigger .
so that pair we can't resolv .
but we do know these relationship .
okai f1 is the smallest and f4 is the smallest biggest ! ! .
and you also notic that c4 is a sink scc and the sink ha no outgo arch and you think about it that's a total gener consequ of thi lema .
so in a simpl group of contradict will go as follow .
consid thi scc with the maximum f valu .
suppos it wa not a sink scc that it ha an outgo arch , follow that outgo arch to get some other scc by the lema the scc you've got into ha even bigger maximum finish time .
so that contradict the fact that you start in the scc with a maximum finish time .
okai .
so just like in thi cartoon , where the uniqu sink scc ha to have the largest finish time , that's total gener .
as anoth saniti check , we might return to the nine node graph , where we actual ran kasaraja's algorithm and look at the ford version of the graph , which is the on on the bottom , we see that the maximum finish time in the three fcc ar <num> , <num> and <num> .
and it turn out that the same as the leader node which is not an accid if you think about it for a littl while and again you'll observ the maximum finish time in thi graph name <num> is inde in the left most scc which is the onli scc with no outgo ark .
okai but it's total gener basic you can keep follow ark and you keep see bigger and bigger finish time so the biggest on of all it ha to be somewher where you get stuck where you can't go forward but there's no outgo ark and that's what i'm call a sink scc .
okai .
so assum the lemma is true we know that the corollari is true .
now us thi corollari let's finish the proof of correct , of kasaraja's algorithm , modul over proof of the kei lima .
so i'm not go to do thi super rigor , although everyth i sai is correct and can be made , made rigor .
and if you want a more rigor version i'll post some note on the cours websit which you can consult for more detail .
so what the previou corollari accomplish , it allow us to locat , the node with maximum finish time .
we can locat it in somewher in some sink scc .
let me remind you about the discuss we had at the veri begin of talk about comput strong compon .
we're tryna understand depth first search would be a us workhors for find the strong compon .
and the kei observ wa that it depend , where you begin that depth first search .
so for exampl in thi , graph with four scc's shown in blue on the right .
a realli bad place to start .
df call depth for search would be somewher in c1 .
somewher in thi sourc scc , so thi is a bad df .
why is it bad ?
well rememb what depth for search doe ; it find everyth findabl from it start point .
and from c1 you can get to the entir world , you can get to all the node in the entir graph .
so you can discov everyth .
and thi is total useless becaus we want to discov much more fine grain structur .
we want to discov c1 , c2 , c3 and c4 individu .
so that would be an disast if we invok depth first search somewher from c1 .
fortun that's not what's go to happen , right ?
we comput thi magic order in the first pass to insur that we look at the node with the maximum finish time and , by the corollari , the maximum finish time is go to be somewher in c4 .
that's gonna be a good df , in the sens that , when we start explor from anywher in c4 , there's no outgo arc .
so , of cours , we're gonna find everyth in c4 .
everyth in c4's strongli connect to each other .
but we can't get out .
we will not have the option of trespass on other strong compon , and we're not gonna find'em .
so we're onli gonna find c4 , noth more .
now , here's where i'm gonna be a littl inform .
although , again , everyth i'm gonna sai is gonna be correct .
so what happen now , onc we've discov everyth in c4 ?
well , all the node in c4 get mark as explor , as we're do depth first search .
and then thei're basic dead to us , right ?
the rest of our depth first search loop will never explor them again .
thei're alreadi mark as explor .
if we ever see'em , we don't even go there .
so the wai to think about that is when we proce with the rest of our for loop in df loop it's as if we're start afresh .
we're do depth first search from scratch on a smaller graph , on the residu graph .
the graph g with thi newli discov strong compon 'c<i>'< i> delet .
so in thi exampl on the right , all of the node in c4 ar dead to us and it's as if we run df anew , just on the graph contain the strong compon c1 , c2 and c3 .
so in particular , where is the next indic of depth first search go to come from ?
it's go to come from some sink scc in the residu graph , right ?
it's go to start at the node that remain and that ha the largest finish time left .
so there's some ambigu in thi pictur .
again recal we don't know whether f2 is bigger or f3 is bigger .
it could be either on .
mayb f2 is the largest remain finish time in which case the next df indic's gonna begin somewher more from c2 .
again , the onli thing outgo from c2 ar these alreadi explor node .
their effect delet .
we're not gonna go there again .
so thi is essenti a sink fcc .
we discov , we newli discov the node in c2 and noth els .
those ar now effect delet .
now , the next indic of df will come from somewher in f3 , somewher in c3 .
that's the onli remain sink scc in the residu graph .
so the third call , the df will discov thi stuff .
and now , of cours , we're left onli with c1 .
and so the final indic of df will emerg from and discov the node in c1 .
and in thi sens becaus we've order the node by finish time when df wa revers graph , that order ha thi incred properti that when you process the node in the second pass we'll just peel off the strongli connect compon on at a time .
if you think about it , it's in revers topolog order with respect to the direct asypr graph of the strongli connect compon .
so we've construct a proof of correct of kosaraju's , algorithm for comput strongli connect compon .
but again , there's a hole in it .
so we complet the argument assum a statement that we haven't prove .
so let's fill in that last gap in the proof , and we'll we done .
so what we need to do is prove the kei lemma .
let me remind you what it sai .
it sai if you have two adjac scc , c1 and c2 and is an arc from a node in c1 , call it 'i' to a node in c2 , sai j .
then the max finish time in c2 is bigger than the max finish time in c1 .
where , as alwai , these finish time ar comput in that first pass of depth first search loop in the revers graph .
all right , now the finish time ar comput in the revers graph , so let's actual revers all the arc and reason about what's happen there .
we still have c1 .
it still contain the node i .
we still have c2 , which still contain the node j .
but now of cours the orient of the arc ha revers .
so the arc now point from j to i .
recal we had a quiz which said , ask you to understand the effect of revers all arc on the scc's and in particular there is no effect .
so the scc's in the revers graph ar exactli the same as in the forward graph .
so now we're go to have two case in thi proof and the case correspond to where we first encount a node of c1 and union c2 .
now rememb , when we do thi df loop , thi second pass , becaus we have thi outer four loop that iter over all of the node we're guarante to explor everi singl node of the graph at some point .
so in particular we're gonna have to explor at some point everi node in c1 and c2 .
what i want you to do is paus the algorithm .
when it first , for the first time , explor some node that's in either c1 or c2 .
there's go to be two case , of cours , becaus that node might be in c1 , you might see that first .
or it might be in c2 , you might see someth from c2 first .
so our case on is go to be when the first node that we see from either on happen to lie in c1 .
and the second case is where the first node v that we see happen to lie in c2 .
so clearli exactli on of these will occur .
so let's think about case on .
when we see a node of c1 befor we see ani node of c2 .
so in thi case where we encount a node in c1 befor we encount ani node in c2 , the claim is that we're go to explor everyth in c1 befor we ever see anyth in c2 .
why is that true ?
the reason is there cannot be a path that start somewher in c1 , for exampl , like the vertex v , and reach c2 .
thi is where we ar us the fact that the meta graph on scc is a cyclic .
right c1 is strong connect , c2 is strong connect , you can get from c2 to c1 and , if you can also get from c1 back to c2 thi all collaps into a singl strongli connect compon .
but that would be a contract , we're assum c1 and c2 ar distinct strongli connect compon , therefor you can't have path in both direct .
we alreadi have a path from right to left , via ji , so there's no path from left to right .
that's why if you origin a depth first search from somewher insid c1 like thi vertex v , you would finish explor all of c1 befor you ever ar go to see c2 , you're .
onli gonna see c2 at some later point in the outer for loop .
so , what's the consequ that you complet finish with c1 befor you ever see c2 ?
well it mean everi singl finish time in c1 is go to be smaller than everi singl finish time in c2 .
so that's even stronger that what we're claim , we're just claim that the biggest thing in c2 is bigger than the biggest of c1 .
but actual finish time in c2 total domin those in c1 , becaus you finish c1 befor you ever see c2 .
so let's now have a look at case on actual in action .
let's return to the nine node graph , the on that we actual ran kosaraju's algorithm to complet .
so if we go back to thi graph which ha the three connect compon , then rememb that the bottom version is the forward version , the top version is the revers version .
so if , if you think about the middl scc as be c1 , pull the row of c1 and the left most .
scc plai the role of c2 , then what we have exactli is case on of the kei lemma .
so , which wa the first of these six vertic visit dure the df loop in the revers graph ?
well that would just be the node with the highest name , so the node nine .
so thi wa the first of these six vertic that depth first search look at in the first pass , that li in what we're call c1 .
and inde everyth in c1 wa discov in that pass befor anyth in c2 and that's why all of the finish time in c2 , the <num> , <num> , <num> ar bigger than all of the finish time in c1 the <num> , <num> , and <num> .
so we're good to go in case two .
we've proven sorri , in case on , we've proven the lemma .
when it's the case that , amongst the vertic in c1 and c2 , depth first search in the first pass see someth from c1 first .
so now , let's look at thi other case , thi grei case , which could also happen , total possibl .
well , the first thing we see when depth first search in the first pass , is someth from c2 .
and here now is where we truli us the fact that we're us a depth first search rather than some other graph search algorithm like breadth first search .
there's a lot of place in thi algorithm you could swap in breadth first search but in thi case two , you'll see why it's import we're us depth first search to comput the finish time .
and what's the kei point ?
the kei point is that , when we invok depth first search begin from thi node v , which is now assum the line c2 .
rememb depth first search will not complet .
we won't be done with v until we've found everyth there is to find from it , right ?
so we recurs explor all of the outgo arc .
thei recurs explor all of the outgo arc , and so on .
it's onli when all path go out of v have been total explor and exhaust , that we final backtrack all the wai to v , and we consid ourselv done with it .
that is , depth first search .
in the revers graph initi at v .
won't finish until everyth findabl ha been complet explor .
becaus there's an arc from c2 to c1 , obvious everyth to c2 is findabl from v , that's strongli connect .
we can from c2 to c1 just us thi arc from j to i .
c1 be strongli connect we can then find all of that .
mayb we can find other strongli connect compon , as well , but for sure depth first search start from v will find everyth in c1 to c2 , mayb some other thing .
and we won't finish with v until we finish with everyth els , that's the depth first search properti .
for that reason the finish time of thi vertex v will be the largest of anyth reachabl from it .
so in particular it'll be larger than everyth in c two but more to the point , it'll be larger than everyth in c1 which is what we ar try to prove .
again let's just see thi quickli in action in the nine node network on which we trace through kosaraju's algorithm .
so to show the rule that case two is plai in thi concret exampl let's think of the right most strongli connect compon as be c1 .
and let's think of the middl scc as be c . <num> .
now the last time .
we call the middl on c1 and the leftmost on c2 .
now we're call the rightmost on c1 and the middl on c2 .
so again , we have to ask the question , you know , of the six node in c1 and in c2 , what is the first on encount in the depth first search that we do in the first pass .
and then again , is the node nine ?
the , the node which is origin label not .
so it's the same node that wa relev in the previou case , but now with thi relabel of the compon , nine appear in the strongli connect compon c <num> , not in the on label c <num> .
so that's the reason now we're in case two , not in case on .
and what you'll see is , what is the finish time that thi origin label nine node get .
it get the finish time six .
and you'll notic six is bigger than ani of the other finish time of ani of the other node in c1 or c2 .
all , the other five node have the finish time on through five .
and that's exactli becaus when we ran depth first search in the first pass , and we start it at the node origin label nine , it discov these other five node and finish explor them first befor final back track all the wai back to nine , and deem nine fulli explor .
and it wa onli at that point that nine got it finish time after everyth reachabl from it had gotten there lower finish time .
so that wrap it up .
we had two case depend on whether in these two adjac scc's , the first vertex encount wa in the c1 , or in c2 .
either wai it doesn't matter , the largest finish time ha to be in c2 .
sometim it's bigger than everyth , sometim it's just bigger than the biggest in c <num> , but it's all the same to us .
and to re cap how the rest of the proof goe , we have a corollari base on thi lemma , which is maximum finish time have to lie in sink scc and that's exactli where we want our depth first search to initi .
if you're initi in a strong compon with no outgo arc , you do df .
the stuff you find is just the stuff and that strongli connect compon .
you do not have ani avenu by which to trespass on other strong compon .
so you find exactli on scc .
in effect , you can peel that off and recurs on the rest of the graph .
and our slick wai of implement thi recurs is to just do the singl second , df pass , where you just treat the node in decreas order of finish time , that in effect , unveil , unveil all of the scc in revers topolog order .
so that's it , kosaraju's algorithm , and the complet proof of correct .
a blazingli fast graph primit that , in ani direct graph , will tell you it strong compon .
so he's now put in a lot of work design and analyz super fast algorithm for reason about graph .
so in thi option video , what i want to do is show you why you might want such a primit , especi for comput on extrem larg graph .
specif , we're go to look at the result of a famou studi that comput the strongli connect compon of the web graph .
so what is the web graph ?
well , it's the graph in which the vertic correspond to web page .
so for exampl , i have my own web page where i , you know , list my research paper , and also link to cours such as thi on and the edg ar go to be direct , and thei correspond precis to hyperlink .
so , the link that , bring you from on web page to anoth .
note of cours that these ar direct edg , where the tail is the page that contain the hyperlink , and the head is the pa , page that you go to if you click the hyperlink , and so thi is a direct graph .
so from my home page you can get to my paper .
you can get to my cours .
sometim i have random link up to thing i like , like sai , my favorit record store and of cours for mani of these web page there ar addit link go out and go in .
so for exampl from my paper i might link some to my co author , some of my co author might be link from their home page to me , or of cours there's web page out there that list the current avail free onlin cours and so on so obvious thi is just part of a , massiv , web graph , just a tini , tini piec of it .
so the origin of the web were probabl around <num> or so but it start to realli explod in the mid'90s .
and by the year <num> it wa alreadi beyond comprehens .
even though in internet year the year <num> is sort of the stone ag rel to right now , rel to <num> but still , even by <num> peopl were so overwhelm with the massiv scale of the web graph ; thei want to understand anyth about it , even the most basic thing .
of cours on issu with understand what the graph look like is you don't even have it local , right ?
it's distribut over all these differ server over the entir world .
so the first thing peopl realli focus on when thei want to answer thi question wa on techniqu for crawl .
so have softwar which just follow lot of hyperlink report back to the home base , from which you can assembl .
at least some kind of sketch of what thi graph actual is .
so , but then the question is even onc you have thi crawl inform even if , onc you've access a good chunk of the node and the edg of thi of thi network .
what doe it look like ?
so what make thi a difficult question , more difficult than , sai , for ani other direct graph you might encount ?
well , it's simpli the massiv scale of the web graph , it's just so big .
so for the graph us in the particular studi i'm gonna discuss , you know , like we said , it wa in the year <num> , which wa sort of the stone ag compar to <num> , so the graph wa small , rel , but still , the graph wa realli , realli , big .
so , it wa someth like <num> million node and on billion edg , realli , on and a half billion edg .
so the refer for the work i'm gonna discuss is a paper by a number of author .
the first author is andrei broder and then he ha mani co author and thi wa a paper that appear in the dub , dub , dub confer of the year <num> .
that's the world wide web confer and i encourag you , those of you who ar interest , to go track down the , the paper on line and read the origin sourc .
so andrei broder the lead author .
at thi time he wa at a compani that wa call alta vista .
so how mani of you rememb a compani call alta vista ?
well , some of you , especi , you know , the youngest on among you mayb have never heard of alta vista and the youngest on among you mayb can't even conceiv of a world in which we didn't have googl but in fact there wa a time when we had web search that it would , but googl did not yet exist .
that wa sort of in the , you know , mayb <num> or so .
and so thi wa in the veri embryon year of googl and thi , thi data set actual came out of , out of alta vista instead .
so etc all want to get some answer to thi question .
what doe the web graph look like ?
and thei approach it in a few wai , but the on i'm go to focu on here is thei ask , well .
you know , what's the most detail structur we can get about thi web graph without do an infeas amount of comput ?
realli just stick to linear time algorithm , at , at the worst and what have we seen ?
we've seen that in a direct graph you can get full connect inform just realli us first search .
you can comput strongli connect compon in linear time with small constant , and that's on of the major thing that thei did in thi studi .
now if you want to do thi same comput todai you'd have on thing go against you and on thing go for you .
the obviou thing that you'd have go against you is that the web is still veri much bigger than it wa here .
certainli by an order of magnitud the thing that you'd have go for you is now there's special system which ar meant to oper on massiv data set and in particular , thei can do thing like comput connect inform on graph data .
so what you have to rememb , for those of you who ar awar of these term , in <num> , there wa no map reduc .
there wa no there were no tool for autom process larg data set .
these gui realli had to do it from scratch .
so let me tell you about what broder et al found when thei did strong connect comput on the web graph .
thei explain their result in what thei call the bow tie pictur of the web .
so let's begin with the center of the knot of the bow tie .
so in the middl we have what we're gonna call a giant , strongli connect compon .
with the interpret be thi is the core of the web in some sens .
all right , so all of you know what a , an scc is at thi point .
a strongli connect compon is the reg ion from which you can get from ani point to ani other point along a direct path .
so in the context of the web graph , with thi giant scc , what thi mean is that from ani webpag insid thi you can get to ani other webpag insid thi just by travers a sequenc of hyperlink .
and hopefulli , it doesn't strike you as too surpris that a big chunk of the web is strongli connect , is well connect in thi sens right ?
so if you think about all these differ univers in the world .
you know , probabl all of the web page correspond to all of the differ univers .
you can get from ani on place to ani other place for exampl , from the homepag on which i put my paper , that often includ link to my co author which veri commonli ar other univers so that alreadi provid a web link from some stanford page to some page at sai , berkelei or cornel or whatev and of cours i'm just a on person .
i'm just on of mani faculti member at stanford .
so you put all of these togeth , you would expect all of the differ scc correspond to differ univers to collaps into a singl on and so on for other sector s well .
an then of cours if you knew that a huge chunk of the web wa in the same shell ham compon , let's sai ten percent of the web , which would be ten of million of web page , you wouldn't expect there to be a second on , right , it would be super weird if there were two differ blob , ten million web page each , that somehow were not mutual reachabl from each other .
that would just , all it take to collaps two scc into on , is a lone arc go from on to the other , and then a lone arc in the revers direct .
and then those two scc collaps into on .
so we do expect a giant scc , just sort of think anecdot about what the web look like , and then onc we realiz there's on giant scc , we don't expect there to be more than on .
all right , so is that the whole stori ?
is the web graph just on big scc ?
well , on of the perhap inter sting find of thi paper is that , you know there is a giant scc , but it doesn't actual take up the whole web , or anyth realli that close to the entir web .
so what els would there be in such a pictur ?
well , there's the other two end of the bow tie .
which ar call the in and the out region in the out region , you have a bunch of strongli connect compon , not giant scc .
we've establish that shouldn't be ani other giant scc , but small scc which you can reach from the giant strongli connect compon , but from which you cannot go back to the giant strongli connect compon .
i encourag you to think about what type of web site you would expect to see in thi out part of the bow tie .
i'll give you onc exampl , veri often if you look at a corpor site includ those of well known corpor , which you would definit expect to be reachabl from the giant sec .
there's actual a corpor polici that no hyperlink can go from someth in the corpor site to someth outsid the corpor site .
so that mean the corpor site is go to be a collect of web page which ar certainli strongli connect , becaus it's a major corpor you can certainli get there from .
the giant ha , but becaus of thi corpor polici you can't get back out .
symmetr , in the in part of the bow tie , you have a strong of compon , gener small on , from which you can reach the giant scc but you cannot get through them from the giant scc .
again i encourag you to think about all the differ type of web page you might expect to see in thi import of the bow tie certainli on realli obviou exampl would be new web page .
so if you just creat someth , and then , you know if i just creat a web page and point it to stanford univers that would immedi be in thi in compon , in thi in collect of compon .
now , if you think about it , thi doe not exhaust all of the possibl of where note can log .
there's a few other case .
thei frankli ar pretti weird , but thei're there .
you can have passiv hyperlink which bypass the giant fcc , and go straight from the in part of the bowti to the out part so suggest call these tube .
and then there's also a kind of veri curiou outgrowth go out of the in compon , but which don't make it all the wai to the giant scc , and similarli there's stuff which goe into the out compon and recommend call these strang creatur' tendril and then in fact you can also just have some weird isol island of scc that ar not connect even weekli to the giant scc .
so thi is the pictur that emerg from broderal's strong compon comput on the web graph and here's qualit some of the main find that thei came up with .
so first of all , that pictur on the previou slide i drew roughli to scale in the sens that all four part , so the giant scc , the in part , the out part and then their residu stuff ; the tube and tendril , have roughli the same size .
you know , more or less <num> percent of the node in the graph .
i think thi surpris some peopl .
i think some peopl might have thought that the core that the giant scc might have been a littl bit bigger than just <num> or <num> .
but is a lot of other stuff outsid of the stronger connect core .
you might wonder if thi is , is just an artifact of the , thi data set be from the stone ag , be from <num> or so .
but , peopl have rerun thi experi on , on the web graph again in later year .
and of cours the number ar chang becaus the graph is grow rapidli , but these qualit find , qualit find have seem pretti stabl , throughout subsequ , reevalu of the structur of the web .
on the other hand , while the .
core of the web is not as big as you might have expect .
it's extrem well connect perhap better connect than you might have expect .
now you'd be right to ask the question .
you know , what could i mean by unusu well connect ?
we've alreadi establish that thi giant core of the web is strong and connect .
you can get from ani on place to ani other place , via a sequenc of hyperlink .
what els could you want ?
well , in fact .
it ha a veri richer notion of connect call the small world properti .
so let me tell you about the small world properti or the phenomenon colloqui known as six degre of separ .
so thi is an idea which had been in the air at least sinc the earli twentieth centuri , but it realli it wa kind of studi .
in a major wai and popular by norgren wa a social scientist back in <num> .
so norgren wa interest in understand you know ar peopl at great distanc in fact connect by short chain of intermediari .
so the wai he evalu thi he ran the follow experi .
he gave he identifi a friend in boston massachusett .
a doctor i believ and so thi wa gonna be the target and then he identifi a bunch of peopl who were thought to be far awai both cultur and geograph .
specif omaha so for those of you who don't live in the us just take it on faith that mani peopl in the us would regard boston and , and omaha as be fairli far apart geograph and otherwis and what he did is he wrote each of these resid of omaha the follow letter .
he said look here's the name and address of thi doctor who live in boston .
okai , your job is to get thi letter to thi doctor in boston .
now , you're not allow to mail the letter directli to the doctor , instead you need to mail it to an intermediari , someon who you know on a first name basi .
so of cours if you knew the doctor on a first name basi you could mail it straight to them , but that wa veri unlik .
so you know what peopl would do in omaha is , thei'd sai , well .
i don't know ani doctor , or i don't know anyon in boston , but at least i know somebodi in pittsburgh , and at least that's closer to boston than omaha , that's further eastward or mayb someon would sai , well i don't realli know anyon at e ast coast , but at least i do know some doctor here in omaha and so thei give the letter to somebodi depend on a first name basi in omaha and then , the situat would repeat .
whoever got the letter , again thei'd be given the same instruct if you know thi doctor in boston on a first name basi , send him the letter .
otherwis , pass the letter on to somebodi who seem more like closer to them then you ar now of cours mani of these letter never reach their destin but shock at least to me is that a lot of them did .
so someth like <num> percent at least of the letter that thei start with made it all the wai to boston .
which i think sai someth about peopl in the late sixti just have more free time on their hand than thei do in the earli 21th centuri and i find thi hard to imagin but it's a fact .
so you had dozen and dozen of letter reach thi doctor in boston .
and thei were abl to trace exactli which path of individu the letter went along befor it eventu reach thi doctor in boston and so then what thei did is thei look at the distribut of chain link .
so how mani intermediari were requir to get from some random person in omaha to thi doctor in boston ?
some were as few as two .
some were as big as nine but the averag number of hop , the averag number of intermediari wa in the rang of five and on half or six and so thi is what ha given rise to the colloqui even the name of a popular plai the six degre of separ .
so that's the origin myth .
that's where thi phrase come from .
these sort of experi with physic letter but now , in network scienc , the small world properti is meant to be a network , which , on the on hand , is richli connect but also , in some sens , there ar enough cue about which link ar like to get closer to some target .
so that if you need to rout inform from point a , to point b .
not onli is there a short path but if you , in some sens , follow your nose , then you'll actual exhibit a short path .
so in some sens , rout inform is easi in small world network and thi is exactli the properti that identifi with scc , veri rich with short path and if you want to get from point a to point b just follow your nose and you'll do great .
you don't need a veri sophist path algorithm to find a short path .
some of you mai have heard of stanlei migrim , not for thi small world experi but for anoth famou , or mayb infam , he did earlier in the sixti which consist into trick volunt into think thei were subject other human be to massiv dose of electr shock .
so that wound up , you know , end up caus a rewrit to certain standard of ethic in experiment psycholog .
you don't hear about that so much when peopl ar talk about network , but that's anoth reason why migram's work is well known and just as a point of contrast , outsid of thi oppon which ha thi rich small world structur veri poor conduct in other part of the web graph .
so there's a lot of cool research go on these dai about the studi of inform network like , like the web graph .
so thei want you to get the impress that the entir interact between algorithm and think about inform network is just be as on strong compon competit in <num> .
of cours , there is all kind of interact , i have just singl on out that wa easi to explain and also highli influenti and interest back in the dai but you know these dai ; lot of stuff ar go on .
peopl ar think about inform network kind of differ wai and of cours algorithm and almost everyth explain a veri fundament law .
so let me just dash off to a few exampl .
i'm whet your appetit , mayb you want to go explor thi topic in greater depth outsid of thi cours .
so on super interest question is , rather than look at a static snapshot of the web like we were do so far in thi video all right , the web is chang all the time .
new page ar get creat , new link ar get creat and destroi and so on and how doe thi evolut proce ?
can we have a mathemat model which faithfulli reproduc the most import first order properti of thi evolutionari process ?
so the second issu is to not think not just about the dynam of the graph itself , but the dynam of inform that get carri by the graph .
and you could ask thi both about the web graph and about other social network like sai , facebook or twitter .
anoth realli import topic which there's been a lot of work on , but we still don't fulli understand by ani mean , is get at the finer grain structur in network , includ the web graph and particularli what we realli like to do is that full proof method for identifi commun .
so group of note is go to be web page and web graph for individu and social network which we should think of as group togeth .
we discuss thi when we talk about applic of cut .
on motiv of cut is to identifi commun everyth .
commun ha been rel dens connect insid and spars connect outsid and that's just a , and that's just babi step , realli we need much better techniqu for both defin and comput commun in these kind of network .
so i think these question ar super interest , both from a mathemat technic level , but also thei're veri time .
answer them realli help us understand our world better .
unfortun these ar gonna be well outsid the cours of just the bread and better design analysi of algorithm , which is what the , on task with cover here .
we've arriv at anoth on of comput scienc's greatest hit , name dijkstra's shortest path algorithm .
so let me tell you about the problem .
it's a problem call singl sourc , shortest path .
basic what we wanna do is comput someth like drive direct .
so we're given as input a graph , in thi lectur i'm gonna work with direct graph , although the same algorithm would work with undirect graph with cosmet chang .
as usual , we'll us m to denot the number of edg and n to denot the number of vertic .
the input also includ two extra ingredi .
first of all , for each edg e , we're given , as input , a non neg length , which i'll denot by l sub b .
in the context of a drive direct applic , l sub b could denot the , the mileag , how long , thi particular road is , or it could also denot , the expect travel time along the edg .
the second ingredi is a vertex from which we ar look for path .
thi is exactli the same as we had in breadth first search and depth first search .
we have an origin vertex which we'll call here , the sourc .
our respons then is to given thi input to comput for everi other vertex v in thi network the length of a shortest path from the sourc vertex s to that destin vertex v .
and so , just to be clear , what is the length of a path that ha , sai , three edg in it ?
well , it's just the sum of the length of the first edg in the path , plu the length of the second edg in the path , plu the length of the third edg in the path .
so if you had a path like thi with three edg , and length on , two , and three , then the length of the path would just be six .
and then we defin the shortest sv path in the natur wai .
so amongst all of the path direct from s to v each on ha it own respect path length .
and then the minimum overal sv path is the shortest path distanc in the graph g .
so , i'm go to make two assumpt for these lectur .
on is realli just for conveni .
the other is realli import .
the other assumpt without which dijkstra's algorithm is not correct , as we'll see .
so , for conveni we'll assum that there is a direct path between s and everi other vertex v in the graph .
otherwis the shortest path distanc is someth we'd defin to be plu infin .
and the reason thi is not a big assumpt is if you think about it , you could detect which vertic ar not reachabl from s just in a pre process step us sai breadth first or depth first search and then you could delet the irrelev part of the graph and run dijkstra's algorithm as we'll describ it on what remain .
altern dijkstra's algorithm will quit natur figur out which vertic there ar path to from s and which on there ar not .
so thi won't realli come up .
so , to keep it simpl just think about , we have an input graph where you can get to s to v .
for everi differ vertex v and the challeng then is amongst all the wai to get from s to v what is the shortest wai to do it ?
so the second assumpt alreadi appear in the problem statement , but i want to reiter it just so it's realli clear .
when we analyz dijkstra's algorithm , we alwai focu on graph where everi length is non neg .
no neg edg length ar allow .
and we'll see why a littl bit later in the , in the video .
now in the context of a drive direct applic it's natur to ask the question , why would you ever care about neg edg length .
until we invent a time machin it doesn't seem like neg edg length ar gonna be relev when you're comput liter path through liter network .
but again rememb that path can be thought of as more abstractli as a just sequenc of decis .
and some of the most power applic of shortest path ar come up with optim wai such sequenc .
so , for exampl , mayb you're engag in financi transact , and you have the option of both bui and sell asset at differ time .
but if you sell , then you get some kind of profit , and that would correspond to a neg edg length .
so there ar quit interest applic in which neg edg length ar relev .
if you ar deal with such an applic , dijkstra's algorithm is not the algorithm to us .
there is a differ shortest path algorithm , a coupl other on , but the most well known on is call bellman ford .
that's someth base on dynam program , which we mai well cover in a sequel cours .
okai , so for dijkstra's algorithm we alwai focu on graph that have onli non neg edg length .
so for the next quiz i just wanna make sure that you understand the singl sourc shortest path problem .
okai let me draw for you here a simpl four node network and ask you for what ar the four shortest path length .
so from the sourc for text s to each of the four vertic in the network .
all right , so the answer to thi quiz is the final option , zero , on , three , six .
to see why that's true , well all of the option had zero as the shortest path distanc from s to itself so that just seem kinda obviou .
so the empti path will get you from s to itself and have zero length .
now suppos you want to get from s to v .
well , there's actual onli on wai to do that , you have to go along thi on hop path .
so the onli path ha length on .
so , the shortest path distanc from s to v is on .
now w's more interest .
there's a direct on hop path , sw , that ha length four .
but that is not the shortest path from s to w .
in fact , the two hop path , that goe through v as an intermediari , ha total path length three , which is less than the length of the direct arc from s to w .
so therefor , the shortest path distanc from s to w is go to be three .
and final , for the vertex t , there's three differ path go from s to t .
there's the t , two hop path that goe through v .
there's the two hop path which goe through w .
both of those have path length seven .
and then there's the three hop path which goe through both v and w , and that actual ha a path length of on plu two plu three equal six .
so despit have the largest number of edg , the zigzag path is , in fact , t he shortest path from s to t , and it ha length six .
all right .
so befor i tell you how dijkstra's algorithm work , i feel like i should justifi the exist of thi video a littl bit .
all right , becaus thi is not the first time we've seen shortest path .
you might be think , rightfulli so .
we alreadi know how to comput shortest path .
that wa on of the applic of breadth first search .
so the answer to thi question is both ye and no .
breadth first search doe inde comput shortest path .
we had an entir video about that .
but it work onli in the special case where the length of everi edg of the graph is on .
at the moment we're try to solv a more gener problem .
we're try to solv shortest path when edg can have arbitrari non neg edg length .
so for exampl , in the graph that we'd explor in the previou quiz , if we ran breadth first search , start from the vertex s , it would sai that the shortest path distanc from s to t is two .
and that's becaus there's a path with two hop go from s to t , put differ , t is in the second layer eman from s .
but as we saw in the quiz , there is not in fact a shortest two hop path from s to t if you care about the edg length , rather , the minimum length path , the shortest path , with respect to the edg weight , is thi three hop path , which ha a total length of six .
so breadth first search is not go to give us what we want , when the edg length ar not all the same .
and if you think about an applic like drive direct , then needless to sai , it's not the case that everi edg in the network is the same .
some road ar much longer than other , some road will have much larger travel time than other ; so we realli do need to solv thi more gener shortest path problem .
similarli if you're think more abstractli about a sequenc of decis like financi transact , in gener , differ transact will have differ valu ; so you realli want to solv gener shortest path not in the special case that breadth first search solv .
now if you're feel particularli sharp todai , you might have the follow object to what i've just said .
you might sai , yeah , big deal .
gener edg weight , unit edg weight .
it's basic the same .
sai you have an edg that ha length three .
how is that fundament differ than have a path with three edg , each of which ha length on ?
so why not just replac all the edg with a path of edg of the appropri length ?
now we have a network in which everi edg ha unit length and now we can just run breadth first search .
so , put succinctli , isn't it the case that comput shortest path with gener edg weight reduc to comput shortest path with unit edg weight ?
well the first comment i want to make is i think thi will be an excel object to rais .
inde , as programm , as comput scientist thi is the wai you should be think .
if you see a problem that seem superfici harder than anoth on , you alwai want to ask .
mayb with a clever trick i can reduc it to a problem i alreadi know how to solv .
that's a great attitud in gener for problem solv .
and inde if all of the edg length were just small number like <num> , <num> and <num> and so on , thi trick would work fine .
the issu is when you have a network where the differ edg can have veri differ length .
and that's certainli the case in mani applic .
definit road network would be on , where you have both , sort of , long highwai and you have neighborhood street .
and potenti , in financi transact base network , you would also have a wide varianc between the valu of differ transact .
and the problem then is , some of these edg length might be realli big .
thei might be a <num> .
thei might be a <num> .
it's veri hard to put a priori bound on how larg these edg weight could be .
so if you start .
wantonli replac singl edg with these realli long path of length <num> , <num> , you've blown up the size of your graph wai too much .
so you do have a faith represent of your old network , but it's too wast .
so even though breadth first search run in linear time , it's now on thi much larger graph , and we'd much prefer someth which is linear time or almost linear time that work directli on the origin graph and that is exactli what dijkstra's shortest path algorithm is go to accomplish .
let's now move on to the pseudocod for dijkstra's shortest path algorithm .
so thi is anoth on of those algorithm where no matter how mani time i explain it , it's super fun to teach .
and the main reason is becaus it expos the beauti that pop up in good algorithm design .
so , the pseudo code , as you'll see in a second , is itself veri eleg .
we're just go to have on loop and then in each iter of the loop , we will comput the shortest path distanc to on addit vertex .
and by the end of the loop we'll have complet shortest path distanc to everybodi .
the proof of correct , which we'll do in the next video , is a littl bit subtl but also quit natur , quit pretti .
and then final , dijkstra's algorithm will give us our first opportun to see the interplai between good algorithm design and good data structur design .
so with a suitabl applic of the heap data structur , we'll be abl to implement dijkstra's algorithm so it run blazingli fast , almost linear time .
name , m time log n .
but i'm get a littl ahead of myself .
let me actual show you the pseudo code .
at a high level , you realli should think of dijkstra's algorithm as be a close cousin of breadth first search .
and inde , if all of the edg length ar equal to on , dijkstra's algorithm becom breadth first search .
so thi is sort of a slick gener of breadth first search when edg can have differ length .
so , like our gener graph search procedur , we're go to start at the sourc for a text s .
and in each iter , we're go to conquer on new vertex .
and we'll do that onc each iter after n minu on iter , we'll be done .
and in each iter , we'll correctli comp ut the shortest path distanc to on new possibl destin vertex , v .
so let me just start by initi some notat .
so capit x is go to denot the vertic we've dealt with so far .
and by dealt with i mean we've correctli comput shortest path distanc from the sourc vertex to everi vertex in x .
we're go to augment x by on new vertex in each iter of the main loop .
rememb that we're respons for output n number , on for each vertex .
were not just comput on thing we're comput the shortest path distanc from the sourc vertex s to everi other vertex .
so , i'm go to frame the output in term of thi arrai capit a .
so for each vertex we're go to have an entri in thi arrai a and the goal is at the end of the algorithm , a will be popul with the correct shortest path distanc .
now to help you understand dijkstra's algorithm , i'm gonna do some addit bookkeep which you would not do in a real implement of dijkstra's algorithm .
specif , in addit to thi arrai capit a , in which we comput shortest path distanc from the sourc vertex to veri other destin , there's gonna be an arrai capit b in which we'll keep track of the actual shortest path itself , from the sourc vertex s to each destin v .
so the arrai a and b will be index in the same wai .
there'll be on entri for each possibl destin vertex v .
capit a will store just a number for each destin's shortest path distanc .
the arrai b in each posit will store an actual path , so the path , the shortest path from s to v .
but again , you would not includ thi in an actual implement .
i just find in my experi it's easier for student to understand thi algorithm if we think of the path be carri along as well .
so now that i've told you the semant of these two arrai , i hope it's no surpris how we initi them for the sourc vertex itself s .
the shortest path distanc from s to itself is zero , the empti path get you from s to s with length zero , there's no neg edg by assumpt , so there's no wai you can get from s back to s with non posit length , so thi is definit the shortest path distanc for s .
by the same reason , the shortest path from s to s is just the empti path , the path with no edg in it .
so now let's proce to the main while loop .
so the plan is we wanna grow thi set capit x like a mold that cover the entir graph .
so in each iter it's go to grow and cover up on new vertex and that vertex will then be process and at the time of process we're respons for comput the shortest path distanc from s to thi vertex and also figur out what the actual shortest path from s to thi vertex is .
so in each iter we need to grow x by on node to ensur that we've made progress .
so the obviou question is , which node should we pick ?
which on do we add the x next ?
so there's gonna be two idea here .
the first on we've alreadi seen in term of all of these gener graph search procedur , which is we're go to look at the edg and vertic which ar on the frontier .
so we're go to look at the vertic that ar just on hop awai from the vertic we've alreadi put into x .
so that motiv at a given iter of the while loop to look at the stuff we've alreadi process that's x .
and the stuff we haven't alreadi process , that's v minu x .
s of cours start in x and we never taken anyth out of x so s is still there , you know .
and some gener iter of the while loop , we might have some other vertic that ar in x .
and in a gener iter of thi while loop , there might be multipl vertic which ar not in x .
and now as we've seen in our graph search procedur there in gener ar edg cross thi cut , so there ar edg which have on end point in each side , on end point in x and on end point outsid of x .
thi is a direct graph so thei can cross in two direct , thei can cross from left to right or thei can cross from right to left .
so you might have some edg intern to x .
those ar thing we don't care about at thi point .
you might have edg which ar intern to v minu x .
we don't care about those , at least not quit yet .
and then you have edg which can cross from x to v minu x .
as well as edg that can cross in the revers direct from v minu x back to x .
and the on we're gonna be interest in , just like when we did graph search on direct graph , ar the edg cross from left to right .
the edg whose tail is amongst the vertic we've alreadi seen and whose head is some not yet explor vertex .
so the first idea is that at each iter of the while loop we scan or we examin all of the edg with tail in x and head outsid of x .
on of those is gonna lead us to the vertex that we pick next .
so that's the first idea , but now we need a second idea , becaus thi is again quit under determin .
there could be multipl vertic , which meet thi criterion .
so for exampl in the cartoon in the bottom left part of thi slide , you'll notic that there's on vertex here which is the head of an arc that cross from left to right , and there is yet anoth vertex down here in v minu x , which again is the head of an arc that cross from left to right .
so there ar two option , of which of those two to suck into our set x .
and we might want some guidanc about which on to pick next .
so the kei idea in dijkstra is to give each vertex a score correspond to how close that vertex seem to the sourc vertex s , and then to pick among all candid vertic the on that ha the minimum score .
lemm be more precis .
so among all cross edg , with tail on the left side and head on the right side , we pick the edg that minim the follow criterion , the shortest path distanc , that we previous comput from s to the vertex v , plu the length of the edg that connect v to w .
so thi is quit an import express , so i will call thi dijkstra's greedi criterion .
thi is a veri good idea to us thi method to choos which vertex to add to the set x , as we'll see .
i need to give a name to thi edg which minim thi quantiti over all cross edg .
so let's call it v star w star .
so for exampl in the cartoon in the bottom left mayb of the two edg cross from left to right , mayb the top on is the on that ha a smaller valu , of dijkstra's greedi criterion .
so in that case thi would be the vertex v star , and the other end of the edg would be the vertex w star .
so thi edg , v star w star is go to do wonder for us .
it will both guid us to the vertex that we should add to x next .
that's gonna be w star .
it's go to tell us how we should comput the shortest path distanc to w star as well as what the actual shortest path from x to w star is .
so specif in thi iter of the while loop , after we've chosen thi edg v star w star , we add w star to x .
rememb by definit w star wa previous not in capit x .
so we're make progress by ad it to x .
that's on more vertex in x .
now , x is suppos to repres all of the node we've alreadi process .
so an invari of thi algorithm is that we've comput shortest path distanc for everybodi in x as well as the actual shortest path , so now that we're put w star in x , we're respons for all of thi inform , shortest path inform .
so what we're gonna do is we're gonna set the , our estim of w star's shortest path distanc from s to be equal to the valu of thi dijkstra greedi criterion for thi edg .
that is , whatev our previous comput shortest path distanc from s to v star wa , plu the length of the direct edg from v star to w star .
now a kei point is to realiz thi code doe make sens , by which i mean , if you think about thi quantiti , av , thi ha been previous comput .
and that's becaus an invari of thi algorithm is we've alwai comput shortest path distanc to everyth that's in x .
and of cours , the same thing hold when we need to assign w star it shortest path distanc becaus v star wa a member of x , we'd alreadi comput it shortest path distanc so we can just look up the v star entri posit in the arrai a .
so over on our pictur on our left we would just sai , well what did we comput the shortest path distanc to v star previous ?
mayb it's someth like seventeen .
and then we'd sai you know , what is the length of thi direct edg from v star to w star ?
mayb that's six .
then we would just add seventeen and six , and we would put <num> as our estim of the shortest path distanc from s to w star .
so we someth analog with the shortest path itself and the arrai b .
that is again we ar respons sinc we just ad w star to capit x we're respons for suggest a path from s to w star in the b arrai .
so what we're gonna do is we're just go to inherit the previous comput path to v star and were just gonna tack on the end on extra hop , name the direct edg from v star to w star .
that'll give us a path from s all the wai to w star via v star as an intermedi pit stop .
and that is the entireti of dijkstra's algorithm .
i have explain all of the ingredi about how it work at a conceptu level .
so the two thing that i ow you is , you know , why is it correct , why doe it actual comput shortest path directli to all the differ vertic , and secondli , how fast can we implement it .
the next two video ar go to answer both of those question .
but befor we do that , let's go through an exampl to get a better feel of how thi algorithm actual work .
and i also wanna go through a non exampl so that you can appreci how it break down when there ar neg edg and that will make it clear why we do need a proof of correct .
becaus it's not correct without ani assumpt about the edg length .
so let's just see how it work on the same exampl we trace here earlier .
so we sorta have just by initi thing in the obviou wai .
so , the shortest path distanc from s to itself is zero .
and the shortest path from s to itself is just the empti path .
and initi x is go to be just the sourc vertex itself .
so now we enter them in while loop .
and so rememb , in the while loop , we sai , well , let's scan all of the edg whose tail is in the vertic we've alreadi look at .
whose tail is in x , and whose head is outsid of x .
now in thi first iter , there ar two such edg .
there's the edg sv , and the edg sw .
so how do we know which of these two to us .
well we evalu dijkstra's greedi criterion .
you gui rememb what that is .
dijkstra's greedi score for a given edg vw that's cross the frontier , is just the previous comput shortest path distanc for the a tail of the arc plu the length of the arc itself .
so at thi point sv ha a greedi score of zero plu on , which is on , and the arc s comma w ha a greedi score of zero plu four , which is four .
so obvious sv is go to be the shorter of those two , so we us the edg sv , thi is plai the role of v star w star on the previou slide , and the algorithm them suggest we should add v to our set x , so we suck in v , and our new x consist of s and v .
and it also tell us how to comput the shortest path distanc and the shortest path from s to v , name in the a arrai , we just write down what wa the greedi , the dijkstra's greedi score for thi particular edg , and that wa zero plu on , or on .
it also tell us how to comput the shortest path for v , name we just inherit the shortest path to the tail of the arc , which , in thi case , wa the empti path from s to itself , and then we tack on the end , we append the arc we us to get here , the arc s to v .
so now we go to the next iter of the while loop .
so with our new set consist of s and v .
now again we wanna look at all edg which ar cross the fr ontier .
edg that have tail in x and head outsid x .
and know we see there is three such cross edg .
there is sw there is vw and there is vt all of those have the tail in x and the head outsid of x , so we need to comput dijkstra's greedi score for each of those three and then pick the minimum , so let's go from bottom to top , so first of all we can look at the arc svsw , excus me .
and the greedi score here is the shortest path distanc for the tail , so it's zero , plu the length of the arc , which is four .
so here we get a four in thi iter .
then , if we do thi crossbar edg , thi vw edg , the dijkstra greedi score is the a valu , or the shortest path distanc valu of the tail , and we comput that last iter .
the a of v valu is on .
we add to that the length of the arc , which in thi case is two .
so thi edg three , thi edg vw ha a score of three .
final there's the arc vt , and here , we're gonna add on , which is the shortest path distanc of the tail of the arc , plu the edg length which is six .
so that ha the worst score .
so sinc the edg vw ha the smallest score , that's the on that guid how we supplement x , and how we comput the shortest path distanc in the shortest path for the newli acquir vertex w .
so the chang ar , first of all , we enlarg x .
so x is now everyth but t .
and then how do we comput thing for w ?
well the shortest path , so our entri in a arrai is just go to be dijkstra's greedi score in the previou set of ration so that wa on plu two so that's go to be equal to three .
and then what is the shortest path , how do we fill up the arrai b ?
well we inherit the shortest path to the tail of the arc .
which in thi case is the arc sv and then we append the arc that we us to choos thi new vertex w so that's the arc vw .
so the new path is just the svw path , okai ?
so we comput the shortest path from s to w in thi graph .
so now we proce to the final iter of dijkstra's algorithm .
we know what vertex we're go to bring into x .
it's gonna be the vertex t .
that's the onli on left .
but we still have to comput by which edg we discov t and bring it into the set x .
so we have to comput the score for each of the two cross arc . . .
vt and wt .
and in thi final iter the score for the arc v , t is unchang .
so thi is still gonna be the a valu of it tail , on , plu the length of the arc , six .
so the score here is still seven .
and now , for the first time , wt is a cross edg of the frontier , and when we comput it score , it's the a valu of it tail w , which is three , plu the length of thi arc , which is three , so we get a rescor of six .
so by dijkstra's greedi criterion , we pick the edg wt instead of the edg vt , and of cours , that doesn't matter who get brought into x , but it doe matter how we comput the a and b valu for t .
so in the final irat .
we comput at to be the dijkstra greedi score of the edg that we pick , which is the edg , wt and the score wa six .
so we comput the shortest path distanc from s to t to be six .
and then what is the path itself ?
well , we inherit the shortest path from the tail of the arc that we us to discov t .
so that's the shortest path to w , which we previous comput as be the path through v .
and then we append the edg we us to discov t , so we append the edg , wt .
so the shortest path from s to t , we're go to comput as the zig zag path .
s .
goe to v , goe to t , sorri , goe to w , goe to t .
and then now inde x is all the vertic .
we've comput it for everyth .
thi is our final output .
the content of the , especi the a arrai , thi , the final output .
shortest path distanc from s to all of the four possibl destin .
and if you go back and compar thi to the exampl you went through to the quiz , you will see at least on thi exampl , inde .
dijkstra's algorithm correct pa , the shortest path distanc .
now , i've said it befor and i'll sai it again .
if someon show you their algorithm work just on some exampl , especia lly a pretti simpl four note exampl , you should not jump to the conclus that thi algorithm alwai work .
sometim the algorithm work fine on small exampl , but break down onc you go to more interest complic exampl .
so i definit ow you a proof that dijkstra's algorithm work not onli in thi network , but in ani network .
and actual , it doesn't work in ani network .
it's onli gonna work in ani network with non neg edg length .
so to help you appreci that , let's conclud thi video with a nonexampl show what goe wrong in dijkstra's algorithm when you have network with neg edg length .
so befor i actual give you a , a real non exampl let me just answer preliminari question which you might have and should be have veri good question if it someth that's occur to you .
the question would be well , ya why is it , why ar these neg instanc such a big deal .
why can't we just reduc shortest path competit with neg edg link to the problem of comput shortest path with non neg edg link .
right so whatev just clear thing out we just add big number to all the edg that make them all non neg and now we just run dijkstra's algorithm and we're good to go .
so thi is exactli the sort of question you should be look to ask , if , as a comput scientist , as a seriou programm .
when confront with a problem you alwai want to look for wai to reduc it to simpler problem that you alreadi know how to solv .
and thi is a veri natur idea of how to reduc a seemingli harder sort of path problem to on we alreadi know how to solv us dutch algorithm .
the onli problem is it doesn't quit work .
on isn't gonna work .
well if you , let's sai you have a graph , and the most neg edg is ten .
so all the edg length ar neg ten and abov .
so then what you'd want to do is add ten to everi singl edg in the network , and that insur that all of the edg length ar non neg , run dijkstra's algorithm , get your shortest path .
the is sue is that differ .
path between a common origin and destin have differ number of edg .
so some might have five edg , some might have two edg .
now if you add ten to everi singl edg in the graph you're go to chang path length by differ amount .
if a path ha five edg , it's go to go up by <num> , when you add ten to everi edg .
if a path ha onli two edg , it's onli go to go up by twenti , when you add ten to everi edg .
so as soon as you start chang the path length of differ path by differ amount , you might actual screw up which path is the shortest .
the path which is shortest under the new edg length need not be the on that's shortest under the old edg length .
so that's why thi reduct doesn't work .
to be concret , let's look at thi veri simpl three vertex graph with vertic s , v , and t and edg length as shown .
on minu five and minu two .
now what i hope is clear , is that in thi graph .
the shortest path .
the on with the minimum length is the two hot path svt , that ha length minu four .
the direct str ha length minu two which is bigger than minu four .
so the upper path is the shortest path .
now , suppos we tri to massag thi by ad a constant to everi edg so all edg length were non neg .
we'd have to add five to everi edg becaus that's the biggest neg number the vt edg .
so that would give us new edg length of six .
and zero end three .
and now the problem is , we've chang which path is the shortest on .
we ad ten to the top path and onli five to the bottom path and as a result , thei've revers .
so now the bottom path st is actual the shorter on .
so if you run dijkstra on thi graph , it's go to come with the path st even though that's not in fact the shortest path in the origin network , the on that we actual care about .
okai , so that's why you can't just naiv reduc shortest path with neg edg length to shortest path with non neg edg length .
moreov on thi veri same , super simpl thre e node graph , you know , we can try to run , run dike for shortest path algorithm .
it's perfectli well defin .
it will produc some output .
but it's actual go to be wrong .
it is not go to comput shortest path distanc , correctli in thi graph .
so let me show you why .
unless of cours initi will work as it alwai doe .
so it's gonna start by sai the shortest path distanc from s to itself is zero via the empti path .
and then , what's it go to do next ?
it's go to sai well we need to enlarg the set capit x by on vertex and there ar two cross edg , it's the xv edg and the st edg .
and what's it go to do .
it's go to us the dijkstra greedi score .
so the score of thi upper edg is go to be on , and the score of thi bottom edg .
is go to be neg two .
'caus rememb , you take the previous comput shortest path of the tail , that's zero in both case .
and then you add the edg length .
so the edg length ar on and minu two , so the score ar on and minu two .
which of these is smaller ?
well evid , the st arc ha the smaller score minu two .
so what is dijkstra's algorithm gonna do ?
it's go to sai ye , let's go for thi edg st .
let's bring t into the set capit x .
t is now part of the conquer territori .
and of cours as soon as you bring a node into the set x , into the territori , you have to commit or dijkstra's algorithm choos to commit to a shortest path distanc and a shortest path .
what is the definit of it shortest path distanc , as comput by djikstra ?
well it's just a degre score .
so it's go to assign the vertex t the shortest path distanc of minu two , and the path is go to be just the arc s t .
but notic that thi is in fact wrong .
the shortest path distanc from s to t is not minu two , in thi graph .
there is anoth path , name the on that goe thorough v that ha length minu four , less than minu two .
so , comput incorrect shortest path distanc on thi trivial <num> note graph .
so t o summar the stori so far , we've describ dijkstra's algorithm .
i've shown you that it work in a veri simpl exampl that doesn't have neg edg line and i've show you that it doesn't work in and even simpler exampl that doe have neg edg line .
so , i've both given you some plausibl that it might work gener at least for non neg edg link .
but i've also tri to sew some seed of doubt that it's not an all clear if at thi point if dijkstra's algorithm is alwai clear correct or not even if you have non neg edg length and certainli if it is alwai correct there better be a fool proof argument to why .
you should be demand and explan of a claim if dijkstra's is correct in ani kind of gener .
that's the subject of the next video .
in thi video i'll prove to you that dijkstra's algorithm doe inde comput correct shortest path in ani direct graph where all edg length ar nonneg .
let me remind you about what is dijkstra's algorithm .
it's veri much in the spirit of our graph search primit , in particular , breadth first search .
so there's go to be a subset x of vertic , which ar the on that have been process so far .
initi , x contain onli the sourc vertex .
of cours , the distanc from the sourc vertex to itself is zero , and the shortest path from s to itself is the empti path .
so then we'll have a main loop .
there's gonna be n minu on iter .
in each iter , we'll bring on vertex which is not current in x into capit x .
and the variant we're go to maintain is that all the vertic in x we would have comput estim of the shortest path distanc from s to that vertex , and also will have comput the shortest path itself from s to that vertex .
rememb our stand assumpt state in the previou video , we're alwai gonna assum there's at least on path from the sourc vertex s to everi other destin v .
our job is just go to comput the shortest on , and also we have to assum that the edg length ar non neg , as we've seen otherwis dijkstra's algorithm might fail .
now , the kei idea in dijkstra's algorithm is a veri care choic of which vertex to bring from outsid of x into capit x .
so what we do is , we scan the edg cross the frontier .
mean , given the current edg vertic that we've alreadi process , we look at all of the edg whose tail ha been process , and whose head ha not been process .
so the tail is in capit x , the head is outsid of x .
that is , thei cross .
the cut from left to right in the diagram that we usual draw .
now , there mai be mani such edg .
how do we decid amongst them ?
well , we comput the dijkstra greedi score for each .
the dijkstra greedi score is defin as the shortest path distanc we comput for the tail .
and tha t's been previous comput cuz the tail in capit x .
and then we add to that the length contribut by thi edg itself , by the edg v , w , which is cross the cut from left to right .
so amongst all edg cross from left to right , we comput all those dijkstra greedi score , we pick the edg with the smallest greedi score , call that edg just v star w star for the purpos of notat .
w star is the on that get ad to x , so it's the head of the arc with the smallest greedi score , and we comput the shortest path distanc of that new vertex w star to the be the shortest path distanc to v star plu the length contribut by thi edg v star w star and what is the shortest path .
it's just the shortest path previous comput to v star , plu , thi extra edg v star , w star , tack on to the end , is the formal statement we ar go to prove .
for thi video we're not go to worri at all about run time .
that will be the discuss of the next video .
we'll discuss both the run time of the basic algorithm and a super fast implement that us the heat data structur .
for now we're gonna just focu on correct .
so the claim is that for everi direct graph , not just the <num> node , <num> arc exampl we studi , as long as there's no neg edg length , dijkstra's algorithm work perfectli , comput all of the correct shortest path distanc .
so just to remind you about the notat , what doe it mean to correct all shortest path distanc correctli ?
it mean that what the algorithm actual comput , which is a of v .
is exactli the correct shortest path distanc , which we were denot by capit l and v in the previou video .
so both the algorithm and the proof of correct wa establish by edsger dijkstra , thi is back in the late 1950s .
edsger dijkstra wa a dutch comput scientist and certainli you know , on of the forefath of the field as a , as a , as realli as a scienc , as an intellectu disciplin .
he wa award the acm ture award , that is the nobel prize in comput sci enc , effect , i believ it wa <num> and he work for a long time in the netherland but also spend a lot of hi later career at ut austin .
so the wai thi proof is gonna go is by induct .
and basic what we're go to do is we're go to sai everi iter when we have to commit to a shortest path distanc to some new vertex , we do it correctli .
and so then the form of the induct will be , well that given that we've made all of our previou decis correctli , we comput all our earlier shortest path in the correct wai , that remain true for the current iter .
so formal it's induct on the number of iter of dijkstra's algorithm .
and as is more often than not the case in proof by induct , the base case is complet trivial .
so that just sai befor we start the while loop , what do we do ?
well , we commit to the shortest path distanc from s to itself .
we set it to zero .
we set the shortest path to be the empti path .
that is of cours true .
of cours even here , we're us the fact that there ar no edg with neg edg length .
that make it obviou that sort of have a nonempti path can't get you neg edg length better than zero .
so the first shortest path comput we do from s to s is trivial correct .
the hard part , of cours , is the induct step justifi all of the futur decis done by the algorithm .
and of cours , mind of that exampl , that non exampl we had at the end of the previou video .
in the proof by induct , we'd better make us of the hypothesi that everi edg ha non neg length , okai ?
otherwis , the theorem would be fals .
so we'd better , somewher in the proof , us the fact that edg cannot be neg .
so let's move on to the induct step .
rememb the induct step the first thing to do is to state the induct hypothesi .
you're assum you haven't made ani mistak up to thi point .
let's be a littl bit more formal about that .
so that is everyth we comput in the past okai what did we comput in the past ?
well for each vertex which is in our set capit x for each vertex that we've alreadi process .
we want to claim our comput shortest path distanc match up exactli with the true correct shortest path distanc .
so in our run notat for everi alreadi process vertex so for all vertic v in our set capitol x .
what we comput as our estim of the shortest path distanc for v is in fact the real shortest path distanc .
and also .
the comput shortest path .
is in fact a true shortest path from stv .
so again , rememb , thi is a proof by induct .
we ar assum thi is true , and we're go to certainli make us of thi assumpt when we establish the correct of the new iter , the current iter .
so what happen in an iter ?
well , we pick an edg , which we've been call v star , w star .
and we add the head of thi edg w star to the set x so let's get our bear and rememb what dijkstra's algorithm comput as the shortest path and shortest path distanc for thi new vertex w star .
so by the definit of the algorithm , we assign as a shortest path .
report shortest path from s to w star .
the previous comput reportedli shortest path from s to v star .
and then we tack on the end the direct arc v star w star .
so pictori we alreadi had some path that start at s and end up at v star and then we tack on the end thi arc go to w star in on hop .
and thi whole sha bang , is what we're go to assign as , paw star .
so let's us the induct hypothesi .
the hypothesi sai that all previou iter ar correct .
so that is , ani shortest path we comput in a previou iter is in fact , a bona fide shortest path from the sourc s to that vertex .
now , v star , rememb , is in x , so that wa previous comput .
so , by the induct hypothesi , thi path bv star , from s to v star , is in fact a true shortest path from s to v star in the graph .
so therefor .
it ha length l of e star .
rememb , l of e star is just , by definit , the true shortest path distanc in the g raph from s to v star .
and now , given that the path that we've exhibit from s to w star is just the same on as we inherit the v star , plu thi extra edg tack on .
it's pretti obviou what the length of the left hand side is , so it ha length .
just the length of the old path , which we just argu is the shortest path distanc from s to v star , plu the length of thi arc that we tack on .
so that's gonna be l of v star , w star .
so by the definit of the algorithm , what we comput for w star is just score which is just the comput shortest path distanc to the tail .
v star plu the length of the direct edg , by the induct hypothesi we've correctli comput all previou shortest path distanc .
v star is someth we've comput in the past by conduct hypothesi is correct so thi is equal to l of v star .
by the induct hypothesi .
all right .
so don't worri if you're feel a littl lost at thi point .
we actual realli done no content in thi proof yet .
we haven't done the interest part of the argument .
all we've been do is set up our domino , get them readi to be knock down .
so , what have we done in the current iter ?
well first of all our estim of the shortest path distanc from the sourc to w star to the new vertex that we're includ in the x is the true shortest path distanc to v star , plu the length of the edg from v star to w star .
that's the first thing .
secondli , the path that we have in the b arrai .
is a bonifi path from s .
to w .
star with exactli thi distanc .
and the point is now it's clear what ha to be proven for us to complet the induct step , and therefor the proof of dijkstra's algorithm .
so what do we need to prove ?
we need to prove that thi isn't just ani old path that we've exhibit from s to thi vertex w star .
that it's the shortest path of them all .
but differ , we need to show that everi other s w star path in thi graph ha length at least thi circl valu .
so let's proce .
let's show that no matter how you get from the sourc vertex s .
to thi destin w .
star , the total length of the path that you travel is go to be at least thi circl valu , at least l of v star , plu l of v star , w star .
now on the on hand , we don't have a lot go for us , becaus thi path p could be almost anyth .
it could be a crazi look path .
so how do we argu that it ha to be long ?
well , here's the on thing we've got go for us for ani path p that start in s and goe to w star .
ani such path must cross the frontier .
rememb it start on the left side of the frontier .
it start at the sourc vertex which is initi and forev in the set capit x .
and rememb that we onli choos edg that cross the frontier whose head is outsid of x .
and w start exactli at the head of the edg we chose in thi iter , so thi is not an x .
so ani path that start in x and goe outsid of x , at some point it cross from on to the other .
so let's think about the graph and it two piec , that to the left of the frontier and to the right , the stuff is alreadi process and the stuff that , which ha not yet been process .
s , of cours , is in the left hand side .
and at the begin of thi iter of the while loop , w star wa on the right hand side .
ani path , no matter how wacki , ha to at some point cross thi frontier .
mayb it doe it a bunch of time , who know .
but it's gotta do it onc .
let's focu on the first time it cross the frontier , and let's sai that .
it cross the front here with the vertex y go to the vertex z .
that is , ani path p ha the form where there's an initi prefix , where all the vertic ar in x , and then there's some first point at which it cross the frontier , and goe to .
a vertex which is not an x , we're call the first such vertex outsid of x , z , and then i can skip back and forth , who know , but certainli it end up in the vertex w star , which is not in x .
so we're gonna make us of just thi minim inform about arbitrari path p , and yet thi will give us enough of a foothold to lower bound it length , and thi lower bound will be strong enough that we can conclud that our path , that we comput , is the best .
smaller than ani possibl competitor .
so let's just summar where we left off on the previou slide .
we establish that everi direct path from s to w star p no matter what it is it ha to have a prescrib form .
where it ambl for awhil insid x .
and then the portal through which it escap x .
for the first time , we're call y .
and then the first vertex it see outsid of x .
is z , and there ha to be on .
and then it perhap ambl further , and eventu reach w .
star .
it could well be that z and w star were exactli the same that's total fine for thi argument .
so here's on of our competitor thi path p and it show it's least as long as our path .
so we need a lower bound on the link of thi arbitrari path from s to w star .
so , it's get that lower bound by argu it by each piec separ and then invok dijkstra's greet criterion .
so rememb we said we better us the hypothesi that edg link ar non neg otherwis we ar toast , otherwis we know that the algorithm is not correct .
so here's where we us it .
thi final part of the .
path from z to w star .
if it's nonempti , then it's gotta have nonneg length .
right ?
everi edg , as part of thi subpath , ha nonneg edg length .
so , total length of thi part of the path is nonneg .
so y .
to z .
by construct is direct arc .
rememb thi is the first arc that path p .
us to go from x to get outsid of x .
okai ?
so it's how it escap , the conquer territori x .
and thi just ha some length l of yz , so that leav the first part of thi path the prefix of the path that li entir in capit x , so how do we get a lower bound on the length to thi path .
well , let's begin with some trivial , thi is some path from s to y so certainli it's at least as long as a shortest path from s to y .
and now we're gonna us the induct hypothesi again .
so thi vertex y , thi is somet hing we treat in a previou iter , right ?
thi belong to the set capit x .
we've alreadi process it .
we've alreadi comput or estim the shortest path length and the induct hypothesi assur us that we did it correctli .
so whatev valu we have hang out in our arrai capit a , that is inde the length of a true shortest path .
so the length of the shortest sy path is l of y by definit .
and it's a y by the induct hypothesi .
and now we're in busi .
all right , so what doe thi mean we can sai about the total length of thi arbitrari path p ?
well , we've broken it into three piec and we have a lower bound on the link for each of the three piec .
our lower bound ar , our comput shortest path distanc to y , the length of the direct edg from y to z , and zero .
so ad those up we get that the length of path p is at least .
our comput shortest path distanc to y plu the length of the arch from y to z .
so why is thi us ?
well , we've got on remain trick up our sleev .
is a hypothesi which is presum veri import , which i have not yet invok .
and that is the choic of dijkstra's greedi criterion .
at no point in the proof , yet , have we us the fact that we select which vertex to add next accord to dijkstra's greedi score .
so that is gonna be the final nail in the coffin , that is go to complet the proof .
how do we do that ?
well , we've taken an arbitrari path p .
we've lower bound it length in term of the comput shortest path distanc up to the last vertex of thi prefix y , plu the arc length to get from x to outsid of x , c y , z .
so , rememb thi mean y is in the left part .
of the frontier and z is not and therefor , in thi iter , the edg yz wa total a candid for us to us to enlarg our frontier .
rememb , we look at all of the edg cross from left to right , yz is on such edg , and amongst all of them we chose the on with the smallest dijkstra greedi score .
that wa the dijkstra greedi criterion .
so what have we shown ?
we've shown that th e length of our path .
is no more then what's a lower bound on the length of thi arbitrari other path p .
so thi complet the proof .
so let me just remind you of all the ingredi , in case you got lost along the wai .
so what we start out with is we realiz our algorithm , or dijkstra's algorithm , it doe comput some path from s to w star , right .
it just take the path it comput previous to v star and it just append thi final hop at the end .
so that give us some end from s to w star .
moreov , it wa easi to figur out exactli what the length of that path is , and the length of the path that we came up with is exactli the circl quantiti at the bottom of the slot .
it's the shortest path distanc .
comma apostroph star , plu the length of the direct arc from v star to w star .
so that wa how well we did .
but we had to ask the question , is it possibl to do better ?
well , we're try argu that our algorithm doe the best possibl , that no compet path could possibl be shorter than our .
so how did we do that ?
well , we consid an arbitrari compet path p .
the onli thing we know about it is that it start at s and end at the w star and we observ that ani path .
can be decompos into three piec ; a prefix , a direct edg and a suffix .
then we give a lower bound on thi path p , right ?
the direct edg , you know the length is just whatev it is , the suffix we just us the trivial lower bound that will be zero and that's where we us the hypothesi that everi edg ha non neg edg length and for the prefix , becaus that's all in the stuff we alreadi comput , we can invok the induct hypothesi and sai , well whatev thi path is , it goe from s to some vertex in y so at least the shortest path distanc from s to y .
which is someth we comput in a previou iter .
we lower bound the length of ani other path in term of the dijkstra greedi score for that path , sinc we choos the path with the best greedi score .
that's why we end up , we wind up with the shortest path of them all from s to w star .
thi , of cours , is embed in a outer proof by induct on the number of iter .
but thi is the induct step which justifi a singl iter , sinc we can justifi everi iter given correct of the previou on .
that mean , by induct , all of them ar correct , so all of the shortest path ar correct .
and that is why dijkstra's algorithm correctli comput shortest path .
ani direct graph with non neg edg link .
in thi video we'll discuss how we actual implement dijkstra's shortest path algorithm .
and in particular , us the heap data structur , we'll give a blazingli fast implement , almost linear time .
let me just briefli remind you of the problem we're solv .
it's the singl sourc , shortest path problem .
so we're given the direct graph and a sourc vertex , s .
we're assum that there's a path from s to everi other vertex , v .
if that's not true , we can detect it with an easi pre process step , so our task then is just to find the shortest path amongst all of them from the sourc vertex , s to each possibl destin , v .
moreov , everi edg of the graph ha a non neg edg length which we're denot , els of v .
so recal that dijkstra's algorithm is driven by a singl y loop .
so we're go to add on addit vertex to an evolv set capit x as the algorithm proce .
so x is the vertic that have been process so far .
we maintain the invari that for everi process vertex we've comput what we think the shortest path distanc is to that vertex .
so initi x is just the sourc vertex s .
of cours , the shortest path distanc from s to itself is zero .
and then the clever of dijkstra's algorithm is in how we figur out which vertex to add to the set capit x each iter .
so the first thing we do is we .
focu onli on edg that cross the frontier edg that have their tail in capit x and their head outsid of capit x .
now , of cours , there mai be mani such edg , edg that cross thi frontier and we us dijkstra's gradi criterion to select on of them .
so for each cross edg , each edg with a tail and x and head outsid x , we comput the dijkstra gradi score that is defin as the previous comput shortest path distanc to the tail of the arc plu the length of the arc .
so we comput that for each cross edg and then the minimum edg we're call it v star w star .
that determin how we proce .
so we add the head of that arc w star to the set capit x and then w e comput the shortest path distanc to w star to give the previou .
see comput shortest fast distanc to v star plu the length of thi extra v star , w star .
now back when i explain thi algorithm i did it us two arrai , arrai capit a and arrai capit ba is what comput the shortest path distanc , and rememb that's what the problem ask us to comput .
and for clariti i also fill up thi arrai capit b just to keep track of the shortest path themselv .
now if you look at the code of thi algorithm , we don't actual need the arrai capit b for anyth .
when we fill in the arrai capit a , we don't actual refer to the b arrai .
and so now that we're gonna talk about real implement of dijkstra ; i'm actual gonna cross out all of the instruct that correspond to the b arrai .
okai ?
becaus you would not , as i told you earlier , us thi in a real implement of dijkstra .
you would just fill in the shortest path distanc themselv .
so in the next quiz , what i want you to think about is the run time of thi algorithm if we implement it more or less as is , accord to the pseudo code on thi slide without ani special data structur .
and in the answer to the quiz , we're go to be us the usual notat where m denot the number of edg in the graph , and n denot the number of vertic of the graph .
so the correct answer to thi quiz is the fourth on that the straightforward implement of dijkstra's algorithm would give you a run time proport to the product of the number of edg and the number of vertic .
and the wai to see that is to just look at the main while loop and look at how mani time it execut and then how much work we do per iter of the while loop if we implement it in a straightforward wai .
so there's gonna be n minu on iter of the while loop .
and the reason is , is that the algorithm termin onc everi singl vertex ha been ad to capit x .
their end vertic , initi there's on vertex in x .
so after n m inu on iter , we'll have suck up all of the vertic .
now what's the work done in each wild loop ?
well basic , we do naiv a linear scan through all of the edg .
we go through the edg .
we check if it's an elig edg , that is if it tail is in x and it head is outsid of x .
we can keep track of that just by have an auxiliari bullion variabl for each vertex .
rememb whether it's an x or not .
and then amongst all of the illeg edg , those cross the frontier , we just , by exhaust .
search rememb which edg ha the smallest dijkstra store score , now we can comput the dijkstra score in constant time for each of the edg .
so that's a reason algorithm .
we might be abl to get awai with graph that have sai hundr or thousand of vertic us the straight forward of implement , but of cours , we'd like to do better .
we'd like the algorithm to scale up to much larger graph , even graph with potenti sai a million vertic so the answer is ye , we can do better .
not by chang the algorithm , but , rather , chang how we organ the data as the algorithm proce .
so thi will be the first time in the cours where we us a data structur to get an algorithm speed up .
so we're gonna see a realli love interplai between , on the on hand , algorithm design and , on the other hand , data structur design in thi implement of dijkstra's algorithm .
so you might well ask what's the clue that indic that a data structur might be us in speed up dijkstra's shortest path algorithm .
and the wai you'd figur thi out is you'd sai , well , where is all thi work come from ?
why ar we do a linear amount of work in the edg for a linear number in the vertic iter ?
well , at each iter of thi while loop , what we're do is , we're just do an exhaust search to comput a minimum .
we look at everi edg , we look at those that cross the frontier , and we comput the on with the minimum dijkstra score .
so we could ask ourselv , oh , if we're do minimum comp utat over and over and over again , is there some data structur which , whose raison d ? ? tre , whose reason for be is in fact to perform fast minimum comput ?
and in fact there is such a data structur .
it's the heap data structur .
so in the follow descript of a fast implement of dijkstra's algorithm , i'm go to assum you're familiar with thi heap data structur .
for exampl , that you watch the review video elsewher on the cours site that explain it .
so let me just remind you with a lightn quick review of what we learn in that video , so heap ar gener logic thought of as a complet binari tree , even though thei ar usual implement as a laid out linear arrai .
and the kei properti that you get to leverag but that you also have to maintain in a heap is the heap properti that at everi node the kei at that node ha to be at least as small as that of both of the children .
thi properti ensur that the smallest kei of them all ha to be at the root of thi tree .
to implement extract menu , just pluck off the root .
that's what you return .
that's the minimum element .
and then you swap up the , bottommost rightmost leaf .
the last element , make that the new root , and then you bubbl that down as necessari to restor the heap properti .
when you do insert , you just make the new element the new last leaf , bottommost rightmost leaf , and then you swap up as need to restor the heap properti .
when we us heap in dijkstra's algorithm we're also go to need the abil to delet an element from the middl of the heap .
but again you can do that just by swap thing and doubl up or down as need .
i'll leav it as an exercis for you to think through carefulli , how to delet element from the middl of a heap .
becaus you're maintain the heap as an essenti perfectli balanc binari tree , the height of the tree is roughli the log base two of n , where n is the number of element in the heap .
and becaus for everi oper , you implement it just by do a constant amo unt work at each level of the tree , all of these oper run in o of log n time , where n is the number of item that ar be store in the heap .
as far as the intuit connect between the heap data structur and dijkstra's algorithm .
in the main wild loop of dijkstra's algorithm , we're respons for find a minimum , everi singl iter .
what ar heap good for ?
thei're good for find minimum in logarithm time .
that sound a lot better than the linear time we're spend in the naiv implement of dijkstra's algorithm .
so let's now see how to us heap to speed up dijkstra's shortest path algorithm .
now becaus everi iter of the wild loop is respons for pick an edg , you might expect that we're go to store edg in the heap .
so the first subtl but realli good idea is to actual us a heap to store vertic rather than edg .
go back to the pseudo code algorithm , rememb that the onli reason that we focus on an edg .
well so that we can then deduc which vertex , name the head of that edg , to add to our set capit x .
so we're just go to cut to the chase , we're just go to keep vertic not yet in x and then when we extract them in from the heap , it'll tell us which is the next vertex to add into the set capit x .
so the pictur we're go to wanna have in mind is dijkstra's choic path algorithm at some intermedi iter .
so there'll be a bunch of vertic in the , the set capit x sourc vertex plu a bunch of other stuff that we've suck into the set so far .
and then there'll be all the vertic we haven't process yet .
a big group v minu x .
then there's gonna be edg cross thi cut in both direct from x to v minu x and vice versa .
now befor i explain the second invari , let's just recal what the straightforward implement of dijkstra's algorithm need to do .
what it would do is search through all the edg and it would look for ani elig edg .
those with tail and x , and head and v minu x .
so in thi pictur , there w ould be three such edg .
i've drawn the exampl so that two of the edg , the top two edg , both share a common head vertex wherea the third edg ha it own head vertex .
the straightforward of limit of dysktra's algorithm we ? d comput dystra's greedi score for each of these three edg and rememb , by definit , that's the previous comput shortest path distanc to the tail of the arc v , plu the length of the arc vw .
so the straightforward implement just comput thi .
in thi case , it would comput it for three edg .
and whichev the three edg won had the smallest score .
the head of that edg would be the next vertex that get ad to x .
so let me specifi the second invari , and then i'll tell you how to think about it .
so , becaus we're store vertic rather than edg in the heap , we're go to have to be fairli clever with the wai we defin the kei of a vertex that's in thi heap .
so we're go to maintain the properti that the kei of a vertex v is the smallest greedi dijkstra score of ani ver , ani edg which ha that vertex as it head .
so let me show you what i mean in term of our exampl , where we have three cross edg .
suppos for these three edg in the upper right that happen to have of dijkstra score of seven , three , and five .
let's look at what the kei should be for each of these three vertic i've drawn in v minu x .
now for the timelin vertex thi is pretti interest .
there ar two differ edg whose tail is in x , and have thi vertex as their head .
so what should the kei of thi vertex be ?
well , it should be the smallest dijkstra greedi score of ani of the edg whose tail li on the left hand side that termin at thi vertex .
so there's two candid edg .
on ha dijkstra greedi score three .
on ha dijkstra greedi score seven .
so the kei valu should be three , the smaller of those two .
now , the second vertex , there's onli a singl edg that ha tail in x and that termin at thi vertex .
so the kei for thi vertex should ju t be the score at that weak edg .
so in thi case that's gonna be five .
and then thi poor third vertex , there's actual no edg at all , that , start x and termin at thi vertex .
there's onli on arc go the wrong direct .
so for ani edg , sorri , for ani vertex outsid of x that doesn't have ani elig edg termin at it , we think of the kei as be plu infin .
so the wai i recommend think about these heap kei is that we've taken what us to be on round tournament , winner take all and we've turn it into a two round knockout tournament .
so in our straightforward implement of dijkstra's algorithm , we did a singl linear search through all the edg , and we just comput the dijkstra's score for each and we pick the best .
so in thi exampl we would have discov these three edg in some order .
their score ar three , five , and seven .
and we would have rememb the edg with score three as be the best .
that would have been our winner of thi winner take all tournament .
now when we us the heap , it , we're factor it into two round .
so first , each vertex in v minu x run a local tournament .
to elect a local winner , so each of these vertic in v minu x .
sai , well let me look at all of the edg .
for whom i'm the head and also the tail of that edg is in x .
and amongst all of those edg that start in x and termin at me , i'm go to rememb the best of those .
so that's the winner of the local tournament of the first round .
and now the heap is onli go to rememb thi set of first round winner .
right , there's no point in rememb the exist of edg who aren't even the smallest score that termin at a given vertex , becaus we onli care about the smallest score overal .
now when you extract min from the heap , that's in effect .
execut the second and final round of thi knockout tournament .
so each of the vertic of v minu x ha propos their local winner .
and then the heat in an extract min just choos the best of all of those local winner .
so that's the final propos vertex that come out of the heap .
so the point is that if we can successfulli maintain these two invari , then , when we extract min from thi heap , we'll get exactli the correct vertex , w star , that we're suppos to add to the set capit x next .
that is , the heap will just hand to us on a silver platter exactli the same choic of vertex that our previou exhaust search through the edg would've comput .
the exhaust search wa just comput the minimum in a brute forc wai , in a singl winner take all tournament .
the heap implement in thi wai choos exactli the same winner .
it just doe it in thi <num> round process .
now , in dijkstra's algorithm , we weren't suppos to mere just find the vertex w star to add to x .
we also had to comput it shortest path distanc .
but rememb , we comput the shortest path distanc as simpli the dijkstra greedi score .
and here the dijkstra greedi score is just go to be the kei for thi heap that's immedi from invari number two .
so we're us the fact here that our kei ar , by definit , just .
the smallest greedi score ar edg that stick into that vertex w star so again exactli replic .
the comput that we would have done in the straightforward implement , just in a much slicker wai .
okai ?
but we're ad exactli the same vortic , in exactli the same order , and we're comput exactli the same shortest path distanc in thi heap of notat , provid of cours that we do successfulli maintain these two invari throughout the cours of the algorithm .
so that is now what i ow you .
we have to pai the piper .
we've shown that if we can have a data structur with these properti .
then we can simul the straight forward implement now i have to show you how we maintain these invari without do too much work .
all right .
so maintain invari number on will realli take care of itself .
realli sort of by definit the vertic which remain in the heap ar those that we haven't process ed yet , and those ar the on that ar outsid of capit x .
so realli the trick is , how do we maintain invari number two ?
now befor i explain thi let me point out , that thi is a tricki problem .
there is someth subtl go on .
so as usual , i want you to think about thi shortest path algorithm at some intermedi iter .
okai ?
so take a , take a snapshot .
a bunch of vortic have alreadi been ad to x .
a bunch of vortic ar still hang out in the heap .
thei haven't been ad to x .
there's some frontier , there's a , just cross , possibl in both direct .
and suppos at the end of a current iter we identifi the vortex w , which we're go to extract from the heap and conceptu add to the set x .
now the reason thing complic is when we move a vortex from outsid x to insid x .
the frontier between x and v minu x chang .
so in thi pictur , the old black x becom thi new blue x .
and what's realli interest about the frontier chang is that then the edg which cross the frontier chang .
now , there might be , there ar some edg which us to cross the frontier and now don't .
those ar the on that ar come into w .
those we're not so concern with .
those don't realli plai ani role .
what make thing tricki is that there ar edg which us to not be cross the frontier but now thei ar cross the frontier .
and those ar precis the edg stick out of w .
so in thi pictur there ar three such edg which i will highlight here in pink .
to see why it's tricki when new edg all the sudden ar cross a frontier let's rememb what invari number two sai .
it sai that for everi vertex which is still in the heap , which is not yet in x , the kei for that vertex better be the smallest dijkstra gradi score of ani edg which come from capit x and stick into thi vertex of v .
now in move on vertex into x , name thi vertex w , now there can be new edg stick into vertic which were still on the heap .
as a result , the appropri kei valu for vertic i n the heap might be smaller .
now the w ha been move into x .
and the candid for the vertic in the heap whose kei might have drop ar precis those vertic on the other end of edg stick out of w .
so summar , the fact that we'd ad a new vertex to capit x and extract someth from the heap , it's potenti increas the number of cross edg across the frontier , becaus the frontier ha chang .
and therefor , for vertic that remain in the heap , the smallest greedi score of an edg that stick into them from the set x might have drop .
so we need to updat those kei to maintain invari number two .
now , that's the hard part .
here's what we have go for us .
we've damag the kei perhap by chang the frontier , but the damag is local .
we can understand exactli whose kei might have drop , so as suggest by the pictur , the vertic whose kei we need to updat ar precis those at the head of edg that stick out of w .
so for each outgo edg from w , the vertex we just extract from the heap , we need to go to the other end of the edg and check if that vertex need it kei to be decreas .
so here's the pseudo code to do thi .
so when we extract the vertex w from the heap , that is when we conceptu add a new vertex w .
to the set x , therebi chang the frontier , we sai , well , you know , we know the onli vertic that might have to have their kei chang , thei're the on on the other side of these outgo arc from w .
so we just have a simpl iter over the outgo edg , w v , from the vertex v .
now i haven't shown you ani edg in the pictur like thi , but there might well be some edg where the head of the arc v is also in the set x , is also alreadi been process .
but anyth in x is not in the heap .
rememb , the heap is onli the stuff outsid of x .
so we could care less about the stuff outsid .
of the heat , for not maintain their kei .
so we do an extra check .
if the head of thi edg is in fact still in the heap , that is if it's not in x so i n the pictur , for exampl , thi would be true for all three of the vertic that ar on the other end of arc point out of w .
and for each of these vertic v , we updat it kei .
and the wai we're go to updat it kei is , we're just go to rip thi vertex out of the heap .
we're go to recomput it kei and constant time , and then we're go to reinsert it into the heap .
and sinc all heap oper take logarithm time , thi kei updat will be logarithm time .
as an addit optim , i wanna point out that if on of these vertic v's kei doe chang , it can onli chang in on wai .
so rememb , what is the kei ?
the kei is the smallest gradi dijkstra score of all of the edg that start next and stick into thi vertex .
so that's the local tournament or the first round tournament happen at thi vertex v .
now the onli thing which ha chang .
befor and after we ad thi vertex , w to x , is that now on new edg is stick into thi vertex , v .
all of the old edg stick into it from x ar still stick into it , and now there's on extra candid in it local tournament , name thi edg , wv .
so either wv is the local winner ; either it ha the smallest dyxtra greedi score of them all .
that termin thi vertex , or it doesn't , in which case the previou winner is still the new winner .
so if that is , the new kei valu can onli be on of two thing .
either it's the old kei valu that's the case where thi .
extra entranc , the edg from w to v is irrelev .
or , if it's chang , it ha to have chang to the score of thi edg , w v .
and the formula for that is the shortest path distanc .
that we just comput for w where w ha been process at thi point plu the link of the direct arch from w m v .
and again conceptu thi formula is just a greedi dijkstra score for the arc wv .
the new entranc in v's local first round tournament .
so now , have updat v's kei appropri , so that invari two is restor .
and onc again , the kei of everi vertex doe reflect the sma llest greedi , dijkstra greedi score of ani edg stick into it from the set x .
we can safe reinsert thi node back into the heap with it new kei valu .
and these three line togeth ar just a kei updat in logarithm time , for on of these vertic that's at the other end of an arc stick out of the vertex w .
so let's talli up the run time in thi new implement .
on thing i want you to check , and thi will definit help you understand thi refin implement of dijkstra's algorithm , is that essenti all the work done is through the heap api .
that is , all of the run time that we have to account for is in heap oper .
we don't realli do nontrivi work outsid of heap oper .
and again recal that the run time of ani heap oper is logarithm in the number of element in the heap .
our heap is store vertic .
it's never gonna have more than n thing in it .
so the run time of everi heap oper is big o of log n .
so what ar the heap oper that we do .
well , we extract men and we do it onc per iter of the wild loop .
so there's n minu on iter of the wild loop , just like befor , but now instead of do an exhaust search through the edg , we just do a simpl extract men from the heap and it give us on a silver platter the vortex we should add next .
so what do we do besid extract min ?
well , we have to do thi work pai the piper .
we have to maintain invari two .
and everi time we extract a min , that then trigger some subsequ kei updat .
and rememb , each of these kei updat is a delet of an element , from the heap follow by an insert .
so how mani delet and insert do we do ?
well , at first thi might seem a littl bit scari .
right ?
becaus we do a roughli linear number of extract min .
and a vertex might have as mani as n <num> outgo arc .
so it seem like a vertex could trigger as mani as n <num> kei updat , which is theta of n oper .
and if we sum that up over the n iter of the wild loop that w ould give us n squar heap oper .
so , and inde , in dens graph , that can be the case .
it is true that a singl vertex might trigger a linear in n number of oper .
but that's the wrong wai to think about it .
rather than have thi vertex centric perspect on what , who's respons for heap oper , let's have an edg centric view .
so for each edg at the graph , let's think about when can thi be respons for some heap oper , in particular a decreas in kei in the result insert and delet .
if you have an edg and it point from the vertex v to the vertex w .
there's actual onli on situat in which thi edg is go to be respons for a , a decreas in kei .
and that's in the case where the tail of the edg , v .
get suck into the set x befor the head w of thi edg get suck into the set x .
if that happen , if v get suck into x and w is still outsid of x , then inde we're gonna have to decreas the kei of w , just like we did in the exampl .
but that's all that's gonna happen v can onli get suck into x onc and never gonna leav it .
so it's onli respons for thi singl decreas in kei of it head w .
and that's on insert and on delet .
and in fact , if the endpoint of thi edg get suck into x in the opposit order , if the tail of , excus me , if the head of thi edg w get suck into x first .
that doesn't even trigger a , a kei decreas for v , and v will never have it de kei decreas , becaus of thi particular arc , from v to w .
so the upshot is that each edg vw of the graph trigger at most on insert delet combo .
so what doe thi mean , thi mean that the number of heap oper .
is big o of n , that's for the extract min .
plu big o of m .
that's for the insert the leak combo trigger by edg dure the decreas kei .
now just to , i'm gonna write thi in a , in a simplifi wai .
thi is just o of m , the number of edg .
and thi is becaus of our assumpt that's there's a path to s from everi other vertex .
if yo u think about it that mean that the graph is at least weakli connect if you pick it up it would stai togeth in on piec .
so that mean it at least contain a tree , at least an in an undirect sens , which mean it contain at least n minu on edg .
so we're in the case of weakli connect graph where n domin m .
m is alwai as big as n at least up to a plu on .
so what that mean is the run time of dijkstra's algorithm , with thi heap implement , is just a log factor larger .
rememb , everi heap oper take time logarithm .
so we do a linear in m number of oper ; each take time logarithm in n .
so the run time is m log n .
with , i should sai , quit good consist .
so thi is a realli , realli impress fast algorithm , for comput such a us problem as shortest path .
so we got a littl bit spoil in our discuss of graph search connect , where it seem ani problem we care about we could solv in linear time , over m plu n .
so here we're pick up thi extra logarithm factor , but i mean , come on , thi is still awesom .
a run time of m log n is unbeliev faster than a run time of m time n , which is what we had in the straightforward implement .
so thi deft us of the heap data structur ha given us a truli blazingli fast algorithm for an extrem well motiv problem , comput shortest path .
in thi video , i'm gonna tell you a littl bit about my approach toward teach data structur in thi cours .
so know when and how to us basic data structur is an essenti skill for the seriou programm .
data structur ar us in pretti much everi major piec of softwar .
so let me remind you of what's the point , the raison d'etr of the data structur ?
it job is to organ data in a wai that you can access it quickli and usefulli .
there's mani , mani exampl of data structur and hopefulli you've seen a few of them and perhap even us a few of them in your own program .
and thei rang from veri simpl exampl like list , stack and queue to more intric but still veri us on like heap , search tree , hash tabl .
rel thereof like balloon filter .
union find structur and so on .
so why do we have such a laundri list of data structur ?
why is there such a bewild assort ?
it's becaus differ data structur support differ set of oper and ar therefor well suit for differ type of task .
let me remind you of a concret exampl that we saw back when we were discuss graph search .
in particular , breadth first search and depth first search .
so we discuss how implement breadth first search the right data structur to us wa a queue .
thi is someth that support fast , mean constant time insert to the back , and constant time delet from the front .
depth first search by contrast is a differ algorithm with differ need .
and becaus of it recurs natur , a stack is much more appropri for depth first search .
that's becaus it support constant time delet from the front , but constant time insert to the front .
so the last in first out support of a stack is good for depth first search .
the first in first out oper of a queue work for breadth first search , now becaus differ data structur ar suitabl for differ type of task you should learn the pro and con of the basic on .
gener speak the fewer oper th at a data structur support the faster the oper will be .
and the smaller the space overhead requir by the data structur .
thu , as programm , it's import that you think carefulli about the need of your applic .
what ar the oper that you need a data structur to export ?
and then you should choos the right data structur , mean the on that support all of the oper you need .
but ideal no superflu on .
let me suggest four level of data structur knowledg that someon might have , so level zero is the level of ignor , for someon who ha never heard of a data structur , and is unawar of the fact that organ your data can produc fundament better softwar .
for exampl , fundament faster algorithm , level on i think of as be cocktail parti level awar .
now obvious , here i'm talk onli about the nerdiest of cocktail parti .
but nonetheless , thi would be someon who could at least hold a convers about basic data structur .
thei've heard of thing like heap and binari search tree , thei're perhap awar of some of the basic oper , but thi person would be shaki us them in their own program or sai in a technic interview context .
now , with level two , we're start to get somewher .
so here i would put someon who ha solid literaci about data structur .
thei're comfort us them as a client in their own program , and thei have a good sens of which data structur ar appropri for which type of task .
now level three , the final level , is the hardcor programm and comput scientist .
and these ar peopl who ar not content to just be a client of data structur , and us them in their own program , but thei actual have an understand of the gut of these data structur .
how thei ar code up , how thei're implement , not mere how thei ar us .
now my guess is that , realli a larg number of you will wind up us data structur in your own program , and therefor , learn about what ar the oper of differ data structur and what ar thei good for , will be a quit empow skill for you as a programm .
on the other hand , i'll bet that veri few of you will wind up have to implement your own data structur from scratch , as oppos to just us as a client the data structur that alreadi come with the variou standard program librari .
so with thi in mind i'm go to focu my teach on take up to level two .
my discuss gonna focu on the oper report by variou data structur and some of canon applic .
so through thi i hope i'll develop your intuit for what kind of data structur ar suitabl for what kind of task .
time permit , howev , i also want to includ some option materi for those of you want to take it to the next level and learn some about the gut of these data structur , the canon limit of how you code them up .
so in thi video , we'll start talk about the heap data structur .
so in thi video i want to be veri clear on what ar the oper support by heap , what run time guarante you can expect from limit and i want you to get a feel for what kind of problem thei're us for .
in a separ video , we'll take a peek under the hood and talk a littl bit about how heap actual get implement .
but for now , let's just focu on how to us them as a client .
so the number on thing you should rememb about a given data structur is what oper it support , and what is the run time you can expect from those oper .
so basic , a heap support two oper .
there's some bell and whistl you can throw on .
but the two thing you gotta now is insert and extract min .
and so the first thing i have to sai about a heap is that it's a contain for a bunch of object .
and each of these object should have a kei , like a number so that for ani given object you can compar their kei and sai on kei is bigger than the other kei .
so for exampl , mayb the object ar employe record and the kei is social secur number , mayb the object ar the edg of a network and the kei ar someth like the length or the weight of an edg , mayb each object indic an event and the kei is the time at which that event is meant to occur .
now the number on thing you should rememb about a given data structur is , first of all what ar the oper that it support ?
and second of all , what is the run time you can expect from those oper ?
for a heap , essenti there's two basic oper .
insert and extract the object that ha the minimum kei valu .
so in our discuss of heap , we're go to allow ti that ar pretti much equal to easi with or without ti .
so , when you extract men from a heap thei mai have duplic kei valu then there's no specif about which on you get .
you just get on of the object that ha a tie for the minimum kei valu .
now , of cours , there's no special reason that i chose to extract the minimum rather than the maximum .
you either you can have a second notion of a heap , which is a max heap , which alwai return the object of the maximum kei valu .
or if all you have at your dispos is on of these extract min type heap , you can just , negat the sign of all of the kei valu befor you insert them , and then extract min will actual extract , the max kei valu .
so , just to be clear , i'm not propos here a data structur that support simultan an extract min oper and an extract max oper .
if you want both of those oper , there'd be data structur that would give it to you ; probabl a binari search tree is the first thing you'd want to consid .
so , i'm just sai , you can have a heap of on off two flavor .
either the heap support extract min and not extract max or the heap will support extract max and not extract min .
so i mention that you should rememb not just the support oper of a data structur , but what is the run time of those oper .
now , for the heap , the wai it's canon implement , the run time you should expect is logarithm in the number of item in the heap .
and it log base two , with quit good constant .
so when you think about heap , you should absolut rememb these two oper .
option , there's a coupl other thing about heap that ar , might be worth rememb some addit oper that thei can support .
so the first is an oper call heapifi .
like a lot of the other stuff about heap , it ha a few other name as well .
but i'm go to call it heapifi , on standard name .
and the point of heapifi is to initi a heap in linear time .
now , if you have n thing and you want to put them all in a heap , obvious you could just invok insert onc per each object .
if you have n object , it seem like that would take n time log n time , log n for each of the n insert .
but there's a slick wai to do them in a batch , which take onli linear time .
so tha t's the heapifi oper , and anoth oper which can be implement , although there ar some subtleti .
is you can delet not just the minimum , but you can delet an ar , arbitrari element from the middl of a heap , again , in logarithm time .
i mention thi here primarili cuz we're gonna us thi oper when we us heap to speed up dijkstra's algorithm .
so that's the gist of a heap .
you maintain object that have kei you can insert in logarithm time , and you can find the on with the minimum kei in logarithm time .
so let's turn to applic , i'll give you sever .
but befor i dive into ani on applic let me just sai ; what's the gener principl ?
what should you to think that mayb you want to us a heap data structur in some task ?
so the most common reason to us a heap is if you notic that your program is do repeat minimum comput .
especi via exhaust search , most of the applic that we go through will have thi flavor .
it will be , there will be a naiv program which doe a bunch of repeat minimum us just brute forc search and we'll see that a veri simpl applic of a heap will allow us to speed it up tremend .
so let's start by return to the mother of all comput problem , sort and unsort arrai .
now , a sort algorithm which is sort of so obviou and suboptim that i didn't even realli bother to talk about it at ani other point in the cours is select sort .
what do you do ?
in select sort , you do a scan through the unsort arrai .
you find the minimum element ; you put that in the first posit .
you scan through the other n <num> element ; you find the minimum among them .
you put that in the second posit .
you scan through the remain n <num> unsort element .
you find the minimum ; you put that in the third posit , and so on .
so evid , thi sort algorithm doe a linear number of linear scan through thi arrai .
so thi is definit a quadrat time algorithm .
that's why i didn't bother to tell you about it earlier .
so thi certainli fit the bill as be a bunch of repeat minimum comput .
or for each comput , we're do exhaust search .
so thi , we should just , a light bulb should go off , and sai , aha !
can we do better us a heap data structur ?
and we can , and the sort algorithm that we get is call heap sort .
and given a heap data structur , thi sort algorithm is total trivial .
we just insert all of the element from the arrai into the heap .
then we extract the minimum on by on .
from the first extract , we get the minimum of all n element .
the second extract give us the minimum of the remain n <num> element , and so on .
so when we extract min on by on , we can just popul a sort arrai from left to right .
boom , we're done .
what is the run time of heap sort ?
well , we insert each element onc and we extract each element onc so that's 2n heap oper and what i promis you is that you can count on heap be implement so that everi oper take logarithm time .
so we have a linear number of logarithm time oper for run time of n log n .
so let's take a step back and appreci what just happen .
we took the least imagin sort algorithm possibl .
select sort , which is evid quadrat time .
we recogn the pattern of repeat minimum comput .
we swap in the heap data structur and boom we get an nlogn sort algorithm , which is just two trivial line .
and rememb , n log n is a pretti good run time for a sort algorithm .
thi is exactli the run time we had for merg sort ; thi wa exactli the averag run time we got for random quick sort .
moreov , heap sort is a comparison base sort algorithm .
we don't us ani data about the kei element we just us as a total set .
and , as some of you mai have seen in an option video , there doe not exist a comparison base sort algorithm with run time better than n log n .
so for the question , can we do better ?
the answer is no , if we us a comparison base sort algorithm like heap sort .
so that's pretti amaz , all we do is swap in a heap and a run time drop from realli quit unsatisfactori quadrat to the optim n log n .
moreov , heapsort is a pretti practic sort algorithm when you run thi it's gonna go realli fast .
is it as good as quick sort ?
hm , mayb not quit but it close it's get into the same so let's talk of anoth applic which frankli in some sens is almost trivial but thi is also a canon wai in which heap ar us .
and in thi applic it will be natur to call a heap by a synonym name , a prioriti queue .
so what i want you to think about for thi exampl is that you've been task with write softwar that perform a simul of the physic world .
so you might pretend , for exampl , that you're help write a video game which is for basketbal .
now why would a heap come up in a simul context ?
well , the object in thi applic ar go to be event record .
so an event might be for exampl that the ball will reach the hoop at a particular time and that would be becaus a player shot it a coupl of second ago .
when if for exampl the ball hit the rim , that could trigger anoth event to be schedul for the near futur which is that a coupl player ar go to vie for the rebound .
that event in turn could trigger the schedul of anoth event , which is on of these player ?
commit , an over the back foul on the other on and knock them to the ground .
that in turn could trigger anoth event which is the player that got knock on the ground get up and argu that a foul call , and so on .
so when you have event record like thi , there's a veri natur kei , which is just the timestamp , the time at which thi event in the futur is schedul to occur .
now clearli a problem which ha to get solv over and over and over again in thi kind of simul is you have to figur out what's the next event that's go to occur .
you have to know what other event to schedul ; you have to updat the screen and so on .
so that's a minimum comput .
so a veri silli thing you could do is just maintain an unord list of all of the event that have ever been schedul and do a linear path through them and comput the minimum .
but you're gonna be comput minimum over and over and over again , so again that light bulb should go on .
and you could sai mayb a heap is just what i need for thi problem .
and inde it is .
so , if you're store these event record in a heap .
with the kei be the time stamp then when you extract them in the hand for you on a silver platter us logarithm time exactli which algorithm is go to occur next .
so let's move on to a less obviou applic of heap , which is a problem i'm go to call median mainten .
the wai thi is gonna work is that you and i ar gonna plai a littl game .
so on my side , what i'm go to do is i'm go to pass you index card , on at a time , where there's a number written on each index card .
your respons is to tell me at each time step the median of the number that i've pass you so far .
so , after i've given you the first eleven number you should tell me as quickli as possibl the sixth smallest after i've given you thirteen number you should tell me the seventh smallest and so on .
moreov , we know how to comput the median in linear time but the last thing i want is for you to be do a linear time comput everi singl time step .
i onli give you on new number ?
do you realli have to do linear time just to re comput the median ?
if i just gave you on new number .
so to make sure that you don't run a linear time select algorithm everi time i give you on new number , i'm go to put a budget on the amount of time that you can us on each time step to tell me the median .
and it's go to be logarithm in the number of number i've pass you so far .
so i encourag you to paus the video at thi point and spend some time think about how you would solv thi problem .
all right , so hopefulli you've thoug ht about thi problem a littl bit .
so let me give you a hint .
what if you us two heap , do you see a good wai to solv thi problem then .
all right , so let me show you a solut to thi problem that make us of two heap .
the first heap we'll call h low .
thi equal support extract max .
rememb we discuss that a heap , you could pick whether it support extract min or extract max .
you don't get both , but you can get either on , it doesn't matter .
and then we'll have anoth heap h high which support extract min .
and the kei idea is to maintain the invari that the smallest half of the number that you've seen so far ar all in the low heap .
and the largest half of the number that you've seen so far ar all in the high heap .
so , for exampl , after you've seen the first ten element , the smallest five of them should resid in h low , and the biggest five of them should resid in h high .
after you've seen twenti element then the bottom ten , the smallest ten , should , should resid in h low , and the largest ten should resid in h high .
if you've seen an odd number , either on can be bigger , it doesn't matter .
so if you have <num> you have the smallest ten in the on and the biggest eleven in the other , or vice versa .
it's not , not import .
now given thi kei idea of split the element in half , accord to the two heap .
you need two realiz , which i'll leav for you to check .
so first of all , you have to prove you can actual maintain thi invari with onli o of log i work in step i .
second of all , you have to realiz thi invari allow you to solv the desir problem .
so let me just quickli talk through both of these point , and then you can think about it in more detail , on your own time .
so let's start with the first on .
how can we maintain thi invari , us onli log i work and time step i , and thi is a littl tricki .
so let's suppos we've alreadi process the first twenti number , and the smallest ten of them we've all work hard to , to put onli in h low .
and the biggest ten of th ''em we've work hard to put onli in h high .
now , here's a preliminari observ .
what's true , so what do we know about the maximum element in h low ?
well these ar the tenth smallest overal and the maximum then is the biggest of the tenth smallest .
so that would be a tenth order statist , so the tenth order overal .
now what about in the , the hi kei ?
what s it minimum valu ?
well those ar the biggest ten valu .
so the minimum of , of the ten biggest valu would be the eleventh order statist .
okai , so the maximum of h low is the tenth order statist .
the minimum of h high is the statist , thei're right next to each other ; these ar in fact the two median right now so when thi new element come in , the twenti first element come in , we need to know which heap to insert it into and well it just , if it's smaller than the tenth order statist then it's still gonna be in the bottom , then it's in the bottom half of the element and need to go in the low heap .
if it's bigger than the eleventh order statist , if it's bigger than the minimum valu of the high heap then that's where it belong , in the high heap .
if it's wedg in between the tenth and eleventh order of statist , it doesn't matter .
we can put it in either on .
thi is the new median anywai .
now , we're not done yet with thi first point , becaus there's a problem with potenti imbal .
so imagin that the twenti first element come up and it's less than the maximum of the low heap , so we stick it in the low heap and now that ha a popul of eleven .
and now imagin the twenti second number come up and that again is less than the maximum element in the low heap , so again we have to insert it in the low heap .
now we have twelv element in the low heap , but we onli have ten in the right heap .
so we don't have a <num> .
<num> , <num> split of the number but we could easili re balanc we just extract the max from the low heap and we insert it into the high heap .
and boom .
now thei both have eleven , and the low heap ha the smallest el even , and the high heap ha the biggest eleven .
so that's how you maintain , the invari that you have thi <num> <num> split in term of the small and the high , and between the two heap .
you check where it li with respect to the max of the low heap and the mid of the high heap .
you put it in the appropri place .
and whenev you need to do some re balanc , you do some re balanc .
now , thi us onli a constant number of heap oper when a new number show up .
so that's log i work .
so now given thi discuss , it's easi to see the second point given that thi invari is true at each time step .
how do we comput the median ?
well , it's go to be either the maximum of the low heap and or the minimum of the high heap depend on whether i is even or odd .
if it's even , both of those ar median .
if i is odd , then it's just whichev heap ha on more element than the other on .
so the final applic we'll talk about in detail in a differ video .
a video concern with the run time of dijkstra's shortest path algorithm .
but i do wanna mention it here as well just to reiter the point of how care us of data structur can speed up algorithm .
especi when you're do thing like minimum comput in an inner loop .
so dijkstra's shortest path algorithm , hopefulli , mani of you have watch that video at thi point .
but basic , what it doe is it ha a central wild loop .
and so it oper onc per vertex of the graph .
and at least naiv , it seem like what each iter of the wild loop doe is an exhaust search through the edg of the graph , comput a minimum .
so if we think about the work perform in thi naiv implement , it's exactli in the wheel hous of a heap , right .
so what we do in each of these loop iter is do an exhaust search comput a minimum .
you see repeat minimum comput , a light bulb should go off and you should think mayb a heap can help .
and a heap can help in dijkstra's algorithm .
the detail ar a bit subtl , and thei're discuss i n a separ video , but the upshot is , we get a tremend improv in the run time .
so we're call that m denot the number of edg .
and n denot the number of vertic of a graph .
with a care deploy of heap in dijkstra's algorithm , the run time drop from thi realli rather larg polynomi .
the product of the number of vertic and the number of edg .
down to someth which is almost linear time .
anywai , o of m log n .
where m is the number of edg and n is the number of vertic .
so the linear time here would be o of m .
the liner of the number of edg we're pick up an extra log factor but still thi is basic as good as sort .
so thi is a fantast fast shortest path algorithm .
certainli , wai , wai better that what you get if you don't us heap and do just repeat exhaust search for the minimum .
so that , that's wrap up our discuss of what i think you realli want to know about heap .
name , what ar the kei oper that it support ?
what is the run time you can expect from those oper ?
what ar the type of problem that the data structur will yield speed up for ?
and a suit of applic .
for those of you that want to take it to the next level and see a littl bit about the gut of the implement , there is a separ option video that talk a bit about that .
so , in thi video , we're gonna take it at least partwai to the next level for the heap data structur .
that is , we'll discuss some of the implement detail .
i . e . , how would you code a , a heat data structur from scratch ?
so rememb what the point of a heap is .
it's a contain , and it contain object , and each of these object , in addit to possibl lot of other data , should have some kind of kei .
and we should be abl to compar the kei of differ object ; you know , social secur number for differ employ , edg weight for differ edg in a network , or time stamp on differ event , and so on .
now rememb , for ani data structur the number on thing you should rememb is what ar the oper that it support , i . e . , what is the data structur good for .
and also , what is the run time you can count on of those oper .
so someth we promis in a previou video .
indic the implement of the miss video is these two primari oper that a heap export .
first of all , you can insert stuff into it , and it take logarithm in the number of object that the heap is store .
and second of all , you can extract an object .
that ha the minimum kei valu , and again we're go to allow duplic in our , in our heap , so if there's multipl object , then i'll have a minimum , a common minimum kei valu , the heap will return on of those .
it's unspecifi , which on .
as i mention earlier , you can also dress up heap with addit oper , like you can do batch insert , and you can do linear time rather than log in time .
you can delet from the middl of the heap .
i'm not gonna discuss those in the video ; i'm just go to focu on how you'd implement insert and extract if you wanna know how heap realli work , it's import to keep in mind simultan two differ view of a heap on , as a tree and on , as a , arrai .
so we're gonna start on thi slide with the tree view .
conceptu , thi will be us to explain how the heap op ration ar implement .
a conceptu will think of a heap not just as ani old tree , but as a tree that's root .
it'll be binari .
mean that , each node will have zero , on or two children node and third , it will be as complet as possibl .
so let me draw for your amus an as complet as possibl binari tree that ha nine node .
so if the tree had , had onli seven node it would have been obviou what , is complet as possibl mean .
it would have meant we just would have had three complet fill in level .
if it had , had fifteen node it would have had four complet fill in level .
if you're in between , these two number that ar power of two minu on , well we're go to call a complet to the tree as just in the bottom level you fill in the leav from left to right .
so here the two extra leav on the fourth level ar both , push as far to the left as possibl .
so , in our mind , thi is how we visual heap .
let me next defin the heat properti .
thi impos an order on how the differ object ar arrang in thi tree structur .
the heap properti dictat that at everi singl node of thi tree it doesn't matter if it's the root if it's a leaf if it's an intern node whatev .
at everi node x the kei of the object store in x should be no more than the kei of xs children .
now x mai have zero children if it's a leaf it mai have on child or it mai have two children whatev those case zero on or two children all those children kei should be at least that of kei at x .
for exampl , here is a heap with seven node .
notic that i am allow duplic .
there ar three differ object that have the kei valu four , in thi heap .
anoth thing to realiz is that while the heap properti impos us structur on how the object can be arrang it in no wai uniqu pin down their structur .
so thi exact same set of seven kei could be arrang differ and it would still be a heap .
the import thing is that , in ani heap , the root ha to have a minimum valu kei .
just like in the se two organ of these seven kei , the root is alwai a four , the minimum kei valu .
so that's a good sign , given that on of the main oper we're suppos to .
quickli implement , is to extract the minimum valu .
so at least we know where it's go to be , it' gonna be at the root of a heap .
so while in or mind we think of heap as organ in a tree fashion , we don't liter implement them as tree .
so in someth like search tree , you actual have pointer at each node and you can travers pointer to go from a to the children from the children to the parent , yada , yada , yada .
turn out it's much more effici in a heap to just directli implement it as an arrai .
let me show you by exampl how a tree like we had on the last slide map natur onto an arrai represent .
so let's look at a slightli bigger heap , on that ha nine element .
so let me draw an arrai with nine posit .
label on , two , three all the wai up to nine and the wai we're go to map thi tree which is in our mind to thi arrai implement is realli veri natur .
we're just go to group the node of thi tree by their level .
so , the root is gonna be the onli node at level zero .
then the children of the root ar level on , their children constitut level two , and then we have a partial level three , which is just these last two note here .
and now we just stick these note into the arrai , on level at a time .
so the root wind up in the privileg first posit , so that's go to be the , the first , the object which is the first copi of the four .
then we put in the level on object , so that's the second copi of the four and the eight , and then we put in level two which ha our third four along with the two nine .
and then we have the last two note from level three round out the penultim and final posit of the arrai .
and you might be wonder how it is we seem to be have our cake and eat it , too .
on the on hand we have thi nice , logic tree structur .
on the other hand we have thi arrai implement and we're not wast ani space on the usual pointer you would have in a tree to travers between parent and children .
so where's the free lunch come from ?
well the reason is that becaus we're abl to keep thi binari tree as balanc as possibl , we don't actual need pointer to figur out who is whose parent and who is whose child .
we can just read that off directli from the posit in the arrai .
so , let me be a littl bit more specif .
if you have a node in the fifth posit , i'm assum i here is not on , right ?
so the , the root doesn't have ani , doe not have a parent but ani other , ani other object in posit i doe have a parent and what the posit that is , depend on , in a simpl wai on whether i is even or odd .
so if i is even , then the parent is just the posit of i <num> , and if i is odd , then it's go to be i <num> .
okai , that's a fraction .
so we take the floor that is we round down to the nearest integ .
if i is odd , so for exampl , the object in posit two and three have as their parent the object in posit on , and those in four and five have the on in posit two as hi parent .
six and seven have as their parent the node in , the object in posit three and so on and of cours we can invert thi function we can equal easili determin who the children ar of a given node so if we have an object in posit i .
then you notic that the children of i ar gonna be at the posit 2i and 2i <num> .
of cours those mai be empti so if you have a leaf of cours that doesn't have ani children and then mayb on node that ha onli on child .
but in the common case of an intern node it's gonna be two children that ar gonna be in posit 2i and 2i <num> .
so rather than travers pointer it's veri easi to just go from a node to it parent to either on of it children just by do these appropri trivial calcul with respect to it posit .
so thi slide illustr , some of the .
lower level reason that heap ar quit a popular data struct ur so the first on , just in term of storag .
we don't need ani overhead at all .
we ar just .
we have these object ; we're store them directli in an arrai , with no extra space .
second of all , not onli do we not have to have a space for pointer but we don't even have to do ani travers .
all we do ar these realli simpl .
divid by two or multipli two oper and us bit shift trick .
those can also be implement extrem quickli .
so , the next two slide let me indic , at a high level , how you would implement the two export oper , name insert and extract in time algorithm in the size of the heap and rather than give you ani pseudo code , i'm just go to show you how these work by exampl .
i think it will be obviou how thei extend the gener case .
i think just base on thi discuss , you'll be abl to code up your own version , of insert and extract , if you so desir .
so let's redraw the <num> node heap that we had on the previou slide and again , i'm gonna keep draw it as a tree and i'm gonna keep talk about it as a tree but alwai keep in mind the wai it's realli implement is in term of these arrai and when i talk about the parent of a node , again , what that mean is you go to the appropri posit given the posit of the node from where you start .
so let's suppos we have an exist heap , like thi blue heap here and we're call upon to insert a new object .
let's sai with a kei valu k .
now rememb heap ar alwai suppos to be perfectli balanc binari tree .
so if we want to maintain the properti that thi tree is perfectli balanc is pretti onli on place we can try to put the new kei k and that's as the next leaf .
that is it's go to be the new right most leaf on the bottom level .
or in term of the arrai implement we just stick it in the first non empti slot in the arrai and if we keep track of the arrai size we're get constant time of cours know where to put the new kei .
now whether or not we can get awai with thi depend on what the actual kei valu k is but , you know , for starter we can sai , what if we insert a kei that's seven ?
well , then we get to sai , whew , we're done .
so , the reason we're done is cuz we have not violat the heap properti .
it is still the case that everi node ha kei no bigger than that of it children .
in particular , thi third copi of a four , it pick up a new child , but it kei seven wa bigger than it kei four .
so , you can imagin that mayb we get lucki with anoth insert .
mayb the next insert is a ten and again , we put that in the next avail spot in the last level and that becom the second child of the third copi of the four and again we have no violat of the heap properti .
no worri still got a heap and in these lucki event insert is even take constant time .
realli all we're do is put element at the end of an arrai and not do ani rearrang .
so where it get interest is when we do an insert that violat the heap properti .
so let's suppos we do yet anoth insert , and the left child of thi twelv , it becom a five .
now we got a problem .
so now we have a as perfectli as possibl balanc binari tree , but the kei properti is not satisfi .
in particular , it's violat at the node twelv .
it ha on child .
the kei of that child is less than it own kei .
that's no good .
so is there some wai we can restor the heap properti ?
well , a natur idea is just to swap the posit of the five and the twelv , and that's someth that of cours can be done in constant time , cuz again from a node , we can go to the parent or the child in constant time just with a suitabl trivial comput .
so we sai okai , for starter we put the five at the end , but that's no good .
so we're gonna swap the five with the twelv and now we see we're not out of the wood .
no longer is there a heap violat at the node twelv .
that's been fix .
we've made it a leaf but we've push up the heap violat .
so now instead it's the eight that ha a problem .
the eight us to h av two children , with kei twelv and nine that wa fine .
now the eight ha two children with kei five and nine .
the five is less than the eight , that's a violat of the heap properti but again , that's the onli violat of the heap properti .
there's no other node you could have screw up , becaus eight wa the onli person whose children we mess around with .
all right so now we just it again .
let's try again , local fix the heap violat by swap the five with the 8s and now we see we've restor order .
the onli place where there could possibl be a violat of the heap properti is at the root .
the root , when we did thi swap , the onli person whose children we realli mess with wa the root four , and fortun it new child ha kei five , which is bigger than it .
on subtl point that you might be think that in addit to screw up at the root node , that mess around with hi children , mayb we could have screw up the twill by mess around with it parent .
all right , it parent us to be five , and now it parent is eight .
so is there some possibl that hi parent would all of a sudden have a kei bigger than it but if you think about it , thi eight and thi twelv , thei were a parent child relationship in the origin heap right ?
so back in the blue heap , the twelv wa under the 8s .
now the twelv is under the eight yet again .
sinc we have the heap properti for that pair befor , we certainli have it now .
so in gener , as you push up thi five up the tree , there's onli go to be on possibl edg that could be out of order and that's between where the five current resid and whatev it parent is .
so when the <num>'s parent wa twelv that wa a violat when <num>'s parent wa eight that wa a violat but now that we've push it up two level and <num>'s parent is four , that's not a violat becaus four is less than five .
so in gener , step two of insert is , you do thi swap , which it's call a lot of differ thing .
i'm gonna call it bubbl up becaus that's how i learn it more year ago than i care to admit but also thi is call , sometim sift up , happili up , and so on .
so now told you to just how to implement insert by repeat bubbl up in a heap data structur and thi is realli how it work , there is noth i haven't told you but you know , i'm not go to realli fill in all the detail but i'll encourag you to do that on your own time , if it's someth that interest you and the two main thing that you should check is first of all , is bubbl up process is gonna stop and when it stop , it stop with the heap properti restor .
the second thing need to be check in thi , i think is easier to see is that we do have the desir on time log make in the number of element in the heap .
the kei observ area that becaus thi is a perfectli balanc binari tree .
we know exactli how mani level there ar .
so thi is basic log base two event level where n is the number of item in the heap and what is the run time of thi insert procedur while you onli do a constant amount of work at each level , just do the swap and comparison and then in the worst case , you'll have to swap at everi singl level and there is a lot number of level .
so that's insert .
let's now talk about how do we implement the extract oper and again i'm gonna do thi by exampl and it's gonna be by repeat the of a bubbl procedur .
so the extract main oper is respons for remov from the heap an object with minimum kei valu and hand it back to the client on a silver platter .
so it pretti much have to whip out the root .
rememb the minimum is guarante to be at the root .
so that's how we have to begin to extract the subroutin is just we pluck out the root and hand it back to the client .
so thi remov of cours leav a gape hole in our tree structur and that's no good .
on of the respons for maintain is that we alwai have an as perfectli balanc as possib le binari tree and if you ar miss a root you certainli don't have an almost perfect .
binari tree so , what ar we go to do about it ?
how do we fill thi hole ?
well , there's pretti much onli on node that could fill thi hole without caus other problem with the tree structur , and that is the veri last node .
so the rightmost leaf at the bottom level on that simpl fix is to swap that up and have that take the place of the origin root .
so in thi case , the thirteen is go to get a massiv promot and get teleport all the wai to be the new root of thi tree .
so now we've resolv our structur challeng .
we now again have a , as perfectli balanc as possibl , binari tree but of cours now we've total screw up the heap properti , right .
so the heap properti sai that at everi node , includ the root , the kei valu at that node ha to be less than both of the children , and now it's all mess up .
right , so at the root , the kei valu's actual bigger than both of the children .
and matter that ar littl bit more tricki than thei were with insert , right , when we insert at the bottom becaus everi note ha a uniqu parent .
if you wanna push a note upward in the tree , there's sort of onli on place you can go , right , all you can do is swap with your parent , unless you're go to try to do someth realli crazi but if you want to do someth local , pretti much you onli have a uniqu parent you got to swap with .
now when you're try to push note down to the right posit in the tree , there is two differ swap you could do , on for the left child , on for the right child and the decis that we make matter to see that concret , let's think about thi exampl .
there's thi thirteen at the root , which is total not where it should be , and there's the two children .
the four and the eight , and we could try swap it with either on .
so suppos we swap it in a misguid wai with the right child , with the eight .
so now the eight becom the root , and the thirteen get push dow n a level .
so on the on hand ; we made some progress becaus now at least we don't have a violat between the thirteen and the eight .
on the other hand , we still have violat involv the thirteen .
the thirteen is still violat with respect to the twelv and nine and moreov , we've creat a new problem between the eight and the four , right ?
so now that the eight is the root , that's still bigger than it left child , thi four .
so it's not even clear we made ani progress at all when we swap the thirteen with the eight , okai ?
so that wa a bad idea and if you think about it would made it a bad idea , the stupid thing wa to swap it with the larger child .
that doesn't make ani sens .
we realli want to swap it with the smaller child .
rememb , everi node should have a kei bigger than both of it children .
so if we're go to swap up either the four or the eight , on of those is go to becom the parent of the other .
the parent is suppos to be smaller , so evid we should take the smaller of the two children and swap the thirteen with that .
so we should swap the thirteen with the figur .
not with e and now we observ a phenomenon veri much analog to what we saw in insert .
when we were bubbl up dure insert , it wasn't necessarili that we fix violat of the heat properti right awai but we would fix on and then introduc anoth on that wa higher up in the tree and we had confid that eventu we could just , push thi violat up to the root of the tree and squash it , just like we're try to win a game of whack a mole .
here , it's the opposit .
it's just in the opposit direct .
so we swap the thirteen with the four .
it's true we've creat on new violat of the heap properti .
that's again involv the thirteen with it children nine and four .
but we haven't creat ani new on .
we've push the heap violat further down the tree and hopefulli again , like in whack a mole .
we'll squash it at the bottom .
so after swap the thirteen and the four , now we just gotta do t he same thing .
we sai , okai , we're not done .
we still don't have a heap .
thi thirteen is bigger than both of it children but now , with our accumul wisdom , we know we should definit swap the thirteen with the four .
we're not gonna try swap with the nine , that's for sure .
so we move the four up here and we , the thirteen take the <num>'s old place .
boom !
now we're done .
so now we have no violat remain .
the thirteen in it new posit ha no children so there's no wai it can have ani violat , and the four becaus it wa the smaller child that's gonna be bigger than the <num>'s so we haven't introduc a huge violat there , and again we have these consecut <num>'s but we know that's not gonna be a problem becaus those were consecut <num>'s in the origin heap as well .
so you won't be surpris to hear that thi procedur by which you push someth down , by swap it with it smaller children , is call bubbl down , and extract men is noth more than take more than , take thi last leaf , promot it to the top of the tree , and bubbl down until the heap violat ha been fix .
so again on a conceptu level that's all of the ingredi necessari for a complet from scratch implement of extract the minimum from a heap and as befor , i'll leav it for you to check the detail .
so first of all you should check that in fact thi bubbl down ha to at some point halt and when it halt you do have a bona fide heap .
the heap properti is definit restor and second of all the run time is , is logarithm .
here the run time analysi is exactli the same as befor so we alreadi have observ that the height of a heap becaus it's perfectli balanc is essenti the log base two of the number of element in the heap and in bubbl down all you do is a constant amount of work per level .
all you have to do is a coupl comparison and swap .
so , that's a peek at what's under the hood in the heap data structur .
a littl bit about the gut of it element .
so after have seen thi , i hope you feel like a littl bit more hard core of a programm , a littl bit more hard core of a comput scientist .
in thi sequenc of video , we'll discuss our last but not least data structur name the balanc binari search tree .
like our discuss of other data structur we'll begin with the what .
that is we'll take the client's perspect and we'll ask what oper ar support by thi data structur , what can you actual us it for ?
then we'll move on to the how and the why .
we'll peer under the hood of the data structur and look at how it's actual implement and then understand the implement to understand why the oper have the run time that thei do .
so what is a balanc binari search tree good for ?
well , i recommend think about it as a dynam version of a sort arrai .
that is , if you have data store in a balanc binari search tree , you can do pretti much anyth on the data that you could if it wa just the static sort arrai .
but in addit , the data structur can accommod insert and delet .
you can accommod a dynam set of data that you're store overtim .
so to motiv the oper that a balanc binari search tree support , let's just start with the sort arrai and look at some of the thing you can easili do with data that happen to be store in such a wai .
so let's think about an arrai that ha numer data although , gener as we've said , in data structur is usual associ other data that's what you actual care about and the number ar just some uniqu identifi for each of the record .
so these might be an employe id number , social secur number , packet id number and network contact , etcetera .
so what ar some thing that ar easi to do given that your data is store as a sort arrai , most a bunch of thing ?
first of all , you can search and recal that search in a sort arrai is gener done us binari search so thi is how we us to look up phone number when we have physic phone book .
you'd start in the middl of the phone book , if the name you were look for wa less than the midpoint , you recurs on the left hand side , otherwis you'd recurs on the right hand side .
as we discuss back in the master method lectur long ago , thi is go to run in logarithm time .
roughli speak , everi time you recurs , you've thrown out half of the arrai so you're guarante to termin within a logarithm number of iter so binari search is logarithm search time .
someth els we discuss in previou lectur is the select problem .
so previous , we discuss thi in much harder context of unsort arrai .
rememb , the select problem in addit to arrai you're given in order statist .
so , if your order statist that your target is seventeen , that mean you're look for the seventeenth smallest number that's store in the arrai .
so in previou lectur , we work veri hard to get a linear time algorithm for thi problem in unsort arrai .
now , in a sort arrai , you want to know the seventeenth smallest element in the arrai .
pretti easi problem , just return whatev element happen to be in the seventeenth posit of the arrai sinc the arrai is sort , that's where it is so no problem .
it's alreadi sort constant time , you can solv the select problem .
of cours , two special case of the select problem ar find the minimum element of the arrai .
that's just if the order statist problem with i 1and the maximum element , that's just i n .
so thi just correspond to return the element that's in the first posit and the last posit of the arrai respect .
well let's do some more brainstorm .
what other oper could we implement on a sort arrai ?
well here's a coupl more .
so there ar oper call the predecessor and successor oper .
and so the wai these work is , you start with on element .
so , sai you start with a pointer to the <num> , and you want to know where in thi arrai is the next smallest element .
that's the predecessor queri and the successor oper return the next largest element in the arrai .
so the predecessor of the <num> is the seventeen , the successor of the <num> would be the <num> .
and again in a sort arrai , these ar trivial , right ?
you just know that predecessor just on posit back in the arrai , the successor is on posit forward .
so given a pointer to the <num> , you can return to <num> or the <num> in constant time .
what els ?
well , how about the rank oper ?
so we haven't discuss thi oper in the past .
so what rank is , thi ha for how mani kei store in the data structur ar less than or equal to a given kei .
so for exampl , the rank of <num> would be equal to <num> .
becaus <num> of the <num> element in the arrai ar less than or equal to <num> .
and if you think about it , implement the rank oper is realli no harder than implement search .
all you do is search for the given kei and wherev it is search termin in the arrai .
you just look at the posit in the arrai and boom , that's the rank of that element .
so for exampl , if you do a binari search for <num> and then when you termin , you discov it is , thei're in posit number six then you know the rank is six .
if you do an unsuccess search , sai you search for <num> , well then you get stuck in between the <num> and the <num> , and at that point you can conclud that the rank of <num> in thi arrai is five .
let me just wrap up the list with the final oper which is trivial to implement in the sort arrai .
name , you can output or print sai the store kei in sort order let's sai from smallest to largest .
and natur , all you do here is a singl scan from left to right through the arrai , output whatev element you see next .
the time requir is constant per element or linear overal .
so that's a quit impress list of support oper .
could you realli be so greedi as to want still more from our data structur ?
well yeah , certainli .
we definit want more than just what we have on the slide .
the reason be , these ar oper that oper on a static data set which is not chang overtim .
but the world in gener is dynam .
for exampl , if you ar run a compani and keep track of the employe , sometim you get new employe , sometim employe leav .
that is on of the data structur that not onli support these kind of oper but also , insert and delet .
now of cours it's not that it's imposs to implement insert or delet in a sort arrai , it's just that thei're go to run wai too slow .
in gener , you have to copi over a linear amount of stuff on an insert or delet if you want to maintain the sort arrai properti .
so thi linear time perform when insert and delet is unaccept unless you bare ever do those oper .
so , the raison d'etr of the balanc binari search tree is to implement thi exact same set of oper just as rich as that's support by a sort arrai but in addit , insert and delet .
now , a few of these oper won't be quit as fast or we have to give up a littl bit instead of constant time , the on in logarithm time and we still got logarithm time for all of these oper , linear time for output the element in sort of order plu , we'll be abl to insert and delet in logarithm time so let me just spell that out in a littl more detail .
so , a balanc binari search tree will act like a sort arrai plu , it will have fast , mean logarithm time insert and delet .
so let's go ahead and spell out all of those oper .
so search is go to run in o log n time , just like befor .
select run in constant time in a sort arrai and here it's go to take logarithm , so we'll give up a littl bit on the select problem but we'll still be abl to do it quit quickli .
even on the special case of find the minimum or find the maximum in our , in our data structur , we're go to need logarithm time in gener .
same thing for find predecessor and successor thei're not , thei're no longer constant time , thei go with logarithm .
rank took as logarithm time and the , even the sort arrai version and that will remain logarithm here .
as we'll see , we lose essenti noth over the sort arrai , if we want to output the kei valu in sort order sai from smallest to largest .
and crucial , we have two more fast oper compar to the sort arrai of data structur .
we can insert stuff so if you hire a new employe , you can insert them into your data structur .
if an employe decid to leav , you can remov them from the data structur .
you do not have to spend linear time like you did for sort of arrai , you onli have to spend the logarithm time wherea alwai n is the number of kei be store in the data structur .
so the kei takeawai here is that , if you have data and it ha kei which come from a total order set like , sai numer kei , then a balanc binari search tree support a veri rich collect of oper .
so if you anticip do a lot of differ process us the order inform of all of these kei , then you realli might want to consid a balanc binari search tree to maintain them .
well then , keep in mind though is that we have seen a coupl of other data structur which don't do quit as much as balanc binari search tree but what thei do , thei do better .
we alreadi , we just discuss in the last slide of the sort arrai .
so , if you have a static data set , you don't need insert and delet .
well then by all mean , don't bother with balanc binari search tree that us a sort arrai becaus it will do everyth super fast .
but , we also sought through dynam data structur which don't do as much but do it , but what thei do , thei do veri well .
so , we saw a heap , so what the heap is good for is it's just as dynam as a search tree .
it allow insert and delet both in logarithm time .
and in addit , it keep track of the minimum element or the maximum element .
rememb in a heap , you can choos whether you want to keep track of the minimum or keep track of the maximum but unlik in a search tree , a heap doe not simultan keep track of the minimum and the maximum .
so if you just need those three oper , insert , delet and rememb the smallest , and thi would be the case for exampl in a prioriti queue or schedul applic as discuss in the heap video .
then , a binari search tree is over kill .
you might want to consid a heap instead .
in fact , the benefit of a heap don't show up in the big o notat here both have logarithm oper time but the constant factor both in space and time ar go to be faster with a heap then with a balanc binari search tree .
the other dynam data structur that we discuss is a hash tabl .
and what hash tabl ar realli , realli good at is handl insert and search , that is look up .
some , sometim , depend on the implement also handl delet realli well also .
so , if you don't actual need to rememb thing like minima , maxima or rememb order inform on the kei , you just have to rememb what's there and what's not .
then the data structur of choic is definit the hash tabl , not the balanc binari search tree .
again , the balanc binari search tree would be fine and we'd give you logarithm look up time but it's kind of over kill for the problem .
all you need is fast look up .
a hash tabl recal will give you constant time look up .
so that will be a notic win over the balanc binari search tree .
but if you want a veri rich set of oper for process your data .
then , the balanc binari search tree could be the optim data structur for your need .
so , in thi video , we'll go over the basic behind implement binari search tree .
we ar not go to focu on the balanc aspect in thi video that will be discuss in later video and we ar go to talk about thing which ar true for binari search tree on gener , balanc or otherwis .
but let's just recal , you know , why ar we do thi , you know , what is the raison d' tre of thi data structur , the balanc version of the binari search tree and basic , it a dynam version of a sort arrai .
so , that's pretti much everyth you can do on a sort arrai , mayb in slightli more expens time .
thei ar still realli fast but in addit to thi dynam , it accommod insert and delet .
so , rememb , if you want to keep a sort arrai data structur , everi time you insert , everi time you delet , you're probabl go to wind up pai a linear factor which is wai too expens in most applic .
by contrast with the search tree , a balanc version , you can insert and delet a logarithm time in the number of kei in the tree .
and moreov , you can do stuff like search in logarithm time , no more expens than binari search on a sort arrai and also you can sort of sai the select problem in the special case , the minimum or maximum .
okai , it's not constant time like in a sort arrai but still logarithm pretti good and in addit , you can print out all of the kei from smallest to largest and in linear time , constant time per element just like you could with the linear scan through a sort arrai .
so , that's what thei're good for .
everyth a sort arrai can do more or less plu insert and delet everyth in logarithm time .
so , how ar search tree organ ?
and again , what i'm go to sai in the rest of thi video is true both for balanc and unbalanc search tree .
we're go to worri about the balanc aspect in the later video .
all right , so , let me tell you the kei ingredi in a binari search tree .
let me also just draw a simpl cartoon exampl in the upper right part of the slide .
so , thi on to on correspond between node of the tree and kei that ar be store .
and as usual in our data structur discuss we're go to act as if the onli thing that we care about , the onli thing that exist at each node is thi kei when gener , thi associ data that you realli care about .
so , each node in the tree will gener contain both the kei plu a pointer to some data structur that ha more inform .
mayb the kei is the employe id number , and then there's a pointer to lot of other inform about that employe .
now , in addit to the node , you have to have link amongst the node and there's a lot of differ wai to do the exact implement of the pointer that connect the node of the tree togeth but the video i'm just go to keep is straightforward as possibl and we're just go to assum that in each node , there's three pointer .
on to a left child , anoth on to the right child and then the third pointer which point to the parent .
now , of cours , some of these pointer can be null and in fact in the five node binari search tree i've drawn on the right for each of the five node , at least on of these three pointer is null .
so , for exampl , for the node with kei on it ha a null left child pointer , there wa no left child .
it's the right child pointer go to point to the node with kei two and the parent pointer wa go to a node that ha kei three .
similarli three is go to have a null parent pointer and the root node in thi case , three is a uniqu node but ha a null parent pointer .
here the node with kei valu three , of cours , ha a left child pointer point to on and ha a right child pointer that point to five .
now , here is the most fundament properti of search tree .
let's just go ahead and call it the search tree properti .
so , the search tree properti assert the follow condit at everi singl node of the search tree .
if the node ha some kei valu then all of the kei store in the left subtre should be less than that kei .
and similarli , all of the kei store in the right subtre should be bigger than that kei .
so , if we have some node who's store kei valu is x and thi is somewher , you know , sai deep in the middl of the tree so upward we think of as be toward the root .
and then if we think about all the node that ar reachabl , after follow the left child pointer from x , that's the left subtre .
and similarli , the right subtre be everyth reachabl via the right child pointer from x , it should be the case that all kei in the left subtre ar less than x and all kei in the right subtre ar bigger than x .
and again , i want to emphas thi properti hold not just to the root but at everi singl node in the tree .
i've defin the search to a properti assum that all of the kei ar distinct , that's why i wrote strictli less than in the left sub tree and strictli bigger than in the right subtre .
but search tree can easili accommod duplic kei as well .
we just have to have some convent about how you handl ti .
so , for exampl , you could sai that everyth in the left subtre is less than or equal to the kei at that node and then everyth in the right subtre should be strictli bigger than that node .
that work fine as well .
so , if thi is the first time you've ever heard of the search tree properti , mayb at first blush it seem a littl arbitrari .
it seem like i pull it out of thin air but actual , you would have revers engin thi properti if you sat down and thought about what properti would make search realli easi in a data structur .
the point is , the search tree properti tell you exactli where to look for some given kei .
so , look ahead a littl bit , steal my fire from a slide to come , suppos you were look for sai , a kei <num> , and you start the root and the root is seventeen .
the point of the search tree properti is you know where <num> ha to be .
if the root is seventeen , you're look to <num> , if it's in the tree , no wai is it in the left subtre , it's got to be in the right subtre .
so , you can just follow the right child pointer and forget about the left subtre for the rest of the search .
thi is veri much in the spirit of binari search where you start in the middl of the arrai and again , you compar what you're look for to what's in the middl and either wai , you can recurs on on of the two side forget forevermor about the other half of the arrai and that's exactli the point of the search tree properti .
we're go to have to search from root on down , the search tree properti guarante we have a uniqu direct to go next and we never have to worri about ani of the stuff that we don't see .
we can also draw a veri loos analog with our discuss of heap and mai recal heap were also logic , we thought of them as a tree even though thei ar implement as an arrai .
and heap have some heap properti and if you go back to review the heap properti , you'll find that thi is not the same thing as the search three properti .
these ar two differ properti and that's go to try to make differ thing easi .
back when we talk about heap , the properti wa that thi is for the extract min version .
parent alwai have to be smaller than their children .
that's differ than the search tree properti which sai stuff to the left , that's smaller than you , stuff to the right is bigger than you .
and heap , we have the heap properti so that identifi the minimum valu wa trivial .
it wa guarante to be at the root .
heap ar design so that you can find the minimum easili .
search tree ar , ar defin so that you can search easili that's why , you have thi differ search tree properti .
if you want to get smaller , you go left .
if you want to get bigger you go right .
on point that's import to understand earli , and thi will be particularli relev onc we did , onc we try to enforc balanc in our subsequ video is that , for a given set of kei , you can have a lot of differ search tree .
on the previou slide , i drew on search tree contain the kei valu on , two , three , four , five .
let me redraw that exact same search tree here .
if you stare to thi tree a littl while you'll agre that in fact that everi singl node of thi tree , all of the thing in the left subtre ar smaller , all of the thing in the right subtre ar bigger .
howev , let me show you anoth valid binari search tree with the exact same set of kei .
so , in the second search three , the root is five , the maximum valu .
and everybodi ha no right children , onli the left children ar popul and that goe five , four , three , two , on in descend order .
if you check here again , it ha the properti that at everi node , everyth in the left subtre is smaller .
everyth in the right subtre , in thi case , empti , is bigger .
so , extrapol from these two cartoon exampl , we surmis that for a given set of n kei , search tree that contain these kei could vari in height anywher from the best case scenario of a perfectli balanc binari tree which just go to have logarithm height to the worst case of on of these link list like chain which is go to be linear in the number of kei n .
and so just to remind you the height of a search tree which is also sometim call the depth is just the longest number of hop it ever take to get to from a root to a leaf .
so , in the first search tree , here the height is two and then the second search tree , the height is four .
if the search tree is perfectli arrang with the number of node essenti doubl at everi level , then the depth is you're go to run out of node around the depth of log2n .
and in gener , if you have a chain of n kei that that's go to be n <num> but we'll just call it n amongst friend .
so , now that we understand the basic structur of binari search tree , we can actual talk about how to implement all of the oper that thei support .
so , as we go through most of the support oper on at a time , i'm just go to give you a realli high level descript .
it should be enough for you to code up on implement if you want or as usual , if you want more detail or actual work code , you can check on the web or in on of the number of good program or algorithm textbook .
so , let's start with realli the primari oper which is search .
search , we've realli alreadi discuss how it's done when we discuss the search tree properti .
again , the search tree properti make it obviou how to look for someth in a search tree .
pretti much you just follow your nose you have no other choic .
so , you start the root , it's the obviou place to start .
if you're lucki , the root is what you ar look for and then you stop and then you return to root .
more like , the root is either bigger than or less than the kei that you're look for .
now , if the kei is smaller , the kei you ar look for is smaller than the kei of the root , where you're go to look ?
well , the search tree properti sai , if it's in the tree , it's got to be in the left subtre so you follow the left sub child pointer .
if the kei you're look for is bigger than the kei at the root , where is it got to be ?
got to be in the right subtre .
so , you're just go to recurs on the right subtre .
so , in thi exampl , if you're search for , sai the kei two , obvious you're go to go left from the root .
if you're search for the kei four , obvious you're go to go right from the root .
so , how can the search termin ?
well , it can termin in on of two wai .
first of all , you might find what you're look for so in thi exampl , if you search for four , you're go to travers to right child pointer then a left child pointer and then boom , you're at the four and you return successfulli .
the other wai the search can termin is with a null pointer .
so , in thi exampl , suppos you were look for a node with kei six , what would happen ?
well , you start at the root , three is too small so you go to the right .
you get to five , five is still too small cuz you're look for six so you try to go right but the right child pointer is null .
and that mean six is not in the tree .
if there wa anywher in the tree , it had to be to the right of the three , it had to be to the right of the five but you tri it and you ran on the pointer so the six isn't there .
and you return correctli with an unsuccess search .
next , let's discuss the insert oper which realli is just a simpl piggi back on the search that we just describ .
so , for simplic the first think about the case where there ar no duplic kei .
the first thing to do on thi insert is search for the kei k .
now , becaus there ar no duplic , thi search will not succe .
thi kei k is not yet in the tree .
so , for exampl , in the pictur on the right , we might think about try to insert the kei six .
what's go to happen when we search for six , we follow a right child pointer .
we go from three to five and then we try to spot anoth on and make it stuck .
there's a null pointer go to the right of five .
then when thi unsuccess search termin at a null pointer , we just rewir that pointer to point to a node with thi new kei k .
so , if you want to permit duplic from the data structur , you got to tweak the code and insert a littl bit but realli bare at all .
you just need some convent for handl the case when you do in counter the kei that we ar about to insert .
so , for exampl , if the current note ha the kei equal to the on you're insert , you could have the convent that you alwai continu on the left subtre and then you continu the search as usual again , eventu termin at a null pointer and you stick the new insert node you rewir to null pointer to point to it .
on good exercis for you to think through which i'm not go to sai more about here is that when you insert a new node , you retain the search tree properti .
that is if you start with the search tree , you start within tree where at everi node stuff to the left is smaller , the stuff to the right is bigger .
you insert someth and you follow thi procedur .
you will still have the search tree properti after thi new node ha been insert .
that's someth for you to think through .
so what i want to do next is test your understand about the search and insert procedur by ask you about their run time .
so of the follow four paramet of a search tree that contain n differ kei , which on govern the worst case time of a search or insert .
so the correct answer is the third on .
so , the height of a search tree govern the worst case time of the search or of an insert .
notic that mean mere know the number of kei n is not enough to deduc what the worst case search time is .
you also have to know someth about the structur of the tree .
so , to see that , just let's think about the two exampl that we've been run so far .
on of which is nice and balanc .
and the other of which , which contain exactli the same five kei is super unbalanc , it's thi crazi link list in effect .
so , in ani search tree , the worst case time to do is search or insert is proport to the largest number of pointer left to right child pointer that you might have to follow to get from the root all the wai to a null pointer .
of cours in a success search , you're go to termin befor you encount a null pointer but in the worst case , you want insert you go all the wai to a null pointer .
now on the tree on the left you're go to follow at most <num> such pointer .
so for exampl , if you're search for <num> .
you're go to follow a left pointer follow by a right pointer .
by anoth pointer and that on is go to be null .
so we're go to follow three pointer .
on the other hand , in the right tree , you might follow as mani as five pointer befor that fifth pointer is null .
for exampl , if you search for the kei zero , you're go to travers five left pointer in a row and then you're final go to encount the null at the end .
so , it is not constant time certainli , you have to get to the bottom of the tree .
it's go to be from proport to logarithm , logarithm in the number of kei if you have a nice balanc binari search tree like thi on on the left .
it's go to be proport to the number of kei n as in the fourth answer if you have a realli lousi search tree like thi on on the right and in gener .
search time or the insert time is go to be proport to the height .
the largest number of hop we need to take to get from the root to the leaf of the tree .
let's move on to some more oper that search tree support but that , for exampl , the dynam data structur of heap and hash tabl do not .
so let's start with the minimum and the maximum .
so , by contrast and a heap rememb , you can choos on or the two .
you can either find the minimum , usual you find the maximum easili but not both .
and the search tree is realli easi to find , either the min or the max .
so , let's start with the minimum .
on wai to think of it is that you do a search for neg infin in the search tree .
so , you start the root .
and you just keep follow left child pointer until you run out , until you hit a null .
and whatev the last kei that you visit ha to be the smallest kei of the tree , right ?
becaus , think about it , suppos you start the root .
suppos that the root wa not the minimum , then where is the minimum got to be , it's got to be in the left sub tree so you follow the left child pointer and then you just repeat the argument .
if you haven't alreadi found the minimum , where it's got to be with respect to current place , it's got to be in the left sub tree and you just iter until you can't go to the left ani further .
so for exampl , in our run search tree .
you'll notic that if we just keep follow left child pointer , we'll start at the three , we'll go to the on , we'll try to go left from the on .
we'll hit a null pointer and we'll return on and on is inde the minimum kei in thi tree .
now , given that we've gone over how to comput the minimum , no prize to guess how we comput the maximum .
of cours , if we want to comput the maximum instead of follow left child pointer we follow right child pointer by symmetr reason as guarante upon the largest kei in the tree .
it's like search for the kei plu infin .
all right .
so what about comput the predecessor ?
so rememb thi mean you're given kei in the tree , in the element of the tree and you want to find the next smallest element so for exampl the predecessor of the three is two .
the predecessor of the two in thi tree is the on .
the predecessor of the five is the four .
the predecessor of the four is the three .
so , here i'll be a littl hand wavi just in the interest of get through all of the oper in reason amount of time but let me just point out that there is on realli easi case and then there is on slightli trickier case .
so the easi case .
is when the node with the kei k ha a non empti left sub tree .
if that's the case , then what you want is simpli the biggest element in thi node left sub tree .
so , i'll leav it for you to prove formal that thi is inde the correct wai to comput predecessor for kei that do have a non empti left sub tree , let's just verifi in our exampl by go through the tree that have a left sub tree and check thi is in fact what we want .
now , if you look at it , there's actual onli two node that have a non empti left sub tree .
the three ha a non empti left sub tree and inde the largest kei in the left sub tree three is the two and that is the predecessor of the three so that work out fine .
and then the other node with a non empti left subtre is the five and it's left subtre is simpli the element four of cours the maximum of that tree is also four .
and then you'll notic that is inde the predecessor of five in thi entir search tree .
so , the trickier case is what happen if you know the kei with no left subtre at all .
okai .
so , what ar you go to do if you not in the easi case , well , given at thi node with kei k , you onli have three pointer and by assumpt , the left on is null so that's not go to get you anywher , now , the right childpoint if you think about it is total pointless for comput the predecessor .
rememb , the predecessor is go to be a kei less than the given kei k .
the right subtre by definit of a search tree onli ha kei that ar bigger than k .
so , it stand for reason to find the predecessor we got to follow the parent pointer .
mayb in fact more than on parent pointer so to motiv exactli how we're go to follow parent pointer , let's look at a coupl of exampl in our favorit search tree here on the right .
so , let's start with a node two .
so , we know we got to follow a parent pointer .
when we follow to thi parent pointer , we get to on and boom , on in fact is two's predecessor in thi tree so that wa realli easi to comput two's predecessor .
it seem that all we have to do is follow the parent pointer .
so , for anoth exampl though which think about the node four .
now , four when we follow which parent pointer , we get to five and .
five is not <num>'s predecessor , it's <num>'s successor .
what we want a kei that is less than where we start , we follow the parent pointer and it wa bigger .
but , if we follow on more parent pointer , then we get to the three .
so , from the two we need to follow on parent pointer , from the four we need to follow two parent pointer .
but the point is , you just need to follow parent pointer until you get to a node with kei smaller than your own .
and at that point you can stop and that's guarante to be the predecessor .
so , hopefulli , you would find thi intuit .
i should sai , i have definit not formal prove that thi work and that is a good exercis for those of you that want to have a deeper understand of search tree and thi magic search tree properti and all of the structur that it grant you .
the other thing i should mention is anoth wai to interpret the , the termin criteria .
so what i've said is you stop your search of parent pointer as soon as you get to through smaller than your if you think it about a littl bit , you'll realiz you'll get to a kei smaller than your , the veri first time you take a left turn .
so the veri first time that you go from a right child to it's parent .
look at the exampl , when we start from two , we took a left turn , right ?
we went from upper link go leftward to it's a right child of on , and that's when we got to the predecessor in just on step .
by contrast when we start from the four , our first step wa to the right .
so , we got to a node that wa bigger than where we start for five is four's left child which is go to be smaller than five .
but the first time we took a left turn on the next step , we got to a node that is not onli smaller than five but actual smaller from four , smaller from the start point .
so , in fact , you're go to see a kei smaller than your start point at veri first time , you take a left turn , the veri first time you go from a node to a parent and in fact , that node is that parent's right child .
so thi is anoth statement which i think is intuit but which formal is not total obviou .
and again i encourag you to think carefulli about why these two descript of the termin criteria ar exactli the same so it doesn't matter if you stop when you first find a kei smaller than your start point .
it doesn't matter if you first stop when you follow a parent pointer that goe from a node that's the right child of a node .
either wai you're go to stop at exactli the same time so i encourag you to think about why those ar the exact same stop condit .
a coupl of other detail if you start from the uniqu node that ha no predecessor at all , you'll never go to trigger thi termin condit so for exampl if you start from the node on in the search tree , not onli is the left subtre empti which sai you're suppos to start travers parent pointer but then when you travers a parent pointer , you onli go to the right .
you never turn left and that's becaus there is no predecessor so that's how you detect that you're at the minimum of a search tree .
and then of cours if you want to be the successor of the kei instead of the predecessor , obvious you just flip left and right through out thi entir descript .
so that's the high level explan of all of these differ order oper , minimum and maximum predecessor and successor work in a search tree .
let me ask you the same question i ask you when we talk about search in insert .
how long that these oper take in the worst case ?
well , the answer is the same as it wa befor .
it's proport to the height of the tree and the explan is exactli the same as it wa befor .
so to understand the depend on the height wa just focus on the maximum oper that ha the state within the question .
the other three oper , the run time is proport to the height in the worst case for exactli the same reason .
so , what is the max oper do when you start the root and you just follow the right child pointer until you run out them so you hit null .
so , you know , that the run time is go to be no wors in the longest such path .
it's particular path from the root to essenti a leaf .
so instead we're go to have a run time more than the height of the tree , on the other hand for all you know .
the path from the root to the maximum kei might well be the longest on in the tree .
it might be the path that actual determin the height of the search tree .
so , for exampl in our run unbalanc exampl , that would be a bad tree for the minimum oper if you look for the minimum in thi tree , then you have to travers everi singl pointer from five all the wai down to on .
of cours there's an analogi bad search tree for the maximum oper where the on is the root and the five is all the wai down to the left .
anoth thing you can do is search tree which mimic what you can do with sort arrai is you can print out all of the kei in the sort order in linear time with constant time per element .
obvious , in the sort arrai thi is trivial .
us your for loop start ing at the begin at the arrai point up the kei on at a time and there's a veri eleg recurs implement for do the exact same thing in a search tree .
and thi is known as an in order travers of binari search tree .
so as alwai you begin at the begin name at the root of the search tree .
and a littl bit of notat of which call , all of the search tree that start at r's left child t sub l and the search tree rout at r's right child t sub r .
in our run exampl of cours the root is three t sub l with correspond in the search tree compris onli the element on and two , t sub r would correspond to the sub tree compris onli the element five and four .
now , rememb we want to print out the kei in increas order .
so in particular , the first kei we want to print out is the smallest of them all .
so it's someth we definit don't want to do is we don't want to first print out the kei at the root .
for exampl in our search tree exampl , the root's kei is three , we don't want to print that out first .
we want to print out the on first .
so where is the minimum lie ?
well , by the search tree properti , it's got to lie in the left subtre t sub l , so we're just go to recurs on t sub l .
so by the magic of recurs or if you prefer induct , what re curs on t sub l is go to accomplish is we're go to print out all of the kei in t sub l in increas order from smallest to largest .
now that's pretti cool becaus t sub l contain exactli the kei that ar smaller than the kei of the root .
rememb that's the search tree properti .
everyth bigger than the root's kei ha to be in the left sub tree .
everyth bigger than the root's kei have to be in it right sub tree .
so in our concret exampl of thi first recurs call is we're go to print the kei on and then two .
and now , if you think about it it's the perfect time to print out the kei at the root , right ?
we want to print out all the kei in increas order we've alreadi done everyth less than the root's kei where re curs and on the right hand side will take you everyth bigger in it so in between the two recurs call , thi is why it's call an in order travers , that's when we want to print out .
r's kei .
and clearli thi work in our concret exampl , the first recurs call print out on and two , it's the perfect time to print out three and then a recurs call of print out four and five .
and more gener , the recurs call on there right subtre will print out all of the kei bigger than the root kei and increas order again by the magic of recurs or induct so , the fact that the pseudo code is correct .
the fact that the so call in order travers inde print out the kei in increas order .
thi is a fairli straightforward proof by induct .
it's veri much in the spirit or the proof by induct , correct of divid and conquer algorithm that we've discuss earlier in the cours .
so what about the run time of an in order travers ?
the claim is that the run time of thi procedur is linear .
it's o of n where n is the number of kei in the search tree .
and the reason is , there's exactli on recurs call for each node of the tree and constant work is done in each of those recurs call .
and a littl more detail , so what is the in order travers do , it will print out the kei in increas .
in particular it print out each kei exactli onc .
each recurs call print out exactli on kei's valu .
so there's exactli n recurs call and all of the recurs call doe is print on thing .
so n recurs call constant time for each that give us a run time of o n overal .
in most data structur , delet is the most difficult oper and in search tree .
there ar no except .
so let's get into it and talk about how delet work , there ar three differ case .
so the first order of busi is to locat the node that ha the kei k , locat the node that we want to get rid off .
right so for starter , mayb we're try to delet the kei two from our run exampl search tree .
so the first thing we need to do is figur out where it is .
so , there ar three possibl for the number of children that a node in a search tree might have and might have zero children that might have on child it might have two children , correspond to those three case that the delet pseudo code will also have three case .
so , let's start with the happi case where there's onli zero children like in thi case where delet the kei <num> from the search tree .
then of cours , we can , without ani reserv just delet the node directli from the search tree , noth can go wrong , there's no children depend on that node .
then there is the medium difficult case .
thi is where .
the node contain k ha on child .
an exampl here would be , if we want to delet five from the search tree so the medium case is also not too bad .
all you got to do is splice out the node that you want to delet .
that creat a hole in the tree but then that node , delet node's uniqu child assum the previou posit of the delet node .
i can make a nerdi joke about shakespear right here but i'll refrain .
for exampl , in our five node search tree if we want to , let's sai we haven't actual delet two out of thi on , if we want to delet the five .
the five when we take it out of the tree that would leav a hole but then we just replac the posit previous held by five by it's uniqu child four .
and if you think about it that work just fine in the sens of that preserv the search tree properti .
rememb the search tree properti sai that everyth in sai , a right subtre ha to be bigger than everyth in the node kei , so we've made four the new right child of three but four and ani children that it might have were alwai part of <num>'s right subtre so all that stuff ha got to be bigger than three so there's no problem put four and possibl all of it descend .
as the right child of three .
the search tree properti is in fact retain .
so , the final difficult case then is when the node be delet ha both of it children , ha two children .
so , in our run exampl with five node , thi would onli transpir if you want to delet the root , you want to delet the kei three from the tree .
the problem , of cours , is that , you know , you can try rip out thi node from the tree but then , there's thi hole and it's not clear that it's go to work to promot either child .
into that spot .
you might stare at our exampl search tree and try to understand what would happen if you try to bring on up to be the root or if you try to bring five up to be the root .
problem would happen , that's what would happen .
thi is an interest contrast to when we face the same issu with heap .
becaus the heap properti in some sens is perhap less stringent , there we didn't have an issu .
when we want to delet someth with two children , we just promot the smaller of the two children assum we want to export and extract them in oper .
here , we're go to have to work a littl harder .
in fact thi is go to be realli neat trick .
we're go to do someth that reduc the case of two children to the previous solv case of zero or on children .
so here's a veri sneaki wai we identifi a node to which we can appli either the case zero or the case on oper .
what we're go to do is we're go to .
start from k and we're go to comput k's predecessor .
rememb , thi is the next smallest kei in the tree .
so , for exampl , the predecessor of the kei three is two .
that's the next smallest kei in the tree .
in gener , let's call case predecessor l .
now , thi might seem complic .
we're try to implement on tree oper and with delet and all of a sudden we're invok a differ tree oper predecessor which we cover a coupl of slide ago .
and to some extent you're right you know , delet , thi is a nontrivi oper .
but , it's not quit as bad as you think for the follow reason .
when we comput thi predecessor , we're actual in the easi case of the predecessor oper conceptu .
rememb how do you get a predecessor , well it depend .
what doe it depend on ?
it depend on whether you got a non empti left sub tree or not .
if you don't have a non empti left sub tree , that's how you got to those thing and follow a parent pointer upward until you find a kei which is smaller than what you've start .
but .
if you've got a left sub tree , then it's easi .
you just find the maximum of the left sub tree and that's got to be the predecessor and rememb , find maximum ar easi .
all you have to do is follow right child pointer until you can't anymor .
now , what's cool is becaus we onli bother with thi predecessor comput in the case where case k's node ha both children .
we onli have to do it in the case where it ha a non empti left subtre .
so realli when we sai comput k's predecessor l .
all you got to do is follow k's left child .
that's not null becaus it ha both children .
and then , follow right child pointer until you can't anymor and that's the predecessor .
now , here's the fairli brilliant part of the wai you do implement delet in the search tree which is you swap these two kei , k and l .
so for exampl in our run search tree , instead of thi three at the root we would put a two there and instead of thi two at the leaf , it would put a three there .
and the first time you see thi , it just strike you as a littl crazi , mayb even cheat or just simpli disregard the role of , rule of search tree .
and actual , it is like check out what happen to our exampl search tree .
we swap the three and the two and thi is not a search tree anymor , right ?
so , we have thi three which is in two left sub tree and a three is bigger than the two and that is not allow .
that is violat of the search tree properti .
oop .
so , how can we get awai with thi and we get awai with thi is we're go to delet three anywai .
so , we're go to wind up with the search tree at the end of the dai .
so we mai have mess up the search tree properti a littl bit but we've swap k in the posit where it realli easi to get rid of .
well how did we comput case predecessor l ?
ultim that wa the result of a maximum comput which involv follow right child pointer until you get stuck and l wa the place we got stuck .
what's the mean to get stuck ?
it mean l's right child pointer is null .
it doe not have two children .
in particular it doe not have a right child .
onc we swap k in the l's old posit , k now doe not have a right child .
it mai or mai not have a left child and the exampl on the right it doe not have a left child either in thi new posit but in gener it might have a left child .
but , it definit doesn't have a right child .
becaus that wa a posit at which a maximum comput got stuck .
and if we want to delet a node that ha onli zero or on child , well that we know how to do .
that we cover in the last slide .
either you just delet it , that's what we do in the run exampl here .
or in the case where k's new node doe have a left child , you would do the splice out oper .
so you would rip out the node that contain k and that the uniqu child of that node would assum the previou posit of that node .
now an exercis which i'm not go to do here but i strongli encourag you think through in the privaci of your own home , is that , in fact , thi delet oper retain the search tree properti .
so roughli speak , when you do the swap , you can violat the search tree properti as we see in thi exampl but all of the violat involv the node you're about to delet so onc you delet that node , there's no other violat of the search properti so bingo , you're left with the search tree .
the run time thi time no get , no prize for guess what it is becaus it's basic just on of these predecessor comput plu some pointer rewir just like the predecessor and search is go to be govern by the height of the tree .
so let me just sai a littl bit about the final two oper mention earlier , select and rank .
rememb select is just a select problem .
i'll give you an order statist like seventeen and i want you to return the seventeenth smallest kei in the tree .
rank is i give you a kei valu and i want to know how mani kei in the tree ar less than or equal to that valu .
so , to implement these oper effici , we actual need on small new idea which is to augment binari search tree with addit inform at each node .
so , now the search tree will contain not just a kei but also inform about the tree itself .
so , thi idea is often call augment your data structur and perhap the most canon augment of the search tree like these is to keep track in each node , not just to the kei valu but also over the popul of tree node in the sub tree that is root there .
so let's call thi size of x .
which is the number of tree node in the subtre root at x .
so to make sure you know what i mean , let me just tell you what the size field should be for each of the five node in our run search tree exampl .
so again exampl , we're think about how mani node ar in the subtre root given node .
or equival , follow child pointer from that node how mani differ tree node can you reach ?
so from the root of cours , you can reach everybodi .
everybodi's in the tree root at the root so the size there is five .
by contrast , you start at the node on , well , you can get to the on or you can follow the right child pointer to get to the two .
so at the on .
the size would be two and the node with the kei valu five for the same reason , the size would be two .
at the two leav , the subtre where the leaf is just the leaf itself so there , the size would be on .
there's an easi wai to comput the size of a given node onc you know the size of it two sub tree .
so , if the given node in the search tree ha children y and z , then , how mani node ar there in the sub tree root x , well , there's those that ar root at y .
there ar those in the left sub tree , there ar those that ar reachabl from z that is there ar the children that ar also children of z and then there's x itself .
now in gener , whenev you augment a data structur , and thi is someth we'll talk about again when we discuss red black tree , you've got to pai the piper .
so , the extra data that you maintain it might be us for speed up certain oper .
but whenev you have oper that modifi the tree , specif insert and delet , you have to take care to keep that extra data valid , keep it maintain .
now , in the case of the subtre size , there ar quit straightforward to maintain under insert and delet without affect the run time of insert and delet veri much but that's someth you should alwai think about offlin .
for exampl , when you perform an insert rememb how that work .
you do as , essenti a search .
you follow left and right child pointer down to the bottom of the tree until you get a null pointer then that's where you stick a new node .
now what you have to do is you have to trace back up that path , all of the ancestor of the new node you just insert and increment their subtre size by on .
so let's wrap up thi video by show you how to implement the select procedur given an nth order statist in a search tree that's been augment so that at everi node you know the size of a subtre root at that node .
well of cours as alwai you start at the begin which in the search tree is the root .
and let's sai the root ha a sub children y and z .
y or z could be null , that's no problem .
we just think of the size of a null node as be zero .
now , what's the search tree properti ?
it sai , everi , these kei that ar less than the kei sort x ar precis the on that ar in the left sub tree of x .
the kei in the tree , thei ar bigger than the kei to x or precis the on that you're go to find in x's right sub tree .
so , suppos we're ask to find the seventeenth order statist in the search three .
seventeenth smallest kei that's store in the tree , where is it go to be ?
where should we look ?
well , it's go to depend on the structur of the tree and in fact it's go to depend on the subtre size .
thi is exactli .
we're keep track of them so we can quickli make decis about how to navig through the tree .
so for a simpl exampl , suppos that x's left subtre contain sai <num> kei .
so rememb y know local exactli what the popul of the subtre is so in constant time from x , we can figur out how mani kei ar in y subtre let's sai it <num> .
now , by the defin properti of search tree , these ar the <num> smallest kei anywher in the tree .
right , x is bigger than all of them .
everyth in x's right subtre is bigger than all of them .
so , the <num> smallest order statist ar all in the subtre root to y , clearli that where we should recurs .
clearli that's where the answer li so in recurs the subtre root of y and then we ar again look for the seventeenth order statist in thi new smaller search tree .
on the other suppos when we start x and we look , we ask why .
how , how mani node ar there in your subtre .
mayb y local have store the number twelv .
so there's onli twelv thing in x's left subtre .
well , okai , x itself is bigger than all of them so that's go to , x is go to be the thirteenth biggest order statist .
it's go to be the thirteenth biggest element in the tree .
everyth els is in the right sub tree .
so , in particular , the seventeenth order statist is go to be in the right sub tree so we're go to recurs in the rght sub tree .
now , what ar we look for , we're not look for the seventeenth order statist anymor .
the twelv smallest thing all in x's sub tree , x itself is the thirteenth smallest so we ar look for the fourth smallest of what remain .
so , the recurs is veri much along the line of what we did in the divid and conquer select algorithm earlier in the cours .
so to fill in some more detail , let's let a denot the subtre size at y .
and if it happen that x ha no left child , we'll , the point would be a to be zero .
so the super lucki case is when there's exactli i <num> node in the left subtre .
that mean the root here , x is itself the ith order statist rememb it's bigger than everyth in it's left subtre it's smaller than everyth in it right subtre .
but , in the gener case we're go to be recurs either on the left subtre or in the right subtre .
we recurs on the left subtre when it popul is larg enough that we guarante it and compass the ith order statist .
and that happen exactli when it side is at least i .
that's becaus the left subtre ha the smallest kei that ar anywher in the search tree .
and in the final case when the left subtre is so small that the onli doe it not contain the ith order statist but also x is too small to be an ith order statist then we recurs in the right subtre know that we have thrown awai a <num> , the a <num> smallest kei valu anywher in the origin tree .
so , correct of thi procedur is pretti much exactli the same as the induct correct for the select algorithm we've discuss earlier in effect to the root of the search tree is act as a pivot element with everyth in the left sub tree be less than the root everyth in the right sub tree be greater than the element in the root so that's why the recurs is correct .
as far as the run time , i hope it's evid from the pseudo code that we do constant time each time thei recurs .
how mani time can we recurs when we keep move down the tree that maximum number of time we can move down the tree is proport to the height of the tree .
so , it wa again is proport to the height .
so , that's the select oper , there is an analog wai to write the rank oper .
rememb , thi is where you're given the kei valu and you want to count up the number of store kei that ar less than or equal to that target valu , again , you us thi augment search tree and again , you can get run time porport to the height and i encourag you to think through the detail of how implement rank offlin .
so , in thi video , we'll graduat beyond the domain of just vanilla binari search tree , like we've been talk about befor , and we'll start talk about balanc binari search tree .
these ar the search tree you'd realli want to us when you want to have real time guarante on your oper time .
cuz thei're search tree which ar guarante to stai balanc , which mean the height is guarante to stai logarithm , which mean all of the oper search tree support that we know and love , will also be a logarithm in the number of kei that thei're store .
so , let's just quickli recap .
what is the basic structur tree properti ?
it should be the case that at everi singl node of your search tree , if you go to the left , you'll onli see kei that ar smaller than where you start and if you go to the right you onli see kei that ar bigger than where you start .
and a realli import observ , which is that , given a set of kei , there's go to be lot and lot of legitim , valid , binari search tree with those kei .
so , we've been have these run exampl where the kei on , two , three , four , five .
on the on hand , you can have a nice and balanc search tree that ha height onli two , with the kei on through five .
on the other hand , you can also have these crazi chain , basic devolv to link list where the height for , and element could be as high as n <num> .
so , in gener , you could have an exponenti differ in the height .
it can be as small , in the best case , as logarithm and as big , in the worst case , as linear .
so , thi obvious motiv search tree that have the addit properti that you never have to worri about their height .
you know thei're go to be well balanc .
you know thei're go to have height logarithm .
you're never worri about them have thi realli lousi linear height .
rememb , why it's so import to have a small height ?
it's becaus the run time of all of the oper of search tree depend on the height .
you want to do search , you want to insert , you want to find predecessor or whatev , the height is go to be what govern the run time of all those properti .
so , the high level idea behind balanc search tree is realli exactli what you think , which is that , you know , becaus the height can't be ani better than logarithm in the number of thing you're store , that's becaus the tree ar binari so the number of node can onli doubl each level so you need a logarithm number of level to accommod everyth that you ar store .
but it's got to be logarithm , let make sure it stai logarithm all the time , even as we do insert and delet .
if we can do that , then we get a veri rich collect of support oper all run in logarithm time .
as usual , n denot , the number of kei be store in the tree .
there ar mani , mani , mani differ balanc search tree .
thei're not super , most of them ar not super differ from each other .
i'm go to talk about on of the more popular on which ar call red black tree .
so , these were invent back in the '70s .
these were not the first balanc binari search tree data structur , that honor belong to avl tree , which again ar not veri differ from red black tree , though the invari ar slightli differ .
anoth thing you might want to look up and read about is a veri cool data structur call splai tree , due to sleator and tarjan , these , unlik red black tree and avl tree , which onli ar modifi on insert and delet , which , if you think about it , is sort of what you'd expect .
splai tree modifi themselv , even when you're do look up , even when you're do search .
so , thei're sometim call self adjust tree for that reason .
and it's super simpl , but thei still have kind of amaz guarante .
and then final , go beyond the , just the binari tree paradigm mani of you might want to look up exampl of b tree or also b tree .
these ar veri relev for implement databas .
here what the idea is , in a given node you're go to have not just on kei but mani kei and from a node , you have multipl branch that you can take depend where you're search for fall with respect to the multipl kei that ar at that node .
the motiv in a databas context for go beyond the binari paradigm , is to have a better match up with the memori hierarchi .
so , that's also veri import , although a littl bit out of the scope here .
that said , what we discuss about red black tree , much of the intuit will translat to all of these other balanc tree data structur , if you ever find yourself in a posit where you need to learn more about them .
so , red black tree ar just the same as binari search tree , except thei also alwai maintain a number of addit invari .
and so , what i'm go to focu on in thi video is , first of all , what the invari ar , and then how the invari guarante that the height will be logarithm .
time permit , at some point , there will be option video more about the gut , more about the implement of red black tree name how do you maintain these invari under insert and delet .
that's quit a bit more complic , so that's appropri for , for option materi .
but understand what the invari ar and what role thei plai in control the height is veri access , and it's someth i think everi programm should know .
so , there , i'm go to write down four invari and realli , the bite come from the second two , okai , from the third and the fourth invari .
the first two invari you know , ar just realli cosmet .
so , the first on we're go to store on bit of inform addition at each node , beyond just the kei and we're go call thi bit as indic whether it's a red or a black node .
you might be wonder , you know , why red black ?
well , i ask my colleagu , leo guiba about that a few year ago .
and he told me that when he and professor sedgewick were write up thi articl the journal were , just had access to a certain kind of new print technolog that allow veri limit color in the print copi of the journal .
and so , thei were eager to us it , and so thei name the data structur red black , so thei could have these nice red and black pictur in the journal articl .
unfortun , there wa then some snafu , and at the end of the dai , that technolog wasn't actual avail , so it wasn't actual print the wai thei were envis it but the name ha stuck .
so , that's the rather idiosyncrat reason why these data structur got the name that thei did , red black tree .
so , secondli we're go to maintain the invari that the root of the search tree is alwai black , it can never be red .
okai .
so , with the superfici pair of invari out of the wai , let's go to the two main on .
so , first of all , we're never go to allow two red in a row .
by which , i mean , if you have a red node in the search tree , then it children must be black .
if you think about for a second , you realiz thi also impli that if a notic red , and it ha a parent , then that parent ha to be a black node .
so , in that sens , there ar no two red node in a row anywher in the tree .
and the final invari which is also rather sever is that everi path you might take from a root to a null pointer , pass through exactli the same number of black node .
so , to be clear on what i mean by a root null path , what you should think about is an unsuccess search , right ?
so , what happen in an unsuccess search , you start at the root depend on whether you need to go smaller or bigger , you go left or right respect .
you keep go left right as appropri until eventu you hit a null pointer .
so , i want you to think about the process that which you start at the root and then , eventu , fall off the end of the tree .
in do so , you travers some number of node .
some of those node will be black some of those node will be red .
and i want you to keep track of the number of black node and the constraint that a red black tree , by definit , must satisfi , is that no matter what path you take through the tree start from the root termin at a null pointer , the number of black node travers , ha to be exactli the same .
it cannot depend on the path , it ha to be exactli the same on everi singl root null path .
let's move on to some exampl .
so , here's a claim .
and thi is meant to , kind of , whet your appetit for the idea that red black tree must be pretti balanc .
thei have to have height , basic logarithm .
so , rememb , what's the most unbalanc search tree ?
well , that's these chain .
so , the claim is , even a chain with three node can not be a red black tree .
so , what's the proof ?
well , consid such a search tree .
so , mayb , with the kei valu on , two and three .
so , the question that we're ask is , is there a wai to color the node , these three node , red and black so that all four of the invari ar satisfi .
so , we need to color each red or black .
rememb , variant two sai , the root , the on ha to be black .
so , we have four possibl for how to us the color two and three .
but realli , becaus of the third invari , we onli have three possibl .
we can't color two and three both red , cuz then we'd have two red in a row .
so , we can either make two red , three black , two black , three red , or both two and three black .
and all of the case ar the same .
just to give on exampl , suppos that we color the node two , red , and on and three ar black .
the claim is invari four ha been broken and invari four is go to be broken no matter how we try to color two and three red and black .
what is invari four sai ?
it sai , realli on ani unsuccess search , you pass through the same number of black node .
and so , on unsuccess search would be , you search for zero .
and if you search for a zero , you go to the root , you immedi go left to hit a null pointer .
so , you see exactli on black node .
name on .
on the other hand , suppos you search for four , then you'd start at the root , and you'd go right , and you go to two , you'd go right , and you go to three , you'd go right again , and onli then will you get a null pointer .
and on that , unsuccess search , you'd encount two black node , both the on and the three .
so , it's a violat of the fourth invari , therefor , thi would not be a red black tree .
i'll leav that for you to check , that no matter how you try to code two and three red or black , you're go to break on of the invari .
if thei're both red , you'd break the third invari .
if at most on is red , you'd break the fourth invari .
so , that's a non exampl of a red black tree .
so , let's look at an exampl of a red black tree .
on , a search tree where you can actual color the node red or black so that all four invari ar maintain .
so , on search tree which is veri easi to make red black is a perfectli balanc on .
so , for exampl , let's consid thi three node search tree ha the kei three , five , and seven and let's suppos the five is the root .
so , it ha on child on each side , the three and the seven .
so , can thi be made a red black tree ?
so , rememb what that question realli mean .
it's ask can we color these three node some combin of red and black so that all four of the invari ar satisfi ?
if you think about it a littl bit , you realiz , yeah , you can definit color these node red or black to make and satisfi for the invari .
in particular , suppos we color all three of the node , black .
we've satisfi variant number on , we've color all the node .
we've satisfi variant number two , and particularli , the root is black .
we've satisfi invari number three .
there's no red at all , so there's certainli no two red in a row .
and , if you think about it , we've satisfi invari four becaus thi tree is perfectli balanc .
no matter what you unsuccessfulli search for , you're go to encount two black node .
if you search for , sai , on , you're go to encount three and five .
if you search for , sai , six , you're go to encount five and seven .
so , all root null path have exactli two black node and variant number four is also satisfi .
so , that's great .
but , of cours , the whole point of have a binari search tree data structur is you want to be dynam .
you want to accommod insert and delet .
everi time you have an insert or a delet into a red black tree , you get a new node .
let's sai , an insert , you get a new node , you have to color it someth .
and now , all of a sudden , you got to worri about break on of these four invari .
so , let me just show you some easi case where you can accommod insert without too much work .
time permit we will includ some option video with the notion of rotat which do more fundament restructur of search tree so that thei can maintain the four invari , and stai nearli perfectli balanc .
so , if we have thi red black tree where everyth's black , and we insert , sai , six , that's go to get insert down here .
now , if we try to color it black , it's no longer go to be a red black tree .
and that's becaus , if we do an unsuccess search now for , sai , <num> , we're go to encount three black node , where if we do an unsuccess search for on , we onli encount two black node .
so , that's not go to work .
but the wai we can fix it is instead of color the six black , we color it red .
and now , thi six is basic invis to invari number four .
it doesn't show up in ani root null path .
so , becaus you have two black node in all root in all path befor , befor the six wa there , that's still true now that you have thi red six .
so , all four invari ar satisfi onc you insert the six and color it red .
if we then insert , sai , an eight , we can pull exactli the same trick , we can call it an eight red .
again , it doesn't particip in invari four at all so we haven't broken it .
moreov , we still don't have two red in a row , so we haven't broken invari number three either .
so , thi is yet anoth red black tree .
in fact , thi is not the uniqu wai to color the node of thi search tree , so that it satisfi all four of the invari .
if we , instead , recolor six and eight black , but at the same time , recolor the node seven , red , we're also golden .
clearli , the first three invari ar all satisfi .
but also , in push the red upward , consolid the red at six and eight , and put it at seven instead , we haven't chang the number of black node on ani given path .
ani black , ani path that previous went through six , went through seven , anyth that went through eight , went through seven so there's exactli the same number of red and black node on each such path as there wa befor .
so , all path still have equal number of black node and invari four remain satisfi .
as i said , i've shown you here onli simpl exampl , where you don't have to do much work on an insert to retain the red black properti .
in gener , if you keep insert more and more stuff and certainli if you do the delet , you have to work much harder to maintain those four invari .
time permit , we'll cover just a tast of it in some option video .
so , what's the point of these seemingli arbitrari four invari of a red black tree ?
well , the whole point is that if you satisfi these four invari in your search tree , then your height is go to be small .
and becaus your height's go to be small , all your oper ar go to be fast .
so , let me give you a proof that if a search tree satisfi the four invari , then it ha super small height .
in fact , no more than doubl the absolut minimum that we conceiv have , almost two time log base two of n .
so , the formal claim , is that everi red black tree with n node , ha height o of log n , were precis in those two time log base two of n <num> .
so , here's the proof .
and what's clear about thi proof is it's veri obviou the role plai by thi invari three and four .
essenti , what the invari guarante is that , a red black tree ha to look like a perfectli balanc tree with at most a sort of factor two inflat .
so , let's see exactli what i mean .
so , let's begin with an observ .
and thi , thi ha noth to do with red black tree .
forget about the color for a moment , and just think about the structur of binari tree .
and let's suppos we have a lower bound on how long root null path ar in the tree .
so , for some paramet k , and go ahead and think of k as , like , ten if you want .
suppos we have a tree where if you start from the root , and no matter how it is you navig left and right , child pointer until you termin in a null pointer .
no matter how you do it , you have no choic but to see at least k node along the wai .
if that hypothesi is satisfi , then if you think about it , the top of thi tree ha to be total fill in .
so , the top of thi tree ha to includ a perfectli balanc search tree , binari tree of depth k <num> .
so , let me draw a pictur here of the case of k three .
so , if no matter how you go from the root to a null pointer , you have to see at least three node along the wai .
that mean the top three level of thi tree have to be full .
so , you have to have the root .
it ha to have both of it children .
it ha to have all four of it grandchildren .
the proof of thi observ is by contradict .
if , in fact , you were miss some node in ani of these top k level .
we'll that would give you a wai of hit a null pointer see less then k node .
so , what's the point is , the point is thi give us a lower bound on the popul of a search tree as a function of the length of it root null path .
so , the size n of the tree must includ at least the number of node in a perfectli balanc tree of depth k <num> which is <num> k <num> , so , for exampl , when k <num> , it's <num> <num> two cube <num> , or <num> that's just a basic fact about tree , noth about red black tree .
so , let's now combin that with a red black tree invari to see why red black tree have to have small height .
so again , to recap where we got to on the previou slide .
the size n , the number of node in a tree , is at least <num> k <num> , where k is the fewest number of node you will ever see on a root null path .
so , let's rewrit thi a littl bit and let's actual sai , instead of have a lower bound on n in term of k , let's have an upper bound on k in term of n .
so , the length of everi root null path , the minimum length of everi root null path is bound abov by log base two of quantiti n <num> .
thi is just ad on to both side and take the logarithm base two .
so , what doe thi bui us ?
well , now , let's start think about red black tree .
so now , red black tree with n node .
what doe thi sai ?
thi sai that the number of node , forget about red or black , just the number of node on some root null path ha to be the most log base two of n <num> .
in the best case , all of those ar black .
mayb some of them ar red , but in the , in , the maximum case , all of them ar black .
so , we can write in a red black tree with n node , there is a root null path with at most log base two of n <num> , black node .
thi is an even weaker statement than what we just prove .
we prove that it have some , somehow must have at most log base two , n <num> total node .
so , certainli , that path ha the most log base two of n <num> black node .
now , let's , now let's appli the two knockout punch of our two invari .
all right , so fundament , what is the fourth invari tell us ?
it's tell us that if we look at a path in our red black tree , we go from the root , we think about , let's sai , that's an unsuccess search , we go down to a null pointer .
it sai , if we think of the red node as invis , if we don't count them in our talli , then we're onli go to see log , basic a logarithm number of node .
but when we care about the height of the red black tree , of cours , we care about all of the node , the red node and the black node .
so , so far we know , that if we onli count black node then we're good , we onli have log base two of n <num> node that we need to count .
so , here's where the third invari come in .
it sai , well actual , black node ar a major of node in the tree .
in a strong sens , there ar no two red in a row , on ani path .
so , if we know the number of black node is small , then becaus you can't have two red in a row , the number of total node on the path is at most twice as larg .
in the worst case , you have a black rout , then red , then black , then red , then black , then red , then black , et cetera .
at the worst case , the number of red node is equal to the number of black node , which doubl the length of the path onc you start count the red node as well .
and thi is exactli what it mean for a tree to have a logarithm depth .
so , thi , in fact , prove the claim , if the search tree satisfi the invari on through four , in particular if there's no two red in a row and all root null path have an equal number of black node , then , know noth els about thi search tree , it's got to be almost balanc .
it's perfectli balanc up to a factor of two .
and again , the point then is that oper in a search tree and the search tree ar go to run in logarithm time , becaus the height is what govern the run time of those oper .
now , in some sens , i've onli told you the easi part which is if it just so happen that your search tree satisfi these four invari , then you're good .
the height is guarante to be small so the oper ar guarante to be fast .
clearli that's exactli what you want from thi data structur .
but for the poor soul who ha to actual implement thi data structur , the hard work is maintain these invari even as the data structur chang .
rememb , the point here is to be dynam , to accommod insert and delet .
and search and delet can disrupt these four invari and then on ha to actual chang the code to make sure thei're satisfi again , so that the tree stai balanc , ha low height , even under arbitrari sequenc of insert and delet .
so , we're not go to cover that in thi video .
it can be done , without significantli slow down ani of the oper .
it's pretti tricki , take some nice idea .
there's a coupl well known algorithm textbook that cover those detail .
or if you look at open sourc and limit of balanc search tree , you can look at code that doe that implement .
but , becaus it can be done in a practic wai and becaus red black tree support such an origin arrai of oper , that's why you will find them us in a number practic applic .
that's why balanc search tree should be part of your programm tool box .
in thi video and the next we ar go to take you to the next level and here under the hood into implement of balanc pioneer research tree .
now frankli when ani great detail of all balanc binari research tree implement get pretti complic and if you realli want to understand them at a fine grain level there is no substitut for read an advanc logarithm textbook that includ coverag of the topic and or studi open sourc implement of these data structur .
i don't see the point of regurgit all of those detail here .
what i do see the point in do , is give you the gist of some of the kei idea in these implement .
in thi first video i want to focu on a kei primit , that of rotat which is common to all balanc by the of limit .
whether is red , black tree , evl tree , b or b tree , whatev all of them us rotat .
in the next video we'll talk a littl bit more about the detail of red , black tree in particular .
so , what is the point behind these magic rotat oper ?
well the goal is to do just constant work , just re wire a few pointer , and yet local re balanc a search tree , without violat the search tree properti g properti .
so there ar two flavor of rotat , left rotat and right rotat .
in either case , when you invok a rotat it's on a parent child pair , in a search tree .
if it's a right child of the parent , then you us a left rotat .
we'll discuss that on thi slide .
and a right rotat is , in some sens , an invers oper which you us when you have a left child of the parent .
so what's the gener pictur look like when you have a node x in a search tree and it ha some right child y ?
well x , in gener , might have some parent .
x might also have some left subtre .
let's call that left subtre of x , a .
it could , of cours , be and then y ha it's <num> subtre , let call the left subtre of y b and the right subtre c .
it's go to be veri import that rotat preserv the search tree properti so to see why that's true let just be clear on exactli which element ar bigger then which other element in thi pictur .
so first of all y be a right child of x , y's go to be bigger than x .
now all of the kei which line the subtre a becaus thei're to the left of x these kei ar go to be even smaller than x .
by the same token anyth in the subtre capit c , that's to the right of y so all that stuff's go to be even bigger than y .
what about sub tree capit b ?
what about the node in there ?
well , on the on hand , all of these ar in x's right sub tree , right ?
to get to ani node in b , you go through x and then you follow the right child to y .
so that mean everyth is b is bigger than x , yet , at the same time , it's all in the left sub tree of y so these ar all thing smaller than y .
summar all of the node in b have kei strictli in between x and y .
so now that we're clear on how all of these search kei in the differ part of thi pictur relat to each other , i can describ the point of a left rotat .
fundament the goal is to invert the relationship .
between the node x and y .
current x is the parent and y is the child .
we want to rewir a few pointer so that y is now the parent and x is the child .
now what's cool is given that is our goal is pretti much a uniqu wai to put all these piec back togeth to accomplish it .
so let just follow our nose .
so rememb the goal y should be the parent and x should be the child .
well , x is less then and why and there's noth we can do about that .
so if x is go to be a child of y , it got to the left child .
so your first question might be well what happen that x is parent .
so x us to have some parent let call it p and x's new parent is y .
similarli y us to have parent x and there's a question what should y new parent be ?
well y is just go to inherit x's old parent p .
so thi chang ha no bear for the search tree properti .
either of thi collect of node wa p's left sub tree , in that case all these node were less than p , or thi sub tree wa p's right sub tree which in that case all of these ar bigger than p , but p can realli care less , which of x or y is it's direct descend .
now let move on to think about how . . . what we should do with the sub tree a , b and c .
so , we have <num> sub tree we need to re assembl into thi pictur , and fortun we have <num> slot avail .
x ha both of it child pointer avail and y ha it right child avail .
so what can we put where ?
well , a , is the sub tree of stuff which is less than both x and y .
so , that should sensibl be x's left child .
that's exactli as it wa befor .
by the same token , capit c is the stuff bigger than both x and y , so that should be , y , the bigger node child , right child just as as befor .
and what's more interest is what happen to subtre capit b .
so b us to be y's left child but that can't happen anymor , 'caus now x is y's left child .
so , the onli hope is to slot capit b into the onli space we have for it , x's right child .
fortun for us thi actual work , slide b into the onli open space we have for it .
x's right child doe inde preserv the switch tree properti .
recal we notic that everi kei in capit b is strictli between x and y , therefor it better be and x's right sub tree and it better be in y's right sub tree , but it is , that's exactli where we put it .
so that's a left rotat , but if you understand a left rotat then you understand a right rotat as well .
becaus a right rotat is just the invers oper .
so that's when you take a parent child pair , where the child is the left child of the parent , and now you again want to invert the relationship .
you want to make the old child the new parent and the old parent the new child .
and onc again given thi goal there's realli a uniqu wai to reassembl the compon of thi pictur so that the goal's accomplish , so that y is now the parent of x .
so what ar the laudabl properti of rotat ?
well first of all i hope it's clear that thei can be implement .
in a constant time or you were do a rewir a constant number of pointer .
further more as have discuss thei preserv the search tree properti .
so these nice properti ar what make rotat the ubiquit primit common to all balanc search tree implement .
so thi of cours , is not the whole stori .
in a complet specif of a balanc search tree implement , you have to sai exactli when and how you deploi these rotat .
you'll get a small tast of that in the next video but if you realli want to understand it in more depth , i again encourag you to check out either a comprehens data structur textbook .
check out a number of balanc search tree demonstr , which ar readili avail on the web .
or have a peek at an open sourc implement of on of these data structur .
for thi final video on binari search tree i want to talk a littl bit about implement , implement detail for the red black tree data structur in particular the insert oper .
as i've said in the past it realli doesn't make sens for me to spell off all of the gori detail about how thi is implement .
if you want to understand them in full detail .
detail you should check out variou demonstr readili avail on the web , or a comprehens textbook , or an open sourc implement .
red black tree , you'll recal satisfi four invari and the final two invari in particular ensur that the red black tree alwai ha logarithm height and therefor all of the support oper run in logarithm time .
the problem is we've got to pai the piper .
whenev we have a oper that modifi the data structur , it potenti destroi on or more of the invari , and we have to then restor that invari .
without do too much work .
now amongst all of the support oper there ar onli two that modifi the data structur insert and delet .
so from thirti thousand feet the approach to implement insert and delet is to just implement them as if it's a normal binari search tree as if we didn't have to worri about these invari and then if an invari is broken we try to fix it with minim work and two tool that we have our dispos to try to restor an invari ar first of all .
recolor , flip the color of node from to black and second of all left and right rotat as cover in the previou video .
my plan is to discuss the insert oper not in full detail but i'll tell you about all of the kei idea .
now delet you got to rememb that even in a regular binari search tree delet is not that trivial and in a red black tree it down right pain .
so , that i'm not go to discuss onto for you to text book or onlin resourc to learn more about delet .
so here's how insert is go to work .
so suppos we have some new node with the kei x .
and we're insert it into a red black tree .
so we first just forget about the invari , and we insert it as usual .
and rememb , that's easi .
all we do is follow left and right shot pointer , until we fall off the end of the tree until we get to a null pointer , and we instal thi new node with kei x , where we fell off the tree .
that make x a leaf in thi binari search tree .
let's let y denot x's parent , after it get insert .
now in a red black tree everi node ha a color .
it's either red or black .
so we have a decis to make .
we just ad thi new node with kei x and we gotta make get either red or black .
and we're sort of between a rock and a hard place , whichev color we make it we have the potenti of destroi on of the invari .
specif , suppos we color it red .
well rememb what the third invari sai , it sai you cannot have two red in a row .
so if y , x's new parent is alreadi red , then when we color x red , we have <num> red in a row .
and we've broken invari number <num> .
on the other hand , if we color thi new node , x , black , we've introduc a new black node to certain root null path in thi tree .
and rememb , the 4th invari insist , that all the root null path have exactli the same number of black .
note , so by ad a black note to some but not all of the path , we're in gener , go to destroi that invari , if we color x black .
so what we're go to do is , we're go to choos the lesser of two evil , and in thi context the lesser of the two evil is to color x red .
again , we might destroi the third invari , we'll just deal with the consequ later .
so why you ask , is color x red and destroi the third invari , the lesser of two evil ?
well , intuit , it's becaus thi invari violat is local .
the flaw in our not quit red black tree is small and manag , it's just a singl doubl red and we know exactli the word is it's x and y .
so . thi sort of more hope in squash it with minim work .
i can't trust if we coat x black then we violat thi much more global type of properti involv all of the rout in all path and that's a much more intimid violat to try to fix .
then just as local on of have a doubl red between x and it's parent .
inde some of the time we'll just get lucki and it will just so happen that x is parent y is color black and then we're golden .
thi new node x that's color red , it doesn't creat a doubl red , there's no other violat of the other invari and so boom , we've got a new red black tree and we can stop .
so , the tricki case then is when x's parent y is also red in thi case we do not have a red , black tree we have a doubl red and we have to do some more work to restor the third invari .
so suppos y is red .
what do we then know ?
well rememb , befor we insert x , we had a red black tree , all <num> of the invari were satisfi .
so therefor y , by virtu of be red , it could not have been the root .
it ha to have a parent .
let's call that parent w .
moreov by the third invari there wa no doubl red in thi tree befor we insert x so by virtu of y be red , it's parent w must have been black .
so , now the insert oper branch into <num> differ case and it depend on the color , on the statu of w's other child .
so in the first case we're go to assum that w's other child that is not y but the other child of w exist in it color red .
in the second case , we're go to treat when w either doesn't have a second child .
y is it onli child or when it other child is color black .
so let's recap where thing stand .
so we just insert thi new node , and it ha the kei x .
and our algorithm color thi node red .
so x is definit red .
now , if it's parent y wa black , we alreadi halt .
so we've alreadi dealt with that case .
so now , we're assum that y .
x's parent is also red , that's what's interest .
now by virtu of y be red , we know that y's parent , that is x's grandpar w , ha to be color black .
and , for case two of insert , we ar assum that w ha a second child , call it z , and that z is color red .
so , how ar we go to quash thi doubl red problem ?
we again , we have <num> tool at our dispos .
on is to re color node .
the second is to do rotat .
so for case <num> , we're onli go to actual have to do re color .
we're not even go to have to bust out per rotat .
in particular what we're go to do is , we're go to recolor z and y black and we're go to recolor w red .
so , in some sens we take the red that ar at z and y and we consolid them at w .
the import properti of thi recov is that it doe not break the fourth invari , rememb the forth invari sai that no matter which path you take from the root to a no pointer you see exactli the same number of black node .
so why is invari still true after thi recolor , well for ani path from a rout to a no pointer which doesn't go through the vertex w it relev .
none of these node ar on that path , so the number of black dot is exactli the same .
so think about a path which doe go through w .
well if it goe through w to get to a no pointer ha to go through exactli on of z or y .
so befor we did the recolor thi path pick up a black node via w and it did not pick up a black node via z or y both of those were red .
now ani such path doe not pick up a black node w that's now red but it doe pick up exactli on black node either z or y .
so , for everi singl path in the tree , the number of black node it contain is exactli the same befor or after thi recolor , therefor sinc the fourth invari held previous , it also hold after thi recolor .
the other great thing is that it seem like we've made progress on restor the third invari .
the properti that we don't want ani doubl red at all in the entir tree .
rememb , befor we did thi recolor , we onli had a singl doubl red .
it involv x and y .
we just recod y from red to black .
so certainli we no longer have a doubl rede wall x and y and that wa the onli on in the tree .
so ar we done , do we now have a bonafi red black tree ?
well the answer depend , and it depend on the core of w's parent .
so rememb w just got recolor from black to red .
so there's now a possibl that w be thi new red node particip in some new doubl red violat .
now w's children , z and y , ar black .
so those certainli can't be doubl red .
but w also ha some parent , and if w's parent is red , then we get a doubl red involv w and it parent .
of cours , if w's parent wa black , then we're good to go .
we don't get a doubl red by recolor doubl .
w red , so we have no w red in the tree , and we can just stop .
summar , thi recolor preserv the fourth invari , and either it restor the third invari , or if it fail to restor the third invari , at least it propag the doubl red violat upward into the tree , closer to to the root . .
we're perfectli happi with the progress repres by propag the doubl red upward .
why ?
well , befor we insert thi new object x , we had a red black tree .
and we know red black tree have logarithm height .
so the number of time that you can propag thi doubl red upward is bound abov by the height of the tree , which is onli logarithm .
so we can onli visit case <num> a logarithm number of time befor thi w is propag all the wai to the top of the tree , all the wai of the root .
so we ar not quit done , the on final detail is what happen when thi recolor procedur actual recolor the root .
so , you could for exampl look at thi green pictur on the right side and ask , well what if w is actual the root of thi red black tree and we just recolor it red ?
now notic in that situat where the , we ar deal with the root of the tree we're not go to have a doubl red problem .
so invari three is inde restor when we get to the top of the tree , but we have a violat of invari number two which state that the root must alwai be black .
well if we find ourselv in thi situat , there's actual a super simpl fix which is thi red root , we just recolor it black .
now clearli that's not go to introduc ani new doubl red .
the worri instead is that it break invari four .
but , the special properti of the root for text is that it a li exactli onc on everi rout on all path .
so if we flip the color of the roof from red to black it increas the number of black node on everi singl routin path by exactli <num> .
so if thei all have the same number of black node befor , thei'll have the same number of black node now , after the recolor .
that complet case <num> of how insert work .
let's move on to case <num> .
so case <num> get trigger when we have a doubl red and the deeper node of thi doubl red pair , call it x , it uncl , that is if it ha grandpar w , parent y and w's other child , other than y either .
doesn't exist or if it exist it's label it's color black .
that is case <num> .
i want to emphas you might find yourself in case <num> right awai when you insert thi new object x it might be there immedi it ha some uncl which is cover x or it might be that if alreadi visit case <num> a bunch of time propag thi doubl red up the tree and now at some point .
the deeper red node x ha a black uncl .
either wai , as soon as that happen , you trigger case <num> .
well it turn out , case <num> is great in the sens that , with nearli constant work , you can restor in variant number <num> and get rid of the doubl red without break ani of the other invari .
you do have to put to us both of the tool we have avail in gener .
both recolor and rotat , left and right rotat , as we discuss in the previou video .
but , if you do just a constant number of each , recolor and rotat , you can get all four of the invari simultan .
there ar unfortun a coupl of sub case depend on exactli the relationship between x , y , z , and w .
for that reason i'm not go to spell out all the detail here , check out a textbook if you're interest , or , even better , work it out for yourself .
now that i've told you that two to three rotat plu some recolor is alwai suffici in case two to restor all of the in varianc , follow your nose and figur out how it can be done .
so let's summar everyth that we've said about how insert work in a red black tree .
so , you have your new node with kei x , you insert it as usual .
so you make it a leaf , you tent color it red .
if it's parent is black , your done .
you have a red black tree , and you can stop .
in gener , the interest case is thi new .
and you know that x's parent is red .
that give you a doubl red of violat of invari three .
now , what happen is you visit thi case <num> , propag thi doubl red upward imageri .
thi upward propag process can termin in on of three wai .
first of all , you might get lucki and at some point the doubl red doesn't propag , you do the recolor in case <num> .
and it just so happen you don't get a new doubl red .
at that point you have a red black tree and you can stop .
the second thing that can happen is the doubl red propag can make it all the wai to the root of the tree , then you can just recolor the root black and you can stop with all of the invari satisfi .
altern at some point when you're do thi upward propag you might find yourself in case <num> as wa discuss on thi slide .
where the lower red node on the doubl red pair x ha a black or non exist uncl , z .
in that case , with constant time , you can restor all of the fourier theori .
so the work done overal is domin by the number of doubl red propag you might have to do , that's bound by the height of thi tree and that's bound by o of log n .
so in all of the case you restor all <num> invari , you do onli a logarithm amount of work , so that give you a logarithm insert oper for red black tree , as promis .
in thi video we'll begin our discuss of hash tabl ; we'll focu first on the support oper , and on some of the canon applic .
so hash tabl ar insan us .
if you want to be a seriou programm or a comput scientist you realli have no choic but to learn about hash tabl .
i'm sure mani of you have us them in your own program in the past in fact .
now on the on hand what's funni is thei don't actual do that mani thing in term of the number of support oper , but what thei do , do thei do realli , realli well .
so what is a hash tabl ?
well conceptu , ignor all of the aspect of the implement , you mai wanna think of a hash tabl as an arrai .
so on thing that arrai do super well is support immedi random access .
so if you're wonder what's the posit number seventeen of some arrai , boom , with a coupl of machin instruct you can find out , wanna chang the content of posit number <num> in some arrai ?
done , in constant time .
so let's think about an applic in which you want to rememb your friend phone number .
so if you're lucki your friend parent were all u nu , unusu unimagin peopl and all of your friend name ar integ let's sai between on and <num> , <num> .
so if thi is the case then you can just maintain an arrai of link <num> , <num> .
and to store the phone number of sai , your best friend , <num> , you can just us posit <num> of thi modest size arrai .
so thi arrai base solut would work great , even if your friend chang over time , you gain some here you lose some there , as long as all your friend name happen to be integ between <num> <num> , <num> .
now , of cours , your friend have more interest name alic , bob , carol , whatev .
and last name as well .
so in princip you could have an arrai with on posit in the arrai for everi conceiv name you might encount , with at least <num> letter set .
but of cours thi arrai would be wai too big .
it would be someth like <num> rais to the thirtieth power and you could never implement it .
so what you'd realli want is you'd want an arrai of reason size , sai , you know ballpark the number of friend that you'd ever have , so sai in the thousand or someth , where it's posit ar index not by the number , not integ .
between on and <num> , <num> , but rather by your friend name and what you'd like to do is you'd like to have random access to thi arrai base on your friend's name .
so you just look up the quot unquot alic posit of thi arrai and .
boom , there would be alic's phone number in constant time .
and thi , on a conceptu level is basic what a hash tabl , can do for you .
so there's a lot of magic happen under the hood of a hash tabl and that's someth we'll discuss to some extent in other video .
so you have to have thi map between the kei that you care about , like your friend' name , and , numer posit of some arrai .
that's done by what's call a hash function , but properli implement , thi is the kind of function that hash tabl give you , so like an arrai with it posit index by the kei that you're store .
so you can think of the purpos of the hash tabl as to maintain a possibl evolv set of stuff .
where of cours the set of thing that you're maintain , you know , will vari with the applic .
it can be ani number of thing .
so if you're run an e commerc websit , mayb you're keep track of transact .
you know , again , mayb you're keep track of peopl , like for exampl , your friend and variou data about them .
so mayb you're keep track of i p address , for exampl if you wanna know , who wa , were there uniqu visitor to your websit .
and so on .
so a littl bit more formal , you know , the basic oper , you need to be abl to insert stuff into a hash tabl .
in mani , but not all applic , you need to be abl to delet stuff as well .
and typic the most import oper is look up .
and for all these three oper you do it in a kei base wai .
where as usual a kei should just be a uniqu identifi for the record that you're concern with .
so , for exampl , for employe you might be us social secur number .
for transact you might have a transact id number .
and then ip address could act as their own kei .
and so sometim all you're do is keep track of the kei themselv .
so , for exampl , in ip address , mayb you just want to rememb a list of ip address .
you don't actual have ani associ data but in mani applic , you know , along with the kei , is a bunch of other stuff .
so along with the employe's social secur number , you gotta rememb a bunch of other data about that employe .
but when you do the insert , when you do the delet , when you do the look up , you do it base .
on thi kei , and then for exampl , on look up you feed the kei into the hash tabl and the hash tabl will spit back out all of the data associ with that kei .
we sometim hear peopl refer to data structur that support these oper as a dictionari .
so the main thing the hash tabl is meant to support is look up in the spirit of a dictionari .
i find that terminolog a littl mislead actual .
you know , most dictionari that you'll find ar in alphabet order .
so thei'll support someth like binari search .
and i want to emphasi someth a hash tabl doe not do is maintain an order on the element that it support .
so if you're store stuff and you do want to have order base oper , you wanna find the minimum or the maximum , or someth like that , a hash tabl's probabl not the right data structur .
you want someth more .
you wanna look at a heap or you wanna look at a , a search tree .
but for applic in which all you have to do is basic look stuff up you gotta , you gotta know what's there and what's not , then there should be a light bulb that goe off in your head .
and you can sai , let me consid a hash tabl , that's probabl the perfect data structur for thi applic .
now , look at thi menu support oper , you mai be left kinda unimpress .
all right , so a hash tabl , in some sens , doesn't do that mani thing ; but again , what it doe , it doe realli , realli well .
so , to first order .
what hash tabl give you is the follow amaz guarante .
all of these oper run in constant time .
and again thi is in the spirit of think of a hash tabl as just like an arrai .
where it posit ar conveni index by your kei , so just like an arrai support random access in constant time , you can see if , you know , there's anyth in the arrai posit , and what it is .
as similarli a hash tabl will let you look up base on the kei in constant time .
so what is the fine print ?
well , there's basic two caveat .
so the first thing is that hash tabl ar easi to implement badli .
and if you implement them badli you will not get thi guarante .
so thi guarante is for properli implement hash tabl .
now , of cours if you're just us a hash tabl from a well known librari , it's probabl a pretti good assumpt that it's properli implement .
you'd hope .
but in the event that you're forc to come up with your own hash tabl and your own hash function and unlik mani of the other data structur we'll talk about , some of you probabl will have to do that at some point in your career .
then you'll get thi guarante onli if you implement it well .
and we'll talk about exactli what that mean in other video .
so the second caveat is that , unlik most of the problem that we've solv in thi cours , hash tabl don't enjoi worst case guarante .
you cannot sai for a given hash tabl that for everi possibl data set you're gonna get cost and time .
what's true is that for non patholog data , you will get cost and time oper in a properli implement hash tabl .
so we'll talk about both of these issu a bit more in other video , but for now just high order bit ar , you know , hash tabl , constant time perform , subject to a coupl of caveat .
so now that i've cover the oper that hash tabl support and the recommend wai to think about them , let's turn our attent to some applic .
all of these applic ar gonna be in some sens , you know , kinda trivial us of hash tabl , but thei're also all realli practic .
these come up all the time .
so the first applic we'll discuss , which again is a conic on , is remov duplic from a bunch of stuff , also known as the dedupl problem .
so in the de duplic problem , the input is essenti a stream of object .
where , when i sai a stream i have kinda , you know two differ thing in mind as canon exampl .
so first of all you can imagin you have a huge file .
so you have , you know , a log of everyth that happen on some websit you're run .
or all of the transact that were made in a store on some dai , and you do a pass through thi huge file .
so you're just in the middl of some outer for loop go line by line through thi massiv file .
the other exampl of a stream that i had in mind , is , where you're get new data over time .
so here , you might imagin that you're run softwar to be deploi on an internet router .
and data packet ar come through thi router at a constant extrem fast rate .
and so you might be look at , sai , the ip address and the sender , and us your data packet which is go through your router .
so it would be anoth exampl of a stream of object .
and now , what do you gotta do ?
what you gotta do is you gotta ignor the duplic .
so rememb just the distinct object that you see in thi stream .
and i hope you find it easi to imagin why you might want to do thi task in variou applic .
so , for exampl , if you're run a websit you might want to keep track of the distinct visitor that you ever saw in a given dai or a given week .
if you're do someth like a web crawl , you might want to identifi duplic document and onli rememb them onc .
so , for exampl , it would be annoi if in search result both the top link and the second link both led to ident page at differ url , okai , so search engin obvious want to avoid that , so you want to detect duplic web page and onli report uniqu on .
and the solut us a hash tabl is laughabl simpl .
so everi time a new object arriv in the stream , you look it up .
if it ? s there , then it ? s a duplic and you ignor it .
if it ? s not there , then thi is a new object and you rememb it .
qed , that's it .
and so then after the string complet , so for exampl after you finish read some huge file , if you just want to report all of the uniqu object , hash tabl gener support a linear scan through them and you can just report all of the distinct object when thi stream finish .
so let's move on to a second applic slightli less trivial mayb but still quit easi , and thi is the subject of program project number five .
so thi is a problem call the two sum problem .
you're given as input an arrai of n number .
these imag ar in no particular order .
you're also given a target sum , which i'll call t .
and what you want to know is ar there two integ from amongst these n you ar given that sum to t .
now the most obviou and naiv wai to solv thi problem is just to go over all n , choos two pair of integ in the input , and check each on separ .
so that's clearli a quadrat time algorithm .
but now , of cours , we need to ask , can we do better ?
and , ye , we can .
and first of all let's see what you'd do if you couldn't us ani data structur .
so if you were clever , but you didn't us ani data structur like a hash tabl , here would be a reason improv over the naiv on .
so the first step of a better solut is to sort a upfront , for exampl , us word sort or heap sort , someth that run in end log and time .
so you mai be ask about the motiv for sort .
well , again , you know , on thing is just , you know whenev you're try to do better than n squar ; you might think that sort your data somehow help .
right and you can sort of do it almost for free in n log n time .
now , why would sort the arrai up front help us ?
well , then the clever insight is that for each entri of the arrai a , sai the first entri , now we know what we're look for to achiev thi given target , right .
if the target that we're try to get to is sum to <num> and the first entri in the sort arrai is <num> , then we know we're look for a <num> somewher els in .
thi now sort arrai .
and we know that search a sort arrai is pretti easi , right .
that just binari search .
that just take logarithm time .
so for each of the n arrai entri , we can look for a complementari .
entri , name of reach x we can look for t x us binari search .
and to us binari search take log n time .
so the sort upfront speed up thi entir batch of n search .
so that's why it's a win .
so , in the second step , becaus we do a linear number of binari search , again , thi is just n , the number of search , time log n , the time per search .
so , thi is just anoth theta of n log n factor .
all right , so that's pretti cool .
you , i don't think you could come up with thi n log n solut without have some basic , facil with algorithm .
thi is alreadi a realli nice improv over the naiv n squar .
but we can do even better .
it is no reason we're stuck with an n log n lower bound for the problem .
obvious , becaus the arrai is unsort , we have to look at all the integ .
so we're not gonna do better than linear time .
but we can do linear time via a hash tabl .
so a good question you might ask at thi point is what's the clue about thi problem , about thi task that suggest we want to us a hash tabl .
well , so hash tabl ar go to dramat speed up ani applic where the bulk of the word is just repeat look up .
and if we examin thi n log n solut , onc we have thi idea of do a search for t minu x for each valu of x , we realiz actual , you know , the onli thing we need the sort arrai for wa to support look up .
that's all binari search here is do , is just look stuff up .
so we sai , ah ha .
all of the work here in step two is from repeat look up .
we're pai an exorbit rel , logarithm per amount of time per look up , wherea hash tabl can do them in cost and time .
so , repeat look up , ding , ding , ding , let's us a hash tabl ; and inde that's what give us linear time in thi problem .
so from the amaz guarante of hash tabl , we get the follow amaz solut for the true problem , although again thi is subject to the same fine print about you better us it properli implement hash tabl and you better not have patholog data .
so rather than sort , you just insert everyth in the arrai into a hash tabl .
so insert cost time .
so thi is gonna be linear time rather than the end log we were pai befor .
onc all the stuff is in the hash tabl , we just do the same thing as in the n log n solut .
for each x in the arrai , we look for it match element , t x in the hash tabl us the cost and time look up oper export by the hash tabl .
and of cours if for some x , you do find the match element t minu x .
then you can just report x and t minu x .
that prove that there is inde a pair of integ of target sum t .
if for everi singl element of the input arrai a , you fail to find thi match element t minu x in the hash tabl .
then , for sure there is no pair of integ in the input that sum to t .
so thi solv the problem correctli .
moreov , constant time insert , so that mean thi first step is go to be o of end time .
and constant time look up .
so that mean that the second step is also gonna be linear time .
that leav subject to the caveat that we discuss on the previou slide .
so it's kind of amaz how mani differ applic of comput scienc boil down in their essenc to repeat look up oper .
therefor , have a super fast look up oper , like that support by a hash tabl , permit these applic to scale to fantast size .
it's realli amaz , and it drive a lot of modern technolog .
so let me just mention a coupl exampl .
again , if you look around or do some research on the web , you'll quickli find mani more .
so origin what prompt research to think hard about data structur that support super fast look up , wa back when peopl were first build compil .
so thi is a long time ago .
thi is in the fifti or so .
and these repeat look up to figur out , you know , what ha and ha not been defin befor wa , wa emerg as a bottleneck in compil .
back in the earli dai of program languag .
and that wa on of the earli applic of hash tabl .
wa to support super fast look up to speed up compil time .
to keep track of the function of variabl name and thing like that .
hash tabl technolog is also super us for softwar on router in the internet .
so , for exampl , you might want to block network traffic from certain sourc .
so , for exampl , mayb you suspect that a certain ip address ha been taken over by spammer and so ani traffic come from that ip address you just want to ignor .
and you don't wanna even let it get to the end host , to the comput on someon's desktop , or to someon's mobil devic but rather insid the internet .
you wanna just drop packet that ar come certain , certain center .
so what is that problem boil down to ?
well , you might have a blacklist of ip address that you're refus traffic from and then the task face by the router is realli the look up problem .
so if data packet come in at some insan fast data rate , and when you wanna .
you immedi , just look up , is thi in the blacklist or not , and if it is in the blacklist then you drop the packet , if it ? s not , then you let it go through .
so a veri differ applic is for speed up search algorithm .
and when i sai a search algorithm , what i'm think about here is someth like a chess plai program .
so someth that doe game tree explor .
so we've alreadi talk a fair amount about graph search in thi class , but in our discuss of breadth first and depth first search , we were think about graph that you could basic write down .
you could store them in the main memori of your machin or , in the worst case , on some big cluster .
so mayb graph , you know , about the size of the web graph or possibl smaller .
but in a context of someth like a chess plai program the graph that you're interest in is wai , wai , wai bigger than the web graph .
so what's the graph we care about for a chess plai program ?
well , the node of the graph ar go to correspond to all possibl configur of chess piec on a chess board .
so everi chess board that you might ever encount in a game of chess .
so that's a .
massiv , massiv number of configur .
and you're never gonna be abl to write down these vertic .
the edg in thi graph ar go to take you from on configur to anoth .
and there gonna correspond to legal move .
so if you can move a bishop from .
on place to anoth place , and you get from on configur to anoth configur , there's an edg in the graph correspond to that move .
now you can't write down thi graph .
so you can't implement breadth versu depth versu search exactli as we discuss it befor .
but , you'd still like to do graph explor , right ?
so you'd like to have your comput program , reason about the at least short term ramif of your possibl next move .
so that will correspond to search through thi graph .
now , how ar you gonna , it's rememb graph search a realli import properti wa you don't want to do redund work , you don't want to re explor thing you've alreadi explor .
that would be silli and might lead into loop and so on .
and you can't write down the graph just rememb where you've been , is suddenli a non trivial problem ; but what is rememb where you've been , fundament ?
fundament that's a look up oper .
so that is exactli what hash tabl ar for .
so to be a littl more concret , you know , on where you us the hash tabl in , sai , a chess plai program , is you'd stake , take the initi configur .
you would sort of imagin try all possibl move from thi configur .
and then you'd try , you'd sort of have all move from your oppon and then you'd have all your move in respons .
and you would alwai rememb , as you were do thi reason , everi chessboard configur you'd ever look at befor and you'd stick in the hash tabl .
and befor you go explor some configur , you'd look it up in your hash tabl to see if you've alreadi explor it in the past .
and if you have , you don't bother .
you've alreadi seen it .
all right .
so chess plai program oper by explor systemat as mani configur as thei'd have time for .
you know , obvious , in a budget of three minut or whatev you don't wanna wast ani work explor ani given configur more than onc .
how do you rememb where you've been ?
well everyth you've explor you stick in a hash tabl befor you explor a configur you look it up in a hash tabl and see if you've alreadi done it .
so these of cours ar just scratch the surfac .
i just want to highlight a coupl , you know , fairli differ look applic , you know to convinc you that hash tabl come up all the time .
and the reason thei come up all the time is becaus you know the need for fast look up come up all the time .
it's kind of amaz how much technolog is be driven just by you know repeat fast look up .
so as homework i encourag you to just sort of think about you know your own life , or think about technolog out there in the world , and come up with some .
you know , guess about where probabl hash tabl ar make someth out there run blazingli fast .
i think it won't take you more than a few minut to come up with some good exampl .
so in thi video we'll take a peek under the hood of hash function .
and i'll discuss some of the high level principl by which their implement .
so let's briefli review the raison d'etr of a hash tabl .
so the purpos in life for a hash tabl is to support super fast lookup .
so mayb you're keep track of the transact that happen on your websit yesterdai .
mayb you're keep track of your employe ; mayb you're keep track of ip address in an internet router .
mayb you're keep track of chess configur in a , in a chess plai program , whatev , the point is , you want to be abl to insert stuff into a hash tabl , and later rememb whether someth's there or whether someth's not there .
so the implement we'll discuss will gener also support delet .
but that's pretti much it .
it's a veri restrict set of oper .
but the hash , it wa go to execut them at veri , veri well .
so , basic in constant time , again subject to some fine print , which we'll discuss a littl bit in thi video but , then more deepli in a separ option video .
so , the two caveat ar first of all the hash tabl had better be properli implement .
it's actual pretti easi to screw up a hash tabl to screw up hash function .
we'll talk a bit about that in a few minut and , then , also , the data should , in some sens , be non patholog , and that , we will discuss more deepli in a separ video .
all right , so let me give you an initi glimps of some of the magic that's happen under the hood in hash function .
so , at first let me sai exactli what the setup is .
the first step is to identifi all the thing that you might want to be store .
so , in other word , the univers of your applic , so thi would be someth like , all possibl i p address , of which there's <num> <num> .
all possibl name you might encount , perhap with a maximum of , sai , <num> charact .
all possibl configur of a chessboard , and so on , and on thing i hope you can appreci from these exampl is that , in mani case , thi univers is realli big .
soth number of ip address is , quot unquot , onli two <num> <num> .
the number of all name , you're probabl talk more like <num> rais to the <num> .
all chessboard configur i don't even wanna think about .
and what you wanna accomplish is , you wanna maintain an evolv subset of thi univers u .
so mayb you wanna keep track of all the ip address you've seen on your websit in the last <num> hour .
you wanna keep track of the phone number of all of your friend .
you wanna keep track of the chessboard configur that you've explor in the past three second , whatev .
and again i hope what is clear from the applic we've been discuss , is that the set s is usual of a reason size .
it's , it's someth you could store in main memori .
you know , it mayb it's ten of thousand of ip address .
mayb it's , you know , a few hundr name of your variou friend .
you know , mayb it's in the , you know , million of chessboard configur , but still wai , wai , wai smaller than the size of the univers .
so without data structur , you'd have to resort to other unsatisfactori solut to maintain thi set .
so the first thing you could try , as we discuss in the previou video , would be just have an arrai with on posit for everi imagin thing you might want to store in your set .
so thi is the solut that's go to work well if all of your friend happen to have name that ar integ between <num> and <num> , <num> , but doesn't scale when the univers size becom realli big , as in most of these applic .
so , the good new is , is of cours , is an arrai of and it's of cours fast random access so you can access ani posit in constant time .
so , if you have an arrai base solut index by all the element of the univers , you can do constant time , insert , delet and look up .
the bad new is , is the space requir is proport to the univers .
and again , forget about be unsatisfactori .
that's just literari imposs .
infeas in mani applic in which you'd us hash tabl .
now of cours to get the memori proport to the size of the set stuff that you're store , an easi solut would just be to us a list .
you know , sai a doubli link list .
someth like that .
now with a list base solut the good new is , is your memori is certainli proport to the size of the set that you're store , and independ of the size of the univers from which these element ar drawn .
the bad new is that to figur out whether someth is , or is not , in a list you gener have to travers through most of that list .
and that's gonna take up time proport to the length of the list .
so , realli the question we're face in implement cach tabl is , can we get the best of both world , of these two naiv solut .
and the on hand , we want to have the constant time oper enjoi by the arrai base solut .
but on the other hand , we wanna have the , linear space in the size of the set that we're store ; that we get in the list base solut .
so to get the best of both world , we ar go to us an arrai base solut .
but the arrai will not be big .
it'll not be with size proport to the univers .
the arrai will onli have size , you know , roughli the same as the set that we're store , so somewher in the ball park of the cardin of s .
so the first thing we do is we decid on how big we want our arrai to be .
so that , that length is gonna be call n .
we're gonna have an arrai of length n .
and n is gonna be in the ballpark of the size of s .
it's gonna depend on a few thing .
exactli how n compar to s , but for now think of n as like doubl the size of s .
we're gonna be call each entri of the arrai a bucket , so there's n bucket , and then , the size of s is about <num> percent of the number of bucket , let's sai .
so on object you might legitim rais at thi point is , you know i thought , i said the set wa dynam .
the set s .
right ?
stuff can be ad , stuff can be delet .
so the size isn't alwai the same .
it can fluctuat over time .
so what doe it mean to defin an arrai which is the , roughli the same length as thi chang set .
so for simplic , for the purpos of thi video to focu on the kei point i am go to assum that the set size s .
while s itself can be chang , i'm go to assum that the size of s doesn't fluctuat too much .
so there ar addit bell and whistl you can add to a hash tabl implement , and thei're all quit natur .
i think most of you could probabl figur them out on your own , to deal with the fact that s might be chang size .
so for exampl , you can just keep track of how mani element ar in your hash tabl .
and when it exce a big , a certain threshold , so when it's too big rel to the size of your arrai , you just doubl the arrai .
and then you reinsert all of the element into thi new doubl arrai .
similarli , if you want to , if the set shrink , you can have trick for shrink the arrai dynam as well .
so i'm not gonna discuss these bell and whistl for resiz your hash tabl dynam .
thei ar , of cours import for a real implement , and thei ar part of the implement in the standard program librari .
but i view those as sort of a , a second order point in the implement of a hash tabl .
and i wanna focu on the first order point , in thi video .
so , summar , think of the set s .
there ar insert and delet we have to accommod .
but , you know , s is gonna be roughli the same size .
and the number of bucket will be , you know , within a constant factor of the size of the set .
all right so there we have our arrai with total reason space , space proport to the size of the set that we ar store .
and now what we want is we want is some wai of translat between the thing that we care about , sai our friend name or whatev the element in the univers ar to the posit in thi arrai .
so the object respons for that translat from kei drawn from thi univers to posit in thi arrai is call a hash function .
so formal , a hash function take as input a kei .
so thi is gonna be an ip address or the name of somebodi or a chessboard configur or whatev .
and it's go to spit out an posit in thi arrai .
so i'm gonna label the arrai entri from <num> to n <num> for thi lectur .
obvious at the moment thi is super underspecifi .
there's a zillion function you could choos .
which on you us , we'll talk about that , but for now there's just gonna be some hash function map from element of the univers to bucket , to posit in thi arrai .
now , as far as the semant of thi hash function , what the hash function is do , it's tell us in which posit we should store a given kei from the univers .
so , if we have some new friend name alic .
and we run alic , we kei alic through the hash function and it give us a <num> .
it sai we should store alic's phone number in posit <num> of the arrai .
if we have some crazi chessboard configur , we feed it into a hash function and it spit out <num> , it sai we should rememb thi chessboard configur in the 172nd bucket of thi arrai .
so again , given x , which is some kei from thi univers , we invok a hash function to get a posit in thi arrai , to get a bucket .
and then that is where we try to store thi x and ani associ data with it .
so that's the high level idea of how you implement a hash tabl , but we're quit far from done , and in particular there is a seriou issu , that we're go to have to deal with , that's fundament to implement hash tabl , and that's the notion of a collis .
so probabl mani of you mai have alreadi notic that thi problem might occur .
which is well what happen if we're store our friend's phone number , and you know alic show up and we ask our hash function where to store alic's phone number , and it sai oh bucket number <num> , and then our friend bob show up , and we ask our hash function where to store bob's phone number , and what if the hash function also sai bucket number <num> for bob ?
what do we put in bucket at <num> ?
do we put alic there , do we put bob there , do we put them both there ?
how do we deal with these so call collis ?
so , the next quiz is meant to give , to get you think about collis , and in some sens , how truli unavoid thei realli ar .
, all right .
so the correct answer to thi question is the first answer , believ it or not .
all you need is <num> peopl in a room befor you're equal like to have two peopl with the same birthdai as not .
so if you're look to , to skim a littl monei off of your non mathemat friend , thi is on wai you can do it .
go to cocktail parti with about <num> peopl and place bet with peopl that there ar two peopl in the room with the same birthdai .
so if you have <num> peopl , well there's onli <num> distinct birthdai , i'm count februari 29th here as on of them .
so by the pigeonhol principl , certainli the probabl is <num> .
by the time you get to <num> .
now , by the time you're at <num> .
you're alreadi at <num> .
so you alreadi have overwhelm probabl to have a duplic birthdai with <num> peopl .
so of cours , with <num> you're gonna be almost at <num> , <num> . <num> .
who know ?
some larg number of <num>'s , and at <num> , you're at <num> .
so mani peopl find thi quit counter intuit that you onli need <num> peopl to get a duplic birthdai on averag .
and so thi is a , thi is a quit famou exampl and it sometim goe by the birthdai paradox .
call it a paradox is sort of a misnom .
a paradox , you know , often suggest some kind of logic inconsist .
there's no logic inconsist here .
it's just that peopl's brain ar not realli wire to have thi intuit , for whatev reason .
so , but it's realli just math .
you can work out the math , and , and , and you can just solv it .
so , more gener , the principl behind the birthdai paradox is the follow .
so suppos you have a calendar , perhap on some differ planet , which ha k dai .
where each , everybodi's equal like to have each of the k dai as their birthdai .
then it's about the squar root of k peopl that you need in a room befor you're equal like to have a duplic , or not have a duplic .
okai , and the reason that you get the squar root effect is becaus if you think about it .
there's a quadrat number of pair of peopl in the room , so that's a quadrat , and the number of peopl opportun to have a duplic .
right ?
so , each pair of peopl could be a duplic , there's a quadrat number of pair .
and so , that's why , onc the number of pair start reach about the number of differ dai , you're , you're about , you're like to see a duplic around that point .
so you might be wonder why i'm tell you about the birthdai paradox in the middl of a lectur about hash , but realli it's quit relev .
so imagin for exampl you defin a hash function in the follow wai .
now to be clear , thi is not a practic hash function , but just for the purpos of discuss , imagin you have a hash function which randomli assign everi singl kei to a uniform bucket .
'kai , so for each , each of the <num> n bucket equal like .
then what the birthdai paradox sai is , even for a veri small dataset , you ar alreadi gonna have a pair of thing collid .
all right , so if you have an n bucket , so mayb your n is like , <num> , <num> , all you need is roughli <num> element in your data set , and despit the fact that the tabl is onli go to be on percent full , you're alreadi go to see a collis , okai ?
so <num> percent of them ar empti , but you're go to have on bucket that ha two , so that's sort of annoi .
so the birthdai paradox sai , you start get collis with the hash function , even with the realli tini data set .
so in thi sens , if you're go to have hash tabl , you've got to deal with collis .
there's go to be a fair number of them , and you need some method for resolv them so , collis ar a fact of life when you're talk about hash .
where again , by collis , what i mean is two differ kei .
so two differ element x and y from the univers that hash to the same bucket , who have the same hash valu , so in gener we can think of a hash function as do a compress of sort .
so we have a huge univers u and we have thi veri modest size arrai a with the onli n bucket .
where n , we're think of as be much , much , much smaller than u .
so , of cours , thi hash function ha to map variou element of u to the same bucket .
so what ar we gonna do about it ?
how ar we go to resolv these collis ?
well , there's two differ solut which ar both quit preval in practic .
so solut number on is call chain , or sometim you'll also see it call separ chain .
and thi is a veri natur solut ; it's also the on that's rel easi to analyz mathemat .
what you do is just for element that hash to the same bucket , you just revert to the list base solut that we talk about in a previou slide .
so , each of the n bucket will not necessarili contain just mere <num> or <num> element , it will contain a list within a principl unbound number of element .
okai , so when we us chain , it's done quit straight forward to figur out how to implement all of the hash tabl oper , name , insert , delet and look up , you just hash someth to the appropri bucket and then you just do insert , delet or look up , as appropri , in the list that's in that bucket .
so just to make clear that everyth is type check , so here h x , thi is the bucket for x .
that's what's specifi by the hash function .
and then , in the h x posit of thi arrai a , in the h x , the bucket is where we find the link list that is go to contain x .
so just to give a cartoon exampl , if you had , sai , four bucket , mayb , you know , the first bucket ha exactli on record .
correspond to alic , mayb the second bucket just ha a null pointer .
no on's been insert in the second bucket .
and then the third bucket we have , let's sai , both bob as well as daniel .
and then mayb in the fourth bucket we have carol .
okai , so becaus we have a collis between bob and daniel , both map to the third bucket , and we resolv that just by have a link list , with bob and daniel in some order .
so the second solut which is trickier to talk about mathemat but still quit import practic is call open address .
and the princip in open address is you're not go to us ani space for pointer .
you're not gonna have list .
so you're onli gonna have on object per bucket of the arrai .
so anoth question is what happen if , you know , you try and insert daniel and you go , you invok the hash function on daniel and it take you to a bucket that alreadi contain bob ?
that mean there's no room for daniel .
so what you're go to do is you're gonna probe the hash tabl in some other posit .
so a hash function is , is now gonna be replac by a hash sequenc , where you try , the hash function tell you the first bucket to try to insert daniel ; fail that , a second bucket in which to try to insert daniel ; fail that , a third bucket to try to insert daniel ; and so on .
and you just keep try till you find an open posit somewher in the arrai .
so there's variou strategi for try to figur out the probe sequenc .
on strategi is if you fail and save bucket <num> , which is where the hash function tell you to go first .
you just try bucket <num> , then <num> , then , <num> , then <num> and so on , until you find your first open slot .
so that's call linear probe .
and anoth approach is doubl hash .
so thi is a solut where you actual have two hash function , hash function <num> and hash function <num> .
and the idea is , suppos you're try to insert , sai , daniel , into a hash tabl with open address , and you evalu both of the hash function .
and the first on come up <num> , and the fir , the second on come up <num> .
so , as usual , the ha , first hash function will specifi where you look first .
so if it evalu on daniel to <num> , you look in the seventeenth posit of the arrai , and if , if it's empti , that's where you insert daniel .
now , if it's full , what you do is you us the second hash valu to be an addit shift , so .
unlik linear probe where after seventeen , you look at eighteen .
with doubl hash , if the second hash function give you <num> , that's gonna be your offset .
so after seventeen , you look at bucket <num> .
if <num> is alreadi full , you look at bucket <num> .
if bucket <num> is alreadi full then you look at bucket <num> .
so you keep ad increment of <num> until you final find a bucket where , that's empti and that's where you insert daniel .
now of cours , if you try to insert some other name , if you try to insert elizabeth , you're gonna get two total differ number in gener .
so mayb you'll get <num> and <num> , and so here the probe sequenc will be <num> , fail that <num> , fail that <num> fail that <num> and so on , so a question you should have at thi point , is , you know .
i've told you two solut to resolv collis in a hash tabl .
and you're probabl ask , well , which on should you us if you have to implement your own hash tabl ?
and , you know , as usual , if i present you with two differ solut for the same problem .
you can probabl rest assur that neither on domin the other , right ?
otherwis i wouldn't wast your time by present both of them to you .
so , sometim chain's gonna perform better , and sometim , open address's gonna perform better .
and of cours , it also depend on what kind of metric that you care about .
so there ar a coupl of rule of thumb that i can tell you .
so first of all if space is at a real premium you might want to consid open address instead of chain , and that's caus with chain you do have thi excess , not huge but you do have thi littl bit of space overhead and deal with all these pointer in thi link list .
so if you want to avoid that , you might want to think about open address .
the second rule of thumb is delet is trickier with open address than with chain , but delet is clearli not difficult at all , either to code or understand when you us chain caus it just reduc chain to a link list which of cours you all know how to do .
open address is , it's not imposs to implement delet but it's much trickier .
so if delet's a , a crucial oper for you , that might steer you toward think about chain .
but ultim , if it's realli kinda mission critic code , probabl the best thing to do is implement both kind of solut and just see which on work better .
it's a littl hard to predict how thei're gonna interact with memori hierarchi and that kind of thing .
thei're both us in their own context .
all right so we've cover the two most preval wai of handl collis .
and we argu that collis ar inevit no matter you design you hash function .
you're stuck with collis and you can do chain or link list per bucket , or you can do address , where you actual have a probe sequenc in order of which you look at bucket until you find an empti on .
and the eleph in the room at thi point is , you know , what is thi hash function ?
i have told you noth about hash function .
all i told you is there is some map from the set of the univers , so ip address , or name , or whatev to a bucket number .
well what kind of function should you us ?
excel question , ton of research on that question , and to thi dai as much art as scienc .
but let's start talk about it .
let's begin by build up some intuit about what we would want from a hash function , now that we know how hash function ar usual implement .
so let's start with a hash function which is implement by chain .
so what's go to be the run time of our lookup , insert , and delet oper in a hash tabl with chain ?
well , the happi oper in a hash tabl with chain is insert .
insert , we can just sai without ani qualif , is constant time .
thi requir the sort of obviou optim that when you do insert , you insert someth at the front of the list in it bucket .
like , there's no reason to insert at the end .
that would be silli .
so the plot thicken when we think about the other two oper , delet and the lookup .
so let's just think about lookup .
delet's basic the same .
so how do we implement lookup ?
well , rememb when we get some kei x , we invok the hash function .
we call h x .
that tell us what bucket to look in .
so if it tell us <num> , we know that , you know , x mai or mai not be in the hash tabl .
but at thi point we know that if it's in the hash tabl , it's got to be in the link list that's in the seventeenth bucket .
so now we descend into thi bucket .
we find ourselv a link list .
and now , we have to resort to just an exhaust search through thi list in the seventeenth bucket , to see whether or not x is there .
so we know how long it take to search a regular list for some element .
it's just linear and the list length .
and now we're start to see why the hash function might matter .
right , so suppos we insert <num> object into a hash tabl with <num> bucket .
if we have a super lucki hash function , then perhap each bucket will get it own object .
there'll be on object in each of the list , in each of the bucket , so .
theta of the list length is just theta of on .
we're do great .
okai ?
so , a constant , constant link to list , mean constant time insert delet .
a realli stupid hash function would map everi singl object to bucket number zero .
okai , so then if you insert <num> object , thei're all in bucket number zero .
the other <num> bucket ar empti .
and so everi time you do insert or delet , it's just resort to the naiv link list solut .
and the run time is go to be linear and the number of object in the hash tabl .
so the largest list length could vari anywher from m n , where m is the number of object , thi is when you have equal link list , to if you us thi ridicul constant hash function , m , all the object in the same list .
and so the main point i'm try to make here , is that you know , first of all , at least with chain , where the run time is govern by the list length , the run time depend crucial on the hash function .
how well the hash function distribut the data across the differ bucket .
and someth analog is true for hash tabl that us open address .
all right so here there aren't ani list .
so you don't , there's no link list to keep track of .
so the run time is go to be govern by the length of the probe sequenc .
so the question is how mani time do you have to look at differ bucket in the hash tabl befor you either find the thing you're look for , or if you're do insert , befor you find an empti bucket in which to insert thi new object .
so the perform is govern by the length of the probe sequenc .
and again , the probe sequenc is go to depend on the hash function .
for realli good hash function in some sens , stuff that spread data out evenli , you expect probe sequenc to be not too bad .
at least intuit , and for sai the constant function you ar go to expect these probe sequenc to grow linearli with the number of object you insert into the tabl .
so again thi point remain true , the perform of a hash tabl in either implement realli depend on what hash function you us .
so , have built up thi intuit , we can now sai what it is what we want from a hash function .
so first we want it to lead to good perform .
i'm us the chain implement as a guid exampl .
we see that if we have a size of a hash tabl , n , that's compar to the number of object , m , it would be realli cool if all of the list had a length that wa basic constant ; therefor we had our constant time oper .
so equal length list is wai better than unequ length list in a hash tabl with chain .
so we , we want the hash function to do is to spread the data out as equal as possibl amongst the differ bucket .
and someth similar is true with open address ; in some sens you want hash function to spread the data uniformli across the possibl place you might probe , as much as possibl .
and in hash , usual the gold standard for spread data out is the perform of a complet random function .
so you can imagin for each object that show up you flip some coin .
with each of the n bucket equal like , you put thi object in on of the n bucket .
and you flip independ coin for everi differ object .
so thi , you would expect , you know , becaus you're just throw dart at the bucket independ , you'd expect thi to spread the data out quit well .
but , of cours , it's not good enough just to spread data out .
it's also import that we don't have to work too hard to rememb what our hash function is and to evalu it .
rememb , everi time we do ani of these oper , an insert or a delet or a lookup , we're go to be appli our hash function to some kei x .
so everi oper includ a hash function evalu .
so if we want our oper to run a constant time , evalu the hash function also better run in constant time .
and thi second properti is why we can't actual implement complet random hash .
so there's no wai we can actual adjust when sai we want to insert alic's phone number , flip a new batch of random coin .
suppos we did .
suppos we flip some random coin and it tell us to put alic's phone number into the 39th bucket , while .
later on , we might do a lookup for alic's phone number , and we better rememb the fact that we're suppos to look in the 39th bucket for alic's phone number .
but what doe that mean ?
that mean we have to explicitli rememb what choic we made .
we have to write down .
you get a list of effect that alic is in bucket number <num> .
in everi singl insert , if thei're all from in the point of coin flip , you have to rememb all of the differ random choic independ .
and thi realli just devolv back to the naiv list base solut that we discuss befor .
so , evalu the hash function is gonna take us linear time and that defeat the purpos of a hash tabl .
so we again we want the best of both world .
we want a hash function which we can store in ideal constant space , evalu in constant time , but it should spread the data out just as well as if we had thi gold standard of complet random hash .
so i want to touch briefli on the topic of how you might design hash function .
and in particular good hash function that have the two properti we identifi on the previou slide .
but i have to warn you , if you ask ten differ , you know , seriou hardcor programm , you know , about their approach to design hash function , you're like to get ten somewhat differ answer .
so the design of hash function is a tricki topic , and , it's as much art as scienc at thi point .
despit the fact that there's a ton of scienc , there's actual a veri beauti theori , about what make good hash function .
we'll touch on a littl bit of that in a , in a differ option video .
and if you onli rememb on thing of , you know , from thi video or from these next coupl slide , the thing to rememb is the follow .
rememb that it's realli easi to inadvert design bad hash function and bad hash function lead to poor hash tabl perform .
much poorer than you would expect given the other discuss we've had in thi video .
so if you have to design your own hash function , do your homework , get some exampl , learn what other expert ar do and us your best judgment .
if you do just someth without think about it , it's quit possibl to lead to quit poor perform , much poorer than you were expect .
so to drive home thi point , suppos that you're think about kei be phone number .
so let's sai , you know , i'm gonna just be veri kinda unit state centric here .
i'm just gonna focu on the , the ten digit phone number insid the us .
so the univers size here is ten to the ten , which is quit big .
that's probabl not someth you realli want to implement explicitli and let's consid an applic where , you know , you're onli sai , keep track of at most , you know , <num> or <num> , <num> phone number or someth like that .
so we need to choos a number of bucket , let's sai we choos <num> , <num> bucket .
let's sai we're expect no more than <num> phone number or so .
so we doubl that , we get a number of bucket to be equal <num> , <num> .
and now i got to come up with a hash function .
and rememb , you know , a hash function by definit .
all it doe is map anyth in the univers to a bucket number .
so that mean it ha to take as input a ten digit phone number and spit as output some number between zero and <num> .
and , beyond that we have flexibl of how to defin thi map .
now , when you ar deal with thing that have all these digit it's veri tempt to just project on to a subset of the digit .
and , if you want a realli terribl hash function , just us the most signific digit of a phone number to defin a map from phone number to bucket .
all right , so i hope it's clear why thi is a terribl choic of a hash function .
all right , so mayb you're a compani base in the san francisco bai area .
the area code for san francisco is <num> .
so if you're store phone number from custom in your area .
you know mayb twenti , <num> , <num> percent of them ar gonna have area code <num> .
all of those ar go to hash to exactli the same bucket , bucket number <num> in thi hash tabl .
so you're gonna get an overwhelm percentag of the data map just to thi on bucket .
meanwhil you know not all <num> possibl of , of these three digit ar even legitim area code .
not all three digit number ar area code in the unit state .
so there'll be bucket of your hash tabl which ar total guarante to be empti .
so you wast a ton of space in your hash tabl , you have a huge list in the bucket correspond to <num> , you have a huge list in the bucket correspond to <num> , the area code at stanford .
you're gonna have a veri slow look up time for everyth that hash to those two bucket and there's gonna be a lot of stuff that hash to those two bucket , so terribl idea .
so a better but still mediocr hash function would be to do the same trick but us the last three digit instead of the first three digit .
thi is better than our terribl hash function becaus there aren't ridicul clump of phone number that have exactli the same last three digit .
but still , thi is sort of assum you're us thi hash function as tantamount to think that the last three digit of phone number ar uniformli distribut among all of the <num> , <num> possibl .
and realli there's no evid if that's true .
okai ?
and so there's go to be pattern and phone number that ar mayb a littl subtl to see with the nake ey , but which will be expos if you try to us a mediocr hash function like thi .
so let's look at anoth exampl .
perhap you ar keep track of object just base by where thei ar laid out in memori .
so in other word the kei for an object is just gonna be it memori locat and if these thing ar in byte , then you ar guarante that everi memori locat will be a multipl of four .
so for a second exampl let's think about a univers where the possibl kei ar the possibl memori locat , so here you're just associ object with where thei're laid in memori , and a hash function is respons for take in as input some memori locat of some object and spit out some bucket number .
now gener , becaus of , you know the structur of byte and so on , our memori locat ar go to be multipl of some power of two .
in particular , memori locat ar go to be even , and so a bad choic of a hash function .
would be to take , rememb , the hash function take the input of the memori locat , which is , you know , some possibl realli big number , and we wanna compress it , we want to output a bucket number .
now let's think of a hash tabl where we choos n equal <num> <num> , or <num> bucket .
so then the question is , you know , how is thi hash function go to take thi big number , which is the memori locat , and squeez it down to a small number .
which is on of the bucket and so let's just us the same idea as in the mediocr hash function , which is we're gonna look at the least signific bit so we can express that us the mod oper .
so let's just think about we pick the hash function h x where h is the memori locat to be x mod <num> there again , you know , the mean of <num> , <num> is that's the number of bucket we've chosen to put in our hash tabl becaus , you know , we're gonna rememb roughli at most <num> differ object .
so don't forget what the mod oper mean , thi mean you just , essenti subtract multipl of <num> , <num> until you get down to a number less than <num> , <num> .
so in thi case , it mean if you write out x base ten , then you just take the last three digit .
so in that sens , thi is the same hash function as our mediocr hash function when we were talk about phone number .
so we discuss how the kei here ar all go to be memori locat ; in particular thei'll be even number .
and here we're take their modulu with respect to an even number .
and what doe that mean ?
that mean everi singl output of thi hash function will itself be an even number .
right , you take an even number , you subtract a bunch of multipl of a <num> , you're still go to have an even number .
so thi hash function is incap of output an odd number .
so what doe that mean ?
that mean at least half of the locat in the hash tabl will be complet empti , guarante , no matter what the kei you're hash is .
and that's ridicul .
it's ridicul to have thi hash tabl <num> percent of which is guarante to be empti .
and again , what i want you to rememb , hopefulli long after thi class is complet is not so much these specif exampl , but more the gener point that i'm make .
which is , it's realli easi to design bad hash function .
and bad hash function lead to hash tabl perform much poorer than what you're probabl count on .
now that we're equip with exampl of bad hash function .
it's natur to ask about , you know , what ar some good hash function ?
well it's actual quit tricki to answer that question .
what ar the good hash function , and i'm not realli go to answer that on thi slide .
i don't promis about hash function that i'm go to tell you about right now , ar good in a veri strong sens of the word .
i will sai these ar not oblivi bad hash function , thei're let's sai , somewhat better hash function .
and in particular if you just need a hash function , and you need a quick and dirti on , you don't want to spend too much time on it .
the method that i'm go to talk about on thi slide is a common wai of do it .
on the other hand , if you're design a hash function for some realli mission critic code , you should learn more than what i'm gonna tell you about on thi slide .
so you , you should do more research about what ar the best hash function , what's the state of the art , if you have a super import hash function .
but if you just need a quick on what's , what we sai on thi slide will do in mani , in most situat .
so the design of a hash function can be thought of as two separ part .
so rememb by definit a hash function take as input someth from the univers .
an ip address , a name , whatev and spit out a bucket number .
but , it can be us to factor that into two separ part .
so first you take an object which is not intrins numer .
so , someth like s string or someth more abstract .
and you somehow turn an object into a number , possibl a veri big number .
and then you take a possibl big number and you map it to a much smaller number , name the index of a bucket .
so in some case i've seen these two step given the name like the first step is formul the hash code for an object , and then the second step is appli a compress function .
in some case , you can skip the first step .
so , for exampl , if your kei ar social secur number , thei're alreadi integ .
if thei're phone number , thei're alreadi integ .
of cours , there ar applic where the object ar not numer .
you know , for exampl , mayb thei're string , mayb you're rememb name .
and so then , the product of thi hash code basic boil down to write a subroutin that take , as input , a string , and output some possibl veri big number .
there ar standard method for do that , it's easi to find resourc to , to give you exampl code for convert string to integ you know , i'll just sai on or two sentenc about it .
so you know each charact in a string it is easi to regard as a number in variou wai .
either you know just sai it is ascii , well ascii code then you just have to aggreg all of the differ number , on number per charact into some overal number and so on thing you can do is you can iter over the charact on at a time .
you can keep a run sum .
and with each charact , you can multipli the run sum by some constant , and then add the new letter to it , and then , if you need to , take a modul list to prevent overflow .
and the point of me give you thi on to two sentenc of the subroutin is just to give you a flavor of what thei're like , and to make sure th at you're just not scare of it at all .
okai ?
so it's veri simpl program you can write for do thing like convert from string to integ .
but again , you know , i do encourag you to look it up on the web or in a program textbook , to actual look at those exampl .
okai ?
but there ar standard method for do it .
so that leav the quest , question of how to design thi compress function .
so you take as input thi huge integ .
mayb your kei ar alreadi numer , like social secur number or ip address .
or mayb you've alreadi some subroutin to convert a string , like your friend's name , into .
some big number , but the point is you have a number in the million or the billion , and you need to somehow take that and output on of these bucket .
and again think of there be mayb a thousand or so bucket .
so the easiest wai to do that is someth we alreadi saw in a previou slide , which is just to take the modulu , with respect to the number of bucket .
now certainli on posit thing you can sai about thi compress function is it super simpl , both in the sens that it's simpl to code and in the sens that it's simpl to evalu .
now rememb that wa our second goal for a hash function .
it should be simpl to store , it is actual noth to store .
and it should be quick to evalu .
and thi certainli fit the bill .
now the problem is , rememb the first .
properti of a hash function that we want is that it should spread the data out equal .
and what we saw in the previou slide is that at least if you choos the number of bucket n poorli , then you can fail to have the first properti .
and in that respect you can fail to be a good hash function .
so if for exampl if n is even and all of your object ar even , then it's a disast , all of the odd bucket go complet empti .
and honestli , you know , thi is a pretti simplist method .
like i said , thi is a quick and dirti hash function .
so , no matter how you choos the number of bucket n , it's not gonna be a perfect hash function in ani sens of the word .
that said , there ar some rule of thumb for how to pick the number of bucket , how to pick thi modulu , so that you don't get the problem that we saw on the previou slide .
so , i'll conclud thi video just with some standard rule of thumb .
you know , if you just need a quick and dirti hash function , you're gonna us the , the modulu compress function , how do you choos n ?
well , the first thing is we definit don't want to have the problem we had in the last slide , where we're guarante to have these empti bucket no matter what the data is .
so what went wrong in the previou slide ?
well .
the problem is that all of the data element were divis by two .
and the hash function modulu , the number of bucket , wa also divis by two .
so becaus thei share a common factor , name two , that guarante that all of the odd bucket remain empti .
and thi is a problem , more gener , if the data share ani common factor with n , the number of bucket .
so , in other word , if all of your data element ar multipl of three , and the number of bucket is also a multipl of three , you got a big problem .
then everyth's gonna hash into bucket number which ar multipl of three , too , that's if your hash tabl will go unfil .
so the upshot is , you realli want the number of bucket to not share ani factor with the data that you're hash .
so , how do you reduc the number of common factor ?
well , you just make sure the number of bucket ha veri few factor , which mean you should choos n to be a prime number , 'kai ?
a number that ha no nontrivi factor , and let me remind you , the number of bucket should also be compar to the size of the set that you're plan on store .
again , at no point did we need n to be , you know , veri close connect to the number of element that you're store just within , sai some small constant factor .
and you can alwai find a prime within a small constant factor of a target number of element to store .
if the number of bucket in your hash tabl isn't too big , if it's just in sai the thousand or mayb the ten of thousand , then , you know , you can just look up a list of all known prime up to some point , and you can just sort of pick out a prime which is about the magnitud that you're look for .
if you're gonna us a realli huge number of bucket in the million or more , then there ar algorithm you can us for primarili test which will help you find a prime in about the rang that you're look for .
so that's the first order rule of thumb you should rememb if you're us the modulu compress function , which is set the number of bucket equal to a prime .
so you're guarante to not have non trivial common factor of the modulu share by all of your data .
so there's also a coupl of second order optim , which peopl often mention .
and you also don't want the prime ; you want the prime to be not too close to pattern in your data .
so what doe that mean , pattern in your data ?
well , in the phone number exampl we saw that pattern emerg in the data when we express it base ten .
so for exampl , there is crazi amount of lump in the first three digit when we express a phone number base ten , becaus that correspond with the area code .
and then , with .
memori locat when we express , express it base two , there ar crazi correl in the low orbit .
and these ar the two most common exampl .
either there's some digit , to the base ten represent or digit in the base two represent where you have , you know , pattern that is non uniform .
so that .
suggest that the prime , that , n that you choos , you know , all els be equal , shouldn't be too close to a power of two , and shouldn't be too close to a power of ten .
the think be that , that will spread more evenli data set that do have these pattern in term of base two represent , or base ten represent .
so in close , thi is a recip i recommend for code of hash function if what you're look to do is sort of minim program ming , programm time , subject to not come up with a hash function , which is complet broken .
but i want to reiter , thi is not the state of the art in hash function design .
there ar hash function which ar in some sens better than the on that expand on thi slide .
if you're respons for some realli mission critic code that involv a hash function , you should realli studi more deepli than we've been abl to do here .
we'll touch on some issu in , of the differ option video , but realli you should do addit homework .
you should find out about the state of the art about hash function design .
you should also look into implement of open address in those probe strategi .
and abov all you realli should consid cold , code up multipl prototyp and see which on work the best .
there's no silver bullet , there's no panacea in the design of hash tabl .
i've given you some high level guidanc about the differ approach .
but ultim it's gonna be up to you to find the optim implement for your own applic .
thi sequenc of video ar go to take it to the next level with hash tabl and understand more deepli the condit under which thei perform well , amazingli well in fact as you know , we've constant time perform for all of their oper .
the main point of thi first video is to explain in sens in which everi hash function ha it own kyptonit .
a patholog data set for it which then motiv the need to tread carefulli with mathemat in the subsequ video .
so , a quick review so rememb that the whole purpos of a hash tabl is to enabl extrem fast look up , ideal constant time lookup .
now , of cours , to have anyth to look up , you have to also allow insert .
so all hash tabl ar go to export those two oper and then , sometim , a hash tabl also allow you to delet element from it .
that depend a littl bit on the underli implement .
so certainli when you have it implement us chain , that's when you have on link list per bucket , it's veri easi to implement delet .
sometim , with open address , it's tricki enough , you're just gonna wind up punt on delet .
so when we first start talk about hash tabl , i encourag you to think about them logic .
much the wai you do as an arrai , except instead of be index just by the posit of an arrai , it's index by the kei that you're store .
so just like an arrai via random access support constant time look up , so doe a hash tabl .
there wa some fine print howev with hash tabl .
rememb there were these two caveat .
so the first on is that the hash tabl better be properli implement and thi mean a coupl of thing .
so , on thing that it mean is that the number of bucket better be commensur with the number of thing that you're store in the hash tabl , we'll talk more about that in a second .
the second thing it mean is you better be us a descent hash function .
so we discuss in a previou video the peril of bad hash function , and it will be even more stringent with our demand on hash function in the video to come .
the second caveat which i'll try to demystifi in a few minut is that you better have non patholog data .
so , in some sens , for ever hash tabl there's kyptonit , a patholog data set that will render it perform to be quit poor .
so in the video on implement detail we also discuss how hash tabl inevit have to deal with collis .
so you start see collis wai befor your hash tabl's start fill up so you need to have some sort of method for address two differ kei that map to exactli the same bucket .
which on do you put there ?
do you put both there or what ?
so there's two popular approach let me remind you what thei ar first call chain .
so thi is a veri natur idea , where you just keep all of the element that hash to a common bucket in that bucket .
how do you keep track of all of them ?
well , you just us a link list .
so , in the seventeenth bucket , you will find all of the element which ever hash to bucket number <num> .
the second approach which also ha plenti of applic in practic is open address .
here the constraint is that you ar onli go to store on item on kei in each bucket .
so if two thing map the bucket number seventeen , you gotta find a separ place for on of them and so the wai that you handl that is you demand from your hash function not mere on bucket but rather a whole probe sequenc .
so the sequenc of bucket so that if you try to hash someth into bucket number seventeen , <num>'s alreadi occupi then you go on to the next .
bucket in the probe sequenc .
you try to insert it there and if it , you fail again you go to third bucket in the sequenc .
you try to insert it there , and so on .
so we mention briefli the sorta two wai you can specifi probe sequenc .
on is call linear probe .
so thi is where if you fail in bucket seventeen you move on to eighteen and then nineteen and then twenti and then <num> .
and you stop onc you find an empti on and that's where you insert the new element and anoth on is doubl hash and thi is where you us a combin of two hash function where the first hash function specifi the initi bucket that you probe .
the second hash function specifi the offset for each subsequ probe .
so for exampl if you have a given element sai the name alic and the two hash function give you the number <num> and <num> then the correspond find probe sequenc is go to be initi <num> , fail that we'll try <num> , still fail that we'll try <num> , fail that we'll try <num> and so on .
so in a cours on the design and analysi of algorithm like thi on you typic talk a littl bit more about chain than you do open address .
that's not to impli that chain is somehow the more import on both of these ar import but chain is a littl easier to talk about mathemat .
so we will talk about it a littl bit more cuz i'll be abl to give you complet proof for chain wherea complet proof for open address would be outsid the scope of thi cours .
so i'll mention it in pass but the detail will be more about chain just for mathemat eas .
so , there's on veri import paramet which plai a big role in govern the perform of a hash tabl , and that's known as the load factor , or simpli the load , of a hash tabl .
and it's a veri simpl definit .
it just talk about how popul , a typic bucket of the hash tabl is .
so it's often denot by alpha and in the numer is the number of thing that have been insert , and not subsequ delet in the hash tabl .
divid by the number of bucket in the hash tabl .
so , as you would expect , as you insert more and more thing into the hash tabl , the load grow , keep the number of item in the hash tabl fix as you scale up the number of bucket , the load drop .
so just to make sure that the notion of the load is clear , and that also you're clear on the differ strategi for resolv collis , the next quiz will ask you about the rang of relev alpha for the chain and open address implement of the hash tabl .
all right , so the correct answer to thi quiz question is the third answer .
load factor bigger than on do make sens thei mai not be optim but thei at least make sens for hash tabl that implement with chain but thei don't make sens for hash tabl with open address .
and the reason is simpl rememb in open address you ar requir to store onli on object per bucket so as soon as the number of object exce the number of bucket there is no where to put the remain object .
so the hash the hash tabl will simpli crash if , if load factor is bigger than on .
on the other hand a hash tabl with chain there is no obviou problem with load factor bigger than on so you can imagin a load factor equal to two sai , sai you insert <num> , <num> object into a hash tabl with <num> , <num> bucket .
you know that mean , hopefulli at least in the best case .
each bucket just gonna have a length list with two object in it .
so there's no big deal .
with have load factor bigger than on , and hash tabl with chain .
all right , so let's then make a , a quit easi but also veri import observ about a necessari condit for hash tabl to have good perform and thi goe into the first caveat that you better properli implement the hash tabl if you expect to have good perform .
so the first point is that you're onli gonna have constant time look up if you keep the load to be constant .
so for a hash tabl with open address , thi is realli obviou , becaus you need alpha not just o <num> but less than on , less than <num> percent full , otherwis the hash tabl is just gonna crash , cuz you don't have enough room for all of the item , but even for hash tabl that you implement us chain , where thei at least make sens for load factor which ar bigger than on , you'd better keep the load not too much bigger than on if you want to have constant time oper .
right , so if you have , sai , a hash tabl with .
n bucket and you hash in nlogn object , then the averag number of object in a given bucket is gonna be logarithm and rememb , when you do a lookup , after you hash to the bucket , you have to do an exhaust search through the link list in that bucket .
so if you have nlogn object and you hash it with n bucket , you're expect more like logarithm lookup time not constant lookup time and then , as we discuss with open address , of cours , you need not just alpha o <num> , but alpha less than on .
and in fact , alpha better be well below on .
you don't want to let an open address tabl get to a <num> percent load or someth like that .
so i'm go to write need alpha less than less than on .
so that just mean you don't want to let the load grow too close to <num> , you will see perform degrad .
so again , i hope the point of thi slide is clear .
if you want good hash tabl perform , on of the thing you're respons for is keep the load factor under control .
keep it at most a small constant with open address and keep it well below <num> .
so you might wonder what i mean by control the load .
after all , you know you write thi hash tabl when you have no idea what some client's gonna do with it .
thei can insert or delet whatev thei want .
so how do you , how do you control alpha ?
well , what you can control under the hood of your hash tabl implement is the number of bucket .
you can control the denomin of thi alpha so if the numer start grow at some point the denomin is go to grow as well .
so what actual implement of hash tabl do is thei keep track of the popul of the hash tabl .
how mani object ar be store , and as thi numer grow , as more and more stuff get insert , the implement ensur that the denomin grow at the same rate so that the number of bucket also increas .
so if alpha exce some target , you know , that could be sai <num> , . <num> , or mayb it's . <num> .
then what you can do is you can doubl the number of bucket , sai in your hash tabl .
so you defin a new hash tabl , you have a new hash function with doubl the rang , and now have doubl the denomin .
the load ha drop by a factor two .
so that's how you can keep it under control .
option , if space is at a real premium , you can also shrink the hash tabl if there's a bunch of delet , sai , in a chain implement .
so that's the first take awai point about what ha to be happen correctli under the hood in order to get the desir guarante for hash tabl perform you gotta control the load .
so you have to have a hash tabl whose size is roughli the same as the as the number of object that you ar store .
so the second thing you've gotta get right and thi is someth we've touch on in the implement video is you better us a good enough hash function and so what's a good hash function ?
it's someth that spread the data out evenli amongst the bucket and what would realli be awesom would be a hash function which work well independ of the data and that's realli been the theme of thi whole cours so far , algorithm solut which work independ of ani domain assumpt .
no matter what the input is , the algorithm is guarante to , for exampl , run blazingli fast and i can appreci that thi is exactli the sort of thing you would want to learn from a cours like thi , right ?
take a class in the design analysi of algorithm and you learn the secret hash function which alwai work well .
unfortun i'm not go to tell you such a hash function and the reason is not cuz , i didn't prepar thi lectur .
the reason is not becaus peopl just haven't been clever enough to discov such a function .
the problem is much more fundament .
the problem is that such a function cannot exist .
that is for everi hash function it ha it own kryptonit .
there is a patholog dataset under which the perform of thi hash function will be as bad as the most miser constant hash function you'd ever seen .
and the reason is quit simpl ; it's realli an inevit consequ of the compress that hash function ar effect implement from some massiv univers to some rel modest number of bucket .
let me elabor .
fix ani hash function as clever as you could possibl imagin .
so thi hash function map some univers through the bucket index from <num> to n <num> .
rememb in all of the interest situat of hash function , the univers size is huge , so the cardin of u should be much , much bigger than n .
that's what i'm go to assum here .
so for exampl , mayb you're rememb peopl's name , and then the univers is string , which have , sai at most , <num> charact , and n , i assur you in ani applic is go to be much , much , much smaller than sai , <num> rais to the thirtieth power .
so now let's us a variant on the pigeon hole principl , and acclaim that at least on of these n bucket ha to have at least a <num> n fraction of the number of kei in thi univers .
that is , there exist a bucket i , somewher between <num> and n <num> such that , at least the cardin of the univers over n kei hash to i get map to i under thi hash function h .
so the wai to see thi is just to rememb the pictur of a hash function map in princip ani kei from the univers , all kei from the univers to on of these bucket .
so the hash function ha to put each kei somewher in on of the n bucket so on of the bucket ha to have at least a <num> n fraction abov all of the possibl kei .
on more concret wai of think , think about it is that you might want to think about a hash tabl implement with chain .
you might want to imagin , just in your mind , that you hash everi singl kei into the hash tabl .
so thi hash tabl is go to be insan over popul .
you'll never be abl to store it on the comput becaus it will have the full cardin of u object in it , but it ha u object , it onli ha n bucket .
on of those bucket ha to have at least u n fraction of the popul .
so the point here is that no matter what the hash function is no matter how clever you build it there's gonna be some bucket sai bucket number <num> which get it fair share of the univers map to it .
so have identifi thi bucket , bucket number <num> where it get at least it fair share of the univers map to it now to construct our patholog dataset all we do is pick from amongst these element that get map to bucket number <num> .
so for such as a data set and we can make thi data set basic as larg as we want becaus the cardin of u n is unimagin big , becaus u itself is unimagin big then in thi data set , everyth collid .
the hash function map everi singl on to bucket number <num> and that's gonna lead to terribl hash tabl perform .
hast tabl perform which is realli no better than the naiv link list solut .
so for exampl an hash tabl with collis , in bucket <num> , you'll just find a link list everi singl thing that's ever been insert into the hash tabl and for open address mayb it's a littl harder to see , what's go on but again if everyth collid , you're gonna basic wind up with linear time perform a far cry from constant time perform .
now , for those of you to whom thi seem like just sort of pointless , abstract mathemat , i want to make two point .
the first is that at the veri least these patholog data set .
tell us , indic , that we will have to discuss hash function in a wai differ than how we've been discuss algorithm so far .
so when we talk about merg sort , we just said it run in n log n time .
no matter what the input is .
whether we discuss the dijkstra's algorithm it run in n log n time , no matter what the input is .
that first search , that first search mani a time no matter what the input is .
we're gonna have to sai someth differ about hash function .
we'll not be abl to sai that a hash tabl ha good perform , no matter what the input is .
thi slide show that's fals .
the second point i want to make is that while thi patholog data set of cours ar not like to aris just by random chanc .
sometim you're concern about somebodi construct a patholog data set for your hash function , for exampl in a denial servic attack .
so there's a veri clever illustr of exactli thi point in a research paper , from <num> by crosbi and wallach .
so the main point of crosbi and wallach wa that there's a number of real world system and mayb there , most interest applic wa a network intrus detect system , for which you could bring them to their knee by exploit badli design hash function .
so these were all applic that made crucial us of hash tabl and , the feasibl of these system realli , complet depend on get cost and time perform from the hash tabl .
so if you could exhibit a patholog data set for these hash tabl and make the perform devolv to linear , devolv to that of a simpl link list solut , the system would be broken , thei would just crash of thei'd fail to function .
now we saw in the last slide that everi hash tabl doe have it own kryptonit .
ha a patholog data set but the question is .
how can you come up with such a patholog data set if you're try to do a denial of servic attack on on of these system ?
and so the system that crosbi and wallach look at gener share two properti .
so first of all , thei were open sourc .
you could inspect the code .
you could see what hash function it wa that thei were us and second of all , the hash , function wa often veri simplist .
so it wa engin for speed more than anyth els and as a result , it wa easi to , just be inspect the code , revers engin a data set .
that realli did break the hash tabl .
that led it that devolv the perform to linear .
so for exampl , in the network intrus detect applic , there wa some hash tabl that wa just rememb the ip address of packet that we re go through , becaus it wa look for pattern of pack , data packet that seem to indic some sort of intrus .
and crosbi and wallach just show how send a bunch of data packet to thi system with cleverli chosen sender ip's realli did just crash the system becaus the hash tabl perform blew up to an unaccept degre .
so how should we address thi fact of life that everi hash function ha a patholog data set ?
and that question is meaning both from a practic perspect , so what hash function should we us if we ar concern about someon construct patholog data set , for exampl , the implement denial of servic attack , and secondli , mathemat , if we can't give the kind of guarante we've given so far , data independ guarante , how can we mathemat sai that hash function have good perform .
so let me mention two solut , the first solut is , is meant more just on the practic point .
you know what hash function should you implement if you ar concern with someon creat patholog data set ?
so there ar these thing call cryptograph hash function .
there for exampl , on cryptograph hash function or realli famili of hash function , for differ number of bucket , is sha <num> and these ar realli outsid the scope of thi cours .
i mean , you'll learn more about thi in a in a cours on cryptographi and obvious us these keyword you can look it up on the web and read more about it .
the on point i want to make is , you know these cryptograph hash function like sha2 , thei themselv , thei do have patholog dataset .
thei have their own version of kryptonit .
the reason that thei work well in practic is becaus it's infeas to figur out what thi patholog dataset is so unlik the veri simplist hash function which crosbi and wallach found in the sourc code of their applic , where it wa easi to revers engin bad dataset , for someth like sha2 nobodi know how to revers engin a bad dataset for it and when i sai infeas , i mean in the usual cryptograph sens , in a wai similar to how on would sai it's infeas to break the rsa cryptosystem if you implement it properli , or it's infeas to factor larg number except in veri special case , and so on .
so that's all i'm go to sai about cryptograph hash function .
i also want to mention a second solut , which would be reason both in practic , and also for which we can sai a number of thing mathemat about it which is to us random .
so more specif we're not go to design , a singl .
clever hash function becaus again , a singl hash function we know must have a patholog data set but we're gonna design a veri clever , famili of hash function and then , at run time .
we're go to pick , on of these hash function at random .
anoth kind of guarante you ar go to want , we ar go to be abl to prove on a famili affair function would be veri much in the spirit of quick sort .
so you would call that in the quick sort algorithm , pretti much .
for ani fix pivot sequenc there is a patholog input for which quick sort will devolv to quadrat run time .
so our solut wa random quick sort which is rather than commit up front to ani particular method of choos pivot at run time we ar gonna pick pivot randomli .
what did we prove about quick sort ?
we prove that for ani possibl input for ani possibl arrai the averag run time with quick sort wa o nlogn with the averag wa over the run time random choic of quick sort .
here we ar gonna do the same thing we'll now be abl to sai for ani dataset .
on averag , with respect to our run time choic of a hash function .
the hash function will perform well , in the sens that it will spread out the data of the data set evenli .
so we flip to the quantifi from the previou slide .
there , we said if we pre commit to a singl hash function .
if we fix on hash function , then there's a data set that break the hash function .
here we're flip it , we're sai for each fix data set a random choic of a hash function is go to do well on averag on that data set .
just like in quick sort .
thi doesn't mean notic that we can't make our program open sourc .
we can still publish code which sai here is our famili of hash function and in the code we would be make a random choic from thi set of hash function but the point is by inspect the code you'll have no idea what wa the real time random choic made by the algorithm .
so you'll know noth about what the actual hash function is so you won't be abl to revers engin patholog dataset for the real time choic of the hash function .
so the next coupl of video ar gonna elabor on the second solut of us a real time random choic of a hash function as a wai of sai .
you do well on everi data set at least on averag .
so let me just give you a road map of where we're go to go from here .
so i'm go to break the discuss of the detail of a random solut into three part , spread over two video .
so in the next video we're go to begin with the definit of what do i mean by a famili of hash function , so that if you pick on at random , you're like to do pretti well .
so that's a definit that's call a univers famili of hash function .
now a mathemat definit by itself is worth approxim noth .
for it to be valuabl , it ha to satisfi two properti .
so first of all there have to be interest and us exampl that meet the definit .
so that is , there better be us look hash function that meet thi definit of a univers famili .
so the second thing will be to show you that thei do inde exist and then the other thing a mathemat definit need is applic .
so that if you can meet , the definit .
then , good thing happen .
so now that we understand why we can't have a singl hash function which alwai doe well at everi singl data set , that is everi hash function is subject to a patholog data set .
we'll discuss the random solut of how we can have a famili of hash function and if you make a real time decis about which hash function to us , you're guarante to do well on averag , no matter what the data is .
so let me remind you of the three prong plan that i have for thi part of the materi .
so in thi video , we'll be cover the first two .
so part on , which we'll accomplish in the next slide , will be to propos a mathemat definit of a good random hash function .
so formerli , we're go to defin a univers famili of hash function .
now , what make thi definit us , well , two thing .
and so , part two , we'll show that there ar exampl of simpl and easi to comput hash function that meet thi definit , that ar univers in the sens describ on the next slide .
so that's import .
and in the third part , which we'll do in the next video , will be the mathemat analysi of the perform of hash , specif with chain when you us univers hash .
and we'll show that if you pick a random function from a univers famili , then , the expect perform of all of the oper ar constant .
assum , of cours , of the number of bucket is compar to the number of object in the hash tabl which we saw earlier is a necessari condit for good perform .
so let's go ahead and get start and let's sai what we mean by a good random hash function .
so for thi definit , we'll assum that the univers is fix .
so mayb it's ip address , mayb it's our friend name .
mayb it's configur of a chessboard , whatev .
but there's some fix univers u and we'll also assum we've decid on the number of bucket n .
and we call the set h univers if and onli if it meet the follow condit .
in english , the condit sai that for each pair of distinct element , the probab iliti that thei collid should be no larger than with the gold standard of perfectli uniform random hash .
so for all distinct kei from the univers , call them x and y , what we want is that the probabl if we choos a random hash function , h , from the set script h , the probabl that x and y collid .
and again , just to be clear , what that mean is that , x and y hash to exactli the same bucket under thi hash function h , thi should be no more than <num> n , and don't forget n is the number of bucket .
again , to interpret thi , you know , <num> n , where doe thi come from ?
so , we said earlier that an impract but in some sens , gold standard hash function would be to just independ for each kei , assign it bucket uniformli and random with differ kei be assign independ .
rememb the reason thi is not a practic hash function is becaus , you'd have to rememb where everybodi went .
and then that would basic requir maintain a list which would devolv to the list solut , so you don't want that .
you want hash function where you have to store almost noth and we can evalu them in constant time .
but , if we throw out those requir of small space and small time then , random function should spread stuff out pretti evenli , right ?
i mean that's what thei ar do .
thei're throw dart complet at random at these n bucket .
so what would be the collis probabl of two given kei sai , of alic and of bob if you ar do everyth independ and uniformli at random .
well , you know , first alic show up and it goe to some total random bucket , sai , bucket number seventeen .
now , bob show up .
so , what's the probabl that it collid with alic ?
well , we have these n bucket that bob could go to , each is equal like , and there's a collis between alic and bob if and onli if bob goe to bucket seventeen .
sinc , each bucket's equal like , that's onli a on in n probabl .
so realli what thi condit is sai , is that , for each pair of element , the collis probabl should be as small , as good as with the holi grail of perfectli random hash .
so thi is a pretti subtl definit , perhap the most subtl on that we see in thi entir cours .
so , to help you get some facil with thi , and to forc you to think about it a littl deepli , the next quiz which is probabl harder than a typic in class quiz , ask you to compar thi definit of univers hash with anoth definit and ask you to figur out to what extent thei're the same definit .
so the correct answer to thi quiz question is the third on that there ar hash function famili h , that satisfi the condit on thi slide that ar not univers .
on the other hand , there ar hash function famili h , which satisfi thi properti and ar univers .
so , i'm go to give you an exampl of each .
i'd encourag you to think carefulli about why thi is an exampl and a non exampl offlin .
so an easi exampl to show that sometim the answer is ye , you have univers hash function famili h , which also satisfi thi properti of the slide , would be to just take h to be the set of all function from map the univers to the number of bucket .
so that's an aw lot of function , that's a huge set , but it's a set nonetheless .
and , by symmetri of have all of the function , it both satisfi the properti of thi slide .
it is inde true that exactli a on on on refract of all function , map arbitrari kei k to arbitrari bucket i .
and by the same reason , by the same symmetri properti , thi is univers .
so realli , if you think about it choos a function at random function from h , is now just choos a complet random function .
so it's exactli what we've been call perfect , random hash .
and as we discuss in the last slide , that would inde have a collis probabl of exactli <num> n for each pair of distinct kei .
so , thi show sometim you can have both thi properti and be univers .
an exampl where you have the properti in thi slide , but you're not universa l , would be to take h to be a quit small famili , a famili of exactli n function , each of which is a constant function .
so it's go to be on function which alwai map everyth to bucket zero , that's a total stupid hash function .
there's go to be anoth hash function which alwai map everyth to bucket number on , that's a differ but also total stupid hash function , and so on .
and then the end function will be the constant function that alwai map everyth to bucket n <num> .
and if you think about it , thi veri silli set h doe inde satisfi thi veri reason look properti on thi slide .
fix ani kei , fix ani bucket , you know sai bucket number <num> what's the probabl that you pick a hash function that map thi kei to bucket number <num> ?
well , independ of what the kei is , it's go to be the probabl that you pick the constant hash function whose output is alwai <num> .
sinc there's n differ constant function , there's a on in n probabl .
so , that's an exampl show that in some sens , thi is not as us a properti as the properti of univers hash .
so thi is realli not what you want .
thi is not strong enough .
univers hash , that's what you want for strong guarante .
so now that we've spent some time try to assimil probabl the subtlest definit we've seen so far in thi class , let me let you in on a littl secret about the role of definit in mathemat .
so on the on hand , i think mathemat definit often get short shrift , especi in , you know , the popular discuss of mathemat research .
that said , you know , it's easi to come up with on reason why that's true , which is that ani schmo can come up and write down an mathemat definit .
nobodi's stop you .
so , what you realli need to do is you need to prove that a mathemat definit is us .
so how do you indic us of a definit ?
well you gotta do two thing .
first of all , you have to show that the definit is satisfi by object of interest .
for us right now , object of interest , ar hash function , we might imagin implement .
so thei should be easi to store , easi to evalu .
so there better be such hash function mean , that complic univers hash function definit .
the second thing is , is someth good better happen if you meet the definit .
and in the context of hash , what good thing do we want to have happen ?
we want to have good perform .
so those ar the two thing that i ow you in these lectur .
first of all , a construct of practic hash function that meet that definit , that's what we'll start on right now .
second of all , why meet that definit is a suffici condit for good hash tabl perform .
that will be the next video .
so in thi exampl , i'm go to focu on ip address although the hash function construct is gener , as i hope will be reason clear .
and as mani of you know , an ip address is <num> bit integ consist of four differ eight bit part .
so let's just go ahead and think of an ip address as a four two fold , the wai you often see it .
and sinc each of the four part is eight bit , it's go to be a number between zero and <num> .
and the hash function that we're go to construct , it's realli not go to be so differ than the quick and dirti function as we talk about in the last video although in thi case we'll be abl to prove that the hash function famili is in fact , univers .
and we're again go to us the same compress function .
we're go to take the modula with respect to a prime number of bucket .
the onli differ is we're go to multipli these xi's by a random set of coeffici .
we're go to take a , a random linear combin of x1 , x2 , x3 and x4 .
so i'm go to be a littl more precis .
so we're go to choos a number of bucket , n , and as we sai over and over , the number of bucket should be chosen so it's in the same ball park of the number of object you ar store .
so you know , let's sai that n should be roughli doubl the number of object that you ar store as initi rule of thumb .
so , for exampl , mayb we onli want to maintain someth in the ball park of <num> ip address and we can choos n to be a prime like <num> .
so here's the construct .
rememb , we want to produc not just on hash function , but the definit is about a univers famili of hash function .
so we need a whole set of hash function that we're ultim go to chose on member from , at random .
so , how do we construct a whole bunch of hash function in a simpl wai ?
here's how we do it .
so you defin on hash function , which i'm go to note by h sub a , a here is a four tupl .
the compon of which i'm go to call a1 , a2 , a3 and a4 .
and , all of the compon of a ar integ between zero and n <num> .
so thei're exactli in correspond with the indic of the bucket .
so if we have <num> bucket , then each of these ai's is an integ between zero and <num> .
so it's clear that thi defin , you know , a whole bunch of function .
so in fact , for each of the four coeffici , that's four independ choic , you have n option .
okai so each of the integ between zero and n <num> for each of the four coeffici .
so that's fine , not give a name to end of the four differ function , but what is ani given function ?
how do you actual evalu on of these function ?
just rememb what a hash function is suppos to do .
rememb you know , how it type check it take as input someth from the univers in thi case an ip address , and output a bucket number .
and the wai we evalu the hash function h sub a , and rememb a here is a <num> tupl .
and rememb ip address is also a <num> tupl , okai , so each compon of the ip address is between zero and <num> .
each compon of a is between zero and n <num> , so for exampl , between zero and <num> .
and what we do is just take the dot product or the inner product of the vector a and the vector x , and then we take the modulu with respect the number of bucket .
so that is we take a1 x1 a2 x2 a3 x3 a4 x4 .
now of cours , rememb the x's lie be tween zero and <num> , the ai's lie between the zero and n <num> , so sai zero and <num> , you know , so you do these form of multipl now make it a pretti big number , you might well over shoot the number of bucket n .
so to get back in the rang of what the bucket ar actual index that in the end we take the modul , modulu the number of bucket .
so in the end we do output , a number between zero and n <num> as desir .
so that's a set of a whole bunch of hash function , n to the fourth hash function .
and each on meet the criteria of be a good hash function from an implement perspect , right ?
so rememb , we don't want to have to store much to evalu a function .
and for a given hash function in thi famili , all we gotta rememb ar the coeffici , a1 , a2 , a3 and a4 .
so you just gotta rememb these four number .
and then to evalu a hash function on an ip address , we clearli do a constant amount of work .
we just do these four multipl , the three addit , and then take the modulu by the number of bucket n .
so it's constant time to evalu , constant space to store .
and what's cool is , us just these veri simpl hash function which ar constant time to evalu and constant space to store , thi is alreadi enough to meet the definit of a univers famili of hash function .
so thi fulfil the first promis that i ow you , after subject you to that definit of univers hash .
rememb the first promis wa , there ar simpl , there ar us exampl that meet the definit , and then of cours , i still ow you why .
mean thi definit is us , why doe it leav the good perform .
but i want to conclud , thi video of actual prove thi theorem to you , argu that thi is , in fact , a univers famili of hash function .
right .
so thi should be a mostli complet proof and certainli will have all of the conceptu ingredi of why the proof work there will be on spot where i'm a littl hand wavi becaus we need a littl number theori , and i don't want to have a big detour into number theori .
and if you think about it , you shouldn't be surpris that basic number theori plai at least some role .
like as i said , we should choos the number of bucket to be prime .
so that mean at some point in the proof , you should expect us to us the assumpt that n is prime .
and pretti much alwai you're go to us that assumpt will involv at least elementari number theori , okai ?
but i'll be clear about where i'm be hand wavi .
so what do we have to prove ?
let's just quickli review a definit of a univers hash function .
so we have our set h that we , that we know exactli what it is .
what doe it mean that it's univers ?
it mean for each pair of distinct kei , so in our context it's for each pair of ip address , the probabl that a random hash function from our famili script h caus a collis , map these two ip address to the same bucket should be no wors than with perfectli random hash .
so no wors than <num> n where n is the number of bucket , sai like <num> .
so , the definit we need to meet is a condit for everi pair of distinct kei .
so let's just start by fix two distinct kei .
so i'm go to assum for thi proof that these two ip address differ in their fourth compon .
that is that i'm go to assum that x4 is differ than y4 .
so i hope that it's intuit clear that , you know , it shouldn't matter , you know , which , which set of <num> bit i'm look at .
so thei're differ ip address .
thei differ somewher .
if i realli want , i could have four case that were total ident depend on whether thei differ in the first eight bit , the next <num> bit , the next <num> bit , or the last <num> bit .
i'm go to show you on case , becaus the other three ar the same .
so let's just think of the last <num> bit as be differ .
and now , rememb what the definit ask us to prove .
it ask us to prove that the probabl that these two ip address ar go to collid is at most , <num> n .
so we need an upper bound on the collis probabl w ith respect to a random hash function from our set of n to the fourth hash function .
so i want to be clear on the quantifi .
we're think about two fix ip address .
so for exampl , the ip address for the new york time websit and the ip address for the cnn websit .
we're ask for these two fix ip address , what fraction of our hash function caus them to collid , right ?
we'll have some hash function which map the new york time and cnn ip address to the same bucket , and we'll have other hash function which do not map those two ip address to the same bucket .
and we're try to sai , that the overwhelm major , send them to differ bucket , onli a <num> n fraction at most , send them to the same bucket .
so we're ask about the probabl for the choic of a random hash function from our set h that the function map the two ip address to the same place .
so the next step is just algebra .
i'm just go to take thi equat which indic when the two ip address collid over a hash function .
i'm go to expand the definit of a hash function , rememb it's just thi inner product modulo the number of bucket n , and i am go to rewrit thi condit in a more conveni wai .
all right , so after the algebra , and the dust ha settl .
we're left with thi equat be equival to the two ip address collid .
so again , we're interest in the fraction of choic of a1 , a2 , a3 , and a4 , such that thi condit hold , right ?
sometim it'll hold for some choic of the ai's , sometim it won't hold for other choic and we're go to show that it almost never hold .
okai , so it fail for all but a <num> n fraction of the choic of the ai's .
so next we're go to do someth a littl sneaki .
thi trick is sometim call the principl of defer decis .
and the idea is when you have a bunch of random coin flip , it's sometim conveni to flip some but not all of them .
so sometim fix part of the random clarifi the role that the remain random is go to plai .
that's what's go to happen here .
so let's go ahead and flip the coin , which tell us the random choic of a1 , a2 , and a3 .
so again rememb , in the definit of a univers hash function , you analyz collis probabl under a random choic of a hash function .
what doe it mean to choos a random hash function for us ?
it mean a random choic of a1 , and a2 , and a3 , and a4 .
so we're make four random choic .
and what i'm sai is , let's condit on the outcom of the first three .
suppos we knew , that a1 turn up <num> , a2 show up <num> and a3 show up <num> , but we don't know what a4 is .
a4 is still equal like to be ani of zero , on , two all the wai up to n <num> .
so rememb that what we want to prove is that at most <num> n fraction of the choic of a1 , a2 , a3 , and a4 , caus thi underlin equat to be true , caus a collis .
so what we're go to show is that for each fix choic of a1 , a2 , and a3 , at most a <num> n fraction of the choic of a4 caus thi equat to hold .
and if we can show that for everi singl choic of a1 , a2 , and a3 , no matter how those random coin flip come out , almost a <num> n fraction of the remain outcom satisfi the equat , then we're done .
that mean that at most of <num> n fraction of the overal outcom can caus the equat to be true .
so if you haven't seen the principl of , for these decis befor , you might want to think about thi a littl bit offlin , but it's easili justifi by just sai two line of algebra .
okai , so we're done with the setup and we're readi for the meat of the argument .
so we have done is , we've identifi an equat which is now in green , which occur if and onli if we have a collis between the two ip address .
and the question we need to ask is , for a fix choic of a1 , a2 and a3 , how frequent will the choic of a4 caus thi equat to be satisfi ?
caus a collis ?
now , here is why we did thi trick of the principl of defer decis .
by fix a1 , a2 , and a3 , the right hand side of thi equat is now just some fix number b etween zero and n <num> .
so mayb thi is <num> , right ?
the xi's were fix upfront , the yi's were fix upfront .
we fix a1 , a2 , a3 at the begin , at the end of the last slide , and those were the onli on involv in the right hand side .
so thi is <num> and over on the left hand side , x4 is fix , y4 is fix but a4 is still random .
thi is an integ equal like to be ani valu between zero and n <num> .
now here's the kei claim , which is that the left hand side of thi green equat is equal like to be ani number between zero and n <num> .
and i'll tell you the reason why thi kei claim is true .
although thi is the point where we need a littl bit of number theori , so i'll be kind of hand wavi about it .
so there's three thing we have go for us , the first is that x4 and y4 ar differ .
rememb our assumpt at the begin of the proof wa that , you know , the ip address differ somewher so why not just assum that thei differ in the last <num> bit of the proof .
again thi is not import if you realli want to be pedant you could have three other case depend on the other possibl bit in which the ip address might differ .
but anywai , so , becaus x4 and y4 ar differ , what that mean is that x4 y4 is not zero .
and in fact , now that i write thi , it's jog my memori of someth that i should have told you earlier , and forgot , which is that the number of bucket n should be at least as larg as the maximum coeffcient valu .
so for exampl , we definit want the number of bucket n in thi equat to be bigger than x4 , and bigger than y4 .
and the reason is , otherwis you could have x4 and y4 be differ from each other , but thei still , the differ still wind up be zero mod n .
so for exampl , suppos n wa four , and x4 wa six and y4 wa ten .
then x4 <num> would be four and that's actual zero modulo four .
so that's get now what you want .
you want to make sure that if x4 and y4 ar differ , then thei're differ is non zero modulo n .
and the wai you ensur that is that you just make sure n is bigger than each .
so you should choos a number of bucket bigger than the maximum valu of the coeffici .
so in our ip address exampl , rememb that the coeffici don't get bigger than <num> .
and i wa suggest a number of bucket equal the same <num> .
now , in gener , thi is never a big deal in practic , if you onli want to us sai , <num> bucket , you didn't want to us <num> , you want <num> , well , then you could just us smaller coeffici , right , you could just break up the ip address , instead of into <num> bit chunk , you could break it into <num> bit chunk , or <num> bit chunk , and that would keep the coeffici size smaller than the number of bucket , okai ?
so you could choos the bucket first , and then you choos how mani bit to chunk your data into , and that's how you make sure thi is satisfi .
so back to the three thing we have go for us in try to prove thi kei claim .
so x4 and y4 ar differ , so their differ is non zero modulo n .
so second of all , n is prime , that wa part of the definit , part of the construct .
and then third , a4 , thi final coeffici is equal like to take on ani valu between zero and n <num> .
so , just as a plausibl argument , let me give you a proof by exampl .
again , i don't want to detour into elementari number theori , although it's beauti stuff , so you know , i encourag those of you who ar interest to go learn some and figur out exactli how you prove it .
you realli onli need the barest elementari number theori to give a formal proof of thi .
but just to show you that is true in some simpl exampl , so let's think about a veri small prime .
let's just sai there's seven bucket and let's suppos that the differ between x4 and y4 is two .
okai , so have chosen the paramet of set n seven , i've set the differ equal to two .
what i want to do is i want to step through the seven possibl choic of a4 , and look at what we get in thi blue circl quantiti , on the left hand side of the green equat .
so , we want to sai the left hand's equal like to be ani of the seven number between zero and six , so that mean that as we try our seven differ choic for a4 , we better get the seven differ possibl number as output .
so , for exampl , if we set a4 zero , then the blue circl quantiti is certainli itself zero .
if we set it equal to on , then it's on two , so we hit two .
for two , we get two two which is four .
for three , we get three two which is six .
now , when we set a4 four , we get four two which is eight , modulo seven is on .
five two seven is three .
six two seven is five .
so as we cycl through a4 , zero through six , we get the valu zero , two , four , six , on , three , five .
so inde we cycl through the seven possibl outcom on by on .
so if a4 is chosen uniformli and random , then inde thi blue circl quantiti will also be uniformli random .
so just to give anoth quick exampl , we could also keep n seven , and think about the differ of x4 and y4 .
again , we have no idea what it is , other than that it non zero .
so , you know , mayb instead of three , mayb , mayb instead of two , it's three .
so now again , let's stop through the seven choic of a4 , and see what we get .
so now we're go to get zero , then three , then six , then two , then five , and then on , and then four .
so again , step through the seven choic of a4 , we get all of the seven differ possibl of thi left hand side .
and it's not an accid that these choic ar paramet .
as long as n is prime , x4 and y4 ar differ , and y rang over all possibl , so will the valu on the left hand side .
so by choos a four uniformli random , inde , the left hand side is equal like to be ani of it possibl valu , zero , on , two up to n <num> .
and so , what doe that mean ?
well , basic it mean that we're done with our proof cuz rememb , the right hand side that circl in pink is fix .
we fix a1 , a2 , and the a3 .
the x's and y's have been fix all along so thi is just some number , like <num> .
and so , we know that there's exactli on choic of a4 that will caus the left hand side to also be equal to <num> .
now a4 ha n differ possibl valu and it's equal like to take on on becau e onli a on end chanc that we're go to get the unlucki choic of a4 that caus the left hand side to be equal to <num> and of cours , there's noth special about <num> .
doesn't matter how the right hand side come out .
so , in thi video , we're go to start reason about the perform of hash tabl .
in particular , we'll make precis thi idea that properli implement thei guarante constant time lookup .
so , let me just briefli remind you of the road map that we're in the middl of .
so , we observ that everi fix hash function is subject to a patholog data set and so explor the solut of make a real time decis of what hash function to us .
and we've alreadi gone over thi realli quit interest definit of univers hash function and that's the propos definit of a good random hash function .
more over , in the previou video i show you that there ar simpl such famili of hash function .
thei don't take much space to store , thei don't take much time to evalu .
and the plan for thi video is to prove formal , that if you draw a hash function uniformli at random from a univers famili of hash function , like the on we saw in the previou video , then you're guarante expect constant time for all of the support oper .
so , here's the definit onc more of a univers famili of hash function .
we'll be us thi definit , of cours , when we prove that these hash function give good perform .
so , rememb , we're talk now about a set of hash function .
these ar all of the conceiv real time decis you might make about which hash function to us .
so , the univers is fix , thi is someth like ip address , the number of bucket is fix .
you know that's go to be someth like <num> , <num> , sai , and what it mean for a famili to be univers is that the probabl that ani given pair of item collid is as good , as small as with the gold standard of complet perfect uniform random hash .
that is for each pair xy of distinct element of the univers , so for exampl , for each distinct pair of ip address , the probabl over the choic of the random hash function littl h from the famili script h is no more than on over n , where n is the number of bucket .
so , if you have <num> , <num> bucket , the probabl that ani given pair of ip address wind up get map to the same bucket is almost on in <num> , <num> .
let me now spell out the precis guarante we're go to prove if you us a randomli chosen hash function from a univers famili .
so , for thi video , we're go to onli talk about hash tabl implement with chain , with on length list per bucket .
we'll be abl to give a complet precis mathemat analysi with thi collis resolut scheme .
we're go to analyz the perform of thi hash tabl in expect over the choic of a hash function littl h drawn uniformli from a univers famili script h .
so , for exampl , for the famili that we construct in the previou video , thi just amount to choos each of the four coeffici uniformli at random .
that's how you select a univers , that's how you select a hash function uniformli at random .
so , thi theorem and also the definit of univers hash function date back to a <num> research paper by carter and wegman .
the idea of hash date back quit a bit befor that , certainli to the 50s .
so , thi just kind of show us sometim idea have to be percol for awhil befor you find the right wai to explain what's go on .
so , carter and wegman provid thi veri clean and modular wai of think about perform of hash through thi univers hash definit .
and the guarante is exactli the on that i promis wai back when we talk about what oper ar support by hash tabl and what kind of perform should you expect , you should expect constant time perform .
as alwai , with hash , there is some fine print so let me just be precis about what the caveat of thi guarante ar .
so , first of all , necessarili thi guarante is an expect .
so , it's on averag over the choic of the hash function , littl h .
but i want to reiter that thi guarante doe hold for an arbitrari data set .
so , thi guarante is quit reminisc of the on we had for the rand omiz quick sort algorithm .
in quicksort , we made no assumpt about the data .
it wa a complet arbitrari input arrai and the guarante said , on averag over the real time random decis of the algorithm , no matter what the input is , the expect run time wa in log in .
here we're sai again , no assumpt about the data .
it doesn't matter what you're store in the hash tabl and expect over the real time random decis of what hash function you us , you should expect constant time perform , no matter what that data set is .
so , the second caveat is someth we've talk about befor .
rememb , the kei to have good hash tabl perform , not onli do you need a good hash function which is what thi univers kei is provid us but you also need to control the load of the hash tabl .
so , of cours , to get constant time perform , as we've discuss , a necessari condit is that you have enough bucket to hold more or less the stuff that you're store .
that is the load , alpha , the number of object in the tabl divid by the number of bucket should be constant for thi theorem to hold .
and final , whenev you do a hash tabl oper , you have to in particular invok the hash function on whatev kei you're given .
so , in order to have constant time perform , it better be the case that it onli take constant time to evalu your hash function and that's , of cours , someth we also discuss in the previou video when we emphas the import of have simpl univers hash function like those random linear combin we discuss in the previou video .
in gener , the mathemat analysi of hash tabl perform is a quit deep field , and there is some , quit mathemat interest result that ar well outsid the scope of thi cours .
but what's cool , in thi theorem i will be abl to provid you a full and rigor proof .
so , for hash tabl with chain and us randomli chosen univers hash function , i'm go to now prove that you do get con tant time perform .
right , so hash tabl support variou oper , insert , delet and lookup .
but realli if we can just bound a run time of an unsuccess lookup , that's go to be enough to bound the run time of all of these oper .
so , rememb in hash tabl with chain , you first hash the appropri bucket and then you do the appropri insert , delet or lookup in the link list in that bucket .
so , the worst case as far as travers though thi length list is if you're look for someth but it's not there cuz you have to look at everi singl element in the list and followup into the list befor you can conclud that the lookup ha fail .
of cours , insert , as we discuss , is alwai constant time , delet and success search , well , you might get lucki , and stop earli befor you hit the end of the list .
so , all we're go to do is bother to analyz unsuccess lookup that will carri over to all of the other oper .
so , a littl more precis , let's let s be the data set .
thi is the object that we ar store in our hash tabl .
and rememb that to get constant time lookup , it realli need to be the case that the load is constant .
so , we ar assum that the size of s is bigger over the number of bucket n .
and let's suppos we ar search for some other object which is not an s , call it x .
and again , i want to emphas we ar make no assumpt about what thi data set s is other than that the size is compar to the number of bucket .
so , conceptu do a lookup in a hash tabl with chain is a veri simpl thing .
you just hash to the appropri bucket and then you scan through the length list in that bucket .
so , conceptu , it's veri easi to write down the what the run time of thi unsuccess lookup is .
it's got two part .
so , the first thing you do is you evalu the hash function to figur out the right bucket .
and again , rememb we're assum that we have a simpl of a hash function and it take constant time .
now , of cours , the magic of hash function is that given that thi hash valu , we can zip right to where the lenght list is to search for x us random access into our arrai of bucket .
so , we go straight to the appropri place in our arrai of bucket and we just search through the list ultim fail to find what we're look for s .
travers a link list , as you all know , it take time proport to the length of the list .
so , we find someth that we discuss inform in the past which is that's the run time of hash tabl oper implement with chain is govern by the list length .
so , that's realli the kei quantiti we have to understand .
so , let call thi , let give thi a name , capit l , it's import enough to give a name .
so , what i want you to appreci at thi point is that thi kei quantiti , capit l , the list of the length in x's bucket is a random variabl .
it is a function of which hash function littl h , we wind up select in a real time .
so , for some choic of our hash function , littl h , thi list length might be as small as zero but for other choic of thi hash function h , thi list , list length could be bigger .
so , thi is exactli analog too in quicksort where depend on the real time decis of which random pivot element you us , your go to get a differ number of comparison , a differ run time .
so , the list length and henc the time for the unsuccess storag , depend on the hash function littl h , which we're choos at random .
so , let's recal what it is we're try to prove .
we're try to prove an upper bound on the run time of an unsuccess lookup on averag , where the on averag is over the choic of the hash function littl h .
we've express that thi lookup time in term of the averag list length in x's bucket where again the averag is over the random choic of h .
summar , we've reduc what we care about , expect time for lookup to understand the expect valu of thi random variabl capit l , the averag list length in x's bucket .
so , that's what we've got to do , we've got to comput the expect valu of thi random variabl , capit l .
now to do that , i want to jog your memori of a gener techniqu for analyz expect which you haven't seen in awhil .
the last time we saw it , i believ , wa when we were do the analysi of random quicksort and count it comparison .
so , here's a gener decomposit principl which we're now go to us in exactli the same wai as we did in quicksort here to analyz the perform of hash with chain .
so , thi is where you want to understand the expect , expect of random variabl which is complic but what you can express as the sum of much simpler random variabl .
ideal , <num> , <num> or indic random variabl .
so , the first step is to figur out the random variabl , capit y , it's what i'm call it here that you realli care about .
now , we finish the last slide , complet step on .
what we realli care about is capit l , the list length in x's bucket .
so , that govern the run time a bit unsuccess look up , clearli that's what we realli care about .
step two of the decomposit principl is well , you know , the random variabl you care about is complic , hard to analyz directli but decompos it as a sum of <num> , <num> indic random variabl .
so , that's what we're go to do in the begin of the next slide .
why is it us to decompos a complic random variabl into the sum of <num> , 1random variabl ?
well , then you're in the wheelhous of linear of expect .
you get that the expect valu of the random variabl that you care about is just the sum of the expect valu of the differ indic random variabl and those expect ar gener much easier to understand .
and that will again be the case here in thi theorem about the perform of hash tabl with chane .
so , let's appli thi three step decomposit principl to complet the proof of the carter wegman theorem .
so , for the record , let me just remind you about the random variabl that we actual realli care about , that's capit l .
the reason that's a random variabl is that becaus it depend on the choic of the hash function , littl h .
thi number could vari between zero and someth much , much bigger than zero , depend on which each you choos .
so , thi is complic , hard to analyz directli , so let's try to express it as the sum of <num> , <num> random variabl .
so , here ar the0 , <num> random variabl that ar go to be the constitu of capit l .
we're go to have on such variabl for each object y in the data set .
now , rememb thi is an unsuccess search , x is not in the data set capit s .
so , if y is in the data set , x and y ar necessarili differ .
and we will defin each random variabl z sub y , as follow .
we'll defin it as on if y collid with x under h and zero otherwis .
so , for a given zy , we have fix object x and y so , x is some ip address , sai , y is some distinct ip address , x is not in our hash tabl , y is in our hash tabl .
and now , depend on which hash function we wind up us , these two distinct ip address mai or mai not get map to the same bucket of our hash tabl .
so , thi indic a random variabl just indic whether or not thei collid , whether or not we unluckili choos a hash function littl h that send these distinct ip address x and y to exactli the same bucket .
okai , so , that's zy , clearli by definit , it's a <num> , <num> random variabl .
now , here's what's cool about these random variabl is that capit l , the list length that we care about decompos precis into the sum of the zy's .
that is , we can write capit l as be equal to the sum over the object in the hash tabl of zy .
so , if you think about it , thi equat is alwai true , no matter what the hash function h is .
that is if we choos some hash function that map these ip address x to , sai , bucket number seventeen , capit l is just count how mani other object in the hash tabl wind up get map to bucket number seventeen .
so , mayb ten differ ob ject got map to bucket number seventeen .
those ar exactli the ten differ valu of y that will have their zy equal to1 , right ?
so , so l is just count the number of object in the data set s that's collid with x .
a given zy is just count whether or not a given object y in hash tabl is collid with x .
so , sum over all the possibl thing that could collid with x , sum over the zy's , we of cours get the total number of thing that collid with x which is exactli equal to the number , the popul of x's bucket in the hash tabl .
all right , so we've got all of our duck line up in a row .
now , if we just rememb all of the thing we have go for us , we can just follow our nose and nail the proof of thi theorem .
so , what is it that we have go for us ?
well , in addit to thi decomposit of the list length in to indic random variabl , we've got linear expect go for us , we've got the fact that our hash function is drawn from a univers famili go for us .
and we've got the fact that we've chosen the number of bucket and to be compar to the data set size .
so , we want to us all of those assumpt and finish the proof that the expect perform is constant .
so , we're go to have a few inequ and we're go to begin with the thing that we realli care about .
we care about the averag list length in x's bucket .
rememb , we saw in the previou slide , thi is what govern the expect perform of the lookup .
if we can prove that the expect valu of capit l is constant , we're done , we've finish the theorem .
so , the whole point of thi decomposit principl is to appli linear of expect which sai that the expect valu of a sum of random variabl equal the sum of the expect valu .
so , becaus l can be express as the sum of these zy's , we can revers the summat and the expect and we can first sum over the y's , over the object in the hash tabl and then take the expect valu of zy .
now , someth which came up in our quicksort an alysi but which you might have forgotten is that <num> , <num> random variabl have particularli simpl expect .
so , the next quiz is just go to jog your memori about why <num> , <num> random variabl ar so nice in thi context .
okai , so the answer to thi quiz is the third on , the expect valu of zy is simpli the probabl that x and y collid , that just follow from the definit of the random variabl zy and the definit of expect , name recal how do we defin zy .
thi is just thi on , if thi object y in the hash tabl happen to collid with the object x that we ar look for under the hash function x and zero otherwis , again , thi will be , thi will be on for some hash function and zero for other hash function .
and then we just have to comput expect .
so , on wai to comput the expect valu of a <num> , <num> random variabl is , you just sai , well , you know , there ar the case where the random variabl evalu to zero and then there's the case where the random variabl evalu to on , and of cours we can cancel the zero .
so , thi just equal the probabl that zy on .
and sinc zy be on is exactli the same thing as x and y collid , that's what give us the answer .
okai , so it's the probabl that x collid with y .
so , plenti of that into our deriv .
now , we again have the sum of all the object y in our hash tabl and the set of the expect valu of zy what's right that in the more interpret form , the probabl that thi particular object in the hash tabl y collid with the thing we ar look for x .
now , we know someth pretti cool about the probabl that a given pair of distinct element like x and y collid with each other .
what is it that we know ?
okai , so i hope you answer the second respons to thi quiz .
thi is realli in some sens the kei point of the analysi .
thi is the role , that be a univers famili of hash function plai in thi perform guarante .
what doe it mean to be univers ?
it mean for ani pair of object distinct like x and y in that proof , if you make a random choic of a hash function , the probabl of collis is as good as with perfectli random hash , hash .
name at most , <num> n where n is the number of bucket .
so , now i can return to the deriv .
what that quiz remind you is that the definit of a univers famili of hash function guarante that thi probabl for each y is at most <num> n , where n is the number of bucket in the hash tabl .
so , let's just rewrit that .
so , we've upper bound the expect list length by a sum over the object in the data set of <num> n .
and thi , of cours , is just equal to the number of object in the data set , the of s divid by n .
and what is thi ?
thi is simpli the load , thi is the definit of the load alpha which we ar assum is constant .
rememb , that wa the third caveat in the theorem .
so , that's why as long as you have a hash function which you can comput quickli in constant time .
and as long as you keep the load under control so the number of bucket is commensur with the size of the data set that you're store .
that's why , univers hash function in a hash tabl with chain guarante expect constant time perform .
in the last video , we discuss the perform of hash tabl that ar implement us chain , us on link list per bucket .
in fact , we prove mathemat that if you us a hash function chosen uniformli at random from a univers famili , and if you keep the bucket , number of bucket , compar to the size of your data set , then in fact , you're guarante constant time expect perform but , recal that chain is not the onli implement of hash tabl .
there's a second paradigm which is also veri import call open address .
thi is where you're onli allow to store on object in each slot , and you keep search for an empti slot when you need to insert a new object into your hash tabl .
now it turn out it's much harder to mathemat analyz hash tabl implement us open address but , i do want to sai a few word about it to give you the gist of what kind of perform you might hope to expect from those sort of hash tabl .
so recal how open address work .
we're onli permit to store on object in each slot so thi is unlik the case with chain where we can have an arbitrarili long list in a given bucket of the hash tabl .
with at most on object per slot , obvious open address onli make sens when the load factor alpha is less than on .
when the number of object you're store in your tabl is less than the number of slot avail becaus of thi requir we have at most on object per slot we need to demand more of our hash function .
our hash function might ask us to put a given object , sai with some ip address into sai bucket number seventeen but bucket number seventeen might alreadi be full , might alreadi be popul .
in that case , we go back to our hash function and ask it where to look for an empti slot next .
so mayb it tell us to next look in bucket <num> .
if <num> is full it tell us to look in bucket number seven and so on two specif strategi for produc a probe sequenc that we mention earlier were doubl hash and linear probe .
d oubl hash is where you us two differ hash function , h1 and h2 .
h1 tell you which slot in which to search first and then everi time you find a full slot you add an increment which is specifi by the second hash function h2 .
linear probe is even simpler you just have on hash function that tell you where to search first and then you just add on to the slot until you find an empti slot as i mention at the begin , it is quit nontrivi to mathemat analyz the perform of hash tabl , us these variou open address strategi .
it's not imposs .
there is some quit beauti and quit inform theoret work .
that doe tell us how hash tabl perform but that's well outsid the scope , of thi cours .
so instead what i wanna do is i want to give you a quick and dirti calcul .
that suggest , at least in an ideal world .
what kind of perform we should expect from a hash tabl with open address if it's well implement as a function of the load factor , alpha .
precis , i'm go to introduc a heurist assumpt .
it's certainli not true but we'll do it just for a quick and dirti calcul , that we're us a hash function in which each of the n factori possibl probe sequenc is equal like .
now , no hash function you're ever go to us is actual go to satisfi thi assumpt , and if you think about it for a littl bit , you realiz that if you us doubl hash or linear program , you're certainli not go to be satisfi that assumpt .
so thi will still give us a kind of best case scenario against to which you can compar the perform of your own hash tabl implement .
so if you hash tabl , and you're see perform as good , as what's suggest by thi ideal analysi , then you're home free .
you know your hash tabl is perform great .
so what is the line in the sand that get drawn , under thi heurist assumpt ?
what is thi ideal , ideal hash function perform ?
as a function of the lo ad alpha well here it is .
what i'm gonna argu next is that under thi heurist assumpt , the expect amount of time to insert a new object into the hash tabl , is go to be essenti on over on minu alpha , where alpha is the load .
rememb the load is the number of object in the hash tabl divid by the number of avail slot .
so if the hash tabl is half full , then alpha's go to be . <num> .
if it's <num> percent full then alpha's go to be three fourth .
so what thi mean is that , in thi ideal scenario , if you keep the load pretti under control .
so , sai if the load is <num> , then the insert time is gonna be great , right ?
if alpha's . <num> and <num> <num> alpha two , so you expect just two probe befor the success insert of the new object and of cours , if you're think about lookup , that's go to be at least as good as insert , so if you're lucki a lookup might termin earli if you find what you ar look for .
in the worst case you go all the wai until an empti slot in an unsuccess search , and that's gonna be the same as insert .
so if alpha is small bound awai from on , you're get constant time perform .
on the other hand , as the hash tabl get full , as alpha get close to on , thi oper time is blow up ; it's such a go to infin as alpha get close to on .
so if you need to have a nice .
<num> percent full hash tabl with open address .
you're gonna start see , ten probe .
so , you realli wanna keep hash tabl with open address .
you wanna keep the load under control certainli no more than probabl . <num> .
mayb even less than that to refresh your memori , with chain , hash tabl ar perfectli well defin even with load factor bigger than on .
what we deriv is that under univers hash , under a weaker assumpt , we had an oper time of on plu alpha , for a load of alpha .
so with chain , you just gotta keep alpha , you know , at most , some reason small constant with open address , and you realli got to keep it well bound a wai below on .
so next let's understand why thi observ is true .
why under the assumpt that everi probe sequenc is equal like do we expect a on over on minu alpha run time for hash tabl with open address ?
so , the reason is pretti simpl .
and we can deriv it by analog with a simpl coin flip experi .
so , to motiv the experi , think just about the veri first probe that we do .
okai , so we get some new object , some new ip address that we want to insert into our hash tabl .
let's sai our hash tabl's current <num> percent full .
sai there's <num> slot , <num> ar alreadi taken by object .
well , when we look at thi first probe , by assumpt it's equal like to be ani on of the <num> slot .
<num> of which ar full , <num> which ar empti so , with probabl of on minu alpha , or in the case , <num> , our first probe will , luckili , find an empti slot and we'll be done .
we'll just insert the new object into that slot if we get unlucki with a probabl , <num> .
we find a slot that's alreadi occupi and then we have to try again .
so we try a new slot , drawn at random and we again check is it full , or is it not full ?
and again , with <num> percent probabl , essenti it's go to be empti and we can stop and if it's alreadi full .
then we try , yet again .
so do random probe , look for an empti slot , is tantamount to flip a coin with the probabl of head <num> alpha , or , in thi exampl , <num> percent and the number of probe you need until you successfulli insert is just the number of time you flip thi last coin until you see a head .
in fact thi bias coin flip experi slightli overestim the expect time for insert and the heurist assumpt and that's becaus in the insert time whenev we're never go to try the same slot twice .
we're go to try all end bucket in some order with each of the impact order equal like so back to our exampl , where we have a hash tabl with <num> slot , <num> of which ar full .
the first probe inde , we have a <num> in <num> chanc of ge tting an empti slot .
if that on fail then we're not go to try the same slot again .
so there is onli <num> residu possibl .
again , <num> of which ar empti .
the on we check last time wa full .
so we actual have a <num> over <num> percent chanc of get an empti slot on the second try .
like <num> over <num> on the third try , if the second on fail , and so on but , a valid upper bond is just to assum a <num> percent success probabl with everi singl probe , and that's precis , what thi coin flip experi will get us .
so the next quiz will ask you to actual comput the expect valu of capit n , the number of coin flip , need to get head when you have a probabl of head of on minu alpha .
as a hint , we actual analyz thi exact same coin flip experi when alpha equal a half , back when we discuss the expect run time of random linear time select .
all right , so the correct answer is the first on .
on over <num> alpha so to see why , let's return to our deriv , where we reduc analyz the expect insert time to thi random variabl .
the expect number of coin flip until we see a head .
so , i'm gonna solv thi exactli the same wai that we did it back when we analyz a random , select algorithm .
and it's quit a sneaki wai , but veri effect .
what we're go to do is we're go to express the expect valu of capit n , in term of itself , and then solv .
so how do we do that ?
well on the left hand side let's write the expect number of coin flip , the expect valu of capit n , and then let's just notic that there's two differ case , either the first coin flip is a head or it's not .
so in ani case you're certainli go to have on coin flip so let's separ that out and count it separ .
with probabl alpha , the first coin flip is gonna be tail and then you start all over again and becaus it's a memori less process , the expect number of further coin flip on requir , given that the first coin flip wa tail , is just the same as the expect number of coin flip in the first place .
so now it's a simpl matter to solv thi on linear equat for the expect valu of n , and we find that it is inde on over on minu alpha , as claim .
summar , under our ideal heurist assumpt , that everi singl probe sequenc is equal like , the expect insert time is upper bound by the expect number of coin flip , which by thi argument is , at most , on over on minu alpha .
so , as long as your load , alpha , is well bound below on , you're good .
at least in thi ideal analysi , you're hash tabl will , will work extrem quickli .
now i hope you're regard thi ideal analysi with a bit of skeptic .
right , from a fals hypothesi you can liter deriv anyth you want .
and we start with thi assumpt which is not satisfi , by hash function you're actual go to us in practic .
thi heurist assumpt , that all probe sequenc ar equal like .
so , should you expect thi on over on minu alpha bound to hold in practic or not ?
well , that depend to some extent .
it depend on what open address strategi you're us .
it depend on , how good a hash function you're us .
it depend on whether the data is patholog or not .
so , just to give cours rule of thumb if you're us doubl hash and you have non patholog data , i would go ahead and look for thi <num> <num> alpha bound in practic .
so implement your hash tabl , check it perform as a function of the load factor alpha and shoot for the <num> <num> alpha curv .
that's realli what you'd like to see .
with linear probe , on the other hand , you should not expect to see thi perform guarante of <num> <num> alpha even in a total ideal scenario .
rememb , linear probe is the strategi where your initi probe , the hash function , tell you where to look first , and then you just skim linearli through the hash tabl until you find what you're look for , an empti slot , the .
that you're look up or whatev so a linear probe , even in a best case scenario , it's go to be subject to clump .
you're go to have contigu group of slot which ar all full , and that's becaus of the linear probe strategi .
now i encourag you to do some experi with implement to see thi for yourself .
so becaus of clump with linear probe , even in the ideal scenario , you're not go to see on over on minu alpha .
howev , you're go to see someth wors , but still in ideal situat .
quit reason so that's the last thing i want to tell you about in thi video .
now needless to sai , with linear probe the heurist assumpt is badli fals .
the heurist assumpt is pretti much alwai fals to no matter what hash strategi you're us , but with linear program it's quot on quot realli fals .
so to see that , the heurist assumpt , sai that all in factori probe sequenc ar equal like .
so your next probe is go to be uniform or random amongst everyth you haven't probe so far but when you're probe , it's total the opposit .
right onc you know the first slot that you're look into sai bucket seventeen , slot a7 is gonna be the first slot , you know the rest of the sequenc becaus it's a linear cancel the tabl .
so it's kind of the opposit from each success probe be independ from the previou on except not explor thing twice .
so to state a conjectur or ideal perform guarante for hash tabl with linear probe , we're go to place , replac the blatant fals heurist assumpt by a still fals , but more heurist reason assumpt .
so what do we know ?
we know that the initi probe with linear probe determin the rest of the sequenc .
so let's assum that these initi probe ar uniform at random , and independ for differ kei .
of cours , onc you have the initi probe , you know everyth els , but let's assum independ and uniform amongst the initi probe .
now , thi is a strong assumpt .
thi is wai stronger than assum you ha ve a univers famili of hash function .
thi assumpt is not satisfi practic , but perform guarante we can deriv under thi assumpt ar typic satisfi in practic by well implement hash tabl that us linear probe .
so , the assumpt is still us for deriv the correct , ideal perform of thi type of hash tabl .
so what is that perform ?
well thi is an utterli classic result from exactli <num> year ago from <num> and thi is a result by my colleagu , the live legend , don canuth , author of art of comput program .
at what can prove is , wa that is that under thi weaker assumpt , suitabl for linear probe .
the expect time to insert an object into a hash tabl with a load factor alpha , when you're us linear probe is wors than on over on minu alpha , but .
it is still a function of the load alpha onli and not a function of the number of object in the hash tabl .
that is with linear program you will not get as good a perform guarante , but it is still the case that if you keep the load factor bound awai from on .
if you make sure the hash tabl doesn't get too full you will enjoi constant time oper on averag so for exampl if with linear probe your hash tabl is <num> percent full then you ar go to get an expect insert time of roughli four probe .
note howev thi quantiti doe approach doe blow up pretti rapidli as the hash tabl grow full .
if it is <num> percent full thi is alreadi go to be someth like a hundr probe on averag .
so you realli don't wanna let hash tabl get too full when you ar us linear probe .
you might well wonder if it's ever worth , implement linear probe , given that it ha the worst perform curv , on over on minu alpha squar .
then the perform curv you'd hope from someth like doubl hash , on over on , minu alpha .
and it's a tricki cost benefit analysi between linear probe and more complic but better perform strategi .
that realli depend on the ap plicat .
there ar reason that you do want to us linear probe sometim , it is actual quit common in practic for exampl , it's often interact veri well with memori hierarchi so again , as with all of thi hash and discuss .
you know the cost and benefit ar , ar veri subtl trade off between the differ approach .
if you have mission critic code that's us a hash tabl and you realli want to optim it .
try a bunch of prototyp , and just test .
figur out which on is the best , for your particular type of applic .
let me conclud the video with a quot from canuck himself where he talk about the raptur of prove thi on of our on man is half a squar theorem and how it wa life chang .
he sai i first formul the follow deriv , mean , the proof of that last theorem in <num> .
ever sinc that dai , the analysi of algorithm ha , in fact , been on of the major theme in my life .
so , in thi video , we're go to discuss bloom filter which is a data structur develop appropri enough by burton bloom back in <num> .
bloom filter ar variant on hash tabl , you'll recogn a lot of the idea from our hash tabl discuss .
the win that you get in bloom filter is that thei ar more space effici than run of the mill hash tabl and thei're go to handl , thei do allow for error , there is a non zero fals posit probabl when you do look up but that's still a win for some applic .
so , it's a veri cool idea , veri cool data structur .
you do see it us quit a bit in practic so let's start talk about it .
so , we'll go through the usual topic that we do whenev we discuss a new data structur .
so first , i want to tell you what oper thei support and what kind of perform you're go to expect from those oper , in other word , what is the api correspond to the data structur .
secondli , i'm go to talk a littl bit about what it's good for .
so , what ar some potenti applic and then we'll take a peek under the hood .
i'll tell you some of the implement detail with an emphasi on explain why you get the kind of perform trade off that you do with bloom filter .
so , to first order , the raison d' tre of bloom filter is exactli the same as a hash tabl .
it support super fast insert , super fast look up .
you can put stuff in there and you can rememb what you put in earlier .
now , of cours , what you should be wonder is what we alreadi know what data structur that support super fast in certain look up , a hash tabl .
why am i bother to tell you about yet anoth data structur with exactli those same oper ?
so , let me tell you about the pro and con of bloom filter rel to run off the mill hash tabl as we've alreadi discuss .
the big win is that bloom filter ar more space effici than hash tabl .
no matter whether thei ar implement with chain or with open address , you can store much less space per object .
in fact , as we'll see , less space than that of an object itself us a bloom filter .
as far as the con , well , first of all , thi is realli for applic where you just want to rememb what kind of valu you see .
you ar not try to store pointer to the object themselv and just try to rememb valu .
so , the first drawback of the bloom filter is that becaus we want to be so space effici , we don't even want to rememb the object itself just whether or not we've seen it befor .
we're not go to be abl to store the object or even pointer to the object in a bloom filter .
we're just go to rememb what we've seen and what we haven't .
so , some of you might know the terminolog hash set for thi kind of variant of a hash tabl as oppos to a full blown hash tabl or hash map .
the second con is at least in the vanilla implement of bloom filter that i'm go to describ here , delet ar not allow .
you can onli insert , you can't delet .
the situat with delet is veri much similar to hash tabl implement with open address .
it's not that you can't have a bloom filter that accommod delet , you can , there ar veri instanc of it but that requir significantli more work and we're not go to discuss it here .
so , the first order at least for vanilla bloom filter , you want to think of them as suitabl for applic or delet or not a first order of oper .
now , the third con and thi is a drawback that we have not see previous us ani data structur is bloom filter can actual make mistak .
now , what kind of mistak could thi kind of data structur possibl make when all you're realli do is look someth up .
well , on of mistak would be a fals neg and that mean you have insert someth previous then you look it up and the hash tabl or the bloom filter sai , it's not there .
so , bloom filter will not have fals neg of thi form .
you've insert someth , you look it up later , it's definit go to confirm that you insert it in the past .
but bloom filter will have fals posit , that mean that despit the fact you have never insert sai , a given ip address into the , into the bloom filter , if you look it up later , it will sai that you have .
so , there will sometim be in some sens phantom object in bloom filter , object which it think have been insert even though thei haven't been .
so , given that , i am now show you two data structur with essenti the same function , hash tabl and bloom filter , at least , if we ignor the delet issu .
you might want to wonder which on is more appropri , which on is more us .
and becaus there is these trade off between the two , the answer as you expect is , it depend on the applic , right ?
so , if it's an applic where space is realli at a premium , you might want to turn to bloom filter especi if a small chanc of a fals posit is not deal breaker .
if you have some kind of applic where fals posit ar absolut out of the question , of cours , you should not us a bloom filter and you want to think about a hash tabl .
so , what ar some situat where peopl actual do us bloom filter where you either realli care about space and or you don't realli care about thi fals posit probabl .
for on of the earliest applic of bloom filter , thi is not long time ago , thi is someth like <num> year ago , wa the spell checker .
so , how would you implement a spell checker us a bloom filter ?
well , first you have thi insert phase where you basic just go through the entir dictionari word by word and you insert everi valid word into the bloom filter .
then , afterward , when you're present with a new document that somebodi ha written , you're go to go through the document word by word for each word , you sai , is thi in the bloom filter ?
that is , is thi on of the legitim word from the dictionari which is previous insert ?
if the bloom filter sai ye , thi word is in the dictionari as in we've store and seen that befor , then you treat is as a correctli spell word and if it's not in the bloom filter , then you treat it as incorrectli spell word .
now , the fals posit probabl mean thi isn't a perfect spell checker .
i mean sometim , you're go to look up a misspel word and the bloom filter won't catch it and it willl actual sai ye , with small probabl , we'll sai , thi is a legitim word .
so , you know , it's not ideal but , you know , the , the english languag is pretti big and space wa definit at a premium , <num> plu year ago .
so , it wa a win for that applic at that time , to us bloom filter to implement a spell checker .
anoth applic which , you know , remain relev todai is to keep track of a list of forbidden password .
now , why would you have forbidden password ?
well , mayb , you want to keep track of password which ar too weak or too easi to guess or too common .
you mai , yourself , have us the piec of softwar or websit at some point where it ask you for a password and if you type in someth which is too simpl or too easi , reject it and ask you to type in anoth on .
so , on wai to implement a list of forbidden password is just with the bloom filter and the idea is similar to the spell checker .
you first , insert into the bloom filter all of the password that you don't want anybodi to us for whatev reason .
then , when a client come and tri to type in a new password , you look it up in the bloom filter and if you get a posit look up , then you tell the user , no , that's no good , you can't us that password , choos anoth on .
and thi is an applic where you realli don't care about the error , you realli don't care about the fact that there's a fals posit rate .
let's assum that the error rate is someth like on percent or <num> .
so , what would that mean in context , that would just mean onc in a while , on in a hundr client or on in a thousand client actual type in a perfectli strong password that get reject by the bloom filter and thei have to type in a second on .
okai , but big deal and if space is at the , the premium , thi is definit a win to us thi super lightweight data structur to keep track of these block password .
these dai certainli on the killer applic of bloom filter is in softwar deploi on network router .
so , the machineri out in the internet which is respons for transmit packet from on place to anoth .
so , what ar the reason why bloom filter have found fertil applic in network router ?
well , first of all , you do have a budget on space , typic on network router .
there's a lot of thing that you got to do and you don't want to wast that much of it on some random data structur to do on's specif task .
so , you do have a budget on space and also , you need super , super fast data structur , right ?
sinc these packet ar come in at thi torrenti rate which you can't even imagin and you want to process these packet in real time , send them off to the next top .
bloom filter ar the work forc behind a lot of differ task that is done in the network router .
you can imagin want to keep track of block ip address , you can imagin keep track of the content of some cach so you don't do spuriou look up .
you can imagin maintain statist to check for denial of servic attack and so on and so forth .
so , summar as a expert programm , what is it that you should rememb about bloom filter , what purpos doe thi tool serv in your tool box ?
well , as far as the oper support which is the same as a hash tabl , the point is to have super fast insert , super fast look up .
but bloom filter ar more lightweight version of a hash tabl .
so , thei ar more space effici but thei do have thi drawback of have a small error probabl .
so , those ar the kei featur you should rememb when decid whether or not you ar work on an applic that could make good us of thi data structur .
so , have discuss on of th e oper and what these data structur ar good for , let's take it to the next level , let's peer under the hood and see how thei ar actual implement .
cuz thi is realli a quit simpl , quit cool idea .
so , like hash tabl , bloom filter have essenti two ingredi .
first of all , there's an arrai and second of all , there's a hash function or in fact , sever hash function .
so , we're go to have a random access arrai except , instead of have n bucket or n slot as we've been call them , each entri in thi arrai is just go to be a singl bit .
each entri in thi arrai can onli take on two valu , zero or on .
and the wai thei think about the space occupi by bloom filter is in term of the number of bit per object that ha been insert into the bloom filter .
so , if you have insert the data set capit s , then the total number of bit is n , the number of object that have been insert is cardin of s .
so , n s is the number of bit in thi data structur that you ar us per entri in the data set .
now , you can tune a bloom filter so thi ratio is ani number of differ quantiti but for now , i encourag you to think of thi ratio as be eight , that is for each object store in the bloom filter , you ar us onli eight bit of memori .
that will help you appreci just how amaz thi data structur ar , right , cuz mayb our data set is someth like ip address which is <num> bit so what i'm sai here , if thi is eight , i'm sai we ar not , definit not actual store the ip address .
so , we have thi <num> bit object we ar insert and we ar onli us eight bit of memori .
thi is how we ar go to rememb whether it there or whether it not .
and again , certainli , eight bit per object is wai less than keep a pointer to some associ memori somewher .
so , thi is a realli impress minim us of space to keep track of what we've seen and what we haven't .
and secondli , we need map of given an object to sai , given the ip address , what ar the relev bit for see if we've seen thi ip address befor or not ?
so , in a bloom filter , it import to have not on hash function , but sever hash function .
so , k is go to denot the number of hash function in the bloom filter which you think of k is some small constant somewher , you know , three , four , five , or someth like that .
so , obvious it's a littl bit more complic to us multipl hash function as suppos to just on hash function .
but it's realli not that big of deal .
so , we'll call from our discuss of sai , univers hash , we have identifi the entir famili of hash function which will work well on averag .
so , instead of choos just us on hash function at random from univers famili , you gave me k independ random choic from univers famili .
in fact , in practic , it seem to typic be enough to just us two differ hash function and then gener k differ linear combin of those two hash function .
but for the purpos of thi video , let's just assum that we've done enough work to come up with k , differ good hash function and that's what we're go to be us in our bloom filter .
so , the code for both insert and delet is veri eleg .
so , let's start by insert .
so , suppos we have some new ip address and we want to stick into these bloom filter , what we do ?
well , we'll just evalu each of our k hash function on thi new object .
each of those tell us an index into our arrai of bit and we'll just set those k bit equal to on .
and when we do thi insert , we don't even bother to look at what the previou valu of these bit were . .
so , zero or on , we don't care .
we'll just blith go in and set thi k bit equal to on , whatev thei were befor .
so , what about look up ?
how ar we go to implement that ?
well , all you have to do is check for the footprint that wa inevit left by a prior insert .
so , if we're look up an ip address and we know wa insert sometim in the past , what happen when we evalu the k hash function , we went t o appropri posit in the arrai and we set all of those bit to on .
so now , i'll just check that , that inde happen , that is when we get a new ip address , we're look it up .
we evalu the hash function , all k of them .
we look at the correspond k posit and we verifi that inde those k bit have been set to on .
so , what i hope is clear fairli quickli from inspect thi veri eleg code is that we will not ever have fals neg , yet , we might have fals posit .
so , let's discuss those on other time .
so , rememb , a fals neg would mean that the bloom filter sai , someth isn't there when in fact , it is , that is we insert someth and we'll look it up later and the bloom filter reject us .
well , that's not go to happen .
cuz when we insert someth , we set the relev k bit to on .
notic when a bit is on , it remain on forevermor .
that bit ar never reset back to zero .
so , if anyth wa ever insert in the sub when we look it up , definit we well confirm that all those bit ar on .
so , we're never go to be reject by someth we insert befor .
on the other hand , it is total possibl that we will have a fals posit .
it's total possibl that there will be a phantom object and we'll do a look up and the bloom filter will turn ye when we never insert that object .
suppos for exampl , the k three .
so , we're us three differ hash function .
consid some ip address , fix ip address , mayb the three hash function tell us the relev bit ar seventeen , <num> , and <num> .
mayb we never insert thi ip address , but we have insert ip address number two and in it insert , the seventeenth bit got set to on .
we insert some other ip address , ip address number three and the twenti third bit got set to on .
and then we insert ip address number four and the 36th bit got set to on .
so , three differ ip address were respons for set these three differ bit but whatev , it not like we ar rememb that .
and that onc , onc we look up thi ip address we realli care about , what do we do , we just inspect bit seventeen , it on .
inspect the <num> , it on .
we inspect the <num> , it also on .
for all we know , thi thing realli wa insert and the bloom filter is go to sai , ye , it's in the tabl .
so , that's how we have fals posit .
all of the bit that ar indic whether or not a given object ar in , ar in the bloom filter were previous set by insert from other object .
so , there ar two point that i hope ar clear at thi stage of the discuss .
first of all , that thi bloom filter , the idea doe suggest a possibl of a super space effici variant of a hash tabl , right .
so , we've been talk about set the number of bit to be sai roughli eight time the number of object that you're store so you're onli us eight bit per object and for most object , that's go to be radic smaller than just the simpl arrai , store the object themselv .
again , if their ip address we're onli have <num> percent much space as we actual store those ip address in just an arrai with no extra bell and whistl .
the second point is that we're inevit go to have error in a bloom filter , we will have fals posit or we look someth up and it sai , it there when in fact , it not .
so , those two point i hope ar clear .
what's actual not clear is the bottom line .
is thi actual a us idea ?
for thi to be us , it'd better be the case that the error probabl can be quit small even while the space per object is quit small .
if we can't get those two thing small simultan , thi is a bad idea and we should alwai just us a hash tabl instead .
so , to evalu the qualiti of thi idea , we're go to have to do littl bit of mathemat analysi .
that's what i'm go to show you in the next coupl of slide .
so , befor we embark in the analysi , what ar we hope to understand ?
well , it seem intuit clear is that there is go to be some trade off between the two resourc of the bloom filter .
on resourc is space consumpt , the other resourc is essenti correct so the more space we us , the larger number of bit , we'd hope that we'd make fewer and fewer error .
and then as we compress the tabl more and more , we us bit more and more for differ object then presum the error rate is go to increas .
so , the goal of the analysi that we're about to do is to understand thi trade off precis at qualit level .
onc we understand the trade off occur between these two resourc , then we can ask is there is a sweet spot which give us a us data structur ?
quit small space and quit manag error probabl .
so the wai we're go to proce with the analysi , we'll be familiar to those of you who watch the open address video about hash tabl so to make the mathemat analysi tractabl , i'm go to make a heurist assumpt the strong assumpt which is not realli satisfi by hash function you would us in the practic .
we're go to us that assumpt to deriv a perform guarante for bloom filter but as all as ani implement you should check that your implement actual is get perform compar to what the ideal analysi suggest .
that said , if you us a good hash function and if you have a non patholog data , the hope and thi is go out mani empir studi is that you will see perform compar to what thi heurist analysi will suggest .
so , what is the heurist assumpt ?
well , it's go to be again familiar from my hash discuss .
we're just go to assum that all the hash is total random .
so , for each choic of a hash function hi and for each possibl object ax , the slot , the posit of the arrai which the hash function give for that object is uniformli random and first of all and it's independen t from all other output of all hash function on all object .
so the set up then is we have n bit .
we have a data set , s which we have insert into our bloom filter .
now our eventu goal is to understand the error rate or the fals posit probabl .
that is the chanc that an object which we haven't insert into the bloom filter look as if it ha been insert into the bloom filter but as a preliminari step , i want to ask about the popul of 1s after we've insert thi data set s into the bloom filter .
so , specif let's focu on a particular posit of the arrai and by symmetri it doesn't matter which on .
and let's ask what is the probabl that a given bit , a given posit on thi arrai ha been set to on after we've insert thi entir data set s ?
all right , so thi , thi is a somewhat difficult quiz question actual .
the correct answer is the second answer .
it's on quantiti on <num> n rais to the number of hash function k the number of object cardin of s , that's the probabl let's sai the first bit of the bloom filter ha been set to on after the data set s ha been insert .
so the , mayb the easiest wai to see thi is to first focu on the first answer .
so , the first answer is go to be the probabl i claim that the first bit is zero after the entir data set ha been insert .
then of cours it's probabl it's a on , is just the on it quantiti which is equal to the second answer .
so we just seem to understand why the first choic is probabl the first bit zero .
well , it's initi zero , rememb stuff is onli set from zero to on .
so we realli need to analyz the probabl that thi first bit surviv all of these dart that ar get thrown to the bloom filter over the cours of thi entir data set be insert .
so there , the cardin of these object each get insert on an insert k dart uniformli at random and independ from each other or effect thrown at the arrai at the bloom filter .
ani posit of the dart hit , get set to on .
mayb it wa on alreadi but if it wa zero , it get set to on .
if it's on then it stai on .
so , how is thi first pick go to stai zero ?
we'll have to be miss by all of the dart .
a given dart , a given bit flick is uniformli like to be ani of the n bit so the probabl of the on that be thi bit is onli <num> n but , if it even it's fortun somebodi els ?
well , that's on <num> n so you have a chanc of surviv a singl dart with probabl on <num> n there is the number of hash function k the number of object cardin that's a dart be thrown .
right k per object that get insert so the overal probabl of elud all of the dart is on on or n rais to the number of hash function k the number of insert cardin of s .
again , the probabl that is on which is the on that quantiti which is the second option in the quiz .
so , let's go ahead and resum our analysi us the answer to that quiz .
so , what do we discov , discov the probabl that a given bit is on , is on quantiti on <num> n or n is the number of posit rais to the number of hash function k the number of insert cardin of s .
so , that's the kind of messi quantiti so let's recal a simpl estim fact that we us onc earlier .
you saw thi when we analyz cardin construct algorithm and the benefit of multipl repetit or cardin contract algorithm .
and the trick here is to estim a quantiti that's on the form of on x or on x by either the x or the x as the case mayb .
so you take the function on x which goe through the point ten and <num> .
and of cours it's a straight line and then you also look at the function e to the x .
well , those two function ar go to kiss at the point <num> , <num> and everywher els e to the x is go to be abov on x .
so for ani real valu of x we can alwai upper bound the quantiti on x by either the x .
so let's appli thi fact to thi quantiti here , on <num> n rais to the k cardin of s .
we're go to take x to the <num> n so that give us an upper bound on thi probabl of on e to th e k the number of insert over n , okai ?
so that's take x to the <num> n .
let's simplifi and final a littl bit further by introduc some notat .
so , i'm go to let b denot the number of bit that were us per object .
so thi is the quantiti i wa tell you to think about as eighth previous .
thi is the ratio n , the total number of bit divid by the cardin of s .
so , thi green express becom on e k where b is the number of bit per object .
and now we're alreadi see thi type of trade off that we're expect .
rememb we're expect that as we us more and more space , then the error rate we think should go down so if you can press the tabl a lot or us bit for lot of differ object that's when you start go to see a lot of fals posit so in thi light blue express if you take the number of bit per object with the number space , the amount of space , littl b if you take that go veri larg expand to infin , thi expon to zero .
so either the zero is on .
so overal , thi probabl of a given bit be on is turn to zero .
so , that is , the more bit you have , the bigger space you have .
the , well , the smaller of the fraction of 1s .
the bigger the faction of 0s .
that should translat to a smaller fals posit probabl unless we will make precis on the next and final slot .
so let's , let's rewrit the upshot form the last slide but probabl that a given bit is equal to on is that at abov by on e to the k over b where k is the number of hash function and b is the number of bit we're us per object .
now thi is not the quantiti that you care about .
the quantiti we care about is a fals posit probabl where someth look like it's in the bloom filter even though it's never been insert so it's focus on some object like some ip address which is never ever been insert into thi bloom filter .
so for a given object x which is not in the data set , that thi ha not been insert into the bloom filter or what ha to happen for us to have a success ful look up for fals posit for thi object ?
well each on of it k bit ha to be set to on .
so , we alreadi comput the probabl that a given bit is set to on .
so , what ha to happen for all k of the bit that indic x's membership in the bloom filter all k of them ha to be set to on .
so we just take the quantiti we comput on the previou slide and we rais that to the kth power .
indic that it ha to happen k differ time .
so believ it or not we now have exactli what we want .
what we set out to do which is deriv a qualit understand of the intuit trade off between the on hand space us and on the other hand on the error probabl .
the fals posit of probabl .
so , we're go to call thi green circl quantiti and name it .
we'll call it epsilon for the error rate and again all error ar fals posit .
and again as b goe to infin , as we us more and more space , thi expon goe to zero so on e to that quantiti is go to zero as well .
and of cours , onc we power it through the kth power , it get even closer to zero .
so if the bigger b get the small of thi error rate epsilon get .
so now let's get to the punch line .
so rememb the question is , is thi data structur actual us ?
can we actual set all of the paramet in a wai that we could both realli usefulli small space but a toler error epsilon ?
and , of cours we wouldn't be give thi video if the answer wasn't ye .
now on thing i've been allud all along is how do we set k ?
how do we choos the number of hash function ?
i told you at the veri begin we think of k as a small constant like <num> .
and now that we have thi realli nice qualit version of how the error rate in the space trade off with each other .
we can answer how to set k .
name set k optim so what do i mean ?
well , fix the number of bit that you're us per object .
eight , sixteen , <num> , whatev .
for fix b , you can just choos the k that minim the screen quantiti .
that minim the error rate epsilon .
so , how do you minim t hi quantiti ?
well , you do it just like you learn in calculu and i'll leav thi as an exercis for you to do in the privaci of your own home .
but for fix b , the wai to get thi green quantiti epsilon as small as possibl is to set the number of hash function k to be roughli the natur log of two .
that's a number of < on notic that's like . <num> b .
so , in other word the number of hash function for the optim implement of the bloom filter is scale linearli than the number of bit that you're us per object .
it's about . <num> the bit per object .
of cours thi is gener not go to be an integ so you just pick k either thi number round up or thi number round down .
but , continu the heurist analysi , now that we know how to set k optim to minim the error for a given amount of space we can plug that valu of k back in and see well , how doe the space and the error rate trade off against each other and we get a veri nice answer .
specif , we get that the error rate epsilon is just under an optim trade to the number of hash function k decreas exponenti in the number of bit that you us per object .
so , it's roughli on half rais to the natur log of two or . <num> roughli the number of bit per object b .
but , again the kei qualit point here is notic that epsilon is go down realli quickli as you scale b .
if you doubl the number of bit that you're alloc per object , you're squar the error rate and for small error rate , squar it make it much , much , much smaller .
and of cours thi is just on equat in two variabl .
if you prefer , you can solv thi equat to express b , the space requir as a function of an error requir .
so if you know that the toler for fals posit in your applic is on percent you can just solv thi for b and figur out how mani bit per object you need to alloc .
and so rewrit what you get is that the number of bit per object that you need is roughli <num> . <num> the log base two of on over epsilon .
so , as expect as epsilon get smaller and smaller , you want fewer and fewer error , the space requir will increas .
so , the final question is , is it a us data structur ?
can you set all the paramet so that you get you know , realli interest space error trade off and the answer is total .
so , let me give you an exampl .
let's go back to have eight bit of storag per object so that correspond to b eight .
then , what thi pick formula indic is we should us five or six hash function and alreadi you have an error probabl of someth like two percent which for a lot of the motiv applic we talk about is alreadi good enough .
and again , if you doubl the number of bit to sai sixteen per object , then thi error probabl would be realli small .
push you know on in <num> , <num> or someth like that .
so , to conclud at least in thi ideal analysi which again , you should check against at ani real world implement although empir , it is definit achiev with well implement bloom filter in nonpatholog data to get thi kind of perform even with realli a ridicul minuscul amount of space per object much less gener than store the object itself , you can get fast insert , fast look up , you do have to have fals posit but with a veri control amount of error rate and that what's make bloom filter a win in a number of applic .
the design and analysi of algorithm is the interplai between on the on hand , gener principl and on the other hand , extanti of those principl to solv specif problem .
while there's no silver bullet in algorithm design and no on techniqu that solv everi comput problem that's ever go to come up , there ar gener design principl which have prove us over and over again over the decad for solv problem that aris in differ applic domain .
those , of cours , ar the principl that we focu on in thi class .
for exampl in part on we studi the divid and conquer algorithm design paradigm and principl of graph search , amongst other .
on the other hand , we studi specif instanti of these techniqu .
so in part i , we studi divid and conquer and how it appli to sai .
stress matrix multipl , merg short , and quick sort .
in graph search we culmin with the rightfulli famou dijkstra's algorithm for comput shortest path .
thi of cours is us not just becaus as ani card carri comput scientist or programm , you want to know about what these algorithm ar and what thei do .
but it also give us a toolbox , a suit of four free primit which we can appli to our own comput problem as a build block in some larger program .
part ii of the cours , we'll continu thi narr .
we'll learn veri gener algorithm design paradigm like greedi algorithm , dynam program algorithm , and mani applic includ a number of algorithm for the greatest hit compil .
and in thi video and the next , i want to wet your appetit for what's to come .
by pluck out two of the applic that we'll studi in detail later in the cours , specificlali in the dynam program section of hte cours .
first of all , for both of these question , i think their import is self evid .
i don't think i'll have to realli discuss why these ar interest problem .
why in some sens .
we realli need to solv these two problem .
secondli , these ar quit tricki comput problem .
and i would expect that most of you do not current know good algorithm for these problem .
and it would be challeng to design on .
third , by the end of thi class you will know effici algorithm for both of these problem .
in fact , you'll know someth much better .
you'll know gener algorithm design techniqu , which solv as a special case these two problem , and have the potenti to solv problem come up in your own project thi as well .
and <num> comment befor we get start on these <num> video , thei're both at a higher level than most of the class by which i mean , there won't be ani equat or math , there won't be ani , concret pseudo code and i'll be gloss over lot of the detail .
the point is just to convei the spirit of what we're go to be studi and to illustr the rang of applic of the techniqu that we're go to learn .
so what i want to talk about first is distribut shortest path rout and why it's fundament to how the internet work .
so let me begin with a kind of veri non mathemat claim .
i claim that we can usefulli think of the internet as a graph , as a collect of vertic and a collect of .
so thi is clearli an ambigu statement as mani thing i might mean as will discuss but here's the primari interpret i want you to have for thi particular video .
so to specifi thi the vertic i intend to be the end host and the router of the internet so machin that gener traffic , machin that consum traffic and machin that help traffic get from on place .
so the edg ar go to be direct and thei're meant to repres physic or wireless connect indic that on machin can talk directli to anoth on via either a physic link between the two or direct wireless .
less connect .
so it's common that you'll have edg in both direct so that machin a can talk to machin b directli , then also machin b can talk directli to machin a .
but you definit want to allow the possibl of asymmetr metric commun .
so for exampl imagin i send an e mail from my stanford account to on of my old mentor's at cornel where i did my graduat studi .
so thi piec of data thi e mail ha to somehow migrat from my machin local at stanford to my mentor's machin over at cornel .
so how doe that actual happen ?
well initi there's a phase of sort of local transport .
so thi piec of data ha to get from my local machin to a place within the stanford network that can talk to the rest of the world .
just like if i wa try to travel to cornel i would have to first us local transport to get to san francisco airport and onli from there could i take an airplan .
so thi machin from which data can escap from the stanford network to the outsid world is call the gatewai router .
the stanford gatewai router pass it on to a network , whose job is to cross the countri .
so last i check , the commerci internet servic provid of stanford wa cogent .
so thei , of cours , have their own gatewai router , which can talk to the stanford on .
everybodi's versa .
and of cours these <num> node and the edg between them ar just thi tini , tini , tini piec embed in thi massiv graph compris all the end host and router of the internet .
so that's the main version of a graph that we're go to talk about in thi video , but let me just paus to mention a coupl of other .
graph that ar relat to the internet , and quit interest in their own right .
so , on graph that ha gener an enorm amount of interest in studi is the graph induc by the web .
so here , the vertic ar go to repres webpag , and the edg , which is certainli direct .
repres hyperlink , that on webpag point to anoth on .
so , for exampl , my homepag is on node in thi massiv , massiv graph , and as you might expect , there is a link from my homepag to the cours page for thi class .
it is of cours essenti to us direct edg to faithfulli model the web there is for exampl no direct edg from thi cours home page to my own home page at stanford .
so the web realli explod around in the mid <num>'s , late <num>'s .
so for the past <num> plu year there's been lot of research about the web graph .
i'm sure you won't be surpris to hear that you around the mid of the last decad peopl got extrem excit about properti of social net .
network , those of cours can also be fruitfulli thought of as graph .
here the vertic ar go to be peopl and the length ar go to denot relationship .
so for exampl , friend relationship on facebook or the follow relationship on twitter .
so notic that differ social network mai correspond to undirect or direct graph .
facebook for exampl , correspond to an undirect graph , twitter correspond to a direct graph .
graph .
so let's now return to the first interpret i want to focu on .
that where the vertic ar in host router .
and it just repres direct physic or wireless connect , indic that <num> machin can talk directli to each other .
so go back to that graph , let's go back to the stori where i'm send an email to somebodi at cornel , and thi data ha to somehow travel from my local machin to some local machin .
at cornel .
so in particular , thi piec of data ha to get from the stanford gatewai router in affect of the airport for stanford's network to the cornel gatewai router , so that the land airport over on cornel site .
so it's not easi to figur out exactli what the structur of the rout between stanford and cornel look like .
but on thing i can promis you is there's not a direct physic .
equal length between the stanford gatewai router and the cornel gatewai router .
ani rout between the two is go to compris multipl hop .
it will have intermedi stop .
and there's not go to be a uniqu such rout .
so if you have the choic between take on rout which stop in houston and then atlanta and then in washington dc , how would you compar that to on which stop in salt lake citi and chicago ?
well hopefulli your first instinct , and a perfectli good idea is , all els be equal , prefer the path that is in some sens the shortest .
now in thi context , shortest could mean mani thing , and it's interest to think about differ definit .
but for simplic , let's just focu on the fewest number of hop .
equival , the fewest number of intermedi stop .
well if we want to actual execut thi idea , we clearli need an algorithm that given a sourc and a destin , comput the shortest path between the <num> .
so hopefulli you feel well equip to discuss the problem becaus <num> of the highlight of part <num> of thi class wa the discuss of dijkstra's shortest path algorithm and a blazingli fast implement us heap , that run in almost linear time .
we did mention on caveat when we discuss dijkstra's algorithm , name that it requir all edg link to be non neg , but in the context of internet rout , almost ani medg , edg metric you'd imagin us will satisfi thi non neg assumpt .
there is howev a seriou issu with try to appli dijkstra's shortest path algorithm off the shelf to solv thi distribut internet rout problem .
and the issu is caus by the just massiv distribut sale of the modern dai internet .
you know , probabl back in the 1960s when you had the <num> note arpanet .
you could get awai with run dijkstra's shortest path algorithm but not in the 21st centuri .
it's not feasibl for thi stanford gatewai router to mainli local , reason accur model of the entir internet graph .
so how can we elud thi issu ?
is it fundament that becaus the internet is so massiv it's imposs to run ani shortest path algorithm ?
well , the rai of hope would be if we could have a shortest path algorithm .
that admit and distribut implement , wherebi a node could just interact , perhap iter with it neighbor , with the machin to which it directli connect and yet somehow converg to have accur shortest path to all of the destin .
so perhap , the fir thing you'd try would be to seek out an implement of dijkstrar's algorithm , where each vertex us onli vocal comput .
that seem hard to do .
if you look at pseudo code of dijkstra , it doesn't seem like a localiz algorithm .
so instead , what we're go to do is differ .
shortest path algorithm .
it's also a classic , definit on the greatest hit compil .
it's call the bellman ford algorithm .
so the bellman ford algorithm , as you'll see , can be thought of as a dynam program algorithm and inde it correctli comput shortest path us onli local local comput .
each vertex onli commun in round with the other vertic to which it's directli connect .
as a bonu we'll see thi algorithm also handl neg edg length , which of cours dijkstra's algorithm doe not .
but don't think dijkstra's algorithm is obsolet .
it still ha faster run time in situat we can get awai with central comput .
now , what's realli kind of amaz here is that the bellman ford algorithm , it date back to the 1950s .
so that's not just pre internet , that's pre arpanet .
so that's befor the internet wa even a glimmer in anybodi's ey .
and yet it realli is the foundat for modern internet rout protocol .
needless to sai there's a lot of realli hard engin work and further idea requir to translat the concept from bellman ford to actual do rout in the veri complex modern dai internet .
but yet , those protocol , at their foundat , go all the wai back the the bellman ford algorithm .
in thi video we'll cover a second problem to whet your appetit for thing to come , name the problem of sequenc align .
so , thi is a fundament problem in comput genom .
if you take a class in the subject , it's veri like to occupi the veri first coupl of lectur so in thi problem you're given two string over an alphabet , and no prize for guess which is the alphabet we're most like to care about .
typic these string repres portion of on or more genom .
and just as a toi run exampl , you can imagin that the two string we're given ar agggct and aggca .
note that the <num> input string do not necessarili need to be of the same length .
inform speak the goal of the sequenc <num> at problem is to figur out how similar the <num> input string ar .
obvious i haven't told you what i mean by <num> string be similar .
that's someth would develop .
over the next coupl of slide .
why might you want to solv thi problem ?
well there's actual a lot of reason , let me just give you two of mani exampl .
on would be to conjectur the function of the region of a genom that you don't understand like sai the human genom , from similar region that exist in genom's that you do understand or at least understand better sai the mous genom .
if you see a string that ha a known function in the well understood genom and you see someth similar in the poorli understood genom you might conjectur it ha the same or similar function .
a total differ reason you might want to compar the genom of two differ speci is to figur out whether on evolv directli from the other .
and when .
a second total differ reason you might want to compar the genom of two differ speci is to understand there evolutionari relationship .
so for exampl mayb you have three speci a , b , and c and you're wonder , whether if b evolv from a , and if c evolv from b , or whether b and c evolv independ from a common ancestor a .
and then you might take genom seril , similar as a measur of proxim in the evolutionari tree .
so have motiv the inform version of the problem , let's work toward make it more formal .
in particular , i ow you a discuss of what i mean by two string be similar .
so to develop intuit for thi , let's revisit the two string that we introduc on the previou slide agggct and aggca .
now , if we just sort of eyebal these two string , i mean clearli thei're not the same string , but , we somehow feel like thei're more similar than thei ar differ .
differ .
so where doe that intuit come from ?
well on wai to make it more precis is to notic that these two string can be nice align in the follow sens .
let's write down the longer string , agggct .
and i'm go to write the shorter string under it and i'll insert a gap , a space to make the two string have the same length .
i'm go to put the space where there seem to be a quot unquot miss g .
and in what sens is thi a nice align ?
well it's clearli not perfect .
we don't get a charact by charact match of the two string but there's onli two minor flaw .
so on the on hand we did have to insert big gap .
and we do have to suffer on mismatch in the final column .
so thi intuit motiv defin similar between two string , with respect to their highest qualiti align , their nicest align .
so we ar get closer to a formal problem statement , but it's still somewhat under determin .
specif we need to make precis why we might compar , why we might prefer on align over anoth .
for exampl , is it better to have three gap and no mix match wa it better to have on gap and on mismatch ?
so for thi video , we're effect go to punt on thi question .
we're go to assum thi problem's alreadi been solv experiment , that it's known and provid as part of the input , which is more costli gap and variou type of mismatch .
so here then , is the formal problem statement .
so in addit to the two string over a , c , g , t we ar provid as part of the input a non neg number indic the cost we incur in align for each gap that we insert .
similarli for each possibl mismatch of two charact like for exampl mismatch an a and t we're given as part of the input a correspond penalti .
given thi input , the respons of a sequenc align algorithm is to output the align that minim the sum of the penalti .
anoth wai to think of thi output , the minimum penalti align is we're try to find in effect the minimum cost explan for how on of these string would have turn into the other .
so we can think of a gap as undo a delet that occur in the past and we can think of a misbatch as repres a mutat .
so thi minimum possibl total penalti .
that is , the valu of thi optim align is famou and fundament enough to have it own name , name the needleman wunsch score .
so , thi quantiti is name after the two author that propos effici algorithm for comput the optim align .
that appear wai back in <num> , in the journal of molecular biologi .
and now , at last , we have a formal definit for what it mean for two string to be similar .
it mean thei have a small nw score , a score close to zero .
so , for exampl , if you had , have a data base with a whole bunch of genom fragment , accord to thi you're go to defin the most similar fragment to be those with the smallest nw score .
so to bring the discuss back squar into the land of algorithm , let me point out that thi definit of genom similar is intrins algorithm .
thi definit would be total useless , unless there exist an effici algorithm that , given two string in these penalti , comput the best align between those two string .
if you couldn't comput thi score you would never us it as a measur of similar .
so thi observ put us under a lot of pressur to devis an effici algorithm for find the best align .
so how ar we go to do that ?
well we could alwai fall back to brute forc search , where we iter over all of the conceiv align of the two string .
comput the total penalti of each of those align , and rememb the best on .
clearli correct is not go to be an issu for group brute forc search , it's correct essenti by definit .
the issu is how long doe it take .
so let's ask a simpler question , let's just thing about how mani differ align there ar , how mani possibl do we have to try ?
so if you're go to creat length imagin if i gave you two string of length <num> , which is a not unreason length .
which of the follow english phrase best describ the number of possibl , the number of align given to string of <num> charact each ?
so i realiz thi is sort of a cheeki question but i hope you can gather that what i wa look for wa part d .
so you know , so how big ar each of these quantiti anywai .
well in a typic version of thi class you might have about <num> , <num> student enrol or so .
so it somewher between <num> <num> and <num> <num> .
the number of peopl on earth is roughli <num> billion , so that's somewher between <num> <num> and <num> <num> .
the most common estim i see for known atom in the univers is <num> <num> .
<num> .
and believ it or not , the number of possibl align of two string of length <num> is even bigger than that .
so i'll leav it for you to convinc yourself that the number of possibl is at least <num> <num> .
the real number is actual , notic bigger than that .
and becaus <num> is at most <num> <num> , we can lower bound thi number by <num> <num> .
quit a bit bigger than the number of atom in the univers .
and , the point , of cours , is just that .
it's utterli absurd to envis implement brute forc search , even at a scale of a few hundr charact .
and you know , forget about these sort of astronom if you will comparison even if you had string length much smaller sai in the .
the , you know , a dozen or two you'd never ever run brute forc it's just not go to work , and a cours notic thi is not the kind of problem thi explos , thi doesn't go awai if you wait a littl while for moor's law to help you .
thi is a fundement limit that sai ; you ar never go to comput align of the string that you care about .
unless you have a fast , clever algorithm .
i'm happi to report that you will inde learn such a fast and clever algorithm later on in thi cours .
even better , it's just go to be a straightforward instanti of a much more gener algorithm design paradigm that of dynam program .
todai , we're go to embark on the discuss of a new algorithm design paradigm name that of design and analyz greedi algorithm .
so to put thi studi of greedi algorithm in a littl bit of context let's just zoom out , let's both review some of the algorithm design paradigm that we've alreadi seen as well look forward to some that we're go to learn later on .
in thi cours .
so it's sort of a sad fact of life , that in algorithm design there's no on silver bullet , there's no magic potion that's the cure for all your comput problem .
so instead , the best we can do , in , in the focu of these cours is to discuss gener techniqu .
that appli to lot of differ problem that aris in lot of differ domain .
so that's what i mean by algorithm design paradigm , high level problem solv strategi that cut across multipl applic .
so let's look at some exampl .
back in part on , we start with the divid and conquer algorithm design paradigm .
a conic exampl of that paradigm be the merg sort algorithm .
so rememb in divid and conquer what you do is you take your problem , you break it into smaller sub problem , you solv the sub problem recurs and then you combin the result into a solut to the origin problem .
like how in merg sort you recurs sort two sub arrai and then merg the result to get a sort version of the origin input arrai .
anoth paradigm that we touch on in part <num> although we didn't discuss it ani where near as thoroughli is that of random algorithm .
so the idea that you can have code , flip coin that is make random choic is insid the code itself often thi lead to simpler , more practic or more .
eleg algorithm .
a kind of that applic here is the quick sort algorithm .
us a random pivot element .
but we also saw applic , for exampl , to the design of hash function .
so the next measur paradigm we're go to discuss is that of greedi algorithm .
so these ar algorithm that iter make myopic decis .
in fact , we've alreadi seen an exampl of a greedi algorithm in part <num> , name dijkstra's shortest path algoithm .
and then the final paradigm , we're go to discuss in thi class , is that of dynam program , a veri power paradigm , which solv in particular , two of the motiv question we saw earlier name sequenc align and distribut shortest path .
so what is a greedi algorithm anywai ?
well , to be honest , i'm not go to offer you a formal definit , in fact , much blood and ink ha been spill over which algorithm precis here , greedi algorithm , but i'll give you an inform descript .
a sort of , rule of thumb for what greedi algorithm usual look like .
gener speak , what a greedi algorithm doe is make a sequenc of decis .
with each decis be made myopic .
that is , it seem like a good idea at the time .
and then you hope that everyth work out at the end .
the best wai to get a feel for greedi algorithm is to see exampl .
and the upcom lectur will give you a number of them .
algorithm .
but i want to point out we've actual alreadi seen an exampl of a greedi algorithm in part on of thi cours , mainli dijkstra's shortest path algorithm .
so , in what sens is dijkstra's algorithm a greedi algorithm ?
well , if you recal the pseudo code for dijkstra's algorithm , you'll recal thi on main wild loop .
and the algorithm process is exactli on new destin vertex in each iter of thi wild loop .
so there's exactli n <num> iter overal .
where n is the number of vertic .
so the algorithm onli get on shot to comput the shortest path to a given destin .
it never goe back and re visit the decis .
in that sens , the decis ar myopic , irrevoc .
and that's the sens in which dijkstra's algorithm is greedi .
so let me paus for a moment to discuss the greedi algorithm design paradigm , gener .
probabl , thi discuss will seem a littl abstract , so i recommend you revisit thi discuss on the slide .
after we've seen a few exampl .
at that point , i think it will realli hit home .
so let me proce by compar it and contrast it to the per a diem we've alreadi studi in depth , that of divid and conquer algorithm .
so you'll recal what you do in the divid and conquer algorithm what you do is you break the problem into sub problem , so mayb to take an imput arrai and you split it into <num> subarrai .
then you solv the smaller subproblem recurs .
and then you combin the result of the subproblem into a solut to the origin input .
so , the greedi paradigm is quit differ in sever respect .
first , both a strength and a weak of the greedi algorithm design paradigm is just how easi it is to appli .
so , it's often .
quit easi to come up with plausibl greedi algorithm for a problem , even multipl differ plausibl greedi algorithm .
i think that's a point of contrast with divid and conquer algorithm .
often it's tricki to come up with a plausibl divid and conquer algorithm and usual you have thi eureka moment , where you final figur out how to decompos the problem in the right wai .
and onc you have the eureka moment , you're good to go .
so secondli , i'm happi to report that analyz run time of greedi algorithm will gener be much easier that it wa with divid and conquer algorithm .
for divid and conquer algorithm , it wa realli unclear whether thei were fast or slow becaus we had to understand the run time over multipl level of recurs .
on the on hand , problem size wa get smaller , on the other hand the number of sub problem wa prolifer .
so we had to work hard , we develop these power tool like the master method and some other techniqu for figur out just how fast an algorithm like merg short run .
or just how fast an algorithm like strassen's fast matrix multipl .
applic algorithm run .
in contrast , with greedi algorithm , it'll often be a on liner .
often it'll be clear that the work is domin by , sai , a sort subroutin and , of cours , we all know that sort take n log n time if you us a sensibl algorithm for it .
now , the catch , and thi is the third point of comparison , is we're gener go to have to work much harder to understand correct issu of greed outcom .
for divid and conquer algorithm we didn't talk much about correct , it wa gener a pretti straightforward induct proof .
you can view the lectur on quick short if you want an exampl of <num> of those canon induct correct proof .
but the game total chang with greedi algorithm .
in fact , given a greedi algorithm we often won't even have veri good intuit .
for whether or not thei ar correct , let alon how to prove thei're correct .
so even with a correct algorithm it's often hard to figur out why it's correct .
and in fact , if you rememb onli on thing from all of thi greedi algorithm discuss mani year from now i hope on kei thing you rememb is that thei're often not correct .
often especi if it's on that you propos yourself , which you're veri bias in favor of .
you will think the algorithm , the greedi algorithm must be correct becaus it's so natur .
but mani of them ar not .
so keep that in mind .
so to give you some immedi practic with the ubiquit incorrect of natur greedi algorithm .
let's review a point that we alreadi cover in part <num> of thi class concern dijkstra's algorithm .
now in part <num> you made a deal about a justli famou algorithm dijkstra's shortest path algorithm is .
it run blazingli fast and it comput all of the shortest path .
what els do you want ?
well , rememb there is an assumpt when we prove the dyke algorithm is correct .
we assum that everi edg of the given network ha a non neg length .
we did not allow neg edg length .
and as we discuss in part on , you know for mani applic you onli care about non neg edg length .
but there ar applic where you do want neg edg length .
so let's review on thi quiz why dijkstra's is actual incorrect despit be so natur .
it's incorrect when edg can have neg length .
so i've drawn in green a veri simpl shortest path network with <num> edg and i've antit the edg with there length .
you'll notic <num> of the edg doe have a neg length .
the edg from v to w with length <num> .
so the question is consid the sourc vertex s , and the destin vertex w .
and the question is , what is the shortest path distanc comput by dykstra's algorithm ?
and you mai have to go and review , just the pseudo code in part <num> , or in , on the web , to answer that part of the question .
and then , what is , in fact , the actual shortest path distanc , from s to w ?
where , as usual , the length of a path is just the sum of the link of the edg in .
in the path .
all right , so the correct answer is d .
so let's start with the second part of the question .
what is the actual length of a shortest path from s to w ?
where there's onli <num> path at all in the graph .
the on straight from s to w .
that ha length <num> .
and the on that goe via the intermedi point v , that ha length <num> <num> .
the on which is shorter .
so svw is the shortest path that ha length on .
y is dijkstra incorrect .
well if you go back to the pseudo code of dijkstra , you'll see that in the veri first iter , it will greedili find the closest vertex to s , in that case thi is w .
w is closer than v .
it'll greedili comput the shortest path distanc to w , know the inform it ha right now .
and all it know is there's thi <num> hop path from s to w .
so it'll irrevoc commut to the shortest path distanc from s to w as <num> .
never reconsid that decis later .
so dykstra will termin with the incorrect output that the shortest path length from s to w is <num> .
thi doesn't contradict anyth we prove in part <num> .
'cuz we establish correct of dykstra .
onli under the assumpt that all edg length ar not neg .
an assumpt which is .
violat in thi particular exampl .
but again the takeawai point here is that , you know , it's easi to write down a greedi algorithm , especi if you came up with it yourself .
you probabl believ deep in your heart that it's got to be correct all the time , but more often than not , probabl your greedi heurist is noth more than a heurist and there will be instanc on which it doe the wrong thi .
so keep that in mind .
in greedi algorithm design .
so now that my consciou is clear , have warn you about the peril of greedi algorithm design .
let's turn to proof of correct .
that is , if you have a greedi algorithm that actual is correct , and we'll see some notabl exampl in the come lectur .
how would you actual establish that fact ?
or if you have a greedi algorithm and you don't know whether or not it's correct .
how would you approach , try to understand which on it is , whether it's correct or not ?
so let me level with you .
prove greedi algorithm is correct frankli is sort of more art than scienc .
so unlik the divid and conquer paradigm where everyth wa somewhat formula .
we had thi black box wai of evalu recurr , we have thi so the templat for prove algorithm correct .
realli , approv correct of greedi algorithm take a lot of creativ .
and it ha a bit of an ad hock flavor .
that's it , as usual , to the extent that there ar recur theme .
that is what i will spend our time togeth emphasis .
so let me tell you , just a littl bit again , about veri high level .
how you might go about thi , you again might want to revisit thi context af , content after you've seen some exampl where i think it will make a lot more sens .
so method on , is our old friend or perhap nemesi depend on your disposit , name proof by induct .
now for greedi algorithm rememb that thei do is thei sequenti make a bunch of irrevoc decis .
so here the induct is go to be on the decis made by the algorithm and if you go back to our proof of correct of dijkstra's algorithm that in fact is exactli how we prove dijkstra's algorithm correct .
it wa by induct on the number of iter and each iter of the main while loop we comput the shortest path to on new destin and we alwai prove that assum all of our previou comput .
were correct , that's the induct hypothesi , then so is the comput and the current iter .
and so then by induct , everyth the algorithm ever doe is correct .
so that's a greedi proof by induct that a greedi algorithm can be correct , and we might see some more exampl of those and for other algorithm in the lectur to come .
some of the textbook call thi method of proof , greedi stai ahead , mean you alwai prove greedi's do the right thing iter by iter .
so a second approach to prove the correct of greedi algorithm , which work in a lot of case , is what's call an exchang argument .
now you haven't yet seen ani exampl of exchang argument in thi class .
so i can't realli tell you what thei ar .
but that's what we're go to proce to next .
i'm go to argu by an exchang argument that a coupl of differ famou greedi algorithm ar , in fact , correct .
it ha a coupl of differ flavor .
on flavor is to approach it by contradict .
you assum for contradict , that a greedi algorithm is incorrect .
and then you show that you take an optim solut .
and exchang two element of that optim solut and get someth even better .
which of cours , contradict the assumpt that you start with an optim solut .
a differ flavor would be to gradual exchang an optim solut into the on output by a greedi algorithm , without make the solut ani wors .
that would show that the up with the greedi algorithm is in fact optim and inform that's done by an induct on the number of exchang requir to transform an optim solut into your , and final i've alreadi said it onc but let me sai it again , there is not a whole lot of formula behind prove greedi algorithm is correct .
you often have to be quit creativ , you might have to switch togeth aspect of method <num> and method <num> , you might have to do someth complet differ .
realli , ani rigor fruit is fair gain .
so , thi set of lectur will be the final applic of the greedi algorithm design paradigm .
it's go to be to an applic in compress .
specif , i'll show you a greedi algorithm for construct a certain kind of prefix free binari code known as huffman code .
so , we're go to spend thi video sort of set the stage .
so , let's begin by just defin a binari code .
so , a binari code is just a wai to write down symbol from gen , some gener alphabet in a manner that can comput can understand that is just a function map each symbol from an alphabet capit sigma to a binari string , a sequenc of <num>'s and <num>'s .
so thi alphabet capit sigma could be ani number of thing but you know it a simpl exampl you could imagin it the letter a through z's in lower case plu mai be the space , charact and some punctuat .
so mai be for a set of size <num> overal .
and if you have <num> symbol you need to encod in binari , well an obviou wai to do it is that happen to be <num> differ binari string of length <num> , so why not just us on of each of those for your symbol .
symbol .
so thi would be a fix length code in the sens we're us exactli the same number of bit , name five to encod each of the symbol in our alphabet .
thi is pretti similar to what's go on with ascii code .
and of cours it's a mantra of thi class to ask when can we do better than the obviou solut .
so in thi context , when can we do better than fix length code .
sometim we can in the import case when some symbol ar much more like to appear than other .
in that case , we can encod inform .
us fewer bit by deploi variabl length code .
and thi is , in fact , a veri practic idea .
the variabl length code ar us all the time in practic .
on exampl is in encod mp3 audio file .
so if you look at the standard for mp3 encod , there's thi initi phase in which you do analog to digit convers but then onc you're in the digit domain you do appli huffman code .
what i'm go to teach you in thi video to compress the length of the file even further .
and as you know compress , especi loss less compress like huffman code , is a good thing .
you want to download a file , you want it to happen as fast as possibl or you want to make file as small as possibl .
so a new issu aris when you pass from fix length code to variabl length code .
so let me illustr that with a simpl exampl .
suppos our alphabet sigma is just four charact a , b , c , d so the obviou fix length encod of these charact would just be <num> , <num> , <num> , and <num> .
well suppos we want to us fewer bit and we want to us a variabl length encod .
an obviou idea would be to try to get awai with onli <num> bit for a coupl of these charact .
so suppos instead of us a <num> for a just us a singl <num> and instead of us a doubl <num> for d we just us a singl <num> .
so , that's onli fewer bit , and it seem like that can onli get getter .
but now , here's the question .
suppos someon hand you and encod transmiss consist of the digit <num> .
what would have been the origin sequenc of symbol that led to that encod version ?
all right so the answer is d , there is not enough inform to know what <num> wa suppos to be an encod of .
the reason is , is that have pass to a variabl length encod there is now ambigu .
there is more than on sequenc of origin symbol that could of lead under thi encod to the output <num> .
specif , answer a and c would both lead to <num> .
the letter a would give you a <num> , the letter b would give you a <num> , so that would give you <num> .
on the other hand , aad would also give you <num> , so there's no wai to know .
contrast thi with fix length encod , if you're given a sequenc of bit with a fix length code .
of cours , you know where on letter end and the next on begin .
for exampl , if everi symbol wa encod with <num> bit , you would just read <num> bit , you'd know which symbol it wa .
you'd read the next <num> bit , and so on with variabl length code .
without further precaut it's unclear where on symbol start and the next on begin .
so that's an addit in issu we have to make sure we take care of with variabl length code .
so to solv thi problem that with variabl length code and without further precaut it's unclear where on symbol end and where the next on begin .
we're go to insist that our variabl length code ar prefix free .
so what thi mean is when we encod a bunch of symbol we're go to make sure that for each pair of symbol i and j from the origin alphabet sigma .
the correspond encod's will have the properti that neither on is a prefix of the other .
so go back to the previou slide , you'll see that that exampl wa not prefix free .
for exampl , we us <num> wa a prefix of <num> .
that led to ambigu .
similarli <num> , the encod for d wa a prefix of <num> , the encod for c , and that also lead to an ambigu .
so if you don't have prefix for each other .
and we'll develop thi more shortli .
then there's no ambigu .
then there's a uniqu wai to decod , to reconstruct what the origin sequenc of symbol wa given just the zero and on .
so lest you think thi is too strong a properti , certainli interest and us variabl length code exist that satisfi the prefix free properti .
so on simpl exampl again , just encod the letter a , b , c , d .
we can get awai with encod the symbol a , just us a singl bit , just us a zero .
now , o cours to be prefix free , it better be the case that our encod of b and c and d all start with the bit on .
otherwis we don't , we're not prefix free .
but , we can get awai with that so let's encod a b with on and then an zero .
and now both c and d , better have the properti that thei start neither with zero , nor with <num> .
that is thei better start with <num> , but let just encod c us <num> and d us <num> .
so that would be a variabl length code , the number of bit vari between <num> and <num> , but it is prefix free .
and again , the reason we might want to do thi , the reason we might want to us a variabl link in code , is to take advantag of non uniform frequenc of symbol from a given alphabet .
so , let me show you a concret exampl of the benefit you can get from these kind of code , on the next slide .
so let's continu with just our four symbol alphabet a , b , c , and d .
and let's suppos we have good statist in our applic domain , about exactli how frequent each of these symbol ar .
so in particular , let's assum we know that a is by far the most like symbol , let's sai <num> of the symbol ar go to be as .
wherea <num> ar bs , <num> ar cs , and <num> ar ds .
so , why would you know these statist ?
well , some , in some domain , you're just go to have a lot of expertis .
in genom , you're go to know the usual frequenc of as , cs , gs , and ts .
for someth like an mp3 file , well , you can liter just take an intermedi version of the file , after you've done the analog to digit transform , and just count the number of occurr of each of the symbol .
so then you know exact frequenc and you're good to go .
so let's compar the perform of the , just sort of obviou fix length code where you us two bit for each of the four charact with that of the variabl length code that's also prefix free that we mention on the previou slide .
and we're go to measur the perform of these code by look , on averag , how mani bit do you need to encod a charact ?
where the averag is over the frequenc of the four differ symbol .
so for the fix length encod , of cours , it's two bit per symbol .
we don't even need the averag , just whatev the symbol is it us exactli two bit .
so , what about the variabl length encod that's shown on the right in pink ?
how mani bit , on averag , for an averag charact given these frequenc the differnet symbol ar need to encod a charact of the alphabet sigma .
okai , so the correct answer is the second on .
it's on averag , <num> . <num> bit per charact .
so what's the comput ?
well , <num> of the time , it's go to us onli <num> bit .
and that's where the big save come from .
<num> bit is all that's need whenev we see an a , and most of the charact ar a's .
we don't do to bad when we see a b either which is <num> of the time , we're onli us <num> bit for each b .
now it is true that c's and d's were pai the price , we're have to us <num> bit for each of those , but their aren't veri mani onli <num> of the time is it a c , and onli <num> of the time is it a d .
and if you ad the result that's take the averag over the simpl frequenc we get the result of <num> . <num> .
so , thi exampl draw our attent to a veri neat algorithm like opportun .
so , name given a offer that and frequenc of the symbol which in gener not uniform .
we now know that the obviou solut fix length code need not be optim .
we can improv upon them us variabl length .
the prefix free code .
so the comput problem you want to solv is which on is the best .
how do we get optim compress ?
which variabl length code give us the minimum averag encod length of a symbol from thi alphabet .
so huffman code ar the solut to that problem .
we'll start develop them in the next video .
hei , so guess what ?
we just design our first dynam program algorithm that linear time algorithm for comput the max rate in the penanc set in the path is inde an extenci of the gener dynam program paradox .
now defer articul the gener principl of the paradox home now but i think thei're best understood through compar exampl now that we have on to relat them to let me tell you about these guid principl , we will in the come lectur see mani more exampl .
exampl .
so , the kei that unlock the potenti of the dynam program paradigm for solv a problem , is to identifi a suitabl collect of subproblem .
and these subproblem have to satisfi a number of properti .
in our algorithm for comput maximum independ set in path graph .
we had n <num> subproblem .
<num> for each prefix of the graph .
so formal , our i th supbroblem in our algorithm .
it wa to comput the max weight independ set of g sub i of the path graph consist onli of the first i vertic .
so , the first properti that you want your collect of subproblem to possess is it shouldn't be too big .
it shouldn't have too mani differ subproblem .
the reason be is , in the best case scenario .
you're go to be spend constant time solv each of those subproblem .
so the number of subproblem is a lower bound than the run time of your algorithm .
now , in the maximum independ set exampl , we did great .
we had mere a linear number of subproblem .
and we did inde , get awai with mere constant work for each of those subproblem , give us our linear run time bound overal .
the second properti you want , and thi on's realli the kicker is there should be a notion of smaller subproblem and larger subproblem .
in the context of independ set of path graph .
thi wa realli easi to understand .
the subproblem were prefix of the origin graph and the more vertic you had the bigger the subproblem .
so in gener in dynam program you systemat solv all of the subproblem .
begin with the smallest on and move on to larger and larger subproblem .
and for thi to work it better be the case that at a given subproblem , given the solut to all of the smaller sub problem , it's easi to infer what the solut of the current subproblem is .
that is , solut to previou subproblem ar suffici to quickli and correctli comput the solut to the current subproblem .
the wai thi relationship between larger and smaller subproblem is usual express is via recurr .
and it state what the optim solut to a given subproblem is , as a function of the optim solut to smaller subproblem .
and thi is exactli how thing plai out in our independ set algorithm .
we did , inde , have a recurr .
it just said that the optim valu , the maximum independ set valu for g sub i wa the better of two candid , and we justifi thi us our thought experi .
either you just inherit the maximum independ set valu from the preced sub problem , from the i <num> sub problem , or you take the optim solut from two sub problem back , from gi <num> and you extend it by the current vertex b sub i .
that is , you add the i th vertic weight to the weight of the optim solut from <num> sub problem back .
so thi is a pattern we're go to see over and over again .
we'll defin sub problem for variou comput problem and we'll us recurr to express how the optim solut of a given sub problem depend onli on the solut to smaller sub problem .
so , just like in our independ set exampl , onc you have such a recurr , it natur lead to a tabl fill algorithm where each entri in your tabl correspond to the optim solut to <num> sub problem .
and you us your recurr to just fill it in , move from the smaller sub problem to the larger on .
so the third properti , you probabl won't have to worri about much .
usual , thi just take care of itself , but needless to sai , after you've done the work of solv all of your subproblem , you'd better be abl to answer the origin .
qeustion .
thi properti's usual automat satisfi becaus in most case , not all , but in most case , the origin problem is simpli the biggest of all your subproblem .
notic , thi is exactli how thing work in independ set .
our biggest subproblem , g sub n wa just the orgin graph .
so onc we fill up the whole tabl boom .
wait for us in the final entri wa the desir solut to the origin problem .
so i realiz , you know , thi is a littl abstract at the moment .
we onli have on concret exampl to relat to these abstract concept .
i encourag you to revisit thi again after we see more exampl and we will see mani more exampl .
someth all of the forthcom exampl should make clear is the power and flexibitl of the dynam program paradigm .
thi is realli just a techniqu that you have got to know .
now when you're try to devis your own dynam program algorithm , the kei , the heart of the matter is to figur out what the right sub problem ar , if you nail the sub problem usual everyth els fall into place in a fairli formula wai .
now if you've got a black belt in dynam program , you might be abl to just stare at a the problem and intuit know what the right collect of sub problem ar .
boom , you're off to the race .
but , of cours , you know , for white belt in dynam program there's still a lot of train to be done .
so rather in the forth come exampl rather than just pluck the sub problem from the sky we're go to go through thi same kind of process we did from independ set and try to figur out how you would ever come up with these sub problem in the first place , by reason about the structur of optim solut .
that's a process you should be abl to mimic in your own attempt at appli thi para dine , the problem that come up in your own project .
so perhap you were hope that onc you saw the ingredi of dynam program all would becom clear why on earth it's call dynam program , and probabl it's not .
so , thi is an anachronist us of the word program , it doesn't mean code .
in the wai i'm sure almost all of you think of it .
it's the same anachron and phrase like mathemat or linear program it more refer to a plan process .
but you know for the full stori let's go ahead and turn to richard bellman himself he's more or less the inventor of dynam program .
you will see hi bellman ford algorithm a littl bit later in the cours .
so he answer thi question in hi autobiographi .
and he sai , he talk about when he invent it , in the 1950s .
and he sai those were not good year for mathemat research .
he wa work at a place call rand .
he sai , we had a veri interest gentlemen in washington name wilson who's the secretari of defens .
and he actual had a patholog fear and hatr of the word , research .
i'm not us the term lightli , i'm us it precis .
hi face would suffus , it would turn red .
and he would get violent if peopl us the term .
research in hi presenc .
you can imagin how he felt then about the term , mathemat .
so the rand corpor wa emploi by the air forc .
and the air forc had wilson as it boss , essenti .
henc , i felt i had to do someth to shield wilson and the air forc from the fact that i wa realli do mathemat insid the rand corpor .
what titl , what name could i choos ?
in the first place , i wa interest in plan and decis make .
but plan , it's not a good word for variou reason .
i decid therefor to us the word program .
dynam ha a veri interest properti as an adject in that it's imposs to us the word dynam in a pejor sens .
try think of some combin that will possibl give it a pejor mean .
mean .
it's imposs .
thu , i thought dynam program wa a good name .
it wa someth not even a congressman could object to .
so , i us it as an umbrella for my activ .
so i hope that pretti much all of you had heard about the p vs .
np question befor you enrol in thi class .
but if you haven't you can pretti much guess what that question is .
i've defin for you both of these class of problem .
p is the class of problem that ar polynomi time solvabl wherea the problem in np have the properti that , at least given a solut , you can quickli verifi that it is inde a correct solut .
it's wide conjectur that p is not equal to np , that is , mere the abil to effici verifi purport solut , is not suffici to guarante .
polynomi time solvabl .
inde , edmund , back in <num> , befor we even had the vocabulari in p .
rememb , that came along onli in '<num> .
edmund , alreadi , in '<num> , wa essenti conjectur that p is not to np .
in the form that he wa conjectur , there's no polynomi time algorithm that solv the travel salesman problem .
.
but , let me emphas .
we genuin do not know the answer to thi question .
there is no proof of thi conjectur .
.
p vs .
np question is arguabl the open question in comput scienc .
it's also certainli on of the most import and deepest open question in all of mathemat .
for exampl , in <num> the clai mathemat institut publish a list of seven millenium prize problem .
the p vs .
np question is on of those seven problem .
all of these problem ar extrem difficult and extrem import .
the onli on that's been solv to date is the poincar conjectur , the riemann hypothesi is anoth exampl on that list .
and thei're not call the millennium prize problem for noth .
if you solv on of these mathemat problem , you get a cash prize or <num> million .
now , while <num> million is obvious noth to sneez at , i think it sort of underst the import of a mathemat question like , p vs np .
and the advanc in our knowledg that a solut to the question would provid , i think would be much more signific than a price check .
so how come so mani peopl think that p is not equal to np , rather than the opposit that p np .
well i think the domin reason is a psycholog reason , mainli that if it were the case that p np , than all you'd have to do to rememb is exibit .
a polynomi time algorithm for just on np complet problem .
and , there ar ton of np complet problem .
and a lot of extrem smart peopl have had np complet problem that thei've realli care about , and either on purpos or accident , thei've been try to develop effici algorithm for them .
no on ha ever succed in over a half centuri of seriou comput .
work .
the second reason is sort of philosoph .
p np just doesn't seem to jive with the wai the world work .
think about , for exampl , when you do a homework problem in a class like thi on , and consid three differ task .
the first task is i give you .
a question , and i ask you to come up with a correct solut , sai a proof , of some mathemat statement .
the second task would be just grade somebodi els's suggest proof .
well , gener , it seem a lot harder to actual come up with .
with a correct argument from scratch , compar to just verifi a correct solut provid by somebodi els .
and yet p np would be sai that these <num> task have exactli the same complex .
it's just as easi to solv homework problem as it is to just read and verifi the correct solut .
so i don't know about you , but it's alwai seem to me to be a lot harder to come up with a mathemat argument from scratch , as oppos to simpli grade somebodi els's solut .
somehow it seem to requir a degre of creativ to pluck out from thi exponenti big space of proof , a correct on for the statement at hand .
yet p np would suggest that that creativ .
could be effici autom .
but , of cours , you know , p vs .
np be a mathemat question .
we'd realli like some mathemat evid of which wai it goe .
for exampl , that p is not to np .
and here , we realli know shockingli littl .
there just isn't that much concret evid at thi point .
that , for exampl , p is not to np .
now mayb it seem bizarr to you that we're struggl to prove that p is not equal to np .
mayb it just seem sort of obviou that there's no wai that you can alwai construct proof in time polynomi in what you need to verifi proof .
but , the reason thi is so hard to prove mathmat , is becaus of the insan rich of the space of polynomi time algorithm .
and inde it's thi rich that we've been exploit all along in these design and analysi of algorithm class .
think , for exampl , about matrix multipl .
had i not shown you strassen's algorithm , i probabl could have convinc you more or less that there wa no wai to solv matrix multipl faster than cubic time .
you just look at the definit of the problem and it seem like you have to do cubic work .
yet , strassen's algorithm and other follow up show you can do fundament better than the naiv cubic run time algorithm for matrix multipl .
so there realli ar some quit counter intuit and hard to discov unusu effici algorithm with in the landscap of polynomi time solut .
and who's to sai that there aren't some more exot speci in thi landscap of polynomi time solvabl that have yet to be discov , which can make ani .
road even on np complet problem .
at thi point we realli don't know , and the veri least ar current primit understand of the fauna within the complex class p is an intimid obstruct to a proof that p is not equal equal to np .
i should also mention that as an interest counterpoint to edmond's conjectur in '<num> , wa a conjectur by godel .
thi is the same logician kurt godel , of godel's complet and incomplet theorem .
he wrote a letter to john von neumann in <num> , where he actual conjectur the opposit .
that p np , so who know ?
so i've tri to highlight for you the most import thing that an algorithm design and seriou programm should know about np problem and np complet .
on thing i haven't explain which you might be wonder about is , what on earth doe np stand for ?
anywai .
a common guess would be not polynomi , but thi is not what it stand for .
the answer's go to be a littl more obscur , and inde it to a bit of an anachron ; non determinist polynomi .
so thi is refer to a differ but mathemat equival wai to defin the complex class np in term of an abstract machin model known as non determinist turn machin .
but gener for somebodi's who's think about algorithm it's gener for the programm .
i would advis against think about problem in np in term of thi origin definit with thi abstract machin model .
and i'd instead strongli encourag you to think about the definit i gave you , in term of the effici recognit , the effici verif of purport solut .
again , thei're mathemat equival and i think effici verif make more sens to the .
the algorithm design .
.
mayb you're think that np is , perhap , not that good in somewhat inscrut definit for what's a super import concept .
but it's not for lack of discuss and effort on the commun's part .
so veri soon after the work of cook and carp .
it wa clear to everybodi work in the west on algorithm and comput , that thi wa a super import concept .
and peopl need to straighten out the vocabulari asap .
so don knuth ran a poll amongst member of the commun .
he report on the result in hi sigact new articl from <num> , a terminolog propos .
and np complet wa the winner .
and that wa then adopt in the landmark book , design and analysi of algorithm by aho , hopcroft , and ullman .
and that's the wai it's been ever sinc .
there is on suggest that wa pass over , which i find quit amus , let me tell you about .
the suggest wa pet .
so , what is pet an acronym for ?
well , it's flexibl .
so , initi , the interpret would be possibl exponenti time problem .
now , suppos if some dai peopl that p is not equal to np , then the mean would chang to provabl exponenti time .
so it not the time to nit pick with the suggest that you could prove p not equal to np without actual prove an exponenti lower bound , mere a super polynomi bound .
let leav object like that asid and ask , what would happen if p actual turn out to be equal to np ?
well then you could call pet , previous exponenti time problem .
but of cours at thi point , pet is noth more than an amus histor footnot .
np complet is the phrase that you gotta know .
thi video is a segu between bad new and good new .
the bad new which we've now discuss is np complet ; the fact that there ar computation intract problem out there in the world , in fact thei're fairli ubiquit and you're like to encount them in your own project .
the good new is that np complet is hardli a death sentenc .
inde , our algorithm toolbox is now rich enough to provid mani differ strategi toward cope with np complet .
np problem .
so , suppos you had identifi a comput problem on which on which the success of compani rest .
mai be you have spent the last sever week throw the kitchen sink ad g .
all the algorithm design for , all the data structur , all the primit noth work .
final you decid to try to prove the problem is np complet and you succe .
now you have an explan for why your week of effort have come to not .
but that doesn't chang the fact that thi is the problem that govern the success of your project .
what should you do ?
well , the good new is np complet is certainli not a death sentenc .
there ar peopl solv , or at least approxim solv , np complet problem all the time .
howev , know that your problem is np complet doe tell you where to set your expect .
you should not expect some gener purpos , super fast algorithm , like we have for other comput problem .
like sai , sort or singl sort shortest path .
unless you're deal with unusu small or well structur input , you're go to have to work pretti hard to solv thi problem , and also possibl make some compromis .
the rest of thi cours is devot to strategi for solv or approxim solv np complet problem .
in the rest of thi video , i'll give you an orient for what those strategi ar , and what you can expect to come .
so as usual , i'm go to focu here on gener purpos strategi , that cut across multipl applic domain .
mean .
as usual , these gener principl should just be a start point .
you should take them and run with them , admit them with whatev domain expertis you have in thi specif problem that you need to solv .
the first strategi is to focu on computation tractabl special case of an np complet problem .
relatedge , relatedli you want to think about what's special about your domain .
about the data set that you're work with , and try to understand if there's special structur which can be exploit in your algorithm .
let me point out that we've alreadi done thi in a coupl case in thi cours .
the first exampl we saw concern the weight independ set .
so we studi thi problem on path graph , but the comput problem make perfect sens in gener graph .
the gener problem is , i give you as input an undirect graph .
everi vertex ha a weight , and i want the maximum weight subset of vertic that's an independ set .
and rememb in an independ set you ar forbidden to to take ani <num> vertic that ar neighbor .
so in an independ set none of the pair of vertic that you've pick ar join by an edg .
in gener graph the weight independ set problem is np complet so we certainli don't expect it to have an polynomi time algorithm .
but in the special case where the graph is a path , as we saw , there's a linear time dynam program algorithm that exactli solv the weight independ set .
so path graph form a special case of the weight independ set problem that's compuc retract solvabl in polynomi or even linear time .
in fact the frontier of tractabl can be push well beyond path graph .
on the homework , i ask you to think through the case of graph that ar tree and notic that you can still do dynam program effici , to comput weight independ set in tree .
you can even get computation effici algorithm for a broader class of graph , known as bound tree width graph .
so the definit of that's a littl outsid the scope of thi cours , but you can go even beyond tree .
so the second exampl follow from my dynam program algorithm for the knapsack problem .
so , we discuss that run time .
and we explain why it's exponenti time .
if the run time of our dynam program napsac algorithm is n , the number of item , time capit w , the napsac capac .
and becaus it onli take log w bit to specifi the capac , capit w , we don't call that a polynomi time algorithm .
but , imagin you onli have to solv a knapsack instanc where the capac is not too big , mayb even sai the capac , capit w , is big o event .
and you definit will see knapsack instanc in practic , which poss that properti , well then our dynam program algorithm just run in time o of n squar and that's a bona fide polynomi time algorithm for thi special case of a small knapsack capac .
so next let me mention a coupl of exampl we're go to see in the forthcom video .
the <num>st <num> is go to concern the 2sat problem .
the 2sat is a type of constraint satisfact problem .
rememb in a constraint satisfact problem you have a bunch of variabl , each on get assign a valu .
so the simplest case is the bullion case where each valu can be zero or on , fals or true .
and then you have a bunch of claus , which specifi the permit joint valu of a collect of variabl .
the <num> and <num> set refer to the fact that each constraint involv the joint valu of onli a pair of variabl .
so a concon constraint in a <num> set instanc is go to for <num> variabl specifi <num> joint assign that allow .
and on that's forbidden .
so , for exampl , mayb it will sai , oh for variabl x3 and x7 , it's okai to set them both to true .
it's okai to set them both to fals .
it's okai to set <num> to true , and <num> to fals .
but it's forbidden to set <num> to fals , and <num> to true .
so that's a canon constraint in the <num> sat instanc .
<num> sat , it's the same thing , except the constraint involv the join valu to a tripl of variabl .
and it's go to forbid <num> out of the <num> possibl .
now the <num> set problem a canon mp complet problem .
that wa realli singl out by cook and levin as be suffici express to encod all problem in mp , but if each constraint ha size .
onli <num> then as we'll see the problem becom polynomi time solvabl .
there's a coupl wai of prove that .
we're go to talk about a local search algorithm that check whether or not there is inde an assign to the variabl that similtan satisfi all of the given constraint .
so the final exampl we'll discuss in more detail later but just veri briefli we're go to discuss the vertex cover problem .
thi is a graph problem and a vertex cover is just the compliment of an independ set .
so while an independ set cannot take two vertic from the same edg , in the vertex cover problem you have to take at least on vertex from everi singl edg , and then what you want is you want the vertex cover that minim the sum of the vertex rate .
yet again thi is an np complet problem in gener but we're go to focu on the special case where the optim solut is small .
that is we're go to focu on graph where there's a small number of vertic .
such that everi singl edg ha at least on endpoint in that small set .
and , we'll see that for that special case , us a smart kind of exhaust search will actual be abl to solv the problem in polynomi time .
so , let me reiter that these tractabl special case ar meant primarili to be build block upon which you can draw in a possibl more sophist approach to your np complet problem .
so , just to make thi a littl more concret , let me just kind of dream up on scenario to let you know what i'm think about .
imagin , for exampl , that you have a project where , unfortun , it's not realli 2sat that you're confront .
it's actual a 3sat instanc .
so you're feel kind of bum , 3sat have been complet and mayb you have <num> , <num> variabl .
and certainli you can't do brute forc search over <num> to <num> , <num> possibl wai of assign valu to your <num> , <num> variabl .
but mayb the good new is , becaus you have domain expertis becaus you understand thi problem instanc .
you know that yeah .
there's a <num> variabl but there's realli <num> that ar crucial .
you have the feel that all of the action basic is boil down to how these <num> core variabl get assign .
well now mayb you can mix togeth some brute forc search with some of these tractabl special case , for exampl you could enumer overal <num> to the <num> wai of assign valu to thi core set of <num> variabl .
to the <num> is roughli a million .
that's not so bad .
and now , what you're go to do is , for each of these million scenario , you check whether there isn't a wai to extend that tent assign of <num> valu to the <num> variabl , to the other <num> variabl , so that all of the constraint get satisfi .
the origin problem is solvabl , if and onli if there exist .
a wai of assign valu to these <num> variabl that successfulli extend the other <num> .
now , becaus these ar the crucial variabl , and it's where all the action is , mayb as soon as you assign all of them <num>'s and <num>'s the residu sat instanc is track .
for exampl mayb it just becom a simpl 2sat instanc and then you can solv it in polynomi time .
so thi give you a hybrid approach , approach , brute forc search at the top level , tractabl special case for each guess of assign of the <num> variabl and you're off to the race .
and i hope it's clear .
i mean thi as just on possibl wai that you might combin the variou build block for develop into a more elabor approach to tackl an np complet problem .
and that's gener what thei take .
thei take a fairli elabor approach becaus after all , thei ar np complet , you've got to respect them .
expect that .
so with that digress complet , let me mention what ar the other <num> strategi we're go to be explor in the lectur to come .
so , the second strategi , which is certainli on veri common in practic , is to resort to heurist .
that is to algorithm which ar not guarante to be correct .
we haven't realli seen exampl of heurist in the cours thu far .
those of you that ar alumni of part <num> .
perhap we could classifi random minimum cut algorithm as a heurist .
'cuz it did have a small failur probabl of , of fail to find , the minimum cut .
but rather , i'm go to focu on some exampl in the upcom lectur .
i'm go to us the knapsack problem as a case studi .
and what we'll see is that our toolbox , which contain variou algorithm design paradigm .
it's us not just for design correct algorithm .
but it's also us for design heurist .
so , in particular , we'll get a pretti good algorithm for the knapsack problem us the greedi algorithm design paradigm .
and we'll get an excel .
heurist for knapsack us the dynam program algorithm design paradigm .
the final strategi is for situat where you ar unwil to relax correct unwil to consid heurist .
now first on can be np complet problem if your hour go to be correct your not , you don't expect to run on polynomi time but there ar still opportun to have algorithm that .
well exponenti time in the worst case ar smarter than naiv brute forc search .
so we have in fact alreadi seen on exampl that can be interpret as a implement of the strategi , that's for the knapsack problem .
so in the knapsack problem naiv brute forc search would just run over all possibl subset of the item , it would check if the subset of item fit in the knapsack if it doe rememb the valu and then it just return the feasibl solut with maximum valu .
that ha time proport to <num> n , where n is the number of item .
our dynam program algorithm ha run time n time time capit w .
now , of cours , cap thi is no better than <num> n , if the knapsack capac is huge , it is itself <num> n , but as we argu , if w is smaller , thi algorithm is go to be faster .
and also , as you learn on the third program assign , sometim even though w is big , dynam program is go to beat the crap out of brute forc search .
so i'll show you a coupl of other exampl we'll talk about the travel salesman problem where naiv brute forc search would take roughli in factori time .
where n is the number of vertic .
we'll give an altern dynam program base solut which run in time onli <num> n which is much better than in factori .
the third exampl , which we'll cover in a forthcom video , we alreadi allud to briefli on the last slide .
it's for the vertex cover problem .
so thi is where you're given a graph .
if your vertex ha a weight and you want the minimum weight subset of vertic that includ at least .
on endpoint from everi edg .
we're go to consid the version of the problem where you want to check whether or not it's possibl to have a vertex cover that us onli k vertic .
whether or not there exist k vertic that includ on endpoint at least from each edg .
the naiiv root brute forc search will run in time , end the k , which get absurd , even when k is quit small .
but we're go to show that there's a smarter algorithm , which is still exponenti in k , but run in time onli <num> k time the size of the graph .
in thi sequenc of lectur we're go to learn asymptot analysi .
thi is the languag by which everi seriou comput programm and comput scientist discuss the high level perform of comput algorithm .
as such , it's a total crucial topic .
in thi video , the plan is to segu between the high level discuss you've alreadi seen in the cours introduct and the mathemat formal , which we're go to start develop in the next video .
befor get into that mathemat formal , howev .
i want to make sure that the topic is well motiv .
that you have solid intuit for what it's try to accomplish .
and also that you've seen a coupl simpl , intuit exampl .
let's get start .
analysi provid basic vocabulari for discuss the design and analysi in algorithm .
more , it is a mathemat concept it is by no mean math for math sake .
you will veri frequent hear seriou programm sai that such and such code run at o of n time , where such and such other code run in o of n squar time .
it's import you know what programm mean when thei make statement like that .
the reason thi vocabulari is so ubiquit , is that it identifi a sweet spot for discuss the high level perform of algorithm .
what i mean by that is , it is on the on hand coars enough to suppress all of the detail that you want to ignor .
detail that depend on the choic of architectur , the choic of program languag , the choic of compil .
and so on .
on the other hand , it's sharp enough to be us .
in particular , to make predict comparison between differ high level algorithm approach to solv a common problem .
thi is go to be especi true for larg input .
and rememb as we discuss in some sens .
larg input ar the interest on .
those ar the on for which we need algorithm enginu .
for exampl , asoton analysi will allow us to differenti between better and wors approach to sort .
better and wors approach to multipli two integ , and so on .
now most seriou programm if you ask them , what's the deal with asymptot analysi anywai ?
thei'll tell you reason , that the main point is to suppress both lead constant factor and lower order term .
now as we'll see there's more to asymptot analysi than just these seven word here but long term , ten year from now , if you onli rememb seven word about asymptot analysi i'll be reason happi if these ar the seven word that you rememb .
so how do we justifi adopt a formal which essenti by definit suppress constant factor and lower order term .
well lower order term basic by definit becom increasingli irrelev as you focu on larg input .
which as we've argu ar the interest input , the on where algorithm ingenu is import .
as far as constant factor these ar go to be highli depend on the detail of the environ , the compil , the languag and so on .
so , if we want to ignor those detail it make sens to have a formal which doesn't focu unduli on lead constant factor .
here's an exampl .
rememb when we analyz the merg sort algorithm ?
we gave an upper bound on it run time that wa <num> time n log n plu <num>n where n wa the input length , the number of number in the input arrai .
so , the lower order term here is the 6n .
that's grow more slowli than n log n .
so , we just drop that .
and then the lead constant factor is the <num> so we supress that well after the <num> supress we're left with a much simpler express n log n .
the terminolog would then be to sai that the run time of merg search is big o of n log n .
so in other word when you sai that an algorithm is big o of some function what you mean is that after you drop the lower order term .
and suppress the leas , lead constant factor , you're left with the function f of n .
intuit that is what big o notat mean .
so to be clear i'm certainli not assert the constant factor never matter when you're design an alg , analyz algorithm .
rather , i'm just sai that when you think about high level algorithm approach , when you want to make a comparison between fundament differnt wai of solv a problem .
asymptot analysi is often the right tool for give you guidanc about which on is go to perform better , especi on reason larg input .
now , onc you've commit to a particular algorithm solut to a problem of cours , you might want to then work harder to improv the lead constant factor , perhap even to improv the lower order term .
by all mean , if the futur of your start up depend on how effici you implement some particular set of line of code , have at it .
make it as fast as you can .
in the rest of thi video i want to go through four veri simpl exampl .
in fact , these exampl ar so simpl , if you have ani experi with big o notat you're probabl just better off skip the rest of thi video and move on the mathemat formal that we begin in the next video .
but if you've never seen it befor i hope these simpl exampl will get you orient .
so let's begin with a veri basic problem , search an arrai for a given integ .
let's analyz the straight forward algorithm for thi problem where we just do a linear scan through , through the arrai , check each entri to see if it is the desir integ t .
that is the code just check each arrai entri in turn .
if it ever find integ t it return true .
if it fall off the end of the arrai without find it it return fals .
so , what do you think ?
we haven't formal defin big o notat but , i've given you an intuit descript .
what would you sai is the run time of thi algorithm as a function of the length of the arrai of capit a .
so the answer i am look for is c , the o n or coval we would sai that the run time of thi algorithm is linear in the input length n .
why is that true ?
well , let's think about how mani oper thi piec of code is go to execut .
actual , the line of code execut is go to depend on the input .
it depend on whether or not the target t is contain in the arrai a , and if so , where in the arrai a it li .
but , in the wors case , thi code will do an unsuccess search .
t will not be in the arrai and the code will scan through the entir arrai a and return fals .
the number of oper then is a constant .
there's some initi setup perhap and mayb it's an oper to return thi final boolean valu , but outsid of that constant which will get suppress in the big annot , it doe a constant number of oper per entri in the arrai .
and you could argu about what the constant is , if it's <num> , <num> , <num> oper per entri in the arrai , but the point it whatev that constant is , <num> , <num> , or <num> , it get conveni suppress by the big o notat .
so as a result , total number of oper will be linear in n , and so the big o notat will just be o of n .
so that wa the first exampl , and the last three exampl , i want to look at differ wai that we could have two loop .
and in thi exampl , i want to think about on loop follow by anoth .
so two loop in sequenc .
i want to studi almost the same problem as the previou on .
where now we're just given two arrai , capit a and capit b , we'll sai both of the same length n , and we want to know whether the target t is in either on of them .
again , we'll look at the straightforward algorithm , where we just search through a , and if we fail to find t in a , we search through b .
if we don't find t in b either , then we have to return fals .
so the question then is exactli the same as last time .
given thi new longer piec of code , what , in big o notat , is it run time ?
well the question wa the same and in thi case the answer wa the same so thi algorithm jsut like the last on ha run time big o of n if we actual count the number of oper it won't be exactli the ssame as last time it will be roughli twice as mani oper .
as the previou piec of code .
that's becaus we have to search two differ arrai , each of length n .
so whatev work we did befor .
we now do it twice as mani time .
of cours , that , too , be a constant independ of the input length n , is go to get suppress onc we pass a big o notat .
so , thi , like the previou algorithm , is a linear time algorithm .
it ha run time big o of n .
let's look at a more interest exampl of two loop where rather than process each loop in sequenc , thei're go to be nest .
in particular let's look at the problem of search whether two given input arrai each of length n contain a common number .
the code that we're go to look at for solv thi problem is the most straightforward on you can , you can imagin where we just compar all possibl .
so for each index i into the arrai a and each index j into the arrai b , we just see if a i is the same number as b j .
if it is , we return true .
if we exhaust all of the possibl without ever find equal element then we're save in return fals .
the question is of cours is , in term of big o notat , asymptot analysi , as a function of the arrai length n , what is the run time of thi piec of code ?
so thi time , the answer ha chang .
for thi piec of code , the run time is not big o of n .
but it is big o of n squar .
so we might also call thi a quadrat time algorithm .
becaus the run time is quadrat in the input length n .
so thi is on of those kind of algorithm , where , if you doubl the input length .
then the run time of the algorithm will go up by a factor of <num> , rather than by a factor of <num> like in the previou two piec of code .
so , why is thi ?
why doe it have run time of n squar ?
well again , there's some constant setup cost which get suppress in the big again , for each fix choic of an entri i into arrai a , and then index j for arrai b for each fix choic for inj .
we onli do a constant number of oper .
the particular constant ar relev , becaus it get suppress in the big o notat .
what's differ is that there's a total of n squar iter of thi doubl four loop .
in the first exampl , we onli had n iter of a singl four loop .
in our second exampl , becaus on four loop complet befor the second on began .
we had onli two n iter overal .
here for each of the n iter of the outer for loop we do n iter of the inner for loop .
so that give us the n time n i . e .
n squar total iter .
so that's go to be the run time of thi piec of code .
let's wrap up with on final exampl .
it will again be nest for loop , but thi time , we're go to be look for duplic in a singl arrai a , rather than need to compar two distinct arrai a and b .
so , here's the piec of code we're go to analyz for solv thi problem , for detect whether or not the input arrai a ha duplic entri .
there's onli <num> small differ rel to the code we went through on the previou slide when we had <num> differ arrai the first surpris , the first chang won't surpris you at all which instead of referenc the arrai b i chang that b to an a so i just compar the ith entri of a to the jth entri of a .
the second chang is a littl more subtl which is i chang the inner for loop so the index j begin .
at i plu <num> .
where i is the current valu of the outer four loop index .
rather than start at the index <num> .
i could have had it start at the index on .
that would still be correct .
but , it would be wast .
and you should think about why .
if we start the inner four loop index at <num> .
then thi code would actual compar each distinct pair of element at a to each other twice .
which , of cours , is silli .
you onli need to compar two differ element of a to each other on .
to know whether thei ar equal or not .
so thi is the piec of code .
the question is the same as it alwai is what in term of bigger notat in the input link n is the run time of thi piec of code .
so the answer to thi question , same as the last on .
big o of n squar .
that is , thi piec of code is also a quad , ha quadrat run time .
so what i hope wa clear wa that , you know ?
whatev the run time of thi piec of code is .
it's proport to the number of iter of thi doubl four loop .
like in all the exampl , we do constant work per iter .
we don't care about the constant .
it get suppress by the big o notat .
so , all we gotta do is figur out how mani iter there ar of thi doubl four loop .
my claim is that there's roughli n squar over two iter of thi doubl four loop .
there's a coupl wai to see that .
inform , we discuss how the differ between thi code and the previou on , is that , instead of count someth twice , we're count it onc .
so that save us a factor of two in the number of iter .
of cours , thi on half factor get suppress by the big o notat anywai .
so the big o , run time doesn't chang .
a differ argument would just sai , you know ?
how mani , there's on iter for everi distinct choic of i and j of indic between on and n .
and a simpl count argument .
sai that there's n choos <num> such choic of distinct i and j , where n choos <num> is the number n time n minu <num> over <num> .
and again , supress lower order term and the constant factor , we still get a quadrat depend on the length of the input arrai a .
so that wrap up some of the sort of just simpl basic exampl .
i hope thi get you orient , you have a strong intuit sens for what big o notat is try to accomplish .
and how it's defin mathemat .
let's now move onto both the mathemat develop and some more interest algorithm .
in the follow seri of video , we'll give a formal treatment of asymptot notat , in particular big oh notat , as well as work through a number of exampl .
big oh notat concern function defin on the posit integ , we'll call it t n we'll pretti much alwai have the same semant for t n .
we're gonna be concern about the worst case run time of an algorithm , as a function of the input size , n .
so , the question i wanna answer for you in the rest of thi video , is , what doe it mean when we sai a function , t n , is big oh of f n .
or hear f n is some basic function , like for exampl n log n .
so i'll give you a number of answer , a number of wai of , to think about what big oh notat realli mean .
for starter let's begin with an english definit .
what doe it mean for a function to be big oh of f n ?
it mean eventu , for all suffici larg valu of n , it's bound abov by a constant multipl of f n .
let's think about it in a coupl other wai .
so next i'm gonna translat thi english definit into pictur and then i'll translat it into formal mathemat .
so pictori you can imagin that perhap we have t n denot by thi blue function here .
and perhap f n is denot by thi green function here , which li below t n .
but when we doubl f n , we get a function that eventu cross t n and forevermor is larger than it .
so in thi event , we would sai that t n inde is a big oh of f n .
the reason be that for all suffici larg n , and onc we go far enough out right on thi graph , inde , a constant multipl time of f n , twice f n , is an upper bound of t n .
so final , let me give you a actual mathemat definit that you could us to do formal proof .
so how do we sai , in mathemat , that eventu it should be bound abov by a constant multipl of f n ?
we see that there exist two constant , which i'll call c and n0 .
so that t n is no more than c time f n for all n that exce or equal n0 .
so , the role of these two constant is to quantifi what we mean by a constant multipl , and what we mean by suffici larg , in the english definit .
c obvious quantifi the constant multipl of f n , and n0 is quantifi suffici larg , that's the threshold beyond which we insist that , c time f n is an upper bound on t n .
so , go back to the pictur , what ar c and n0 ?
well , c , of cours , is just go to be two .
and n0 is the cross point .
so we get to where two f n .
and t n cross , and then we drop the acentod .
thi would be the rel valu of n0 in thi pictur , so that's the formal definit , the wai to prove that someth's bigger of f n you exhibit these two constant c and n0 and it better be the case that for all n at least n0 , c time f n upper bound t n .
on wai to think about it if you're try to establish that someth is big oh of some function it's like you're plai a game against an oppon and you want to prove that .
thi inequ here hold and your oppon must show that it doesn't hold for suffici larg n you have to go first your job is to pick a strategi in the form of a constant c and a constant n0 and your oppon is then allow to pick ani number n larger than n0 so the function is big oh of f n if and onli if you have a win strategi in thi game .
if you can up front commit to constant c and n0 so that no matter how big of an n your oppon pick , thi inequ hold if you have no win strategi then it's not big oh of f n no matter what c and n0 you choos your oppon can alwai flip thi in equal .
by choos a suitabl , suitabl larg valu of n .
i want to emphasi on last thing which is that these constant , what do i mean by constant .
i mean thei ar independ of n .
and so when you appli thi definit , and you choos your constant c and n0 , it better be that n doe not appear anywher .
so c should just be someth like a thousand or a million .
some constant independ of n .
so those ar a bunch of wai to think about big oh notat .
in english , you wanna have it bound abov for suffici larg number n .
i'm show you how to translat that into mathemat that give you a pictori represent .
and also sort of a game theoret wai to think about it .
now , let's move on to a video that explor a number of exampl .
have slog through the formal definit of big o notat , i wanna quickli turn to a coupl of exampl .
now , i wanna warn you up front , these ar pretti basic exampl .
thei're not realli gonna provid us with ani insight that we don't alreadi have .
but thei serv as a saniti check that the big o notat's do what it intend purpos is .
name to supress constant factor and low order term .
obvious , these simpl exampl will also give us some , facil with the definit .
so the first exampl's go to be to prove formal the follow claim .
the claim state that if t n is some polynomi of degre k , so name a<u>k n k .
plu all the wai up to a<u><num> n a<u><num> .
for ani integ k , posit< u>< u>< u> integ k and ani coeffici a<u>i's posit or neg .
then t n is big< u> o of n k .
so thi claim is a mathemat statement and someth we'll be abl to prove .
as far as , you know , what thi claim is sai , it's just sai big o notat realli doe suppress constant factor and lower order term .
if you have a polynomi then all you have to worri about is what is the highest power in that polynomi and that domin it growth as n goe to infin .
so , recal how on goe about show that on function is big o of anoth .
the whole kei is to find thi pair of constant , c and n<u><num> , where c quantifi the constant multipl< u> of the function you're try to prove big o of , and n<u><num> quantifi what you mean< u> by for all suffici larg n . now , for thi proof , to keep thing veri simpl to follow , but admittedli a littl mysteri , i'm just gonna pull these constant , c and n<u><num> , out of a hat .
so , i'm not gonna tell you how i deriv them , < u> but it'll be easi to check that thei work .
so let's work with the constant n<u><num>< u> equal to on , so it's veri simpl choic of n<u><num> and then c we ar gonna pick to< u> be sum of the absolut valu of the coeffici .
so the absolut valu of a<u>k < u> plu the absolut valu of a<u> k <num> , and so on .
rememb i didn't assum that< u> the pol . . . , the origin polynomi , had non neg coeffici .
so i claim these constant work , in the sens that we'll be abl to prove to that , assert , you know , establish the definit of big o notat .
what doe that mean ?
well we need to show that for all n at least on caus rememb we chose n<u><num> equal to< u> on , t n thi polynomi up here is bound abov by c time n k , where c is the wai we chose it here , underlin in red .
so let's just check why thi is true .
so , for everi posit integ n at least on , what do we need to prove ?
we need to prove t n is upper bound by someth els .
so we're gonna start on the left hand side with t n .
and now we need a sequenc of upper bound termin with c time n k our choic of c underlin in red .
so t n is given as equal to thi polynomi underlin in green .
so what happen when we replac each of the coeffici with the absolut valu of that coeffici ?
well , you take the absolut valu of a number , either it stai the same as it wa befor , or it flip from neg to posit .
now , n here , we know is at least on .
so if ani coeffici flip from neg to posit , then the overal number onli goe up .
so if we appli the absolut valu of each of the coeffici we get an onli bigger number .
so t n is bound abov by the new polynomi where the coeffici ar the absolut valu of those that we had befor .
so why wa that a us step ?
well now what we can do is we can plai the same trick but with n .
so it's sort of annoi how right now we have these differ power of n .
it would be much nicer if we just had a common power of n , so let's just replac all of these differ n s by n k , the biggest power of n that show up anywher .
so if you replac each of these lower power of n with the higher power n k , that number onli goe up .
now , the coeffici ar all non neg so the overal number onli goe up .
so thi is bound abov by the absolut valu of a<u>k n k < u> . . . up to absolut valu of a<u><num> n k . . . plu a<u><num> n k . < u>< u> i'm us here that n is at least on , so higher power of n ar onli bigger .
and now you'll notic thi , by our choic of c underlin in red , thi is exactli equal to c time n k .
and that's what we have to prove .
we have to prove that t n is at most c time n k , given our choic of c for everi n at least on .
and we just prove that , so , end of proof .
now there remain the question of how did i know what the correct , what a workabl valu of c and n<u><num> < u> were ?
and if you yourself want to prove that someth is big o of someth els , usual what you do is you revers engin constant that work .
so you would go through a proof like thi with a gener valu of c and n<u><num> and then< u> you'd sai , ahh , well if onli i choos c in thi wai , i can push the proof through . and that tell you what c you should us .
if you look at the option video on further exampl of asymptot notat , you'll see some exampl where we deriv the constant via thi revers engin method .
but now let's turn to a second exampl , or realli i should sai , a non exampl .
so what we're go to prove now is that someth is not big o of someth els .
so i claim that for everi k at least <num> , n k is not o n k <num> .
and again , thi is someth you would certainli hope would be true .
if thi wa fals , there'd be someth wrong with our definit of big o notat and so realli thi is just to get further comfort with the definit , how to prove someth is not big o of someth els , and to verifi that inde you don't have ani collaps of distinct power of ploynomi , which would be a bad thing .
so how would we prove that someth is not big o of someth els ?
the most . . . frequent us proof method is gonna be by contradict .
so , rememb , proof by contradict , you assum what you're try to , establish is actual fals , and , from that , you do a sequenc of logic step , culmin in someth which is just patent fals , which contradict basic axiom of mathemat , or of arithmet .
so , suppos , in fact , n k wa big o of n k <num> , so that's assum the opposit of what we're try to prove .
what would that mean ?
well , we just refer to the definit of big o notat .
if in fact n k hypothet were big o of n k <num> then by definit there would be two constant , a win strategi if you like , c and n<u><num> such< u> that for all suffici larg n , we have a constant multipl c time n k <num> upper bound n k .
so from thi , we need to deriv someth which is patent fals that will complet the proof .
and the wai , the easiest wai to do that is to cancel n k <num> from both side of thi inequ .
and rememb sinc n is at least on and k is at least on , it's legitim to cancel thi n k <num> from both side .
and when we do that we get the assert that n is at most some constant c for all n at least n<u><num> .
and thi now< u> is a patent fals statement .
it is not the case that all posit integ ar bound abov by a constant c .
in particular , c <num> , or the integ right abov that , is not bigger than c .
so that provid the contradict that show that our origin assumpt that n k is big o of n k <num> is fals .
and that prove the claim .
n k is not big o of n k <num> , for everi valu of k .
so differ power of polynomi do not collaps .
thei realli ar distinct , with respect to big o notat .
in thi lectur , we'll continu our formal treatment of asymptot notat .
we've alreadi discuss big o notat , which is by far the most import and ubiquit concept that's part of asymptot notat , but , for complet , i do want to tell you about a coupl of close rel of big o , name omega and theta .
if big o is analog to less than or equal to , then omega and theta ar analog to greater than or equal to , and equal to , respect .
but let's treat them a littl more precis .
the formal definit of omega notat close mirror that of big o notat .
we sai that on function , t of n , is big omega of anoth function , f of n , if eventu , that is for suffici larg n , it's lower bound by a constant multipl of f of n .
and we quantifi the idea of a constant multipl and eventu in exactli the same wai as befor , name via explicitli give two constant , c and n naught , such that t of n is bound below by c time f of n for all suffici larg n .
that is , for all n at least n naught .
there's a pictur just like there wa for big o notat .
perhap we have a function t of n which look someth like thi green curv .
and then we have anoth function f of n which is abov t of n .
but then when we multipli f of n by on half , we get someth that , eventu , is alwai below t of n .
so in thi pictur , thi is an exampl where t of n is inde big omega of f of n .
as far as what the constant ar , well , the multipl that we us , c , is obvious just on half .
that's what we're multipli f of n by .
and as befor , n naught is the cross point between the two function .
so , n naught is the point after which c time f of n alwai li below t of n forevermor .
so that's big omega .
theta notat is the equival of equal , and so it just mean that the function is both big o of f of n and omega of f of n .
an equival wai to think about thi is that , eventu , t of n is sandwich between two differ constant multipl of f of n .
i'll write that down , and i'll leav it to you to verifi that the two notion ar equival .
that is , on impli the other and vice versa .
so what do i mean by t of n is eventu sandwich between two multipl of f of n ?
well , i just mean we choos two constant .
a small on , c1 , and a big constant , c2 , and for all n at least n naught , t of n li between those two constant multipl .
on wai that algorithm design can be quit sloppi is by us o notat instead of theta notat .
so that's a common convent and i will follow that convent often in thi class .
let me give you an exampl .
suppos we have a subroutin , which doe a linear scan through an arrai of length n .
it look at each entri in the arrai and doe a constant amount of work with each entri .
so the merg subroutin would be more or less an exampl of a subroutin of that type .
so even though the run time of such an algorithm , a subroutin , is patent theta of n , it doe constant work for each of n entri , so it's exactli theta of n , we'll often just sai that it ha run time o of n .
we won't bother to make the stronger statement that it's theta of n .
the reason we do that is becaus you know , as algorithm design , what we realli care about is upper bound .
we want guarante on how long our algorithm ar go to run , so natur we focu on the upper bound and not so much on the lower bound side .
so don't get confus .
onc in a while , there will a quantiti which is obvious theta of f of n , and i'll just make the weaker statement that it's o of f of n .
the next quiz is meant to check your understand of these three concept big o , big omega , and big theta notat .
so the final three respons ar all correct , and i hope the high level intuit for why is fairli clear .
t of n is definit a quadrat function .
we know that the linear term doesn't matter much as it grow , as n grow larg .
so sinc it ha quadrat growth , then the third respons should be correct .
it's theta of n squar .
and it is omega of n .
so omega of n is not a veri good lower bound on the asymptot rate of growth of t of n , but it is legitim .
inde , as a quadrat grow function , it grow at least as fast as a linear function .
so it's omega of n .
for the same reason , big o of n cube , it's not a veri good upper bound , but it is a legitim on , it is correct .
the rate of growth of t of n is at most cubic .
in fact , it's at most quadrat , but it is inde , at most , cubic .
now if you want to prove these three statement formal , you would just exhibit the appropri constant .
so for prove that it's big omega of n , you could take n naught equal to on , and c equal to on half .
for the final statement , again you could take n naught equal to on .
and c equal to sai four .
and to prove that it's theta of n squar you could do someth similar just us the two constant combin .
so n naught would be on .
you could take c1 to be on half and c2 to be four .
and i'll leav it to you to verifi that the formal definit of big omega , big theta , and big o would be satisfi with these choic of constant .
on final piec of asymptot notat , we're ar not go to us thi much , but you do see it from time to time so i want to mention it briefli .
thi is call littl o notat , in contrast to big o notat .
so while big o notat inform is a less than or equal to type relat , littl o is a strictli less than relat .
so intuit it mean that on function is grow strictli less quickli than anoth .
so formal we sai that a function t of n is littl o of f of n , if and onli if for all constant c , there is a constant n naught beyond which t of n is upper bound by thi constant multipl c time by f of n .
so the differ between thi definit and that of big o notat , is that , to prove that on function is big o of anoth , we onli have to exhibit on measli constant c , such that c time f of n is upper bound , eventu , for t of n .
by contrast , to prove that someth is littl o of anoth function , we have to prove someth quit a bit stronger .
we have to prove that , for everi singl constant c , no matter how small , for everi c , there exist some larg enough n naught beyond which t of n is bound abov by c time f of n .
so , for those of you look for a littl more facil with littl o notat , i'll leav it as an exercis to prove that , as you'd expect for all polynomi power k , in fact , n to the k minu on is littl o of n to the k .
there is an analog notion of littl omega notat express that on function grow strictli quicker than anoth .
but that on you don't see veri often , and i'm not gonna sai anyth more about it .
so let me conclud thi video with a quot from an articl , back from <num> , about my colleagu don knuth , wide regard as the grandfath of the formal analysi of algorithm .
and it's rare that you can pinpoint why and where some kind of notat becam univers adopt in the field .
in the case of asymptot notat , inde , it's veri clear where it came from .
the notat wa not invent by algorithm design or comput scientist .
it's been in us in number theori sinc the nineteenth centuri .
but it wa don knuth in '<num> that propos that thi becom the standard languag for discuss rate of growth , and in particular , for the run time of algorithm .
so in particular , he sai in thi articl , on the basi of the issu discuss here , i propos that member of sigact , thi is the special interest group of the acm , which is concern with theoret comput scienc , in particular the analysi of algorithm .
so , i propos that the member of sigact and editor in comput scienc and mathemat journal adopt the o , omega , and theta notat as defin abov unless a better altern can be found reason soon .
so clearli a better altern wa not found and ever sinc that time thi ha been the standard wai of discuss the rate of growth of run time of algorithm and that's what we'll be us here .
thi video is for those of you who want some addit practic with asymptot notat .
and we're gonna go through three addit option exampl .
let's start with the first on .
so the point of thi first exampl is to show how to formal prove that on function is big o of anoth .
so the function that i want to work with is two rais to the n plu ten , okai , so it's the two to the n function that you're all familiar with , we're go to shift it by ten and the claim is that thi function is big o of two to the n , so without the ten .
so how would on prove such a claim ?
well let go back to the definit of what it mean for on function to be big o over anoth , what we have to prove is we need to show that there exist two constant , so that for all suffici larg n mean n bigger than n nought , our left hand side , so the function should be n plu ten is bound abov by a constant multipl c time the function on right hand side to the n .
right so for all suffici larg n the function is bound abov by a constant multipl of two to the n .
so unlik the first basic exampl where i just pull the two constant out of a hat let's actual start the proof and see how you'd revers engin the suitabl choic of these two constant .
so , what a proof would look like , it would start with two to the n plu ten , on the left hand side , and then there'd be a chain of inequ , termin in thi , c time two to the n .
so , let's just go ahead and start such a proof , and see what we might do .
so , if we start with two to the n plu ten on the left hand side , what would our first step look like ?
well , thi <num>'s realli annoi , so it make sens to separ it out .
so you could write two to the n plu ten as the product of two term .
two to the ten , and then the two to the n .
also known as just <num> time two to the n .
and now we're in , look in realli good shape .
so if you look at where we ar so far , and where we want to get to , it seem like we should be choos our constant c to be <num> .
so if we choos c to be <num> and we don't have to be clever with n nought we can just set that equal to on , then inde star hold to the desir inequ and rememb to prove that on function is big o of anoth all you gotta do is come up with on pair of constant that work and we've just revers engin it just choos the constant c to be <num> and n nought to be on work so thi prove that two to the n plu ten is big o over two to the n .
next let's turn to anoth non exampl how , of a function which is not big o over anoth .
and so thi will look superfici similar to the previou on .
instead of take , ad ten in the expon of the function two to the n , i'm gonna multipli by ten in the expon .
and the claim is if you multipli by ten in the expon then thi is not the same asymptot as two to the n .
so onc again , usual the wai you prove on thing is not big o of anoth is by contridict .
so we're go to assum the contrari , that two to the ten n is in fact big o of two to the n .
what would it mean if that were true ?
well , by the definit of big o notat , that would mean there ar constant c and n nought .
so that for all larg n , two to the ten n is bound abov by c time <num> to the n .
so to complet the proof what we have to do is go from thi assumpt and deriv someth which is obvious fals but that's easi to do just by cancel thi <num> of the n term from both side .
so if we divid both side by <num> to the n , which is a posit number sinc n is posit , what we find would be a logic consequ of our assumpt would be that two rais to the nine n is bound abov by some fix constant c for all n at least n nought .
but thi inequ of cours is certainli fals .
the right hand side is some fix constant independ of n .
the left hand side is go to infin as n grow larg .
so there's no wai thi inequ hold for arbitrarili larg n .
so that conclud the proof by contradict .
thi mean our assumpt wa not the case , and inde it is not the case that two to the ten n is big o of two to the n .
so our third and final exampl is a littl bit more complic than the first two .
it'll give us some practic us theta notat .
recal that while big o is analog to sai on function is less than or equal to anoth , theta notat is in the spirit of sai on function is equal asymptot to anoth .
so here's gonna be the formal claim we're gonna prove , for everi pair of function f and g , both of these function ar defin on the posit integ , the claim is that it doesn't matter , up to a constant factor , whether we take point wise maximum of the two function or whether we take the point wise sum of the two function .
so let me make sure it's clear that you know i mean by the point wise maximum by max f and g .
so , if you look at the two function , both function of n , mayb we have f be thi green function here and we have g hook to thi red function .
then by the point wise maximum max f , g just mean the upper envelop of these two function .
so that's gonna be thi blue function .
so let now turn to the proof of thi claim that the point wise function of these two function is theta of the sum of two function .
so let's recal what theta mean formal .
what it mean is that the function on the left can be sandwich between the constant multipl of the function on the right .
so we need to exhibit both the usual n nought but also two constant , the small on , c1 , and the big on , c2 , so that the point wise maximum f , g , whatev that mai be , is wedg in between c1 and c2 time f n plu g n , respect .
so to see where these constant c1 and c2 ar go to come from , let's observ the follow inequ .
so no matter what n is , ani posit integ n , we have the follow .
suppos we take the larger of f of n and g of n .
and rememb now , we've fix the valu of n , and it's just some integ , you know , like <num> .
and now f of n and g of n ar theirselv , just number .
you know , mayb thei're <num> and <num> , or whatev .
and if you take the larger of f of n and g of n , that's certainli no more than the sum of f of n plu g of n .
now , i'm us , in thi inequ , that f and g ar posit .
and that's someth i've been assum throughout the cours so far .
here , i wanna be explicit about it , we're assum that f and g cannot output neg number .
whatev integ you feed in , you get out someth posit .
now , the function we care about ar thing like the run time of algorithm , so there's realli no reason for us to pollut our think with neg number .
so , we're just gonna alwai be assum in thi class , posit number .
and i'm actual us it here , the right hand side is the sum of two thing , is bigger than just either on of the constitu summant .
secondli .
if we doubl the larger of f of n and g of n well that's go to exce the sum of f of n plu g of n , right ?
becaus on the right hand side we have a big number plu a small number and then on the left hand side we have two copi of the big number so that is go to be someth larger , now it's gonna be conveni it's gonna be more obviou what's go on if i divid both of these side by two so that the maximum of f of n and g of n is at least half of f of n plu g of n that is at least half of the averag and now we're pretti much home free right so what doe thi sai .
thi sai that for everi possibl n , the maximum wedg between suitabl multipl of the sum .
so on half of f of n plu g of n .
there's a lower bound on the maximum .
thi is just the second inequ that we deriv .
and by the first inequ that's bound abov by onc time the sum .
and thi hold no matter what n is , at least on .
and thi is exactli what it mean to prove that on function is theta of anoth .
we've shown that for all n , not just for insuffi larg , but in fact for all n .
the pointwis maximum of f and g is wedg between suitabl constant multipl of their sum .
and again , just to be explicit , the certifi choic of constant ar n nought equal on .
the smaller constant is on half .
and the bigger constant equal on .
and that complet the proof .
in thi next seri of video , we'll get some more practic appli the divid and conquer algorithm and design paradigm to variou problem .
thi will also give us a glimps of the kind of applic to which it's been successfulli appli .
we're gonna start by extend the merg sort algorithm to solv a problem involv count the number of invers of an arrai .
befor we tackl the specif problem of count the number of invers in an arrai , let me sai a few word about the divid and conquer paradigm in gener .
so again , you've alreadi seen the total canon exampl of divid and conquer , name merg sort .
so the follow three conceptu step will be familiar to you .
the first step , no prize for guess is you divid .
the problem .
into smaller sub problem .
sometim thi divis happen onli in your mind .
it's realli more of a conceptu step than part of your code .
sometim you realli do copi over part of the input into sai new arrai to pass on to your recurs call .
the second step , again no prize here , is you conquer the sub problem just us recurs .
so for exampl , in merg sort , you conceptu divid the arrai into two differ piec .
and then you with the conquer or sort to the first half of the arrai .
and you , you do the same thing with the second half of the arrai .
now , of cours , it's not quit as easi as just do these two step .
divid the problem , and then solv the sub problem recurs .
usual , you have some extra cleanup work after the recurs call , and to stitch togeth the solut to the sub problem into on for the big problem , the problem that you actual care about .
recal , for exampl , in merg sort , after our recurs call , the left half of the arrai wa sort , the right half of the arrai wa sort .
but we still had to stitch those togeth .
merg into a sort version of the entir arrai .
so the step is to combin .
the solut to the subproblem into on problem .
gener the largest amount of ingenu happen in the third step .
how do you actual quickli combin solut to subproblem into on to the origin problem ?
sometim you also get some clever in the first step with divis .
sometim it's as simpl as just splite a rai in two .
but there ar case where the divis step also ha some ingenu .
now let's move on to the specif problem of count invers and see how to appli thi divid and conquer paradygm .
so let begin by defin the problem formal now .
we're given as input an arrai a with a length n .
and you can defin the problem so that the arrai a contain ani ol distinct number .
but , let's just keep thing simpl and assum that it contain the number on through n .
the integ in that rang in some order .
that captur the essenc of the problem .
and the goal is to comput the number of invers of thi arrai so what's an invers you mai ask well an invers is just a pair of arrai i and j with i smaller than j so that earlier arrai entri the i entri is bigger than the latter on the jake on so on thing that should be evid is that if the arrai contain these number in sort order if the arrai is simpli on two three four all the wai up to n then the number of invers is zero .
the convers you might also want to think through if the arrai ha ani other order of the number between on and n other than the assort on , then it's go to have a non .
of zero number of invers .
let's look at anoth exampl .
so'spose we have an arrai of six entri .
so the number on thru six in the follow order .
on , three , five follow by two , four , six .
so how mani invers doe thi arrai have ?
so again what we need to look for ar pair of arrai entri so that the earlier or left entri is bigger than the later or right entri .
so on exampl which we see right here would five and two .
those ar right next to each other and out of order , the earlier entri is bigger than the other on .
but there's other , there's three and two for exampl those ar out of order .
and , five and four ar also out of order .
and i'll leav it to you to check that those ar the onli three pair that ar out of order .
so summar the invers in thi arrai of length six ar <num> <num> , <num> <num> , and <num> <num> .
correspond to the arrai entri , <num> <num> , <num> <num> , and <num> <num> .
pictori , we can think of it thusli , we can first .
write down the number in order , on up to six .
and then we can write down the number again but , order in the wai that their given in the input arrai .
so , on three five two four six .
and then we can connect the dot , mean we connect on to on .
reconnect two to two , and so on .
it turn out , and i'll leav to for you to , to think thi through , that the number of cross pair of line segment prescis correspond to the number of invers .
so we see that there ar on , two , three cross line segment .
and these ar exactli in correspond with the three invers , we found earlier .
five and two , three and two , and five and four .
now , wanna solv thi problem you might ask .
well there's few reason that come up .
on would be to have a numer similar measur that quantifi how close to list ar to each other .
so for exampl , suppos i took you and a friend , and i took , identifi ten movi that both of you had seen .
and i ask each of you to order , or to rank these movi from your most favorit to your least favorit .
now i can form an arrai , and comput invers .
and it quantifi , in some sens , how dissimilar your two rank ar to each other .
so in more detail , in the first entri of thi arrai , i would put down the rank that your friend gave to your favorit movi .
so if you had your favorit movi , star war or whatev .
and your friend onli thought it wa the fifth best out of the ten , then i would write down a five in the first entri of thi arrai .
gener , i would take your second favorit movi .
i would look at how your friend rank that .
i would put that in the second entri of the arrai and so on , all the wai up to the tenth entri of the arrai , where i would put your friend's rank of your least favorit movi .
now , if you have exactli ident prefer , if you rank them exactli the same wai , the number of invers of thi arrai would be zero .
and in gener , the more invers thi arrai ha , it quantifi that your list look more and more differ from each other .
now why might you want to do thi why might you want to know whether two differ peopl rank thing in the similar wai had similar prefer well on reason might be what's call collabor filter , probabl mani of you have had the experi of go to a websit and if you've made a few purchas through thi websit it start recommend further purchas for you , so on wai to solv thi problem under the hood , is to look at your purchas look at what you seem to like , find other peopl who have similar prefer similar histori look at thing thei've bought that you haven't , and then recommend .
new product to you base on what similar custom seem to have bought .
so thi problem captur some of the essenc of identifi which custom or which peopl ar similar base on data about what thei prefer .
so just to make sure we're all on the same page , let me paus for a brief quiz .
we've alreadi notic that a given arrai will have zero invers , if and onli if it's in sort order .
if it onli contain the number of on through n in order .
so , on the other side , what is the largest number of invers an arrai could possibl have ?
let's sai , just for an arrai of size six , like the on in thi exampl here .
so the answer to thi question is the first on .
fifteen .
or in gener in an n .
element arrai the largest number of invers is n .
choos two .
also known as n time n minu on over two .
which , again , in the case of a is go to evalu to fifteen .
the reason is , the worst case is when the arrai is in backward order , revers order , and everi singl pair of indic is invert .
and so the number of indic ij , with i less than j is precis too .
let's now turn our attent to the problem of comput the number of invers of an arrai as quickli as possibl .
so on option that is certainli avail to us is the brute forc algorithm .
and by brute forc i just mean we could set up a doubl four loop .
on which goe through i , on which goe through j bigger than i , and we just check each pair ij individu with i less than j whether that particular pair of arrai entiti ai and aj is invert and if it is then we add it to our run count .
and then we return the final count at the end of the doubl four loop .
that's certainli correct .
the onli problem is , as we just observ , there's n two or a quadrat number of potenti invers so thi algorithm's almost go to run in time quadrat in the arrai link .
now rememb the mantra of ani good algorithm design .
can we do better ?
and the answer is ye .
and the method we'll be us , divid and conquer .
the wai in which we'll divid will be motiv directli by merg sort where we recur e separ on the left and the right half's of the arrai .
we're gonna do the same thing here .
to understand how much progress we can make pure us recurs let's classifi the invers of arrai into on of three type .
so suppos we have an invers of an arrai i , j , and rememb in an invers you alwai have i less than j .
we're gonna call it a left invers .
if both of the arrai indic ar at most n over two , where n is the arrai length .
we're gonna call it a right invers if thei're both strictli greater than n over two .
and we're gonna call it a split invers if the smaller index is at most n over two and the larger index is bigger than n over two .
we can discuss the progress made by recurs in these term .
when we recurs on the left half of an arrai , if we implement our algorithm correctli , we'll successfulli be abl to count all of the invers locat pure in that first half .
those ar precis the left invers .
similarli , a second recurs call on just the right half of an arrai , the second half of an arrai will successfulli count all of the right invers .
there remain the question of how to count the split invers .
but we shouldn't be surpris there's some residu work left over , even after the recurs call do their job .
that , of cours , wa the case at merg short , where magic took care of sort the left half of the arrai , sort the right half of the arrai .
but there wa still , after their return , the matter of merg those two sort list into on .
and here again , after the recurs is gonna be the matter of clean up and count the number of split invers .
so for exampl if you go back to the six element arrai we work through befor , <num> , you'll notic that there , in fact , all of the invers ar split .
so the recurs call will both come back count zero invers .
and all of the work for that exampl will be done by the count split invers subroutin .
so let's summar where thing stand given underspecifi high level descript of the algorithm as we envis it .
there is a base case .
i'll go ahead and write it down for complet , which is if we're given a on element arrai , then there's certainli no invers so we can just immedi return the answer zero .
for ani bigger arrai , we're go to divd and conquer .
so we'll count the left invers with a recurs call .
the right invers with a recurs call .
and then we'll have some current unimpl subroutin that count the split invers .
sinc everi invers is either left or right , or split , and can't be ani more than on of those three , then , have done these three thing , we can simpli return their sum .
so that's our high level attack on how we're gonna count up the number of invers .
and of cours , we need to specifi how we're gonna count the number of split invers .
and moreov , we lack that subroutin to run quickli .
an analog to emerg short , where , outsid the recurs call , we did mere linear work .
out , in the merg subroutin .
here , we'd like to do onli linear work in count up the number of split invers .
if we succe in thi goal , if we produc a correct and linear time of limit to count up the number of split incurs , then thi entir recurs algorithm will run in big o .
of n .
log in time .
the reason the overal out rhythm will run in o .
of n .
log in time is exactli the same reason that merg short ran in n .
log in time .
there's two recurs call .
each on a problem of on half the size .
and outsid of the recurs call we would be do linear work .
so you could copi down exactli the same recurs tree argument we us for merg short .
it would appli equal well here .
altern , veri soon we will cover the master method , and as on veri special case it will prove that thi algorithm , if we can implement it thusli , will run in o .
of n .
log in time .
now on thing to realiz , is thi is a fairli ambiti goal , to count up the number of split invers in linear time .
it's not that there can't be too mani split invers .
there can actual be a lot of them .
if you have an arrai where the first half of the arrai contain the number n over two plu on , up to n .
wherea the second part of the arrai contain the number on up to n over two , that ha a quadrat number of invers , all of which ar split .
so , what we're attempt to do here is count up a quadrat number of thing us onli linear time .
can it realli be done ?
ye is can , as we'll see in the next video .
so far , we've develop a divid and conquer approach to count the number of invers of an arrai , so we're gonna split the arrai in two part , recurs count invers on the left and on the right .
we've indentifi the kei challeng as count the number of split invers quickli .
where a split invers mean that the earlier index in the left half of the arrai , the second index in the right half of the arrai .
these ar precis invers that ar go to be miss by both of our recurs call .
and the crux of the problem is that there might be as mani as quadrat split version , if somehow , thei get the run time we want , we need to do it in a linear time .
so here's the realli nice .
thi idea which is go to let us do that .
the idea is to piggyback on merg short .
by which i mean , we're actual go to demand a bit more of our recurs call to make the job of count the number of split recurs easier .
thi is analog to when you're do a proof by induct .
sometim when make the induct hyphothesi stronger , that's what let you push through the induct proof .
so we're gonna ask our recurs call to not onli count invers in the arrai of their past .
but also along the wai , to sort the arrai .
and hei .
why not ?
we know sort is fast .
merg short will do it in n .
log in time , which is the run time we're shoot for .
so why not just throw that in ?
mayb it will help us in the combin step .
and as we will see it will .
so what doe thi bui us ?
why should we demand more of our recurs call ?
well , as we'll see , in a coupl slide , the merg subroutin almost seem design just to count the number of split invers .
as we'll see as you merg two sort sub arrai , you will natur uncov all the split invers .
so let me just be a littl bit more clear about how our previou high level algorithm is go to now be soup up so that the recurs call sort as well .
so here's the high level algorithm we propos befor where we just recurs count invers on the left side , on the right side , and then we have some current unimpl sub routin count split in which is respons for count the number of split invers .
so we're just gonna augment thi as follow .
so instead of be call count now we're go to call it sort and count .
that's go to be the name of our algorithm the recurs call again just invok sort and count and so now we know each of those will not onli count the number of invers in the sub arrai but also return a sort version so out from the first on we're go to get arrai b back which is the sort version of the arrai that we pass it and we'll get assort arrai c back from the second recurs call the sort version of arrai t hat we pass it and now the count is split in version now in addit to count split version it's respons for merg the two sort sub arrai b and c .
to will be respons .
for output an arrai d . , which is an assort version of the origin input arrai a .
and so i should also renam our unimpl subroutin to reflect it now more ambiti agenda .
so we'll call thi , merg .
and count split in .
now we shouldn't be intimid by ask our combin soviet team to merg .
the two sort separ b and c becaus we've alreadi see we now how to do that in linear time .
so the question is just piggyback on that work , can we also count the number of split invers in an addit linear time .
we'll see that we can , although that's certainli not obviou .
so you should again at thi point have the question why aren't we do thi why ar we just make ourselv do more work .
and again the hope is that the payoff is some how version becom easier by ask our recurs call to so to develop some intuit for why that's true why merg natur uncov the number of split invers let's recal the definit of just the origin merg server team from merg sort wa so here's the same pseudo code we went through sever video ago i have renam the letter of the arrai to be consist with the current notat so .
we're given two sort sub arrai , these come back from recurs call .
i'm call them b and c .
thei both have length n over two and respons for produc the sort combin of b and c .
so that's an output arrai d of length n .
and again the idea is simpl .
you're just take the two sort sub arrai b and c .
and then you take the output arrai d .
which ar reason for popul and us index k you're go to travers the output arrai d from left to right .
that's what thi outer for loop here doe .
and your gonna maintain pointer i and j .
to the sort subarrai , b and c respect .
and the onli observ is that whatev the minimum element that you haven't copi over to d yet is , it's gotta be either the , the leftmost element of b that you haven't seen yet , or the leftmost element of c that you haven't seen yet .
b and c , by virtu of be sort , the minimum remain ha to be , the next on avail to either b or c .
so you just proce in the obviou wai , you compar the two candid for the next on to copi over .
you look at b of i , you look at c of j .
whichev on is smaller , you copi over .
so the first part of the if statement is for when b contain the smaller on .
the second part of the .
the l statement is for when c contain the smaller on .
okai , so that's how merg work .
you go down b and c in parallel popul d in sort order from left to right .
now to get some feel for what on earth ani of thi ha to do with the split invers of an arrai i want you to think about an input arrai a that ha the follow properti , that ha the properti that there ar no split invers .
whatsoev .
so everi invers in thi arrai , in thi input arrai a is gonna be either a left invers , so both indic ar at most n over two or a right invers , so both indic ar strictli greater than n over two .
now the question is , given such an arrai a , what's , onc you're merg .
at the step what do the sort sub arrai b and c look like for input arrai a that ha no split invers .
the correct answer is the second on .
that if you have an arrai with no split invers , then everyth in the first half is less than everyth in the second half .
why ?
well consid the contraposit .
suppos you had even on element in the first half which wa bigger than ani element in the second half .
that pair of element alon would constitut a split invers .
okai ?
so if you have no split invers , then everyth on the left is smaller than everyth on the , in the right half of the arrai .
now , more to the point , think about the execut of the merg subroutin on an arrai with thi properti .
on an input arrai a where everyth in the left half is less than everyth in the right half .
what is merg gonna do ?
all right , so rememb it's alwai look for whichev is smaller the first element of , remain in b or the first element remain in c , and that's what it copi over .
well if everyth in b is less than everyth in c , everyth in b is gonna get copi over into the rai d befor c ever get touch .
okai ?
so merg had an unusu trivial execut on input arrai with no split invers , with zero split invers .
first it just goe through b and copi it over .
then it just concaton c .
okai there's no interleav between the two .
so no split invers mean noth get copi from c until it absolut ha to , until b is exhaust .
so thi suggest that perhap copi element over from the second subarrai , c , ha someth to do with the number of split invers in the origin arrai , and that is , in fact , the case .
so we're gonna see a gener pattern about copi from the second element c , second arrai c expos split invers in the origin input arrai a .
so let's look at a more detail exampl , to see what that pattern is .
let's return to it , the exampl in the previou video .
which is an arrai with six element order on , three , five , two , four , six .
so we do our recurs call , and in fact , the left half of the arrai is sort , and the right half of the arrai is alreadi sort .
so sort what's gonna be done get to zero invers for both our recurs call .
rememb , in thi exampl , it turn out all of the invers ar split invers .
so , now let's trace through the merg sub routin invok on these two sort sub arrai , and try to spot a connect with the number of split invers in the origin <num> element arrai .
so , we initi indic i and j to point to the first element of each of these sub arrai .
so , thi left on is b , and thi right on is c and the output is d .
now , the first thing we do , is we copi the on over from b into the upward arrai .
so , on goe there , and we advanc thi index over to the three .
and , here noth realli interest happen , there's no .
reason to count ani split invers , and inde , the number on is not involv in ani split invers , cuz on is smaller than all of the other element , and it's also in the first index .
thing ar much more interest , when we copi over the element two from the second arrai c .
and , notic at thi point , we have diverg from the trivial execut that we would see with an arrai with no split invers .
now , we're copi someth over from c , befor we've exhaust copi d .
so we're hope thi will expos some split in version .
so we copi over the two .
and we advanc the second pointer j into c .
and the thing to notic is thi expos two split invers .
the two split invers that involv the element two .
and those invers ar three comma two and five comma two .
so why did thi happen ?
well , the reason we copi two over is becaus it's smaller than all the element we haven't yet look at in both b and c .
so in particular , two is smaller than the remain element in b , the three and the five .
but also becaus b is the left arrai .
the indic and the three and five have to be less than the index of thi two , so these ar invers .
two is further to the right than the origin input arrai , and yet it's smaller than these remain element in b .
so there ar two element remain in b , and those ar the two split invers that involv the element two .
so now , let's go back to the emerg subroutin , so what happen next ?
well , next , we make a copi from the first arrai , and we sort of realiz that noth realli interest happen when we copi from the first arrai , at least with respect to split invers .
then we copi the four over , and yet again , we discov a split invers , the remain on which is five comma four .
again , the reason is , given that four wa copi over befor , what's left in b , it's gotta be smaller than it , but by virtu of be in the rightmost arrai , it's also gotta have a bigger index .
so it's gotta be a split invers .
and now the rest of the merg subroutin execut .
without ani real incid .
the five get copi over and we know copi from the left arrai ar bore .
and then we copi the six over and copi from the right arrai ar gener interest but not if the left arrai is empti .
that doesn't involv ani split invers .
and you will recal , from the earlier video that these were invers in the origin arrai , three <num> , five <num> , and five <num> .
we discov them all in autom method , by just keep an ey out when we copi from the right arrai c .
so thi is inde a gener principl , so let me state the gener claim .
so the claim is not just in thi specif exampl , and thi specif execut , but no matter what the input arrai is , no matter how mani split invers there might be , the split invers that involv an element of the second half of the arrai ar precis .
those element remain in the first arrai when that element get copi over to the output arrai .
so thi is exactli what the pattern that we saw in the exampl .
what work on the right arrai .
in c , we had the element two , four , and six .
rememb , everi split invers ha to , by definit , involv on element from the first half , and on element from the second half .
so to count invers , we can just group them accord to which element of the second arrai there , did thei involv .
so out of the two , four , and six , the two is involv in the splitter convers , <num> <num> , and <num> <num> .
the three and the five were exactli the element remain in b .
bit over two .
the split invers involv four is exactli the invers five four and five is exactli the element that wa remain in b when we copi over the four .
there's no split invers involv six and inde the element d wa empti when we copi the six over into the output arrai d .
so what's the gener argument ?
what's quit simpl .
let just zoom in and fixat on a particular element , x that belong to that first half of the arrai that's among the first half of the element and let's just examin which y's so which element of the second arrai the second half of the origin input arrai involv with split version of x .
so there ar two case depend on whether x is copi to the output arrai d befor or after y now if x is copi to the output befor y well then sinc the in sort order that mean x ha got to be shorter than y so there's not go to be ani split in invers .
on the other hand , if y is copi to the output d befor x , then again becaus we popul d left to right in sort order , that's gotta mean that y is less than x .
now x .
is still hang out in the left arrai , so it ha a less index than y .
y come from the right arrai .
so thi is inde a split invers .
so put these two togeth it sai that the .
element x .
of the arrai b .
that form split invers with y .
ar precis those that ar go to get copi to the output arrai , after what ?
so , those ar exactli the number of element remain in b , when y get copi over .
so , that prove the gener claim .
so , thi slide wa realli the kei insight .
now that we understand exactli why , count split invers is easi .
as we're merg togeth two sort sub arrai , it's a simpl matter to just translat thi into code , and get a linear time implement of a sub routin that both merg and count the number of split invers .
which , then , in the overal recurs algorithm , will have n log n run time , just as in merg sort .
so , let's just spend a quick minut fill in those detail .
so .
i'm not gonna write out the pseudocod , i'm just go to write out what you need to augment the merg pseudocod , discuss a few slide ago , by , in order to count split invers as you're do the merg .
and thi will follow immedi from the previou claim , which indic how split invers relat to , the number of element remain on the left arrai as you're do the merg .
so , the idea is the natur on , as you're do the merg , accord to the previou pseudocod of the two sort sub arrai , you just keep a run total of the number of split invers that you've encount , all right ?
so you've got your sort sub arrai b , you've got your sort sub arrai c .
you're merg these into an output arrai d .
and as you travers through d and k from on to n you just start the count at zero and you increment it by someth each time you do a copi over from either from b or c .
so , what's the increment , well what did we just see ?
we saw the copi .
involv b don't count .
we're not gonna look at split invers when we copi over for b .
onli when we look at them from c , right ?
everi split invers involv exactli on element from each of b and c .
so i mai as well count them , via the element in c .
and how mani split invers ar involv with the given element of c ?
well , it's exactli how mani element of b remain when it get copi over .
so that tell us how to increment thi run count .
and it fall immedi from the claim on the previou slide that thi .
implement of thi run total count precis the number of split invers that the origin input arrai a possess .
and you'll recal that the left invers ar count by the first recurs call , the right invers ar count by the second recurs call .
everi invers is either left or right or split , it's exactli on of those three type .
so with our three differ subroutin , the two recurs on and thi on here we successfulli count up all of the invers of the origin input arrai .
so that's the correct of the algorithm , what's the run time ?
what we're call merg sort , we begin by just analyz the run time of merg and then we discuss the run time of the entir merg sort let's do the same thing here briefli .
so what's the run time of the team for thi merg and simultan cut into split version .
work that we do in the merg and we alreadi know that that's linear and then the onli addit work here is .
increment thi run count .
and that's constant time for each element of d , right ?
each time we do a copi over , we do , a singl edit to our run count .
so constant time for element d , or linear time overal .
so i'm be a littl sloppi here , sloppi in a veri convent wai .
but it is a littl sloppi about write o of n plu o of n is equal to o of n .
be care when you make statement like that , right ?
so if you ad o of n to itself n time , it would not be o of n .
but if you add o of n to itself a constant number of time , it is still o of n .
so you might , as an exercis , want to write out , a formal version of what thi mean .
basic , there's some constant c the merg step take a c11 in step .
there's a constant c2 so that the rest of the work is in those c2 time n step .
so when we add them we get , get most quantiti c1 plu c2 time n step , which is still becaus c1 plu c2 is a constant .
okai ?
so linear work for merg , linear work the run count , that's linear work in the subroutin overal .
and no by exactli the same argument we us in merg sort becaus we have two recurs call on half the size and we do linear work outsid of the cursiv call , the overal run time is o of n log n .
in thi video , we'll appli the divid and conquer algorithm design paradigm to the problem of multipli matric .
thi will culmin in the studi of strassen's matrix multipl algorithm .
and thi is a super cool algorithm for two reason .
first of all , strassen's algorithm is complet non trivial .
it is total non obviou , veri clever .
not at all clear how strassen ever came up with it .
the second cool featur , is , it's for such a fundament problem .
so comput , as long as thei've been in us , from the time thei were invent , up'til todai , a lot of their cycl ar spent multipli matric , 'caus it just come up all the time in import applic .
so let me first just make sure we're all clear on what the , what the problem is of .
multipli two matric .
so , we're gonna be interest in three matric , x , y , and z .
thei're all gonna , i'm gonna assum thei all have the same dimens , n by n .
the idea we'll talk about ar also relev for multipli non squar matric , but we're not gonna discuss it in thi video .
the entri in these matric , you know , you could think of it as whatev you want .
mayb thei're integ , mayb thei're ration , mayb thei're from some field .
it depend on the applic .
but the point is , thei're just entri that we can add and multipli .
so how is it that you take two n by n matric , x and y , and multipli them produc a new n by n matrix , z ?
well , recal that the ij entri of z , that mean the entri in the ith row and jth column , is simpli the dot product of the ith row of x with the jth column of y .
so if ij wa thi red squar , thi over in the z matrix , that would be deriv from the correspond row of the x matrix , and the correspond column of the y matrix .
and recal what i mean by dot product .
that just mean you take the product of the individu compon , and then add up the result .
so ultim , the zij entri boil down to a sum over n thing , where each of the constitu product is just the xik entri .
the of the matrix x with the kj entri , of the matrix y , where your k is rang from on to n .
so that's how zij is defin for a given pair of indic , i and j .
on thing to note is where we often us n to denot the input size , here we're us n to note the dimens of each of these matrici .
the input size is not n .
the input size is quit a bit bigger than n .
specif , each of these ar n by n matrici and contain n squar entri .
so sinc presum we have to read the input which ha size and squar .
which happen to produc the output that also ha size and squar .
the best we can realli hope for is multipl hour with the run time n squar .
so the question is how close when we get to it .
befor we talk about algorithm for matrix multipl , let me just make sure we're all crystal clear on exactli what the problem is .
so let's just actual spell out what would be the result of multipli two differ , two byte of matric .
so we can .
<num> gener 2x2 matrici by just give the first on entri a , b , c , and d for these four entri could all be anyth .
and then we're multipli by a second 2x2 matrix , let's call it entri e , f , g , and h .
now , what's the result of multipli these , where again , it's go to be a 2x2 matrix for each entri , it's just the correspond dot product of the relev row of the first matrix and column of the second matrix .
so to get the upper left entri .
you take the doc product of the upper row of the first matrix and the first column of the left column of the second matrix .
so , that result in .
ae plu bg .
to get the upper right entri , we take the dot product of the top row of the left matrix with the right column of the second matrix .
so that give us af bh .
and then fill in the other entri the same wai , we get ce dg and df dh .
you know , so that's multipli two matric , and we've alreadi discuss the definit in gener .
now , suppos you had to write a program to actual comput to the result of multipli two n by n matric .
on natur wai to do that would just be to return to the definit and which defin each of the n squar entri in the z matrix as a suitabl sum of product of entri of the x and y matric .
so on the next quiz , i'd like you to .
figur out what exactli would be the run time of that algorithm as a function of the matrix dimens n where as usual we count the addit or multipl of two individu entri as a constant time oper .
so the correct respons to thi quiz is the third answer , that the run time of the straightforward algorithm run in cubic time rel to the matrix dimens n .
to see thi let's just recal what the definit of the matrix multipl wa .
the definit tell us each entri zij of the output matrix z is defin as the sum from k <num> to n of .
xik time ykj .
that is the product of the row of the x matric and the j column of the y matrix .
certainli assum that we have the matric repres in a wai that we can access a given entri in constant time .
and under that assumpt , rememb each of these , each of these product .
onli take constant time .
and so then to comput zij we just have to add up these end product .
so that's gonna be theta of n time to comput a given zij and then there's an n squar that we have to comput .
there's n choic for i , n choic for j , so that give us n squar time n or cubic run time overal for the natur algorithm , which is realli just a tripl for loop which comput each entri of the output rai separ us the dot product .
so the question as alwai for the keen algorithm design is .
can we do better ?
can we beat , in cube time , by multipli two matric ?
and we might be especi embolden with the progress that we've alreadi seen in term of multipli two integ .
we appli the divid and conquer algorithm , to problem multipli two end digit integ .
and we had , both a naiv recurs algorithm , and a seemingli better .
algorithm due to , which made onli three recurs call .
now we haven't yet analyz the run time of that algorithm .
but as we'll see later , that doe inde beat the quadrat run time of the grade school algorithm .
so it's veri natur to ask , can we do exactli the same thing here ?
there's the obviou algorithm , which follow straight from the definit .
perhap analog to , we could have some clever divid and conquer method , which beat cubic time .
so that's what we're gonna explor next .
let's recal the divid and conquer paradigm , what doe it mean to us it .
well , we first have to identifi smaller problem .
so if we want to multipli by two nxn matrici we have to identifi multipl of smaller matrici that we can solv recurs .
onc we've figur out how we want to divid the given problem into smaller on , then in the conquer step we simpli invok our own algorithm recurs that's go to recurs multipli the smaller matrici togeth .
and then , in gener , we'll have to combin the result of the recurs call to get the solut for the origin problem , in our case , to get the product of the origin two matrici .
from the product of what ever sub matric we identifi .
so how would be appli the divid and conquer paradigm to matric ?
so we're given two n by n matric , and we have to somehow identifi smaller pair of squar matric that we can multipli recurs .
so the idea , i think , is fairli natur .
so we start with a big n by n matrix x .
and so those n row and n column , we have to somehow divid into smaller piec .
now , the first thing you might think about is that you put it in it left half and it right half and now it goe into what we've been do with the rai , but then we're go to break x into two matric which ar no longer squar which ar n over two in on dimens and have length n in the other dimens and we want to recurs call a subroutin that multipli squar matric .
so what seem like the clear thing to do is to divid x into quadrant .
okai , so we have four piec of x .
each is gonna be n over two by n over two , correspond to the differ quarter of thi matrix .
so let's call these differ quadrant or block , in matrix terminolog , a , b , c , and d .
all of these ar n over two by n over two matric .
as usual , for simplic , i'm assum that n is even , and as usual , it doesn't realli matter .
and we can do the same trick with y .
so we'll divid y into quadrant .
and number two by n number two matric which we'll call e , f , g and h .
so on thing that's cool about matric , is when you split them into block , and you multipli them , the block just behav as if thei were atom element .
so what i mean by that is that the product of x and y can be express in term of it quadrant and each of it four quadrant , each of it four corner can be written as a suitabl arithmet express of the quadrant of x and y .
so here's exactli what those formula ar .
thei ar exactli analog to when we just multipli pair of two by two matric .
so i'm not go to formal prove thi fact .
i'm sure mani of you , have seen it befor or ar familiar with it .
and if you haven't , it's actual quit easi to prove .
it's not obviou , you can't see it off the top of your head , necessarili .
but if you go back to the definit , it's quit easi to verifi .
the d , when you multipl x and y , you can express as quadrant to block , in term of the block of x and y .
so what we just did is complet analog to when talk about integ multipl and you want to multipli two integ , littl x and littl y , and we broke them into pair of n over two digit .
and then we just took the expans and we observ how that expans could be written in term of product of n over two digit number .
same thing go on here except with matric .
so now , we're in busi , as far as a recurs approach .
we wanna multipli x and y .
thei're n by n matric .
we recogn we can express that product x time y , in term of the product of n over two by n over two matric .
thing we're abl to multipli recurs , plu addit .
and addit clearli easi to multipli two differ matric with sai , n squar entri each , it's gonna be linear in the number of entri .
so it's gonna be n squar add two matric that ar n by n .
so thi immedi lead us to our first recurs algorithm .
to describ it , let me quickli rewrit that express we just saw on the previou slide .
and now , our first recurs algorithm is simpli to evalu all of these express in the obviou wai .
so specif , in step on , we recurs comput all of the necessari product , and observ that there ar eight product that we have to comput .
eight product of n over two by n over two matric .
there ar four entri in thi expans of x time y .
each of the , each of the block is the sum of two product , and none of the product re occur , thei're all distinct .
so , naiv , if you wanna evalu thi , we have to eight differ product of n over two by n over two matric .
onc those recurs call complet , then all we do is do the , necessari four addit .
as we discuss , that take time proport to the number of entri in a matrix .
so thi is gonna take quadrat time overal , quadrat n , linear in the number of entri .
now , the question you should be ask is .
is thi a good algorithm ?
wa thi good for anyth , thi recurs approach , split x and y into these block , expand the product in term of these block , the recurs comput each of the block .
and i want to sai it's total not obviou , it is not clear what the run time of thi recurs algorithm is .
i'm go to go ahead and give you a spoiler which is go to follow from the master method that we'll talk about in the next lectur .
but it turn out with thi kind of recurs algorithm where you do eight recurs call , each on a problem with dimens half as much as what you start with , and then do quadrat outsid .
the right time is go to be .
cubic .
so exactli the same as with the straightforward iter algorithm that follow from the definit .
that wa cubic , it turn out , and that wa clearli cubic .
thi on , although it's not obviou , is cubic as well .
so no better , no wors than the straightforward iter algorithm .
so in case you're feel disappoint that we went through all thi work and thi sort of seemingli clever divid and conquer approach for matrix multipl , and , and came out at the end no better than the interact algorithm .
well , there's realli no reason to despair , caus rememb , back in integ multipl , we had a straightforward recurs algorithm where we had to do four recurs call , product of n over two digit number .
but then , we had trick which said , oh , if we onli did more clever product and more clever addit and subtract , then we can get awai with onli three recurs call .
and we'll see later that , that isn't even signific save , in the time multipl .
and we've done noth analog .
dous's trick , it wa matrix multipl problem .
all we did is the naiv expans in term of sub matric , and the naiv evalu of the result express .
so .
<num> , <num> question is then , can we do someth clever to reduc the number of recurs call from eight down to someth lower and that is where strassen's algorithm come in .
so at a high level , strassen's algorithm ha two step , just like the first recurs algorithm that we discuss .
it recurs comput some product of smaller matric and over two by two matric .
but there's onli go to be seven of them .
but thei will be much less straightforward , thei will be much more cleverli chosen than in the first recurs algorithm .
and step two , then , is to actual produc the product of x and y , produc each of those four block that we saw , with suitabl , addit and subtract of these seven product .
and again , these ar much less straightforward than in the first recurs algorithm .
and so while the addit and traction involv will be a littl bit more numer , then thei were in the naiv recurs algorithm .
it's onli gonna chang the work in that part of the algorithm by a constant factor .
so we'll still spend onli theta even squar work ad and subtract thing .
and we get a huge win in decreas the number of recurs call from eight to seven .
now , just in case you have the intuit that shave off on of the recurs call .
should onli decreas the run time of an algorithm by on eighth , by <num> , in fact it ha a tremend more amplifi effect becaus we do on less recurs call over and over and over again as we keep recurs in the algorithm .
so it make a fundament differ in the eventu run time of the algorithm , as we'll explor in detail in the next set of lectur , when we discuss the master method .
so again .
a bit of a spoiler alert .
what you're gonna see in the next set of lectur is inde .
rhythm doe beat it's better than you'll have to watch the next set of lectur just so you know what the run time is .
when right now , that call is what chang the cubic .
now , <num> wa obvious , quit a bit befor my time , but .
by all account , from peopl i've talk to who were around then , and from , you know , what the book sai , strassen's algorithm total blew peopl's mind at the time .
everybodi wa assum that there's no wai you could do better than the iter algorithm , the cubic algorithm .
it just seem that matrix multipl intuit fundament requir all of the calcul that ar spell out in the definit .
so strassen's algorithm is an earli glimps of the magic and of the power .
of clever algorithm design .
that if you realli have a seriou ingenu , even for super fundament problem , you can come up with fundament save over the more straightforward solut .
so those ar the main point i want to talk about strassen's algorithm , how you can beat cubic time by save a recurs call with soon to be chosen clever product and clever addit subtract .
mayb a few of you ar wonder , you know , what ar these cleverli chosen product ?
can you realli do thi ?
and i don't blame you .
there's no reason to believ me , just cuz i sort of spell out thi idea .
it's not obviou thi should work .
you might actual want to see the product .
so , for those of you like that , thi last slide is for you .
so here is strau' algorithm in it's somewhat gori detail .
so let me tell you what the seven product ar that we ar go to form .
i'm go to label them p1 through p7 and thei're all go to be defin in term of the block of the inter matric x and y .
so let me just remind you that we think of x in term of it's block , a b c d .
and we think of y in term of it block e , f , g , h .
and rememb , a through h ar all n over two by n over two sub matrici .
so here ar the seven product , p1 through p7 .
p1 is a time quantiti f minu h .
p2 is quantiti a plu b time h .
p3 is c plu d time e .
p4 is d time g e , p5 is quantiti a d quantiti a h .
p six is quantiti b minu d time quantiti g plu h and final p seven is quantiti a minu c e plu f .
so i hope you'll agre that these ar inde onli seven product , and we could comput these with seven recurs call .
we've preprocess with a littl bit of addit and subtract .
we have to comput f minu h , a plu b , c plu d and so on .
we comput all these new matric from the block , and we can then recurs , with seven recurs call , do these seven product that oper on n over two by n over two matric .
now , the question is , why is thi us ?
why on earth would we wanna know the of these seven product ?
so the amaz other part of the algorithm is that from just these seven product , we can , us onli addit and subtract , recov all four of the block of .
a x time y so x time y .
you'll recal we expand becaus of the block .
so we previous comput thi to be ae bg in the upper left corner , and express for the upper right , lower left , and lower right block .
so thi we alreadi know .
so the content of the claim is that these four block also aris from the seven product in the follow wai .
so the claim here is that these two differ express for x time y ar exactli the same and thei're the same block by block .
so in other word , what the claim is then thi .
crazi express .
p five plu p four minu p two plu p six .
where those were four of the product we list abov .
that is precis a plu b g .
similarli we're , we're claim that p1 plu p2 is exactli af plu bh .
that's actual easi to see .
p3 plu p4 is ce plu dg .
that's also easi to see , and then the other on is that p1 plu p5 minu p3 minu p7 is exactli the same as cf plu dh , so all four of those hold .
so let me just so you believ me cuz i don't know why you would believ me unless i actual show you some of thi deriv .
let's just look at the proof of on of the case of the upper left corner .
so that is , let's just expand out thi crazi express .
p5 p4 p2 p6 , what do we get ?
well , from p5 , we get ae ah d dh , and we add p4 , so that's gonna give us , plu dg minu de , then we subtract p2 , so that it give us a minu , minu dh and then we add in p6 .
so that give us a pg plu bh minu dg minu dh .
okai , so what happen next , well now we look for cancel .
so we cancel the h's .
we cancel the d . e . 's , we cancel the d . h . 's .
we cancel the dg .
we cancel the bh .
and holi cow what do we get , we get a , e .
plu b g .
that is , we get exactli what we were suppos to .
in .
the upper left block of x time y .
so we just actual verifi that thi equat hold for the upper left block .
it's quit easi to see that it hold for the upper right and lower left block and a compar calcul verifi it for the lower right block of the two .
so summar , becaus thi claim hold .
becaus , actual , we can recov the four block of s time y from the seven product .
strauss' algorithm work in the follow wai you comput the seven product , p1 through p7 , us seven recurs call .
then you just comput the four block us some extra addit and subtract as shown in the claim .
so seven recurs call on a number two by number two matric , plu .
n squar work to do the necessari addit as we'll see on the master method lectur that is actual suffici for .
sub humid time .
now i sympath with you if you have the follow question .
which is how on earth did strauson come up with thi ?
and inde , thi sort of illustr , the differ between check somebodi's proof , and come up with a proof .
given that i told you the magic seven product and how you , from them , can recov the four desir block of x time y , it's realli just mechan to see that it work .
it's a total differ stori of how you come up with p1 through p7 in the first place .
so how did strassen come up with them ?
honestli , your guess is as good as mine .
so in thi video and the next , we're go to studi a veri cool divid and conquer algorithm for the closest pair problem .
thi is a problem where you're given n point in the plane and you want to figur out which pair of point ar closest to each other .
so thi would be the first tast we get of an applic in comput geometri , which is the part of algorithm which studi how to reason and manipul geometr object .
so those algorithm ar import in , among other area robot , comput vision and comput graphic .
so thi is rel advanc materi , it's a bit more difficult than the other applic of divid and conquer that we've seen .
the algorithm's a littl bit tricki and it ha a quit nontrivi proof of correct , so just be readi for that and also be warn that becaus it's more advanc i'm go to talk about the materi in at a slightli faster pace tha i do in most of the other video .
so let's begin now by defin the problem formal , so we're given as imput endpoint in the plane , so each on just defin by it x coordin and ist y coordin .
and when we talk about the distanc between two point in thi problem , we're go to focu on euclidean distanc .
so , let me remind you what that is briefli , but we're go to introduc some simpl notat for that , which we'll us for the rest of the lectur .
so we're just go to note the euclidean distanc between two point , pi  and pj , by d of pi  pj .
so in term of the x and y coordin of these two point , we just look at the squar differ in each coordin , sum them up , and take the squar root .
and now as the name of the problem would suggest , the goal is to identifi among all pair of point that pair which ha the smallest distanc between them .
next , let's start get a feel for the problem by make some preliminari observ .
first , i want to make an assumpt pure for conveni that there's no ti .
so that is i'm go to assum all endpoint have distinct x coordinat es , and also all endpoint have distinct y coordin .
it's not difficult to extend the algorithm to accommod ti .
i'll leav it to you to think about how to do that .
so next , let's draw some parallel with the problem of count invers , which wa a earlier applic of divid and conquer that we saw .
the first parallel i want , want to out is that , if we're comfort with the quadrat time algorithm , then thi is not a hard problem , we can simpli solv thi by brute forc search .
and again , by brute forc search , i just mean we set up a doubl for loop , which iter over all distinct pair of point .
we comput the distanc for each such pair and we rememb the smallest .
that's clearli a correct algorithm , it ha to iter over a quadrat number of pair , so it run time is go to be theta of n squar .
and , as alwai , the question is can we appli some algorithm ingenu to do better ?
can we have a better algorithm than thi naiv on which iter over all pair of point ?
you might have a , an initi instinct that becaus the problem ask about a quadrat number of differ object , perhap we fundament need to do quadrat work .
but again , recal back in count invers , us divid and conquer , we were abl to get an n log n algorithm despit the fact that there might be as mani as a quadrat number of invers in an arrai .
so the question is , can we do someth similar here for the closest pair problem ?
now , on of the kei to get an n log n time algorithm for count invers wa to leverag a sort subroutin .
recal that we piggyback on merg sort to count the number of invers in n log n time .
so the question is , here , with the closest pair problem , perhap , sort again can be us in some wai to beat the quadrat barrier .
so , to develop some evid that sort will inde help us comput the closest pair of point embed in quadrat time , let's look at a special case of the problem , realli , an easier version of t he problem , which is when the point ar just in on dimens , so on the line rather that in two dimens in the plane .
so in the 1d version , all the point just lie on a line like thi on , and we're given the point in some arbitrari order not necessarili in sort order .
so , a wai to solv the closest pair problem in on dimens , is to simpli sort the point , and then of cours , the closest pair better be adjac in thi order , so you just iter through the n minu <num> consecut pair and see which on is closest to each other so , more formal , here's how you solv the on dimension version of the problem .
you sort the point accord to their onli coordin , becaus you're go to rememb , thi is on dimens .
so as we've seen , us merg sort , we can sort the point in n log n time and then we just do a scan through the point , so thi take linear time .
and for each consecut pair , we comput their distanc and we rememb the smallest of those consecut pair and we return that .
that's gotta be the closest pair .
so , in thi pictur here on the right , i'm just go to circl here in green the closest pair of point .
so thi is someth we discov by sort and then do a linear scan .
now , needless to sai , thi isn't directli us , thi is not the problem i start out with .
we want to find out the closest pair among of point in the plane not point in the line .
but , i want to point out that , thi , even in the line , there ar a quadrat number of differ pair , so brute forc search is still a quadrat time algorythm even in the 1d case .
so at least , with on dimens , we can us sort , piggyback on it , to beat the naiv brute forc search bound and solv the problem in n log n time .
so our goal for thi lectur is go to be to devis an equal good algorithm for the two dimension case , so we want to solv closest pair of point in the plane , again , in n log n , n time .
so we will succe in thi goal .
i'm go to show you an n log n time algo rithm for 2d closest pair .
it's go to take us a coupl step .
so let me begin with a high level approach .
all right .
so the first i need to try is just to copi what work for us in the on dimension case .
so the on dimension case , we first sort the point by their coordin and that wa realli us .
now , in the 2d case , point have two coordin , x coordin and y coordin , so there's two wai to sort them .
so let's just sort them both wai , that is , the first step of our algorithm , which you should realli think of as a preprocess step .
we're go to take the input point .
we invok merg sort onc to sort them accord to x coordin , that's on copi of the point .
and then we make a second copi of the point where thei're sort by y coordin .
so we're go to call those copi of point px , that's an arrai of the point sort by x coordin , and py for them sort by y coordin .
now , we know merg short take n log n time , so thi preprocess step onli take o of n log n time .
and again , given that we're shoot for an algorithm with run time big o of n log n , why not sort the point ?
we don't even know how we're go to us thi fact right now , but it's sort of harmless .
it's not go to effect our goal of get a big of o n log n time algorithm .
and inde , thi illustr a broader point , which is on of the theme of thi cours .
so recal , i hope on of the thing you take awai from thi cours is a sens for what ar the four free primit , what ar manipul or oper you can do on data which basic ar costless .
mean that if your data set fit in the main memori of your comput , you can basic invok the primit and it's just go to run blazingli fast , and you can just do it even if you don't know why .
and again , sort is the canon for free primit , although , we'll see some more later in the cours and so , here , we're us exactli that principl .
so we don't even understand why yet we might wa nt the point to be sort .
it just seem like it's probabl go to be us , motiv by the 1d case , so let's go ahead and make assort copi of the point by x and y coordin upfront .
so reason by analog with the 1d suggest that sort the point might be us , but we can't carri thi analog too far .
so in particular , we're not go to be abl to get awai with just a simpl linear scan through these arrai to identifi the closest pair of point .
so , to see that , consid the follow exampl .
so we're go to look at a point set which ha six point .
there's go to be two point , which i'll put in blue which ar veri close in x coordin , but veri far awai in y coordin .
and then there's go to be anoth pair of point which i'll do in green , which ar veri close in y coordin , but veri far awai in x coordin .
and then there's go to be a red pair of point , which ar not too far awai in either the x coordin or the y coordin .
so in thi set of six point , the closest pair is the pair of red point .
thei're not even go to show up consecut on either of the two arrai , right ?
so in the arrai that's sort by x coordin , thi blue point here is go to be wedg in between the two red point , thei won't be consecut .
and similarli in the , in py , which is sort of by y coordin , thi green coordin is go to be wedg between the two red point .
so you won't even notic these red point if you just do a linear scan if your px and py , or py look at the consecut pair of point .
so , follow our preprocess step where we just invert , invok merg sort twice we're go to do a quit nontrivi divid and conquer algorithm to comput the closest pair .
so realli , in thi algorithm , we're appli the divid and conquer algorithm twice .
first , intern to the sort subroutin , assum that we us the merg sort algorithm to sort .
divid and conquer is be us there to get an n log n run time in thi preprocess step , and the n , we're go to us it again on sort arrai in a new wai and that's what i'm go to tell you about next .
so let's just briefli review the divid and conquer algorithm design paradigm befor we appli it to the closest pair problem .
so , as usual , the first step is to figur out a wai to divid your problem into smaller subproblem .
sometim thi ha a reason amount of ingenu , but it's not go to .
here in the closest pair problem , we're go to proce exactli as we did in the merg sort and count invers problem , where we took the arrai and broke it into it left and right half .
so here , we're go to take the input point set , and again , just recurs on the left half of the point , and recurs on the right half of the point .
so here , by left and right , i mean with respect to the point x coordin .
there's pretti much never ani ingenu in the conquer step , that just mean you take the sub problem you identifi in the first step , and you solv them recurs .
that's what we'll do here , we'll recurs complet the closest pair in the left half of the point , and the closest pair in the right half of the point .
so where all the creativ in divid and conquer algorithm is in the combin step .
given the solut to your sub problem , how do you somehow recov a solut to the origin problem ?
the on that you actual care about .
so for closest pair , the questioni go to be , given that you've comput the closest pair on the left half of the point , and the closest pair on the right half of the point , how do you then quickli recov the closest pair for the whole point set ?
that's a tricki problem , that's what we're go to spend most of our time on .
so let's make thi divid and conquer approach for closest pair a littl bit more precis , so let's now actual start spell out our closest pair algorithm .
the input we're given , it's , thi follow the preprocess step or recal that we invok , merg sort , we get our two sort copi of the poin t set px , sort by x coordin , and py sort by y coordin .
so the first dividend is the divis step .
so given that we have a copi of the point px sort by x coordin , it's easi to identifi the leftmost half of the point , those with the , those n over two smallest x coordin and in the right half , those were the n over two largest x coordin .
we're go to call those q and r respect .
on thing i'm skip over is the base case .
i'm not go to bother write that down , so base case omit , but it's what you would think it would be .
so basic onc you have a small number point , sai two point or three point , then you can just solv the problem in constant time by a brute forc search .
you just look at all the pair and you return the closest pair .
so think of it be at least four point in the input .
now , in order to recurs , to call clo pair again , in the left and right halv , we need sort version of q and r , both by x coordin and by y coordin , so we're just go to form those by do suitabl linear scan through px and py .
and so on thing i encourag you to think through carefulli or mayb even code up after the video is how would you form qx , qy , rx and ry given that you alreadi have px and py .
and if you think about it , becaus px and py ar alreadi sort just produc these sort sublist take linear time .
it's in some sens the opposit of the merg subroutin us in merg sort .
here , we're sort of split rather than merg .
but again , thi can be done in linear time , that's someth you should think through carefulli later .
so that's the divis step , now we just conquer , mean we recurs call closest pair line on each of the two subproblem , so when we invok closest pair on the left half of the point on q we're go to get back what ar inde , the closest pair of point amongst those in q .
so we're go to call those p1 and pq , so among all pair of point that both lie in q , p1 and q1 minim the distanc between them .
similarli , we're go to call q2q2 the result of the second recurs call , that is , p2 and q2 ar amongst all pair of point that both lie in r , the pair that ha the minimum euclidean distanc .
now , conceptu , there's two case .
there's a lucki case and there's an unlucki case .
in the origin point set p , if we're lucki , the closest pair of point in all of p , actual , both of them lie in q or both of them lie in r .
in thi lucki case , we'd alreadi be done if the closest pair in the entir point set thei happen to both lie in q , then thi first recurs call is go to recov them and we just have them in our hand p1q1 .
similarli , if both of the closest pair of point in all of p li on the right side in r , then thei get hand to us on a silver platter by the second recurs call that just oper on r .
so in the unlucki case , the closest pair of point in p happen to be split .
that is , on of the point li in the left half , in q , and the other point li in the right half , in r .
notic , if the closest pair of point in all of p is split , is half in q and half in r , neither recurs call is go to find it .
okai ?
the pair of point is not pass to either of the two recurs call , so there's no wai it's go to be return to us .
okai ?
so we have not identifi the closest pair after these two recurs call , if the closest pair happen to be split .
thi is exactli analag to what happen when we were count invers .
the recurs call on the left half of the arrai count the left invers .
the recurs call on the right half of the arrai count the right invers .
but we still had to count the split invers , so in thi closest pair algorithm , we still need a special purpos subroutin that comput the closest pair for the case in which it is split , in which there is on point in q and on point in r .
so just like in count invers , i'm go to write down that subroutin and i'm go to leav it unimpl for now , we'll figur e out how to implement it quickli in the rest of the lectur .
now , if we have a correct implement of closest split pair , so that take us input the origin point set sort of the x and y coordin , and return the smallest pair that's split or on point in q and on point in r , then we're done .
so then , the split , then the closest pair ha to either be on the lef or onm the right or it ha to be split .
step two through four comput the closest pair in each of those categori , so those ar the onli possibl candid for the closest pair and we just return the best of them .
so that's an argument for y , if we have a correct implement of the closest split para subroutin , then that impli a correct implement of closest pair .
now , what about the run time ?
so the run time of the closest para algorithm is go to be in part determin by the run time of closest split pair .
so in the next quiz , i want you to think about what kind of run time we should be shoot for with a closest split pair subroutin .
so the correct respons of thi quiz is the second on , and the reason is just by analog with our previou algorithm for merg sort and for count invers .
so , what is all of the work that we would do in thi algorithm or we do have thi preprocess step we call merg sort twice , we know that's n log n , so we're not go to have a run time better than n log n caus we sort at the begin .
and then , we have a recurs algorithm with the follow flavor , it make two recurs call .
each recurs call is on a problem of exactli half the size with half the point of the origin on .
and outsid of the recurs call , by assumpt , by , in the problem , we do a linear amount of work in comput the closest split pair .
so we , the exact same recurs tree which prove an n log n bound for merg sort , prove an n log n bound for how much work we do after the preprocess step , so that give us an overal run time bound of n log n .
remem ber , that's what we were shoot for .
we were work n log n alreadi to solv the on dimension version of closest pair and the goal of these lectur is to have an n log n algorithm for the 2d version .
so thi would be great .
so in other word , the goal should be to have a correct linear time implement of the closest split pair subroutin .
if we can do that , we're home free , we get the desir n log algorithm .
now , i'm go to proce in a littl bit to show you how to implement closest split pair , but befor i do that , i want to point out on subtl , the kei idea , which is go to allow us to get thi linear time correct implement .
so , let me just put that on the slide .
so , the kei idea is that we don't actual need a full blown correct implement of the closet split pair subroutin .
so , i'm not actual go to show you a linear time subroutin that alwai correctli comput the closet split pair of a point set .
the reason i'm go to do that is that's actual a strictli harder problem than what we need to have a correct recurs algorithm .
we do not actual need a subroutin that , for everi point set , alwai correctli comput the closest split pair of point .
rememb , there's a lucki case and there's an unlucki case .
the lucki case is where the closest pair in the whole point set p happen to lie entir in the left half of the point q or in the right half of the point r in that lucki case , we , on of our recurs call will identifi thi closest pair and hand it over to us on a silver platter .
we could care less about the split pair in that case .
we get the right answer without even look at the split pair , pair .
now , there's thi unlucki case where the split pair happen to be the closest pair of point .
that is when we need thi linear time subroutin , and onli .
then , onli in the unlucki case where the closest pair of point happen to be split .
now , that's in some sens , a fairli trivial observ , but , there's a lot of ingenu here i n figur out how to us that observ .
the fact that we onli need to solv a strictli easier problem and that will enabl the linear time implement that i'm go to show you next .
so now , let's rewrit the high level recurs algorithm slightli to make us of thi observ that the closest split pair subroutin onli ha to oper correctli in the regim of the unlucki case , when in fact , the closest split pair is closer than the result of either recurs call .
so i've eras the previou step <num> and <num> , that , but we're go to rewrit them in a second .
so , befor we invok close split pair , what we're go to do is we're go to see how well did our recurs call do .
that is , we're go to defin a paramet littl delta , which is go to be the closest pair that we found or the distanc of the closest pair we found by either recurs call .
so the minimum of the distanc between p1 and q1 , the closest pair that li entir on the left , and p2q2 , the closest pair that li entir on the right .
now , we're go to pass thi delta inform as a paramet into our closest split pair subroutin .
we're go to have to see why on earth that would be us and i still ow you that inform , but , for now , we're just go to pass delta as a paramet for us in the closest split pair .
and then , as befor we just do a comparison between the three candid closest pair and return the best of the , of the trio .
and so , just so we're all clear on , on where thing stand , so what remain is to describ the implement of closest split pair , and befor i describ it , let me just be crystal clear on what it is that we're go to demand of the subroutin .
what do we need to have a correct in o of n log n time closest pair algorithm .
well , as you saw on the quiz , we want the run time to be o of n alwai , and for correct , what do we need ?
again , we don't need it to alwai comput the closest split pair , but we need it to comput the closest split pair in the event that there is a split pair of distanc strictli less than delta , strictli better than the outcom of either recurs call .
so now that we're clear on what we want , let's go ahead and go through the pseudocod for thi closest split pair subroutin .
and i'm go to tell you upfront , iit's go to be fairli straightforward to figur out that the subroutin run in linear time , o of n time .
the correct requir of closest split pair will be highli non obviou .
in fact , after i show you thi pseudo you're not go to believ me .
you're go to look at the pseudocod and you'd be like , what ar you talk about ?
but in the second video , on the closest pair lectur , we will in fact show that thi is a correct sub routin .
so , how doe it work ?
well , let's look at a point set .
so , the first thing we're go to do is a filter step .
we're go to prune a bunch of the point awai and so to zoom in on a subset of the point .
and the subset of the point we're go to look at is those that lie in a vertic strip , which is roughli center in the middl of the point set .
so , here's what i mean .
by center dot , we're go to look at the middl x coordin .
so , let x bar be the biggest x coordin in the left half , so that is in the sort version of the point by x coordin , we look at the n over two smallest ex coordin .
so , in thi exampl where we have six point , all thi mean is we draw , we imagin draw a line between the third point , so that's go to be x bar , the x coordin of the third point from the left .
now , sinc we're pass as input , a copi of the point sort by x coordin , we can figur out what x bar is in constant time .
just by access the relev entri of the arrai , px .
now , the wai we're go to us thi paramet delta that we're pass , so rememb what delta is .
so befor we invok the closest split pair subroutin in the recurs algorithm , we make our two recurs call , we find the closest pair on the left , the closest pair on the right , and delta is whatev the smaller of those two distanc ar .
so delta is the paramet that control whether or not we actual care about the closest split pair or not , we care if and onli if there is a split pair at distanc less than delta .
so , how do we us delta ?
well , that's go to determin the width of our strip , so the strip's go to have width <num> delta , and it's go to be center around x .
and the first thing we're go to do is we're go to ignor , forevermor , point which do not line in thi vertic strip .
so the rest of the algorithm will oper onli on the subset of p , the subset of the point that lie on the strip , and we're go to keep track of them sort by y coordin .
so the formal wai to sai that thei line the strip , is that thei have x coordin in the interv with lower endpoint x bar minu delta and upper endpoint x bar plu delta .
now , how long doe it take to construct thi set sy sort by y coordin ?
well fortun , we've been pass as input a sort version of the point py so to extract sy from py , all we need to do is a simpl linear scan through p y check for each point where it x coordin is .
so thi can be done in linear time .
now , i haven't yet shown you why it's us to have thi sort set as y , but if you take it on faith that it's us to have the point in thi vertic strip sort by y coordin .
you now see why it wa us that we did thi merg sort all the wai at the begin of the algorithm befor we even underw ani recurss .
rememb , what is our run time goal for closest split pair ?
we want thi to run in linear time , that mean we cannot sort insid the closest split pair subroutin .
that would take too long .
we want thi to be in linear time .
fortun , sinc we sort onc and for all at the begin of the closest pair algorithm , extract sort sublist from those sort list of point can be done , done in linear time , which is within our goal here .
now , it's the rest of t he subroutin where you're never go to believ me that it doe anyth us .
so , i claim that essenti with a linear scan through sy , we're go to be abl to identifi the closest split pair of point in the interest , unlucki case where there is such a split pair with distanc less than delta .
so here's what i mean by that linear scan through sy .
so as we do the scan , we're , we're go to keep track of the closest pair of point of a particular type that we've seen so far .
so , let me introduc some variabl to keep track of the best candid we've seen so far .
there's go to be a vari , variabl best which will initi to be delta .
rememb , we're uninterest in split pair unless thei have distanc strictli less than delta .
so , and then we're go to keep track of the point themselv , so we'll initi the best pair to be null .
now , here is the linear scan .
so we go through the point of sy in order y coordin .
okai , well , not quit all the point of sy .
we stop at the eighth to last point and you'll see why in a second .
and then , for each posit i of the arrai sy , we investig the seven subsequ point of the same arrai sy .
so for j go from on to seven , we look at the ith , and i plu jth entri of sy .
so if sy look someth like thi arrai here , in ani given point in thi doubl for loop , we're gener look at an index i , a point in thi , in thi of the arrai , and then some realli quit nearbi point in the arrai i plu j , becaus j here's go to be at most seven .
okai ?
so we're constantli look at pair in thi arrai , but we're not look at all pair of all .
we're onli look at pair that ar veri close to each other , within seven posit of each other .
and what do we do for each choic of i and j ?
well , we just look at those point , we comput the distanc , we see if it's better than all of the pair of point of thi form that we've look at in the past and if it is better , then we rememb it .
so we just rememb the best , ie c losest pair of point , of thi particular type for choic of i and j of thi form .
so in more detail , if the distanc between the current pair of point of p and q is better than the best we've seen so far , we reset the best pair of point to be equal to p and q , and we reset the best distanc , the closest distanc seem so far to be the distanc between p and q and that's it .
then , onc thi doubl for loop termin , we just return it the best pair .
so on possibl execut of closest split pair is that it never find a pair of point , p and q , at distanc less than delta .
in that case , thi is go to return null and then in the outer call .
in the closet pair , obvious , you interpret a null pair of point to have an infinit distanc .
so if you call closest split pair , and it doesn't return ani point , then the interpret is that there's no interest split pair of point and you just return the better of the result of the two recurs call p1q1 or p2q2 .
now , as far as the run time of the subroutin , what happen here ?
well , we do constant work just initi the variabl .
then notic that the number of point in sy , well in the worst case , you have all of the point of p .
so , it's go to be the most endpoint , and so , you do a linear number of iter in the outer for loop .
but here is the kei point , in the inner for loop , right , normal doubl for loop give rise to quadrat run time , but in thi inner for loop we onli look at a constant number of other posit .
we onli look at seven other posit and for each of those seven posit , we onli do a constant number of work .
right ?
we just , we want to compar distanc and make a coupl other comparison , and reset some variabl .
so for each of the linear number of outer iter , we do a constant amount of work , so that give us a run time of o of n for thi part of the algorithm .
so as i promis , analyz the run time of thi closest split pair subroutin wa not challeng .
we just , in a straightforward wai , look at all the oper .
again , becaus in the kei linear scan , we onli do constant work per index , the overal run time is big o of n , just as we want .
so thi doe mean that our overal recurs algorithm will have run time o of n log n .
what is total not obviou and perhap even unbeliev , is that thi subroutin satifi the correct requir that we want .
rememb , what we need , we need that whenev we're in the unlucki case , whenev , in fact , the closest pair of point in the whole point set is split , thi subroutin better find it .
so , but it doe , and that's be precis in the follow correct claim .
so let me rephras the claim in term of an arbitrari split pair , which ha distanc less than delta , not necessarili the closest such pair .
so suppos , there exist , a p on the left , a point on the left side and a point on the right side so that is a split pair and suppos the distanc of thi pair is less than q .
now , there mai or mai not be such a pair of point , pq . .
don't forget what thi paramet delta mean .
what delta is , by definit , is the minimum of d of p1q1 , for p1q1 is the closest pair of point that lie entir in the left half of the point set q and d of p2q2 , or similarli , p2q2 is the closest pair of point that entir on the right insid of r .
so , if there's a split pair with distanc less than delta , thi is exactli the unlucki case of the algorithm .
thi is exactli where neither recurs call successfulli identifi the closest pair of point , instead that closest pair is a split pair .
on the other hand , if we ar in the lucki case , then there will not be ani split pair with distanc less than delta , becaus the closest pair li either all on the left or on the right , and it's not split .
but rememb , we're interest in the case where there is a split pair that ha a distanc less than delta where there is a split pair that is the closest pair .
so the claim ha two part .
the first part , part a , sai the follow .
it sai that if there's a split pair p and , and q of thi type , then p and q ar member of sy .
and let me just sort of redraw the cartoon .
so rememb what sy is .
sy is that vertic strip .
and again , the wai we got that is we drew a line through a median x coordin and then we fatten it by delta on either side , and then , we focus onli on point that lie in the vertic strip .
now , notic our count split pair subroutin , if it ever return a pair of point , it's go to return a pair of point pq that belong to sy .
first , it filter down to sy , then it doe a linear search through sy .
so if we want to believ that our subroutin identifi best split pair of point , then , in particular , such split pair of point better show up in sy , thei better surviv the filter step .
so that's precis what part a of the claim is .
here's part b of the claim and thi is the more remark part of the claim , which is that p and q ar almost next to each other in thi sort arrai , sy .
so thei're not necessarili adjac , but thei're veri close , thei're within seven posit awai from each other .
so , thi is realli the remark part of the algorithm .
thi is realli what's surpris and what make the whole algorithm work .
so , just to make sure that we're all clear on everyth , let's show that if we prove thi claim , then we're done , then we have a correct fast implement of a closest pair algorithm .
i certainli ow you the proof of the claim , that's what the next video is go to be all about , but let's show that if the claim is true , then , we're home free .
so if thi claim is true , then so is the follow corollari , which i'll call corollaryl <num> .
so corollari <num> sai , if we're in the unlucki case that we discuss earlier , if we're in the case where the closest point and the whole point of p doe not lie both on the left , doe not lie both on the right , but rather ha on point on the left and on on the right but as it's a split pair , th en in fact , the count split pair subroutin will correctli identifi the closest split pair and therefor the closest pair overal .
why is thi true ?
well what doe count split pair do ?
okai , so it ha thi doubl for loop , and therebi , explicitli examin a bunch of pair of point and it rememb the closest pair of all of the pair of point that it examin .
what doe thi , so what ar the criteria that ar necessari for count split pair to examin a pair point ?
well , first of all , the point p and q both have to surviv the filter step and make it into the arrai sy .
right ?
so count split pair onli search over the arrai sy .
secondli , it onli search over pair of point that ar almost adjac in sy , that ar onli seven posit apart , but amongst pair of point that satisfi those two criteria , count but pair will certainli comput the closest such pair , right ?
it just explicitli rememb the best of them .
now , what's the content of the claim ?
well , the claim is guarante that everi potenti interest split pair of point and everi split pair of point with distanc less than delta meet both of the criteria which ar necessari to be examin by the count split pair subroutin .
so first of all , and thi is the content of part a , if you have an interest split pair of point with distanc less than delta , then thei'll both surviv the filter step .
thei'll both make it into the arrai sy . , part a sai that .
part b sai thei're almost adjac in sy .
so if you have an interest split pair of point , mean it ha distanc less than delta , then thei will , in fact , be at most seven posit apart .
therefor , count split pair will examin all such split pair , all split pair with distanc less than delta , and just by construct , it will comput the closest pair of all of them .
so again , in the unlucki case where the best pair of point is a split pair , then thi claim guarante that the count split pair will comput the closest pair of point .
therefor , have h andl correct , we can just combin that with our earlier observ about run time and corollari <num> just sai , if we can prove the claim , then we have everyth we want .
we have a correct o of n log n implement for the closest pair of point .
so with further work and a lot more ingenu , we've replic the guarante that we got just by sort for the on dimension case .
now again , these corrollari hold onli if thi claim is , in fact , true and i have given you no justif for thi claim .
and even the statement of the claim , i think , is a littl bit shock .
so if i were you i would demand an explan for why thi claim is true , and that's what i'm go to give you in the next video .
all right .
so the plan for thi video is to prove the correct of the divid and conquer closest to pair algorithm that we discuss in the previou video .
so just to refresh your memori , how doe the outer algorithm work ?
well , we're given endpoint in the plane .
we begin by sort them , first by x coordin and then by y coordin .
that take n log in time .
then we enter the main recurs divid and conquer part of the algorithm .
so what do we do ?
we divid the point set into the left half and the right half , q and r , then we conquer .
we recurs comput the closest pair in the left half of the point set q .
we recurs comput the closest pair in the right half of the point set r .
there is a lucki case where the closest pair on the entir point set li either all on the left or all on the right .
in that case , the closest pair is hand to us on a silver platter , by on of the two recurs call .
but there remain the unlucki case where the closest pair is actual split with on point on the left and on point on the right .
so to get our n log n run time bound , analog to merg short in our invers count , we need to have a linear time implement of a subroutin which comput the best , the closest pair of point , which is split , on on the left and on on the right .
well , actual , we don't need to do quit that .
we need to do someth onli a littl bit weaker .
we need a linear time algorithm , which whenev the closest pair in the whole point set is in fact split , then comput that split pair in linear time .
so let me now remind you of how that subroutin work .
so it ha two basic step .
so first , there's a filter step .
so it look at , first of all , a vertic strip , roughli down the middl of the point set .
and it look at , onli at point which fall into that vertic strip .
that wa a subset of the point that we call s sub y , 'caus we keep track of them sort by y coordin .
and then we do essenti a linear scan through sy .
so we go through the point on at a time , and then , for each point , we look at onli the almost adjac point .
so for each index i , we look onli at j's that ar between on and seven posit further to the right , than i .
so among all such point , we compar them , we look at their distanc .
we rememb the best such pair of point .
and then that's what we return from the count split pair subroutin .
so we've alreadi argu , in the previou video , that the overal run time of the algorithm is n log n .
and what remain to prove correct .
and we also argu , in the previou video , that correct boil down to the follow correct claim .
in the sens that , if we can prove thi claim , then the entir algorithm is correct .
so thi is what remain .
our residu work is to provid a proof of the correct claim .
what doe it sai ?
it sai consid ani split pair that is on point p from the left side q , capit q , and anoth point littl q drawn from the right side of the point set capit r .
and fur , further suppos that it's an interest split pair mean that the distanc between them's at most delta .
here delta is recal the paramet pass to the count split pair subroutin , which is the smallest distanc between a pair of point all on the left or all on the right .
and thi is the onli case we're interest in .
there's two claim .
first of all , for p and q , both member of the split pair surviv the filter step .
thei make it into the sort list s sub y , and second of all , thei will be consid by that doubl for loop , in the sens that the posit of p and q in thi arrai , s sub y , differ by at most seven .
so that's the stori so far .
let's move on to the proof .
so let's start with part a which is the easi part rel of the claim .
so rememb what we start with , our assumpt .
we have a point p , let's write it out in term of the x coordin , x1 and y1 , which is from the left half of the point set .
and we have a point q , which we'll call x2y2 , which come from the right half of the point set .
and furthermor , we're assum that these point ar close to each other .
and we're gonna us that hypothesi over and over again .
so the euclidean distanc between p and q is no more than thi paramet delta .
so , first , someth veri simpl , which is that if you have two point that ar close in euclidean distanc , then both of their coordin have to be close to each other , right ?
if you have two point , and thei differ by a lot in on coordin , then the euclidean distanc is gonna be pretti big as well .
so , specif .
by our hypothesi , that p and q have euclidean distanc less than delta , it must be that the differ between their coordin in absolut valu is no more than delta , and as well , the differ between their y coordin is at most delta .
okai , and thi is easi to see if you'd just return to the definit of euclidean distanc that we review at the begin of the discuss of closest point .
okai ?
so if your distanc is at most delta , then in each coordin , you differ by at most delta as well .
now , what doe a sai ?
so proof of a .
so what doe part a of the claim assert ?
it assert that p and q ar both member of sy , ar both member of that vertic strip .
so anoth wai of sai that is that the x coordin of p and q , that is , the number x1 and x2 both ar within delta of xbar .
rememb , xbar wa in some sens the median x coordin .
so the x coordin of the n over two'th leftmost point .
so we're gonna do a proof by pictur , so consid , forget about the y coordin , that's of irrelev right now , and just focu on the x coordin of all of these point .
so on the on hand we have x bar .
thi is the x coordin of the n over two'th point to the left .
and then there ar the x coordin which defin the left and the right border of that vertic strip .
name xbar delta and xbar delta .
and then somewher in here ar x1 and y1 , the x coordin of the point we care about , p and q .
so a simpl observ , so becaus p come from the left half of the point set , and xbar is the rightmost x coordin of the left half of the point set , the x coordin is at most xbar .
right ?
so all point of q have x coordin , at most , xbar , in particular , p doe .
similarli , sinc xbar is the rightmost edg of the left half of the point set , everyth in the right half of the point set ha x coordin , at least xbar .
so in particular , littl q doe as well .
so what doe thi mean .
so thi mean x1 , wherev it is , ha to be at the left of x bar .
x2 wherev it is ha to be to the right of x bar .
what we're try to prove is that thei're wedg in between x bar minu delta and x bar plu delta .
and the reason why that's true is becaus their x coordin also differ by at most delta .
okai , so what you should imagin is .
you can imagin x1 and x2 ar sort of peopl ti by a rope at the waist .
and thi rope ha length delta .
so wherev x1 and x2 move , thei're at most delta apart .
furthermor x1 , we just observ , can't move ani farther to the right than xbar .
so even if x1 move as far to the right as it can , all the wai to xbar , x2 , sinc it's at most delta awai , ti by the waist , can't extend beyond x bar delta .
by the same reason , x2 can't move ani further to the left than xbar , x1 be ti to the waist to x2 , can never drift further to the left than xbar minu delta .
so that's the proof that x1 and x2 both lie within thi region , and that defin the vertic strip .
so that's part a .
if you have ani split pair whose distanc between them is less than delta , thei both have to wind up , in thi vertic strip .
and therefor wind up in the filter set , the proof set , s sub y .
so that's part a of the claim .
let's now move to part b .
recal what part b assert .
it sai that the point p and q , thi split pair that ar distanc onli delta apart .
not onli do thei wind up in thi sort of filter set sy , but in fact , thei ar almost adjac in sy , in the sens that the indic in the arrai differ by , at most , seven posit .
and thi is a part of the claim that is a littl bit shock .
realli what thi sai is that we're get awai with more or less a variant of our on dimension algorithm .
rememb when we want to find the closest pair of point on the line , all we had to do wa sort them by their singl coordin and then look at consecut pair and return the best of those consecut pair .
here what we're sai is realli , onc we do a suitabl filter focu on point in thi vertic strip , then we just go through the point accord to their y coordin .
and okai , we don't just look at adjac pair .
we look at pair within seven posit , but still we basic do a linear sweep through the point in sy .
accord to their y coordin and that's suffici to identifi the closest split pair .
so why on earth will thi be true .
so our workhors in thi argument will be a pictur which i am go to draw on next .
so i'm go to draw eight box , which have a height and width delta over two .
so here , delta is the same paramet that get pass to the closest split pair subroutin .
and it's also the same delta which we're assum p and q ar closer to each other than , right ?
so that's , rememb , that's on of our hypothes in thi claim .
the distanc between p and q is strictli less than delta .
so we're gonna draw eight delta over two box .
and thei're gonna be center at x bar .
so , thi same center of the vertic strip that defin s y .
and the bottom is go to be the smaller of the y coordin of the point p and q .
so it might be p , it might be q .
it doesn't realli matter .
but the bottom is go to be the smaller of the two .
so the pictur then look as follow .
so the center of these collect of eight box , x bar , the bottom is the minimum of y1 , y2 .
we're gonna have two row and four column .
and needless to sai , we're draw thi pictur just for the sake of thi correct proof , right ?
thi pictur is just a thought experi in our head .
we're just try to understand why the algorithm work .
the algorithm , of cours , doe not draw these box .
the subroutin , the , closest split pair subroutin is just that pseudo code we saw in the previou video .
thi is just to reason about the behavior of that subroutin .
now look ahead , i'll make thi precis in two lemma that ar about to come up , what's go to be true is the follow .
so , either p or q is on thi bottom line , right ?
so we defin the bottom to be the lower y coordin of the two .
so mayb , for exampl , q is the on that ha the smaller y coordin , in which case , is gonna be , somewher , sai , down here .
p , you rememb , is from the left half of the point set .
so p is mayb gonna be here or someth .
and we're gonna argu that both p and q have to be in these box .
moreov , we're gonna argu that these box ar spars popul .
everi on contain either zero or on point of the arrai s sub y .
so , what we're gonna see is that there's at most eight point in thi pictur , two of which ar p and q , and therefor , if you look at these point sort by y coordin , it ha to be that thei're within seven of each other , the differ of indic is no more than seven .
so , we're gonna make those two statement precis on at a time by the follow two lemma .
let's start with lemma on .
lemma on is the easi on .
and it state that all of the point of s sub y , which show up in between the y coordin of the point we care about p and q have to appear in thi pictur , thei have to lie in on of these eight box .
so we're go to argu thi in two step .
first , we're go to argu that all such point have to have y coordin within the relev rang of thi pictur between the minimum of y1 and y2 and delta more than that , and secondli that thei have to have x coordin in the rang of thi pictur , name between x bar minu delta and x bar plu delta .
so let's start with y coordin .
so again , rememb thi kei hypothesi we have , okai .
we're deal with a split pair p q that ar close to each other .
the distanc between x and y is strictli less than delta .
so the veri first thing we did at the begin of thi proof is we said well , if their euclidean distanc is less than delta then thei have to differ by at most delta in both of their coordin , in particular in their y coordin .
now rememb whichev is lower of p and q , whichev on ha a smaller y coordin is precis at the bottom of thi diagram .
for exampl , if q is the on with the smaller y coordin , it might be on the black line right here .
so that mean in particular x ha y coordin no more than the top part of thi diagram .
no more than delta bigger than q .
and of cours all point with y coordin in between them ar equal well wedg into thi pictur .
so that's why all point of sy with a y coordin between those of p and q have to be in the rang of thi pictur , between the minimum of the two y coordin and delta more than that .
now what about horizont ?
what about the x coordin ?
well thi just follow from the definit of s sub y .
so rememb , s sub y ar the point that fall into thi vertic strip .
how did we defin the vertic strip ?
well it had center xbar , and then we fatten it by delta on both side .
so just by definit , if you're an sy , you've gotta have x coordin in the rang of thi pictur .
x delta plu minu , sorri , xbar plu minu delta .
so that complet the proof of the lemma .
so thi is not .
thi is just a lemma .
so i'll us a lower case qed .
rememb thi is just a step toward prove the overal correct claim .
but thi is a good step .
and again , the wai you think about thi is it sai we draw thi box .
we know that either p or q is at the bottom .
the other on is go to be on the other side of the black line x bar but will be in some other box so perhap mayb p is here and the lemma is sai that all the relev point of sy have to be somewher in thi pictur .
now rememb in our doubl for loop we onli search seven posit awai , so the concern is that thi is a sorta super highli popul collect of eight box .
that's the concern , but that's not go to be the case and that's exactli what lemma two is go to sai .
not onli do the point between p and q in y coordin show up in thi diagram , but there have to be veri few .
in particular , everi box ha to be spars , with popul either zero or on .
so , let's move on to lemma two .
so formal the claim is , we have at most on point of the point set in each of these eight box .
and thi is final where we us , in a real wai , the definit of delta .
thi is where we final get the payoff from our realiz long ago , that when defin the closest split pair subroutin , we onli realli need to be correct in the unlucki case .
in the case we're not hand the right answer by on of our recurs call .
we're final gonna us that fact in a fundament wai .
so we're gonna proce by contradict .
so we're go to think about what happen if there ar two point in a singl box and from that we'll be abl to deriv a contradict .
so , call the point that wind up in the same box a and b .
so , to the contrari , suppos a and b lie in the same box .
so , mayb thi is a here , and thi is b here , at antipod corner of thi particular box .
so from thi supposit , we have two consequ .
first of all .
i claim that a and b lie on the same side of the point set .
thei're either both in the left side , q or thei're both in the right side , r .
so why is thi true ?
well it's becaus everi box li either entir on the left half of the point set or on the right half of the point set .
recal how we defin x bar .
x bar is the x coordin of the right most point amongst the left half of the point set capit q .
so therefor point with x coordin at most x bar have to lie insid the left half q .
point with x coordin at least x bar have to lie insid the right half of the point set capit r .
so that would be like in thi exampl .
a and b both lie in a box which is to the right of x bar .
so thei both have to come in the right half of the point set capit r .
thi is on place we ar us that there ar no ti in x coordin , so if there's a point with x , x coordin or x bar , we can count it as part of the left half .
so everi box , by virtu of be either to the left of xbar or to the right of xbar , can onli contain point from a common half of the point set .
so that's the first consequ of assum that you have two point in the same box .
the second consequ is , becaus the box ar small , the point gotta be close .
so , if a and b co habit a box , how far could thei be from each other ?
well , the farthest thei could be is like i've drawn in the pictur , with the point a and b .
where thei're at opposit corner of a common box .
and then you bust out pythagorean's theorem , and what do you get ?
you get that the distanc between them is delta over two , the side of the box time root two .
and what's relev for us is thi is strictli less than delta .
okai ?
but , now , here is where we us , final , the definit of delta .
consequ on and two in tandem , contradict how we defin delta .
rememb what delta is .
it's as close as two pair of , a pair of point can get if thei both lie on the left side of the point set , or if thei both lie on the right side of the point set .
that is how we defin it .
as small as a pair of point on a common half can get to each other .
but what have we just done ?
we've exhibit a pair a and b that lie on the same half of the point set , and ar strictli closer than delta .
so that contradict the definit of delta .
so that complet the proof of lemma two .
let me just make sure we're all clear on why have prove lemma on and lemma two we're done with the proof part b of the claim and therefor the entir claim becaus we alreadi prove part on , now a long time ago .
so let's interpret the <num> lemma in the context of our pictur that we had all throughout .
in term of the eight box of side length delta over two by delta over two .
so again , whichev is the lower of p and q , and again let's just for the sake of concret sai it's q , is at the bottom of the pictur .
the other point is on the other half of the line xbar , and is in on of the other box .
so , for exampl , mayb p is right here .
so lemma on sai that everi relev point , everi point that surviv the filter and make it into sy , by virtu of be in the vertic strip , ha to be in on of those box , okai ?
if it ha y coordin in between p and q .
lemma two sai that you can onli have on point in each of these box from the point set , so that's gonna be at most eight total .
so combin them .
lemma on and two impli , that there ar almost eight point in thi pictur and that includ p and q becaus thei also occupi two of eight box .
so in the worst case , if thi is as dens popul as could possibl be , given lemma on and two , everi other box might have a point and perhap everi on of those point ha a y coordin between p and q .
but thi is as bad as it get .
ani point of the strip with y coordin between p and q occupi a box .
so , at most , there ar these six wedg in between them .
what doe thi mean ?
thi mean if from q you look seven posit ahead in the arrai , you ar guarante to find thi point p .
so a split pair with distanc less than delta is guarante to be identifi by our doubl for loop .
look seven posit ahead in the sort arrai sy is suffici to identifi , to look at everi conceiv interest split pair .
so that complet the assert b of the correct claim and we're done .
that establish that thi suprem clever divid and conquer algorithm is inde a correct o nlog n algorithm that comput the closest pair of a set of n point in the plane .
in thi seri of video , we'll studi the master method , which is a gener mathemat tool for analyz the run time of divid and conquer algorithm .
we'll begin , in thi video , motiv the method , then we'll give it formal descript .
that'll be follow by a video work through six exampl .
final , we'll conclud with three video that discuss proof of the master method , with a particular emphasi on the conceptu interpret of the master method's three case .
so , lemm sai at the outset that thi lectur's a littl bit more mathemat than the previou two , but it's certainli not just math for math's sake .
we'll be reward for our work with thi power tool , the master method , which ha a lot of .
the power it will give us good advic on which divid and conquer algorithm ar like to run quickli and which on ar like to run less quickli , inde it's sort of a gener truism that .
novel algorithm idea often requir mathemat analyi to properli evalu .
thi lectur will be on exampl of that truism .
as a motiv exampl , consid the comput problem of multipli two n digit number .
recal from our first set of lectur that we all learn the iter grade school multipl algorithm , and that , that requir a number of basic oper , addit and multipl between singl digit , which grow quadrat with the number of digit n .
on the other hand , we also discuss an interest recurs approach us the divid and conquer paradigm .
so recal divid and conquer necessit identifi smaller subproblem .
so for integ multipl , we need to identifi smaller number that we wanna multipli .
so we proceed in the obviou wai , break each of the two number into it left half of the digit , and it right half of the digit .
for conveni , i'm assum that the number of digit n is even , but it realli doesn't matter .
have decompos x and y in thi wai , we can now expand the product and see what we get .
so let's put a box around thi express , and call it star .
so we began with the sort of obviou recurs algorithm , where we just evalu the express star in the straightforward wai .
that is , star contain four product involv n over two digit number .
a , c , a , d , v , c , and b , d .
so we make four recurs call to comput them , and then we complet the evalu in the natur wai .
name , we append zero as necessari , and add up these three term to get the final result .
the wai wereason about the run time of recurs algorithm like thi on is us what's call a recurr .
so to intrduc a recurr let me first make some notat .
t of n .
thi is go to be the quantiti that we realli care about , the quantiti that we want to upward boun .
name thi will be the wors case number of oper that thi recurs algorithm requir to multipli two end digit number .
a recurr , then , is simpli a wai to express t of n in term of t of smaller number .
that is , the run time of an algorithm in term of the work done by it recurs call .
so everi recurr ha two ingredi .
first of all , it ha a base case describ the run time when there's no further recurs .
and in thi integ multipl algorithm , like in most divid and conquer algorithm , the base case is easi .
onc you get down to a small input , in thi case , two on digit number , then the run time in just constant .
all you do is multipli the two digit and return the result .
so i'm gonna express that by just declar the t of on , the time need to multipli on digit number , is bound abov by a constant .
i'm not gonna bother to specifi what thi constant is .
you can think of it as on or two if you like .
it's not gonna matter for what's to follow .
the second ingredi in a recurr is the import on and it's what happen in the gener case , when you're not in the base case and you make recurs call .
and all you do is write down the run time in term of two piec .
first of all , the work done by the recurs call and second of all , the work that's done right here , now .
work done outsid of the recurs call .
so on the left hand side of thi gener case we just write t of n and then we want upper bound on t of n in term of the work done in recurs goal and the work done outsid of recurs goal .
and i hope it's self evid what the recurr should be in thi recurs algorithm for integ multipl , as we discuss there's exactli four recurs call and each is invok on a pair of n over two digit number so that give four time the time need to multipli ten over two digit number .
so what do we do outsid of the recurs call well we've had the recurs call with a bunch of zero's and we add them up .
and i'll leav it to you to verifi that grade school addit , in fact run in time linear in the number of digit .
so put it all togeth the amount of work we do outsid of the recurs call is linear .
that is it's big o .
of n .
let's move on to the second , more clever , recurs algorithm for integ multipl which date back to ga .
gauss's insight wa to realiz that , in the express , star , that we're try to evalu , there's realli onli three fundament quantiti that we care about , the coeffici for each of the three term in the express .
so , thi lead us to hope that , perhap , we can comput these three quantiti us onli three recurs call , rather than four .
and , inde , we can .
so what we do is we recurs comput a time c , like befor , and b time d like befor .
but then we comput the product of a plu b with c plu d .
and the veri cute fact .
is if we number these three product on two and three that's the final quantiti that we care about the coeffici of the ten to the n over two term name ad plu bc .
is noth more than the third product minu each of the first two .
so that's the new algorithm , what's the new occurr ?
the base case obvious is exactli the same as befor .
so the question then is , how doe the gener case chang , and , i'll let you answer thi in the follow quiz .
so the correct respons for thi quiz is the second on .
name , the onli thing that chang with respect to the first recurr , is that the number of recurs call drop from four down to three .
a coupla quick comment .
so , first of all , i'm be a littl bit sloppi when i sai there's three recurs call , each on digit , each on number with n over two digit .
when you take the sum a plu b and c plu d , those might well have n over two plu on digit .
amongst friend , let's ignor that , let's just call it n over two digit and each did recurs call .
as usual , the extra plu on is not gonna matter in the final analysi .
secondli , i'm ignor exactli .
what the constant factor is in the linear work done outsid of the recurs call .
inde , it's a littl bit bigger in gau's algorithm than it is in the naiv algorithm with four recurs call .
but it's onli a constant factor and that's gonna be supress in the big o notat .
so let's look at thi occur and compar it to two other recurr , on bigger , on smaller .
so first of all , as we note , it differ from the previou recurr of the naiv recurs algorithm in have on fewer recurs call .
so we have no idea what the run time is of either of these two recurs algorithm but we should be confid that thi on's .
certainli can onli be better , that's for sure .
anoth point of contrast is merg short .
so think about what the recurr would look like for the merg short algorithm .
it would be almost ident to thi on , except instead of a three , we'd have a two , right ?
merg short make two recurs call , each on an arrai of half the size .
and outsid of the recurs call , it doe linear work , mainli for the merg subroutin .
we know the run time of merg short .
it's n log n .
so thi algorithm , gau's algorithm is gonna be wors , but we don't know by how much .
so while we have a coupl clue about what the run time of thi algorithm might be more or less than , honestli , we have no idea what the run time of gau's recurs algorithm for integ multipl realli is .
it is not obviou , we current have no intuit for it .
we don't know what the solut to thi recurr is .
but it will be on super special case of the gener master method , which we'll tackl next .
so have motiv and hype up the , the gener of the master method , and it's us for analyz recurs algorithm .
let's move on to it's precis mathemat statement .
now the master method is , in some sens , exactli what you want .
it's what i'm gonna call a black box for solv recurr .
basic , it take , as input , a recurr in a particular format , and it spit out as output , a solut to that recurr .
an upper the run time of your recurs algorithm .
that is , you just plug in a few paramet of your recurs algorithm , and boom , out pop it run time .
now , the master method doe requir a few assumpt , and let me be explicit about on .
right now .
name , the master method , at least the on i'm gonna give you , is onli gonna be relev for problem in which all of the subproblem have exactli the same size .
so , for exampl , in merg sort , there ar two recurs call , and each is on exactli on half of the arrai .
so merg sort satisfi thi assumpt both so problem have equal size .
similarli , in both of our integ multipl algorithm , all subproblem were on integ with n over two digit with half as mani digit .
so those will also obei thi assumpt .
if , for some reason , you had a recurs algorithm that recurs on the third of the arrai , and then on the other two third of the arrai .
the master method that i'm gonna give you will not appli to it .
there ar gener of the master method that i'm go to show you , which can accomod unbalanc subproblem size , but those ar outsid the scope of thi cours .
thi will be suffici for almost all of the exampl we're go to see .
on notabl except , for those of you that watch the option video on the determinist algorithm for liner time select .
that will be on algorithm which ha two recurs call on differ subproblem size .
so to analyz that recurr , we'll have to us a differ method , not the master method .
next i'm go to describ the format of the recurr to which the master method appli .
as i said , there ar more gener version of the master method which appli to even more recurr , but the on i'm go to give you is go to be reason simpl and it will cover pretti much all the case you're like ever to encount .
so recurr have two ingredi .
there's the rel unimport , but , still necessari base case step .
and we're gonna make the obviou assumpt , which is just satisfi by everi exampl we're ever go to see in thi cours .
which is that , at some point , onc the input size drop two a suffici small amount , then the , then the recurs stop , and the problem is solv , sub problem is solv in constant sinc thi assumpt is pretti much alwai satisfi in everi problem we're go to see , i'm not gonna discuss it much further .
let's move on to the gener case , where there ar recurs call .
so we assum there were recurr given in the follow format .
the run time on an input of link n is bound abov by some number of recurs call .
let's call it a differ recurs call .
and then each of these sub problem ha exactli the same size and it's , on over b fraction of the origin input size .
so there's a recurs call , each of on input of size n over b .
now as usual there's the case where i never be as a fraction and not an integ and as usual i'm go to be sloppi and ignor it and as usual that sloppi ha no implic for the final conclus , everyth that we're go to discuss is true , for the same reason and the gener case where n over b is not an integ , outsid the recurs call we do some extra work and let sai that it's o of n to the d for some paramet d so in addit to the input size n there ar three letter here which we need to be veri clear of what their mean is .
so first of all there's a , which is the number of subproblem , the number of recurs call .
so a could be as small as on or it might be some larger integ .
then there's b .
b is the factor by which the input size shrink befor a recurs call is appli .
b is some constant strictli greater than on .
so for exampl if you re curs oh half of the origin problem then b would be equal to two .
it better be strictli bigger than on so that eventu you stop recurs .
so that eventu that you termin .
final , there's d , which is simpli the expon in the run time of the quot unquot combin step .
that is , the amount of work which is done outsid of the recurs call .
and d could be as small as zero , which would indic constant amount of work outsid of the recurs call .
on point to emphas is that a , b and d ar all constant .
that's all , thei're all number that ar independ of n .
so a , b and d ar gonna be number like on , two , three or four .
thei do not depend on the input size and .
and in fact , let me just redraw the d so that you don't confus it with the a .
so again , a is the number of recurs call .
and d is the expon in the run time govern the work done outsid of the recurs call .
now , on comment about that final turn , that big o of n to the d .
on the on hand , i'm be sorta sloppi .
i'm not keep track of the constant that's hidden insid the big o i'll be explicit with that constant when we actual prove the master method , but it's realli not gonna matter .
it's just gonna carri through the analysi , without affect anyth .
so you can go ahead and ignor that constant within the big o .
obvious .
the constant of the expon , name d .
is veri import .
all right ?
so depend on when d .
is depend on whether that amount of time is constant , linear , quadrat , or so on .
so certainli we care about the constant d .
so that's the input to the master method .
it is a recurr of thi form , so you can think of it as a recurs algorithm which make a recurs call , each on some problem of equal size , each of size n over b , plu it doe n to the d work outsid of the recurs call .
so have setup the notat i can now precis state the master method for you .
so given such a occurr we ar go to get an upper ban on the run time .
so the run time on .
input of size n is go to be upper bound by on of three thing .
so somewhat famous the master method ha three case .
so let me tell you about each of them .
the trigger , which determin which case you're in , is a comparison between two number .
first of all a .
recal a is the number of recurs call made and b rais to the d power .
recal b is the factor by which the input size shrink befor you re curs .
d is the expon in the amount of work done outsid the recurs call .
so we're gonna have on case for when thei're equal , we're gonna have on case for when a is strictli smaller then b to the d and third case is when a .
is strictli bigger than b .
to the d .
and in the first case we get a run time of , big o .
of n .
to the d .
time log in .
again thi is d .
the same d that wa in the final term of the recurr .
okai ?
that work on the outsid of the recurs call .
so the first case , the run time is the , is the same as the run time in the recurr and outsid the recurs call but we pick up an extra log in factor .
in second case where a is smaller than b to the d .
the run time is mere big o of n to the d .
and thi case might be somewhat stun if thi could ever occur .
becaus , of cours , in recurr , what do you do ?
you do some recurs , plu you do n to the d work outsid of the recurs .
so in the second case , it actual sai the work is domin by just what's done outsid the recurs in the outermost call .
the third case will initi seem the most mysteri , when a is strictli bigger than b to the d .
we're gonna get a run time of big o , of n to the log .
base b .
of a .
for again recal a is the number of recurs caus and b is the factor by which the input size shrink befor you recurs .
so that's the master method with it three case .
let me give thi to you in a cleaner slide to make sure there's no ambigu in my handwrit .
so here's the exact same statement , the master method .
onc again , with it's three case depend on how a compar to b to the d .
so on final comment .
you'll notic that i'm be asymmetr sloppi with the two logarithm that appear in these formula .
so let me just explain why .
in particular you'll notic that in case on , with a logarithm .
i'm not specifi the base .
why is that true ?
well , it's becaus the logrithm with respect to ani two differ base differ by a constant factor .
so the logrithm that is at base e , that is the natur logrigthm , and the logrithm base two , for exampl , differ by onli a constant factor independ of the argument n .
so you can switch thi logrithm to whatev consant base you like .
it onli chang the lead constant factor , which of cours is be suppress in the bigger notat anywai .
on the other hand , in case three , where we have a logarithm in the expon .
onc it's in the expon , we definit care about that constant .
constant is the differ between , sai , linear time and quadrat time .
so we need to keep care of the ba , the logarithm base in the expon in case three .
, and that base is precis b , the factor by which the input shrink which each , with each recurs call .
so that's the precis statement of the master method .
and the rest of thi lectur will work toward understand the master method .
so first , in the next video , we'll look at a number of exampl , includ resolv the run time of recurs algorithm for integ multipl .
follow those sever exampl , we'll prove the master method .
and i know now , these three case probabl look super mysteri .
but if i do my job , by the end of the analysi .
in thi video we'll put the master method to us by instanti it for six differ exampl .
but first let's recal what the master method sai .
so the master method take as input recurr of a particular format , in particular recurr that ar paramer by three differ constant , a , b and d .
a refer to the number of recurs call , or the number of sub problem that get solv .
b is the factor by which the sub problem size is smaller than the origin problem size .
and d is the expon in the run time of the work done outsid of the recurs call .
so the recurr ha the form t of n .
the run time put up , n is no more than a , the number of subproblem , time the time requir to solv each subproblem , which is t of n over b .
cuz the input size of a subproblem is a number b o of n to the d .
the work outsid of the recurs call .
there's also a base case , which i haven't written down .
so onc the problem size , drop below a particular constant , then there should be no more recurs , and you can just solv the problem immedi , that is in constant time .
no , given a recurr in thi permit format , the run time is given .
by on of three formula .
depend on the relationship between a .
the number of call .
and b rais to the d power .
case on of the master method is when these two quantiti ar the same , a equal b to the d .
then the run time is n to the d log n .
no more than that .
in case two , the number of recurs call , a , is strictli smaller than b to the d .
then , we get a better run time upper bound , of o of n to the d , and , when a is bigger than b to the d , we get thi somewhat funki look run time of o of n , rais to the log base b of a power .
we'll understand what that , where that formula come from a littl later .
so , that's the master method .
it's a littl hard to interpret the first time you see it , so let's look at some concret exampl .
let's begin with an out rhythm that we alreadi know the answer to , that we alreadi know the run time .
name let's look at merg short .
so again what's so great about the master method is all we have to do is identifi the valu of the three relev paramet a , b , and d , and we're done .
we just plug them in then we get the answer .
so a rememb is the numbr of recurs call .
so in merg sort recal we get two recurs call .
b is the factor by which the sub problem size is smaller than that in the origin .
well we recurs on half the arrai so the sub problem size if half that of the origin .
so b is equal to two .
and recal that outsid of the recurs call all merg sort doe is merg .
and that's a linear time subroutin .
so expon d is on reflect of factor with linear time .
so rememb the kei trigger which determin which of the three case is the relationship between a and b and the d .
so a obvious is two .
and b to the d is also equal to two .
so thi put us in case on .
and rememb in case on .
now that the run time is bound abov by o of n to the d log n .
in our case d equal on , so thi is just o of n log n .
which of cours , we alreadi knew .
okai ?
but at least thi is a saniti check , the master method is at least reconfirm fact which we've alreadi proven by direct mean .
so let's look at a second exampl .
the second exampl is go to be for the binari search in a sort arrai .
now we haven't talk explicitli about binari search , and i'm not plan to , so if you don't know what binari search is pleas read about it in a textbook or just look it up on the web and it'll be easi to find descript .
but the upshot is , thi is basic how you'd look up a phone number in a phone book .
now i realiz probabl the youngest viewer of thi video haven't actual had the experi of us a physic telephon book but for the rest of you .
as you know , you don't actual start with the a's , and then go to the b's , and then go to the c's , if you're look for a given name .
you more sensibl split the telephon book in the middl .
then you recurs on the left or the right half , as appropri , depend on if the element you're look for is bigger or less than the middl element .
now the master method appli equal well to binari search and it tell us what it run time is .
so in the next quiz , you'll go through that exercis .
so the correct answer is the first on .
to see why let's recal what a , b and d mean .
a is the number of recurs call now in binari search you onli make on recurs call thi is unlik merg sort , rememb you just compar the element you're look for to the middl element , if it's less than the middl element you recours on the left half if it's bigger than the middl element you recours on the right half .
so in ani case there's onli on recurs call so a is mere on in binari search .
now in ani case you recur on half the arrai so b two you recur on a valu of half the size , and outsid of the recurs call the onli thing you do is on comparison you just determin whether the element you're look for is bigger than or less than the middl element .
of the arrai that you were first on .
so that's constant time outsid of the recurs call , give us a valu for d .
of zero .
just like merg short thi is again case on of the master method , becaus we have a .
equal , b .
to the d .
both in thi case ar equal to on .
so thi give us a recurr , a solut to our occurr of big o of n to the d log n .
sinc d zero , thi is simpli login .
and again , mani of you probabl alreadi know that the run time of binari search is login , or you can figur that out easili .
again , thi is just us the master method as a saniti check to reconfirm that it's give us the answer that we expect .
let's now move on to some harder exampl , begin with the first recurs algorithm for integ multipl .
rememb , thi is where we .
recurs on four differ product of n over two digit number and then recombin them in the obviou wai us pad by zero and some linear time condit .
so the first which doe not make us of but we do the four differ recurs call we have a the number of recurs call is equal to four .
now , in each case whenev we take a product of two smaller number , the number have n over two digit , so that's half as mani digit as we start with .
so just like in the previou two exampl , b is go to be equal to two .
the input size drop by a by a fact of two when we re curs .
now how much work do we do outsid of re curs or call ?
well again , all it is do is addit and pad by zero and that can be done in linear time .
linear time correspond to a paramet valu of d equal to on .
so next we determin which case of the master method we're in .
a equal four .
b to the d <num> , which , in thi case , is less than a .
so thi correspond to case three of the master method .
and thi is where we get the somewhat strang formula for the run time of the recurr .
t on n is big to the log base b of a .
which , with our paramet valu , is n to the log base two of four , also known as o of n squar .
so let's compar thi to the simpl algorithm that we all learn back in grade school .
recal that the iter algorithm for multipli two integ .
also take an n squar number of oper .
so , thi wa a clever idea to , to attack the problem recurs .
but at least in the absenc of , where you just naiv comput each of the four , necessari , product separ .
you do not get ani improv over the algorithm that you learn in grade school .
either wai , it's an n squar number of oper .
but , what if we do make us of , where we do onli three recurs call instead of four ?
sure , the run time won't be ani wors than n squar .
and hopefulli , it's go to be better .
so i'll let you work out the detail on thi next quiz .
so the correct answer to thi quiz is the fourth option .
it's not hard to see what the relev valu of a , b , and d ar .
rememb , the whole point of trick it to reduc the number of recurs call from four down to three .
so the valu of a is go to be three .
as usual , we're recur on a problem size which is half of that of the origin , in thi case , n over two digit number .
so b remain two , and just like in the more naiv recurs algorithm , we onli do linear work outsid of the recurs call .
all that's need to do some addit and pattern by zero .
so that put paramet valu a , b , and d .
then we have to figur out which case .
the master method of that is so we have a equal three .
b rais to the d equal to two .
so a ha drop by on rel to the more naiv algorithm .
but we're still in case three of the master method .
a is still bigger than b to the d , so the run time is still govern by that rather exot look formula .
name , t of n is big o of n to the log .
base b which in our case is two of a which is now three instead of four okai so the master method just tell us the solut to thi recurr , the run time of thi algorithm is big o to the n to the log base two of three so what is log base two of three well plug it in your comput or calcul and you'll find it's roughli <num> . <num> so we get a run time of n to the <num> . <num> .
which is certainli better than n squar .
it's not as fast as n lo gin , not as fast as the merg sort recurr , which make onli two recurs call .
but it's quit a bit better than quadrat .
so , summar , you did , in fact , learn a sub optim algorithm for integ multipl wai back in grade school .
you can beat the algorithm us a combin of recurs , plu to save on the number of recurs call .
let's quickli move on to our final two exampl .
exampl five is for those of you that watch the video on matrix multipl algorithm .
so recal the and properti of algorithm .
the kei idea is similar to multipl .
first you set up the problem recurs .
on observ that the naiv wai to solv the problem recurs would be to eat sub problem .
but if you're clever about save some comput , you can get it down to just seven recurs call , seven subproblem .
so a in argument is equal to seven .
as usual each sub problem size is half that of the origin on so p is go to be equal two and the amount of work done outsid of the recurs call is linear in the matrix size so quadrat in n quadrat in the mention becaus there's a quadrat number of entri in term of the dimens so n squar work outsid the recurs call equal to the valu of d equal to two .
so as far as which case of the master method we're in it's the same as the last exampl , a seven , d four .
which is less than a , so onc again we're in case three .
and now the run time of strassen's algorithm .
t of n is viggo of n to the log base two seven , which is more or less n to the <num> . <num> .
and again thi is a win .
onc we us , the , the save to get down to just seven call .
thi beat the naiv algorithm which recal , would requir a cubic ton .
so that's anoth win for clever divid and conquer .
and multipl via algorithm .
and onc again , the master's method , just by plug in paramet , it tell us exactli what the right answer to thi recurr is .
so for the final exampl , i feel a littl guilti , becaus i've shown you five exampl , and none of'em have trigger case two .
we've had two in case on of the master method , and three now in case three .
so thi'll be sort of a fictiti recurr , just case two .
but , you know , there ar exampl of recurr that come up , where case two is the relev on .
so let's just look at a .
at the follow recurr .
so thi recurr is just like merg sort .
we recurs twice .
there's two recurs call , each on half the problem size .
the onli differ is in thi recurr we're work a littl bit harder on the combin step .
instead of linear time outsid of the recurs call , we're do a quadrat amount of work , okai ?
so .
a equal two .
b equal two and d equal two , so b to the d wa equal to four , strictli bigger than a and that's exactli the trigger for case two .
now with the run time in case two , it's simpli end with a d , where d is the expon and the combin step , in our case d is two , so we get a run time of n squar .
and you might find thi a littl bit intuit by give the merg sort , all we do with merg sort is chang the combin step from linear to quadrat .
and merg sort ha a run time of and squar log in , and that is over estim .
so master method give us the tighter that it's onli quadrat work .
so , put differ the run time of the entir algorithm is govern by the work outsid of the recurs call just in the outer most call to algorithm .
just at the root of the recurs trim .
in thi video , we'll begin the proof of the master method .
the master method , you'll recal , is a gener solut to recurr of the given form , recurr in which there's a recurs call , each on a sub problem of the same size , size n over b , assum that the origin problem had size n .
and , plu , there is big o of n to the d work done by the algorithm outsid of these a recurs call .
the solut that the master method provid ha three case , depend on how a compar to b to the d .
now .
thi proof will be the longest on we've seen so far by a signific margin .
it'll span thi video as well as the next two .
so let me sai a few word up front about what you might want to focu on .
overal i think the proof is quit conceptu .
there's a coupl of spot where we're go to have to do some comput .
and the comput i think ar worth see onc in your life .
i don't know that thei're worth realli commit to long term memori .
what i do think is worth rememb in the long term howev , is the conceptu mean of the three case of the master method .
in particular the proof will follow a recursionari approach just like we us in the run time analysi of the mertshot algorithm .
and it worth rememb what three type of recurs tree the three case is that the master method correspond to .
if you can rememb that , there will be absolut no need to memor ani of these three run time , includ the third , rather exot look on .
rather , you'll be abl to revers engin those run time just from your conceptu understand of what the three case mean and how thei correspond to recurs tree of differ type .
so , on final comment befor we embark on the proof .
so , as usual , i'm uninterest in formal in it own sake .
the reason we us mathemat analysi in thi cours , is becaus it provid an explan of , fundament , why thing ar the wai thei ar .
for exampl , why the master method ha three case , and what those three case mean .
so , i'll be give you an essenti complet proof of the master method , in the sens that it ha all of the kei ingredi .
i will cut corner on occas , where i don't think it hinder understand , where it's easi to fill in the detail .
so , it won't be <num> percent rigor , i won't dot everi i and cross everi t , but .
there will be a complet proof , on the conceptu level .
that be said , let me begin with a coupl of minor assumpt i m go to make , to make our live a littl easier .
so first , we're gonna assum that the recurr ha the follow form .
so , here , essenti , all i've done is i've taken our previou assumpt about the format of a recurr , and i've written out all of the constant .
so , i'm assum that the base case kick in when the input size is on , and i'm assum that the number of oper in the base case is at most c , and that , that constant c is the same on that wa hidden in the big o notat of the gener case of the recurr .
the constant c here isn't gonna matter in the analysi , it's just all gonna be a wash , but to make , keep everyth clear , i'm gonna write out all the constant that were previous hidden in the big o notat .
anoth assumpt i'm go to make .
now goe to our murtur analysi , is that n is a power of b .
the gener case would be basic the same , just a littl more tediou .
at the highest level , the proof of the master method should strike you as veri natur .
realli , all we're go to do is revisit the wai that we analyz merg short .
recal our recurs tree method work great , and gave us thi log , and the run time of merg short .
so we're just gonna mimic that recurs tree , and see how far we get .
so let me remind you what a recurs tree is .
at the root , at level zero , we have the outermost , the initi indic of the recurs algorithm .
at level on , we have the first batch of recurs call .
at level two , we have the recurs call made by that first batch of recurs call , and so on .
all the wai down to the leav of the tree , which correspond to the base case , where there's no further recurs .
now , you might recal , from the merg sort analysi that we identifi a pattern that wa crucial in analyz the run time .
and that pattern that we had to understand wa , at a given j , at a given level j of thi recurs tree .
first of all , how mani distinct subproblem ar there at level j ?
how mani differ level j ar there ?
and secondli , what is the input size that each of those level j subproblem ha to oper on ?
so think about that a littl bit and give your answer in the follow quiz .
so the correct answer is the second on .
at level j at .
of thi recurs tree , there ar a to , to the j sub problem and each ha an input of size of n over b to the j .
so first of all , why ar there a to the j sub problem ?
well , when j equal zero at the root , there's just the on problem , the origin indic of the recurs algorithm .
and then each .
call to the algorithm make a further call .
for that reason the number of sub problem goe up by a factor of a with each level lead to a to the j sub problem at level j .
similarli , b is exactli the factor by which the input size shrink onc you makea recurs call .
so j level into the recurs .
the input size ha been shrunk j time by a fctor of b each time .
so the input size at level j is n over b to the j .
that's also the reason why , if you look at the question statement , we've identifi the number of level as be log of base b .
of n .
back in merg short , b wa two .
we on half the arrai .
so the leav all resid at level log base two of n .
in gener , if we're divid by a factor b each time , then it take a log base b of n time befor we get down the base case of size of on .
so the number of level overal , zero through log base b event .
for a total of log base b event plu on level .
here then is what the recurs tree look like .
at level zero we have the root correspond to the outer most call .
and the input size here is n .
the origin problem .
children of a node correspond to the recurs call .
becaus there ar a .
recurs call by assumpt , there ar a .
children or a .
branch .
level on is the first batch of precursor call .
each of which oper on an input of size n over b .
that level log base b .
of n .
we've cut the input size by a factor of b .
thi mani time , so we're down to on .
so that trigger the base case .
so now , the plan is to simpli mimic our previou analysi of merg sort .
so let's recal how that work .
what we did is we zoom in , in a given level .
and for a given level j , we count the total amount of work that wa done at level j subproblem , not count work that wa gonna be done later by recurs call .
then , given a bound on the amount of work at a given level j , we just sum up overal , the level , to captur all of the work done by all of the , recurs indic of the algorithm .
so inspir by our previou success let's zoom in on a given level j . , and see how much work get done , with level j .
sub problem .
we're go to comput thi in exactli the wai we did in merg sort .
and we were just go to look at the number of problem that ar at level j and we're go to multipli that by a bound on the work done per sub problem .
we just identifi the number of level j sub problem as a to the j .
to understand the amount of work done for each level j sub problem , let's do it in two part .
so , first of all , let's focu on the size of the input for each level j sub problem .
that's what we just identifi in the previou quiz question .
sinc the input size is be decreas by a factor b each time , the size of each level j sub problem is n over b to the j .
now we onli care about the size of a level j sub problem in as much it determin the amount of work the number of oper that we perform per level j sub problem .
and to understand the relationship between those two quantiti we just return to the re current .
the recurr sai how much work get done in the specif sub problem well there's a bunch of work done by recurs call the a recurs call and we're not count that we're just count the work done here at a level j and the recurr also tell us how much is done outsid of the recurr call .
name it's no more than the constant c time the input size .
rais to the d power .
so here the input size is n over b to the j , so that get multipli by the constant c .
and it get rais to the d power .
okai .
so c .
time quaniti n .
over b .
to the j .
that's the emphas .
rais to the d .
power .
next , i wanna simplifi thi express a littl bit .
and i wanna separ out the term which depend on the level number j , and the term which ar independ of the level number j .
so if you look at it a and b ar both function of j , where the c and end of the d term ar independ of j .
so let's just separ those out .
and you will notic that we have now our grand entranc of the ratio between a and b to the d .
and foreshadow a littl , recal that the three case of the master method ar govern by the relationship between a and b to the d .
and thi is the first time in the analysi where we get a clue that the rel magnitud of those two quantiti might be import .
so now that we've zoom in on a particular label j and done the necessari comput to figur out how much work is done just at that level , let's sum over all of the level so that we captur all of the work done by the algorithm .
so thi is just gonna be the sum of the epress we saw on the previou slide .
now sinc c into the d doesn't depend on j , i can yank that out in front of the sum , and i'll sum the express over all j .
that result in the follow .
so believ it or not , we have now reach an import mileston in the proof of the master method .
specif , the somewhat messi look formula here , which i'll put a green box around , is go to be crucial .
and the rest of the proof will be devot to interpret and understand thi express , and understand how it lead to the three differ run time bound in the three differ case .
now i realiz that at the moment thi express's star probabl just look like alphabet soup , probabl just look like a bunch of mathemat gibberish .
but actual interpret correctli thi ha a veri natur interpret .
so we'll discuss that in the next video .
thi video is the second of three that describ the proof of the master method .
in the first of these three video we mimick the analysi of merg sort .
we us a recurs tree approach which gave us an upper bound of run time of an algorithm .
which is govern by a recurr of the specifi form .
unfortun , that video left us with a bit of an alphabet soup , thi complic express .
and so in thi second video , we're not gonna do ani comput .
we're just go to look at that express , attach some semant to it , and look at how that interpret natur lead to three case , and also give intuit for some of the run time that we see in a master method .
so recal from the previou video that the wai we've bound the work done by the algorithm is resum in on a particular level j of the recurs tree .
we did a comput , which wa the number of sub problem at that level , a to the j , time the work done per sub problem , that wa the constant c time quantiti n over b to the j rais to the d and that gave us thi express .
cn to the d time the ratio of a over b to the d rais to the j .
at a given level .
j .
the express star that we conclud the previou video with wa just the sum of these express over all of the logarithm level , j .
now , as messi as thi express might seem , perhap we're on the right track in the follow sens .
the master method ha three differ case , in which case you're in is govern by how a compar to b to the d .
and hear thi express , we ar see precis that ratio .
a divid by b to the d .
so let's drill down and understand why thi ratio is fundament to the perform of the divid and conquer algorithm .
so realli , what's go on in the master method , is a tug of war between two oppos forc .
on which is forc of good , and on which is forc of evil , and those correspond to the quantiti b to the d and a , respect .
so let me be more precis .
let's start with the paramet a .
so a , you'll recal , is defin as the number of recurs call made by the algorithm .
so it's the number of children that a recurs tree ha .
so fundament , what a is , it's the rate at which sub problem prolifer as you pass deeper in the recurs tree .
it's the factor by which there ar more sub problem at the next level than the previou on .
so let's think of a .
in thi wai .
as the rate of subpropabifli , or r . s . p .
and when i sai rate i mean as a function of the recurs level j .
so these ar the forc of evil .
thi is why our algorithm might slowli , is becaus as we go down the tree there ar more and more sub problem , and that's a littl scari .
the forc of good , what we have go for us , is that with each recurs level j we do less work per sub problem and the extent to which we do less work is precis b to the d .
so i'll abbrevi thi rate of work shrinkag or thi quantiti b .
to the d .
by r .
w .
s .
now perhap you're wonder why is it b of the d .
why is it not b ?
so rememb what b denot .
that's the factor by which the input size shrink with the recurs level j .
so for exampl if b equal two , then each sub problem at the next level is onli half as big .
as that at the previou level .
but we don't realli care about the input size of a subproblem , except inasmuch as it determin the amount of work that we do solv that subproblem .
so that's where thi paramet d come into plai .
think , for exampl , about the case where you have a linear amount of work outsid the recurs call , versu a quadrat amount of work that is consid the case where d equal on or two .
if b two and d on that is if you revers on half the input .
and do linear work , then .
not onli is the input size drop by factor two but so is the amount of work that you do per sub problem and that's exactli the situat we had in merg short where we had linear work outsid the recurs call .
the thing about d two , suppos you did quadrat work per sub problem as a function of the input size .
then again if b two if you cut the input in half , the recurs call's onli gonna do <num> percent as much work as what you did .
at the current level .
the input size goe down by a factor two and that get squar becaus you do quadrat work as a function of the input size .
so that would be b to the d , two rais to the two or four .
so in gener the input size goe down by a factor b , but what we realli care about , how much less work we do per subproblem , goe down by b to the d .
that's why b to the d is the fundament quantiti that quan , that's govern the forc of good , the extent to which we work less hard with each occurs level j .
so the question that is just what happen in thi tug of war between these two oppos forc ?
so fundament , what the three case of the master method correspond to , is the three possibl outcom in thi tug of war between the forc of good , name the rate of word shrinkag and the forc of evil , name the sub problem prolifer .
there ar three case on for the case of a tie on for the case in which the forc of evil win that is in which a is bigger than b to the d and a case in which the forc of good win , that is b to the d is bigger than a .
to understand thi a littl bit better what i want you to think about is the follow .
think about the recurs tree that we drew in the previou slide and as a function of a vers b to the d think about the amount of work you've done per level .
when is that go up per level ?
when is it go down per level ?
and when is it exactli the same at each level ?
so the answer is all of these statement ar true except for the third on .
so let's take them on at a time .
so first of all let's consid the first on .
suppos that the rate of sub problem prolifer a is strictli less than the rate of work shrinkag , b to the d .
thi is where the forc of good , the rate at which we're do less work per sub problem is out , out pace the rate of at which sub problem ar prolifer .
and so the number of sub problem goe up , but the save per sub problem goe up by even more .
so , in thi case it mean that we're gonna be do less work .
with each recurs tree level , the forc of good outweigh the forc of evil .
the second on is true for exactli the same reason .
if sub problem ar prolifer so rapidli that it outpac the save that we get per sub problem , then we're gonna see an increas amount of work .
as we go down the recurs tree , it will increas with the level of j .
given that these two ar true the third on is fals .
we can draw conclus depend on whether the rate of sub problem prolifer is strictli bigger or strictli less than the rate of work shrinkag .
and final , the fourth statement is also true .
thi is the perfect equilibrium between the forc of good and the forc of evil .
sub problem ar prolifer , but our save per sub problem is increas at exactli the same rate .
the two forc will then cancel out and we'll get exactli the same amount of work done at each level of the recurs tree .
thi is precis what happen when we analyz a merd short algorithm .
so let's summar and conclud with the interpret .
and even understand how thi interpret lend us to forecast some of the run time bound that we see in the master method .
summar , the three case of the master method correspond to the three possibl outcom in the battl between sub problem prolifer and the work per sub problem shrink .
on for a tie , on for when sub problem ar prolifer faster , and on for when the work shrinkag is happen faster .
in the case where the rate ar exactli the same , and thei cancel out , then the amount of work should be the same at everi level of the recurs tree .
and , in thi case , we can easili predict what the run time should work out to be .
in particular , we know there's a logarithm number of level , the amount of work is the same at everi level , and we certainli know how much work is get done at the root , right , becaus that's just the origin recurr , which tell us that there's , acentot , n to the d work done at the root .
so , with n to the d work for each of the log level , we expect a run time of n to the d time log n .
as we just discuss , when the rate of .
work done per subproblem is shrink even faster than subproblem prolifer .
then we do less and less work with each level of the recurs tree .
so in particular , the biggest amount of work , the worst level is at the root level .
now , the simplest possibl thing that might be true would be that actual , the root level just domin the overal run time of the algorithm , and the other level realli don't matter up to a constant factor .
so it's not obviou that's true , but if we keep our finger cross and hope for the simplest possibl outcom .
with the root ha the most work , we might expect a run time that's just proport to the run time of the root .
as we just discuss , we alreadi know that that's n to the d , cuz that's just the outermost call to the algorithm .
by the same reason , when thi inequ is flip , and prolifer so rapidli that it's outpac the same as we get for sub problem , the amount of work is increas the recurs level .
and here , the worst case is gonna be at the leav .
that's where the , that level's gonna have the most work compar to ani other level .
and again , if you keep your finger cross and hope that the simplest possibl outcom is actual true , perhap the leav just domin , and .
up to a constant factor , thei govern the run time of the algorithm .
in thi third case , given that we do a constant amount of work for each of the leav , sinc those correspond to base case , here we'd expect a run time in the simplest scenario , proport to the number of leav in the recurs tree .
so let summar what we've learn in thi video .
we now understand that fundament there ar three differ kind of recurs tree .
those in which the work done per level is the same in everi level .
those in which the work is decreas with the level in which case the root is the lowest level .
and those in which the amount of work is increas with the level where the lead ar the lowest level .
further more it's exactli the ratio between a the rate of sub problem prolifer and b to the d the rate of work shrinkag sub problem that govern which of these three recurs tree we're deal with .
further more .
intuit , we've now had predict about what kind of run time we expect to see in each of the three case .
thei're n to the d log in , that we're pretti confid about .
there's a hope that , in the second case , where the root is the worst level , that mayb the run time is n to the d .
and there's a hope in the third case where the ar the wors level , and we do constant time per leaf , per base case , that it's gonna be proport to the number of leav .
let's now stand and check thi intuit against the formal statement of the master method , which we'll prove more formal in the next video .
so in the three case , we see thei match up .
at least two out of three with exactli li .
so in the first case , we see the expect end of the d time log in .
in the second case , where the root is the worst level inde , the simplest possibl outcom of big o of n to the d is the assert .
now , the third case that remain a mysteri to be explain .
our intuit said thi should hopefulli be proport to the number of leav .
and instead , we've got thi funni formula of big o of n in the log base b of a .
so in the next video , we'll demystifi that connect , as well as suppli formal proof for these assert .
let's complet the proof of the master method .
let me remind you about the stori so far , the first thing we did is we analyz the work done by a recurs algorithm us a recurs tree .
so we zoom in on a given level j , we identifi the total amount of work done at level j and then we sum up over all of the level result in thi rather intimid express star .
c into the d time a sum over the level j from zero to log base b of n of quantiti a over b to the b rais to the j .
have deriv thi express star we then spent some time interpret it , attach to it some semant stick .
and we realiz that the roll of thi ratio a to the b over d , is to distinguish between three fundament differ type of recurs tree .
those in which a b to the d and the amount of work is the same at everi level .
those in which a is less than b to the d and therefor the amount of work is go down with the level .
and those where a is bigger than b to the d in which case the amount of work is grow with the level .
thi gave us intuit about the three case of the master method and even gave us predict f or the run time we might see .
so what remain to do is turn thi hope intuit into .
a rigor proof .
so we need to verifi that in fact the simplest possibl scenario outlin in the previou video .
actual occur .
in addit , we need to demystifi the third case and understand what the express ha to do with the number of leav of the recurs tree .
let's begin with the simplest case , which is case on .
we're call case on , we're assum that a equal b to the d .
thi is the case where we have a perfect equilibrium between the forc of good and evil .
where the rate of the sub problem prolifer exactli cancel out with a rate at which we do less work per sub problem .
and now , examin the express , star , we can see how easi our live get when a equal b to the d .
in that case , thi ratio is equal to on .
so natur thi ratio rais to the j is also equal to on for all j .
and then of cours thi sum evalu to someth veri simpl .
name on sum with itself log base b of n plu on time .
so the sum simpli equal log base b of n .
plu on , and that's go to get multipli by .
thi cn to the d term which is independ of the sum .
so summar , when a equal b to the d , we find that star equal cn to the d time log base b of n plu on .
write thi in big o notat , we would write big o of end of a d login .
and again , i'm go to suppress the base of the logarithm .
sinc all logarithm differ onli by a constant factor we don't have to specifi the base .
that's just suppress by the constant hidden in the big o notat .
so that's it for case on .
like i said , thi is the easi case .
so what do we do when a is not equal to b to the d ?
and rememb a could be either less than or bigger than b to the d .
to answer that question , let's take a short detour into geometr seri .
for thi singl slide detour we're go to think about a singl constant number r .
now , what you want to think about is r .
repres that ratio a .
over b .
to the d .
from the previou slot .
but for thi slide onli let's just call it r .
thi is a constant .
it's bigger than zero , and it's not equal to on .
now , suppos we sum up power of r stop , let's sai , at the kth power of r .
i claim that thi sum ha a nice close form formula .
specif it is exactli , r .
to the k .
plu on , minu on .
divid by or a minu on .
now , whenev you see a gener formula like thi , it's us to keep in mind a coupl of canon valu of the paramet that you can plug in to develop intuit .
and for thi express , you might wanna think canon about the case , r <num> , and r <num> <num> .
so when r <num> , or someth that power a two .
on <num> <num> <num> <num> , and so on .
on hour's a half , have on , plu a half , plu a quarter , plu an eighth , and so on .
now i'm not gonna prove thi for you , i'd like you to prove thi yourself .
if you don't alreadi know thi fact .
so the wai to prove thi is simpli by induct .
and i will leav thi an , an exercis .
what i wanna focu on instead is what thi fact can do for us .
the wai that we us thi fact is to formal the idea that , that in recurs tree where the amount of work is increas in the level , the leav domin the overal run time .
and where recurs tree , where the amount of work is decreas in the level , the root domin the run time .
in the sens that we can ignor all of the other level of the recurs tree .
so , and in the vision in thi slide , we have two upshot .
first of all , for the case when r is less than on .
and in thi case , thi express on the right hand side .
r to the q plu on minu on over r minu on can be upper bound by on over on minu r .
so again , rememb , you might want to have a canon valu of r in mind here , name , on half .
so what we're claim here is that the right hand side is nor more than two for the case of r <num> <num> .
and that's easi to see if you think about on plu on half plu a on fourth plu on eighth and so on .
that sum is converg to , to as k grow larg .
so in gener , for our less than on constant , the sum is divid by on minu on over r .
now , we're not actual gonna care about thi formula , on minu on over r .
the point for us is just that thi is a constant .
and by constant , i mean independ of k , independ of how mani term we sum up .
obvious , it depend on r of the ratio , but it doe not depend on how mani thing we sum up on k .
so the wai to think about thi is , when we sum up a bunch of term where r is less than on , then the veri first term domin .
the first term is with a on .
and no matter how mani term we sum up , we never get , grow bigger than the sum constant .
a similar situat hold for the case where r is a constant bigger than on .
when r is bigger than on .
a tini bit of algebra show that we can upper bound the right hand side by r to the k .
time someth which is constant , independ of k .
so again , let's interpret the second upshot in term of a canon valu of r .
name , r equal two .
then our sum is on plu two plu four plu eight plu sixteen , and so on .
and what thi is sai is that no matter how mani term we sum up , the overal sum is never gonna be more than twice .
the largest and final term .
so if we sum up to sai <num> , the sum , you'll notic , will be <num> , which is , at most , twice that largest term , <num> .
and that sai is true for ani k .
the entir sum is no more than twice that of the largest term .
in thi sens , the largest term in the seri domin the whole thing .
so to summar thi slide in just into on sentenc we sum up power of a constant r when r is bigger than on the largest power of that constant domin to the sun when r is smaller than on then the sun is just a constant .
let's now appli thi to prove case two of the master method .
in case two of the master method , we assum that a is less than b to the d .
that is , the rate at which sub problem ar prolifer is drown out by the rate at which we do less work per sub problem .
so thi is the case where the amount of work is decreas with each level of the recurs tree .
and our intuit said that , well , in the simplest possibl scenario , we might hope that all of the work , up to a constant factor , is be done at the root .
so let's make that intuit precis by us the basic sum fact on the previou slide .
so , sinc a is less than b to the d .
thi ration is less than on .
so let's call thi ratio equal to r .
so r , you'll notic , doe depend on the three paramet , a , b and d .
but r is a constant , it doe not depend on n .
so what is thi sum ?
the sum is just , we're just sum up power of thi constant r , where r is less than on .
what did we just learn ?
we just learn that ani such sum is bound abov by a constant , independ of the number of term that you sum up .
so therefor , what is thi express star evalu to .
it evalu to c , which is a constant , time n to the d .
time anoth constant .
so suppress the product of these two constant in big o notat we can sai that the express start upper bound by big o n to the d .
and thi make precis our intuit that inde the overal run time of the algorithm , in thi type of recurs tree with decreas work per level , is domin by the root .
the overal amount of work is onli a constant factor larger than the work done and mere at level zero of the tree .
let's move on to the final and most challeng part of the proof , the final case .
in case three we assum that a is bigger than b to the d .
so in conceptu term , we're assum the rate at which sub problem prolifer is exceed the rate at which we do less work per sub problem .
so these ar recurs tree where the amount of work is increas with each level , with the most work be done at the leav .
and onc again , us the basic sum fact , we can make precis the hope that , in fact , we onli have to worri about the leav .
we can throw awai the rest of work , lose onli a constant factor .
so to see that , you will again denot thi ratio between a and b to the d as r .
and in thi case r is bigger that on .
so thi sum is a sum of a bunch of power of r were r is bigger than on , what did we just learn about that two slide ago in the basic sum fact , we learn that such sum ar domin by the largest and last term of the sum .
okai so the bound it by a constant factor time the largest term .
therefor , we can we can simplifi the express star to the follow .
i'm gonna write it in term of big o notat .
and , like , on the last slide , i'll us it to suppress two differ constant .
on the on hand , i'm gonna be suppress the constant c , which we inherit wai back when from the origin recurr .
and on the other hand , i'm gonna us it to also suppress thi constant that come from the basic sum fact .
so ignor those two constant , what do we have left ?
we have n to the d .
time the largest term of the sum .
so what is the largest term of the sum ?
well , it's the last on so we plug in the biggest valu of j that we're ever go to see .
so what's the biggest valu of j that we're ever go to see ?
we'll it's just thi .
log base b of n .
so , we get the ratio a over b to the d , rais to the log base b of n .
power .
now don't despair how messi thi look .
we can do some remark simplif .
so what i want to do next is i want to focu just on thi on over b to the d , rais to the log base b of n term .
so that's go to be .
you can write that as b to the minu d log base b of n .
which if i factor thi expon into two success part i can write thi as b rais to the log base b of n power .
and onli then rais to the minu d .
and now of cours what happen is that take the logarithm of n base b , follow by take , rais it to the b power , those ar invers oper that cancel , so that leav us just with the n .
so thi result in a end to the minu d .
and now remark thi end to the minu d is just gonna cancel out with thi end to the d .
leav us with mere .
a , rais the log base b event .
and thu , out of thi crazi sea of letter , rise a formula we can actual understand .
so a to the log base b of n , if we step back and pick for a minut , is actual a supernatur quantiti .
describ someth about the recurs tree that we alreadi knew wa suppos to pop up in the analysi .
i'll let , i'll let you think through exactli what that is in the follow quiz .
so the correct answer to thi quiz is the fourth respons .
a rais to the logarithm event is precis the number of leav of the recurs tree .
and rememb in our intuit for case three , recurs tree where the amount of work is increas per level , we thought that perhap the work would be domin by the work done at the leav which is as proport as the number of leav .
so why is thi the answer ?
well just rememb what recurs tree look like at level zero .
we have a singl node , and then with each level we have eight time as mani node as befor .
that is , with each level of the recurs tree , the number of node goe up by a factor of a .
how far doe thi , how long doe thi process go on ?
well , it goe on until we reach down the , the leav .
recal that in the input size start at n up at the root .
it get divid by a factor of b each time , and it termin onc we get down to on .
so the leav presid at precis level log base b of n .
so therefor .
the number of leav is just a branch factor which is a rais to the number of time that we actual multipli by a which is just the number of level which is log base b n .
so each time we go down a level we increas the number of node by a factor of a and we go down a level log base b of n time .
leav us with a number of leav equal to a rais to the log base b of n .
so what we've done is we've mathemat confirm , in a veri cool wai , our intuit about what case three should look like in the master method .
we've proven that in case three when a is .
bigger than b to the d .
the run time is , o of the number of leav in the recurs tree , just as the intuit predict .
but , thi leav us with on final mysteri .
if you go back to the statement of the master method , we didn't sai , a to the log base b of n .
in case three , it sai the run time is , n to the log base b of a .
and , not onli that , we've us thi case three formula over and over again , to evalu gauss's recurs algorithm for integ multipl , to evalu the strassen's matrix multipl algorithm , and so on .
so , what's the stori ?
how come we're not get the same thing , as in the statement of the master method ?
well there's a veri simpl explan , which is simpli that , believ it or not .
a log base b of n , and n to the log base b of a .
ar exactli the same thing .
thi look like the kind of mistak you'd make in freshmen algebra .
but actual , if you think about it , these ar simpli the same quantiti .
if you don't believ me , just take the logarithm base b of both side , and it'll give the same thing in both side .
now , you might well be wonder why i didn't just state in the master method that the run time of case three is thi veri sensibl and meaning express , a rais log base b of n , i . e . , the number of leav in the recurs tree .
well , it turn out that while thi express on the left hand side is the more meaning conceptu .
the right hand side .
n .
to the log base b .
of a .
is the easiest on to appli .
so recal when we work through a bunch of exampl , of the master method , thi right hand side wa super conveni , when we evalu the run time of out rhythm .
when we plug in the number of a .
and b .
in ani case , whether or not you want to think about the run time in case three as proport to the number of leav in the tree or as proport at the end of the log base b of a , we're done .
we've prove it .
that's case three .
that wa the last on .
so we're done with the master method .
qed .
so that wa a lot of hard work for do the master method and i would never expect someon to be abl to regurgit all of the detail of thi proof you know it's someth like a cocktail parti well mayb except the nerdiest of all cocktail parti but i do think there's a few high level conceptu point of thi proof that ar worth rememb in the long term , so we start by just write down a recurs tree for the recurs algorithm and in a gener wai .
and go level by level , we count up the work done by the algorithm .
and thi part of the proof had noth to with how a and b relat to each other .
then we recogn that there ar three fundament differ type of recurs tree .
those with the same amount of work per level , those where it increas with the level , and those where it decreas with the level .
if you can rememb that , you can even rememb what the run time of the three case should be .
in the case where you do the same amount of everi work at each level .
we know there's a logarithm number of level .
we know we do end in d work at the root .
so that give us the run time in case on had end the dai you log in .
when the amount of work is decreas with the level , we now know that the rout domin .
up to a constant , we can throw out the rest of the level , and we know end of the d work get done at the root , so that's the overal run time .
and in the third case , where it's increas in the level , the leav domin .
the number of leav is a rais to the log base of b of n , and that's the same as n , the log base b of a .
and that's proport to run time in case three of the master method .
so now we come to on of my favorit sequenc of lectur , where we go to discuss the famou quicksort algorithm .
if you ask profession comput scientist and profession programm to draw up a list of their top five , top ten favorit algorithm , i'll bet you'd see quicksort on mani of those , those peopl' list .
so , why is that ?
after all , we've alreadi discuss sort .
we alreadi have a quit good and practic sort algorithm , mainli the merg sort algorithm .
well , quicksort , in addit to be veri practic , it's competit with , and often superior to , merg sort .
so , in addit to be veri practic , and us all the time in the real world , and in program librari , it's just a extrem eleg algorithm .
when you see the code , it's just so succinct .
it's so eleg , you just sorta wish you had come up with it yourself .
moreov , the mathemat analysi which explain why quicksort run so fast , and that mathemat analysi , we'll cover in detail , is veri slick .
so it's someth i can cover in just about half an hour or so .
so more precis what we'll prove about the quicksort algorithm is that a suitabl random implement run in time n log n on averag .
and i'll tell you exactli what i mean by on averag , later on in thi sequenc of lectur .
and , moreov , the constant hidden in the big oh notat ar extrem small .
and , that'll be evid from the analysi that we do .
final , and thi is on thing that differenti quicksort from the merg sort algorithm , is it oper in place .
that is , it need veri littl addit storag , beyond what's given in the input arrai , in order to accomplish the goal of sort .
essenti , what quicksort doe is just repeat swap within the space of the input arrai , until it final conclud with a sort version of the given arrai .
the final thing i want to mention on thi first slide is that , unlik most of the video , thi set of the video will actual have an accompani set of lectur note , which i've post on , in pdf , from the cours websit .
those ar larg , redund .
thei're option , but if you want anoth treatment of what i'm gonna discuss , a written treatment , i encourag you to look at the lectur note , on the cours websit .
so , for the rest of thi video , i'm gonna give you an overview of the ingredi of quicksort , and what we have to discuss in more detail , and the rest of the lectur will give detail of the implement , as well as the mathemat analysi .
so let's begin by recal the sort problem .
thi is exactli the same problem we discuss back when we cover merg sort .
so we're given as input an arrai of n number in arbitrari order .
so , for exampl , perhap the input look like thi arrai here .
and then what do we gotta do ?
we just gotta output a version of these same number but in increas order .
like when we discuss merg sort , i'm gonna make a simplifi assumpt just to keep the lectur as simpl as possibl .
name i'm go to assum the input arrai ha no duplic .
that is , all of the entri ar distinct .
and like with the merg sort , i encourag you to think about how you would alter the implement of quicksort so that it deal correctli with ti , with duplic entri .
to discuss how quicksort work at a high level , i need to introduc you to the kei subroutin , and thi is realli the , kei great idea in quicksort , which is to us a subroutin which partit the arrai around a pivot element .
so what doe thi mean ?
well , the first thing you gotta do is , you gotta pick on element in your arrai to act as a pivot element .
now eventu we'll worri quit a bit about exactli how we choos thi magic pivot element .
but for now you can just think of it that we pluck out the veri first element in the arrai to act as the pivot .
so , for exampl , in the input arrai that i mention on the previou slide , we could just us <num> as the pivot element .
after you've chosen a pivot element , you then re arrang the arrai , and re arrang it so that everi , all the element which come to the left of the pivot element ar less than the pivot , and all the element which come after the pivot element ar greater than the pivot .
so for exampl , given thi input arrai , on legitim wai to rearrang it , so that thi hold , is the follow .
perhap in the first two entri , we have the <num> and the <num> .
then come the pivot element .
and then come the element <num> through <num> in some perhap jingl order .
so notic that the element to the left of the pivot , the <num> and the <num> , ar inde less than the pivot , which is <num> .
and the five element to the right of the pivot , to the right of the <num> , ar inde all greater than <num> .
notic in the partit subroutin , we do not insist that we get the rel order correct amongst those element less than the pivot , or amongst those element bigger than the pivot .
so , in some sens , we're do some kind of partial sort .
we're just bucket the element of the arrai into on bucket , those less than the pivot , and then a second bucket , those bigger than the pivot .
and we don't care about , get right the order amongst each , within each of those two bucket .
so , partit is certainli a more modest goal than sort , but it doe make progress toward sort .
in particular , the pivot element itself wind up in it right posit .
that is , the pivot element wind up where it should be in the final sort version of the arrai .
you'll notic in the exampl , we chose as the pivot the third largest element , and it doe , inde , wind up in the third posit of the arrai .
so , more gener , where should the pivot be in the final sort version ?
well , it should be to the right of everyth less than it .
it should be to the left of everyth bigger than it .
and that's exactli what partit doe , by definit .
so , why is it such a good idea to have a partit subroutin ?
after all , we don't realli care about partit .
what we want to do is sort .
well , the point is that partit can be done quickli .
it can be done in linear time .
and it's a wai of make progress toward have a sort version of an arrai .
and it's gonna enabl a divid and conquer approach toward sort the input arrai .
so , in a littl bit more detail , let me tell you about two cool fact about the partit subroutin .
i'm not gonna give you the code for partit here .
i'm gonna give it to you on the next video .
but , here ar the two salient properti of the partit subroutin , discuss in detail in the next video .
so the first cool fact is that it can be implement in linear , that , is o n time , where n is the size of the input arrai , and moreov , not just linear time but linear time with essenti no extra overhead .
so we're gonna get a linear time of mutat , where all you do is repeat swap .
you do not alloc ani addit memori .
and that's kei to the practic perform of the quicksort algorithm .
secondli , it cut down the problem size , so it enabl the divid and conquer approach .
name , after we've partit an arrai around some pivot element , all we have to do is recurs sort the element that lie on the left of the pivot .
and recurs sort the element that lie on the right of the pivot .
and then , we'll be done .
so , that lead us to the high level descript of the quicksort algorithm .
befor i give the high level descript , i should mention that thi , algorithm wa discov by , toni hoar , roughli , <num> or so .
thi wa at the veri begin of hoar's career .
he wa just about <num> , <num> year old .
he went on to do a lot of other contribut , and , eventu wound up win the highest honor in comput scienc , the acm ture award , in <num> .
and when you see thi code , i'll bet you feel like you wish you had come up with thi yourself .
it's hard not to be enviou of the inventor of thi veri eleg quicksort algorithm .
so , just like in merg sort , thi is gonna be a divid and conquer algorithm .
so it take an arrai of some length n , and if it's an arrai of length n , it's alreadi sort , and that's the base case and we can return .
otherwis we're gonna have two recurs call .
the big differ from merg sort is that , wherea in merg sort , we first split the arrai into piec , recours , and then combin the result , here , the recurs call come last .
so , the first thing we're go to do is choos a pivot element , then partit the arrai around that pivot element , and then do two recurs call .
and then , we'll be done .
there will be no combin step , no merg step .
so in the gener case , the first thing you do is choos a pivot element .
for the moment i'm go to be loos , leav the choosepivot subroutin unimpl .
there's go to be an interest discuss about exactli how you should do thi .
for now , you just do it in some wai , that for somehow you come up with on pivot element .
for exampl , a naiv wai would be to just choos the first element .
then you invok the partit subroutin that we'll discuss in the last coupl slide .
so recal that the result in a version of the arrai in which the pivot element p is in it right posit , everyth to the left of p is less than p , everyth to the right of the pivot is bigger than the pivot , and then all you have to do to finish up is recurs on both side .
so let's call the element less than p the first part of the partit arrai , and the element greater than p the second part of the recurs arrai .
and now we just call quicksort again to recurs sort the first part , and then the , recurs sort the second part .
and that is it .
that is the entir quicksort algorithm at the high level .
thi is on of the rel rare recurs , divid and conquer algorithm that you're go to see , where you liter do no work after solv the sub problem .
there is no combin step , no merg step .
onc you've partit , you just sort the two side and you're done .
so that's the high level descript of the quicksort algorithm .
let me give you a quick tour of what the rest of the video's go to be about .
so first of all i ow you detail on thi partit subroutin .
i promis you it can be implement in linear time with no addit memori .
so i'll show you an implement of that on the next video .
we'll have a short video that formal prove correct of the quicksort algorithm .
i think most of you will kinda see intuit why it's correct .
so , that's a video you can skip if you'd want .
but if you do want to see what a formal proof of correct for a divid and conquer algorithm look like , you might want to check out that video .
then , we'll be discuss exactli how the pivot is chosen .
it turn out the run time of quicksort depend on what pivot you choos .
so , we're gonna have to think carefulli about that .
then , we'll introduc random quicksort , which is where you choos a pivot element uniformli at random from the given arrai , hope that a random pivot is go to be pretti good , suffici often .
and then we'll give the mathemat analysi in three part .
we'll prove that the quicksort algorithm run in n log n time , with small constant , on averag , for a randomli chosen pivot .
in the first analysi video , i'll introduc a gener decomposit principl of how you take a complic random variabl , break it into indic random variabl , and us linear of expect to get a rel simpl analysi .
that's someth we'll us a coupl more time in the cours .
for exampl , when we studi hash .
then , we'll discuss sort of the kei insight behind the quicksort analysi , which is about understand the probabl that a given pair of element get compar at some point in the algorithm .
that'll be the second part .
and then there's go to be some mathemat comput just to sort of tie everyth togeth and that will give us the bound the quicksort run time .
anoth video that's avail is a review of some basic probabl concept for those of you that ar rusti , and thei will be us in the analysi of quicksort .
okai ?
so that's it for the overview , let's move on to the detail .
the goal of thi video is to provid more detail about the implement of the quicksort algorithm and , in particular , if you're ever go to drill down on the kei partit subroutin , just let me remind you what the job of the partit subroutin is in the context of sort an arrai .
so recal that kei idea in quicksort is to partit the input arrai around a pivot element .
so thi ha two step .
first , you somehow choos a pivot element , and in thi video , we're not go to worri about how you choos the pivot element .
for concret , you might just want to think about you pick the first element in the arrai to serv as your pivot .
so in thi exampl arrai , the first element happen to be <num> , so we can choos <num> as the pivot element .
now , there's a kei rearrang step .
so you rearrang the arrai so that it ha the follow properti .
ani entri that ar to the left of the pivot element should be less than the pivot element .
wherea ani entri , which ar to the right of the pivot element , should be greater than the pivot element .
so , for exampl , in thi , version of , the second version of the arrai , we see to the left of the <num> is the <num> and the <num> .
thei're in revers order , but that's okai .
both the <num> and the <num> ar to the left of the <num> , and thei're both less than <num> .
and the five element to the right of the <num> , thei're jumbl up , but thei're all bigger than the pivot element .
so , thi is a legitim rearrang that satisfi the partit properti .
and , again , recal that thi definit make partial progress toward have a sort arrai .
the pivot element wind up in it right posit .
it wind up where it's suppos to be in the final sort arrai , to the right of everyth less than it , to the left of everyth bigger than it .
moreov , we've correctli bucket the other n <num> element to the left and to the right of the pivot accord to where thei should wind up in the final sort arrai .
so that's the job , that the partit subroutin is respons for .
now what's cool is we'll be abl to implement thi partit subroutin in linear time .
even better , we'll be abl to implement it so that all it doe , realli , is swap in the arrai .
that is , it work in place .
it need no addit , essenti constant addit memori , to rearrang the arrai accord to those properti .
and then , as we saw on the high level descript of the quicksort algorithm , what partit doe is , it enabl a divid and conquer approach .
it reduc the problem size .
after you've partit the arrai around the pivot , all you gotta do is recurs on the left side , recurs on the right side , and you're done .
so , what i ow you is thi implement .
how do you actual satisfi the partit properti , stuff to the left of the pivot is smaller than it , stuff to the right of the pivot is bigger than it , in linear time , and in place .
well , first , let's observ that , if we didn't care about the in place requir , if we were happi to just alloc a second arrai and copi stuff over , it would actual be pretti easi to implement a partit subroutin in linear time .
that is , us o n extra memori , it's easi to partit around a pivot element in o n time .
and as usual , you know , probabl i should be more precis and write theta of n , ar us in case that would be the more accur stronger statement , but i'm go to be sloppi and i'm just go to write the weaker but still correct statement , us big oh , okai ?
so o n time us linear extra memori .
so how would you do thi ?
well let me just sort of illustr by exampl .
i think you'll get the idea .
so let's go back to our run exampl of an input arrai .
well , if we're allow to us linear extra space , we can just prealloc anoth arrai of length n .
then we can just do a simpl scan through the input arrai , bucket element accord to whether thei ar bigger than or less than the pivot .
and , so for exampl , we can fill in the addit arrai both from the left and the right , us element that ar less than or bigger than the pivot respect .
so for exampl we start with the <num> , we know that the <num> is bigger than the pivot , so you put that at the end of the output arrai .
then we get to the <num> .
the <num> is less than the pivot , so that should go on the left hand side of the output arrai .
when you get to the <num> , it should go on the right hand side , and the <num> should go on the left hand side , and so on .
when we complet our scan through the input arrai , there'll be on hole left , and that's exactli where the pivot belong , to the right of everyth less than it , to the left of everyth bigger than it .
so , what's realli interest , then , is to have an implement of partit , which is not mere linear time , but also us essenti no addit space .
it doesn't re sort to thi cop out of pre alloc an extra arrai of length n .
so , let's turn to how that work .
first , start at a high level , then fill in the detail .
so i'm gonna describ the partit subroutin onli for the case where the pivot is in fact the first element .
but realli thi is without loss of gener .
if , instead , you want to us some pivot from the middl of the arrai , you can just have a preprocess step that swap the first element of the arrai with the given pivot , and then run the subroutin that i'm about to describ , okai .
so with constant time preprocess , the case of a gener pivot reduc to the case of when the pivot is the first element .
so here's the high level idea , and it's veri cool .
the idea is , we're gonna be abl to abl to get awai with just a singl linear scan of the input arrai .
so in ani given moment in thi scan , there's just gonna be a singl for loop , we'll be keep track of both the part of the arrai we've look at so far , and the part that we haven't look at so far .
so there's gonna be two group , what we've seen , what we haven't seen .
then within the group we've seen , we're gonna have definit split further , accord to the element that ar less than the pivot and those that ar bigger than the pivot .
so we're gonna leav the pivot element just hang out in the first element of the arrai until the veri end of the algorithm , when we correct it posit with a swap .
and at ani given snapshot of thi algorithm , we will have some stuff that we've alreadi look at , and some stuff that we haven't yet look at in our linear scan .
of cours , we have no idea what's up with the element that we haven't look at yet , who know what thei ar , and whether thei're bigger or less than the pivot .
but , we're gonna implement the algorithm , so , among the stuff that we've alreadi seen , it will be partit , in the sens that all element less than the pivot come first , all element bigger than the pivot come last .
and , as usual , we don't care about the rel order , amongst element less than the pivot , or amongst element bigger than the pivot .
so summar , we do a singl scan through the input arrai .
and the trick will be to maintain the follow invari throughout the linear scan .
but basic , everyth we have look at the input arrai is partit .
everyth less than the pivot come befor everyth bigger than the pivot .
and , we wanna maintain that invari , do onli constant work , and no addit storag , with each step of our linear scan .
so , here's what i'm gonna do next .
i'm gonna go through an exampl , and execut the partit subroutin on a concret arrai , the same input arrai we've been us as an exampl , thu far .
now , mayb it seem weird to give an exampl befor i've actual given you the algorithm , befor i've given you the code .
but , do it thi wai , i think you'll see the gist of what's go on in the exampl , and then when i present the code , it'll be veri clear what's go on .
wherea , if i present the code first , it mai seem a littl opaqu when i first show you the algorithm .
so , let's start with an exampl .
throughout the exampl , we wanna keep in mind the high level pictur that we discuss in the previou slide .
the goal is that , at ani time in the partit subroutin , we've got the pivot hang out in the first entri .
then , we've got stuff that we haven't look at .
so , of cours , who know whether those element ar bigger than or less than the pivot ?
and then , for the stuff we've look at so far , everyth less than the pivot come befor everyth bigger than the pivot .
thi is the pictur we wanna retain , as we go through the linear scan .
as thi high level pictur would suggest , there is two boundari that we're gonna need to keep track of throughout the algorithm .
we're gonna need to keep track of the boundari between what we've look at so far , and what we haven't look at yet .
so , that's go to be , we're go to us the index j to keep track of that boundari .
and then , we also need a second boundari , for amongst the stuff that we've seen , where is the split between those less than the pivot and those bigger than the pivot .
so , that's gonna be i .
so , let's us our run exampl arrai .
so stuff is pretti simpl when we're start out .
we haven't look at anyth .
so all of thi stuff is unpartit .
and i and j both point to the boundari between the pivot and all the stuff that we haven't seen yet .
now to get a run time reach linear , we want to make sure that at each step we advanc j , we look at on new element .
that wai in a linear number of step , we'll have look at everyth , and hopefulli we'll be done , and we'll have a partit arrai .
so , in the next step , we're go to advanc j .
so the region of the arrai which is , which we haven't look at , which is unpartit , is on smaller than befor .
we've now look at the <num> , the first element after the pivot .
now the <num> itself is inde a partit arrai .
everyth less than the pivot come befor , everyth after the pivot turn out there's noth less than the pivot .
so vacuous thi is inde partit .
so j record delin the boundari between what we've look at and what we haven't look at , i delin amongst the stuff we've look at , where is the boundari between what's bigger than and what's less than the pivot .
so the <num> is bigger than the pivot , so i should be right here .
okai , becaus we want i to be just to the left of all the stuff bigger than the pivot .
now , what's gonna happen in the next iter ?
thi is where thing get interest .
suppos we advanc j on further .
now the part of the arrai that we've seen is an <num> follow by a <num> .
now an <num> and a <num> is not a partit subarrai .
rememb what it mean to be a partit subarrai ?
all the stuff less than the pivot , all the stuff less than <num> , should come befor everyth bigger than <num> .
so <num> , <num> obvious fail that properti .
<num> is less than the pivot , but it come after the <num> , which is bigger than the pivot .
so , to correct thi , we're go to need to do a swap .
we're go to swap the <num> and the <num> .
that give us the follow version of the origin arrai .
so now the stuff that we have not yet look at is on smaller than befor .
we've advanc j .
so all other stuff is unpartit .
who know what's go on with that stuff ?
j is on further entri to the right than it wa befor , and at least after we have done thi swap , we do inde have a partit arrai .
so post swap , the <num> and the <num> , ar inde partit .
now rememb , i delin the boundari between amongst what we've seen so far , the stuff less than the pivot , less than <num> in thi case , and that bigger than <num> , so i is go to be wedg in between the <num> and the <num> .
in the next iter , our life is pretti easi .
so , in thi case , in advanc j , we uncov an element which is bigger than the pivot .
so , thi is what happen in the first iter , when we uncov the <num> .
it's differ than what happen in the last iter when we uncov the <num> .
and so , thi case , thi third iter is gonna be more similar to the first iter than the second iter .
in particular , we won't need to swap .
we won't need to advanc i .
we just advanc j , and we're done .
so , let's see why that's true .
so , we've advanc j .
we've done on more iter .
so , now the stuff we haven't seen yet is onli the last four element .
so , who know what's up with , the stuff we haven't seen yet ?
but if you look at the stuff we have seen , the <num> , the <num> , and the <num> , thi is , in fact , partit , right ?
all the number that ar bigger than <num> succe , come after , all the number smaller than three .
so the j , the boundari between what we've seen and what we haven't is between the <num> and the <num> ; and the i , the boundari between the stuff less than the pivot and bigger than the pivot is between the <num> and the <num> , just like it wa befor .
ad a <num> to the end didn't chang anyth .
so let's wrap up thi exampl in the next slide .
so first , let's just rememb where we left off from the previou slide .
so i'm just gonna redraw that same step after three iter of the algorithm .
and notic , in the next gener , we're go to , again , have to make some modif to the arrai , if we want preserv our variant .
the reason is that when we advanc j , when we scan thi <num> , now again we're scan in a new element which is less than the pivot , and what that mean is that , the partit region , or the region that we've look at so far , will not be partit .
we'll have <num> .
rememb we need everyth less than <num> to preced everyth bigger than <num> , and thi <num> at end is not go to cut it .
so we're go to have to make a swap .
now what ar we go to swap ?
we're go to swap the <num> and the <num> .
so , why do we swap the <num> and the <num> ?
well , clearli , we have to swap the <num> with someth .
and , what make sens ?
what make sens is the left most arrai entri , which is current bigger than the pivot .
and , that's exactli the <num> .
okai , that's the first , left most entri bigger than <num> , so if we swap the <num> with it , then the <num> will becom the right most entri smaller than <num> .
so after the swap , we're gonna have the follow arrai .
the stuff we haven't seen is the <num> , the <num> , and the <num> .
so the j will be between the <num> and the <num> .
the stuff we have seen is the <num> , <num> , <num> , and <num> .
and notic , that thi is inde partit .
all the element , which ar less than <num> , the <num> and the <num> , preced all of the entri , which ar bigger than <num> , the <num> and the <num> .
i , rememb , is suppos to split , be the boundari between those less than <num> and those bigger than <num> .
so , that's gonna lie between the <num> and the <num> .
that is on further to the right than it wa in the previou iter .
okai , so the , becaus the rest of the unseen element , the <num> , the <num> , and the <num> , ar all bigger than the pivot , the last three iter ar easi .
no further swap ar necessari .
no increment to i ar necessari .
j is just go to get increment until we fall off the arrai .
and then , fast forward , the partit subroutin , or thi main linear scan , will termin with the follow situat .
so at thi point , all of the element have been seen , all the element ar partit .
j in effect ha fallen off the end of the arrai , and i , the boundari between those less than and bigger than the pivot , still li between the <num> and the <num> .
now , we're not quit done , becaus the pivot element <num> is not in the correct place .
rememb , what we're aim for is an arrai where everyth less than the pivot is to the left of it , and everyth bigger than the pivot is to the right .
but right now , the pivot still is hang out in the first element .
so , we just have to swap that into the correct place .
where's the correct place ?
well , it's go to be the right most element , which is smaller than the pivot .
so , in thi case , the <num> .
so the subroutin will termin with the follow arrai , <num> .
and , inde , as desir , everyth to the left of the pivot is less than the pivot , and everyth to the right of the pivot is bigger than the pivot .
the <num> and <num> happen to be in sort order , but that wa just sorta an accid .
and the <num> , <num> , <num> and <num> and <num> , you'll notic , ar jumbl up .
thei're not in sort order .
so hopefulli from thi exampl you have a gist of how the partit subroutin is go to work in gener .
but , just to make sure the detail ar clear , let me now describ the pseudocod for the partit subroutin .
so the wai i'm go to denot it is , there's go to be an input arrai a .
but rather than be told some explicit link , what's go to be pass to the subroutin ar two arrai indic .
the leftmost index , which delin thi part of the separ you're suppos to work on , and the rightmost index .
the reason i'm write it thi wai is becaus partit is go to be call recurs from within a quicksort algorithm .
so ani point in quicksort , we're go to be recurs on some subset , contigu subset of the origin input arrai .
l el and r meant to denot what the left boundari and the right boundari of that subarrai ar .
so , let's not lose sight of the high level pictur of the invari that the algorithm is meant to maintain .
so , as we discuss , we're assum the pivot element is the first element , although that's realli without loss of gener .
at ani given time , there's gonna be stuff we haven't seen yet .
who know what's up with that ?
and , amongst the stuff we've seen , we're gonna maintain the invari that all the stuff less than the pivot come befor all the stuff bigger than the pivot .
and j and i denot the boundari , between the seen and the unseen , and between the small element and the larg element , respect .
so back to the pseudocod , we initi the pivot to be the first entri in the arrai .
and again rememb , l denot the leftmost index that we're respons for look at .
initi valu of i , should be just to the right of the pivot so that's gonna be el <num> .
that's also the initi valu of j , which will be assign in the main for loop .
so thi for loop with j , take on all valu from el <num> to the rightmost index r , denot the linear scan through the input arrai .
and , what we saw in the exampl is that there were two case , depend on , for the newli seen element , whether it's bigger than the pivot , or less than the pivot .
the easi case is when it's bigger than the pivot .
then we essenti don't have to do anyth .
rememb , we didn't do ani swap , we didn't chang i , the boundari didn't chang .
it wa when the new element wa less than the pivot that we had to do some work .
so , we're gonna check that , is the newli seen element , a , less than p .
and if it's not , we actual don't have to do anyth .
so let me just put as a comment .
if the new element is bigger than the pivot , we do noth .
of cours at the end of the for loop , the valu of j will get in command so that's the onli thing that chang from iter to iter , when we're suck up new element that happen to be bigger than p .
so what do we do in the exampl , when we suck up our new element less than p ?
well we have to do two thing .
so , in the event that the newli seen element is less than p , i'll circl that here in pink .
we need to do a rearrang , so we , again , have a partit , sub arrai amongst those element we've seen so far .
and , the best wai to do that is to swap thi new element with the left most element that's bigger than the pivot .
and becaus we have an index i , which is keep track of the boundari between the element less than the pivot and bigger than the pivot , we can immedi access the leftmost element bigger than the pivot .
that's just the i th entri in the arrai .
now i am do someth a littl sneaki here , i should be honest about .
which is there is the case where you haven't yet seen ani element bigger than the pivot , and then you don't actual have a leftmost element bigger than the pivot to swap with .
turn out thi code still work , i'll let you verifi that , but it doe do some redund swap .
realli , you don't need to do ani swap until you first see some element bigger than the pivot , and then see some element less than the pivot .
so , you can imagin a differ limit of thi , where you actual keep track of whether or not that's happen to avoid the redund swap .
i'm just gonna give you the simpl pseudocod .
and again , for intuit , you wanna think about the case just like , in the pictur here in blue , where we've alreadi seen some element that ar bigger than the pivot , and the next newli seen element is less than the pivot .
that's realli sort of the kei case here .
now the other thing we have to do after on of these swap is , now the boundari , between where the arrai element less than the pivot and those bigger than the pivot , ha move .
it's move on to the right , so we have to increment i .
so , that's the main linear scan .
onc thi conclud , j will have fallen off the end of the arrai .
and , everyth that we've seen the final element , except for the pivot , will be arrang so that those less than p ar first , those bigger than p will be last .
the final thing we have to do is just swap the pivot into it right posit .
and , recal for that , we just swap it with the right most element less than it .
so , that is it .
that is the partit subroutin .
there's a number of variant of partit .
thi is certainli not the uniqu implement .
if you look on the web , or if you look in certain textbook , you'll find some other implement as well as discuss of the variou merit .
but , i hope thi give you , i mean , thi is a canon implement , and i hope it give you a clear pictur of how you rearrang the arrai us in place swap to get the desir properti , that all the stuff befor the pivot come first , all the stuff after the pivot come last .
let me just add a few detail about why thi pseudocod i just gave you doe , inde , have the properti requir .
the run time is o n , realli theta of n , but again , i'll be sloppi and write o n .
where n is the number of arrai element that we have to look at .
so , n is r el <num> , which is the length of the sub arrai that thi partit subroutin is invok upon .
and why is thi true ?
well if you just go inspect the pseudocod , you can just count it up naiv and you'll find that thi is true .
we just do a linear scan through the arrai and all we do is basic a comparison and possibl a swap and an increment for each arrai entri that we see .
also , if you inspect the code , it is evid that it work in place .
we do not alloc some second copi of an arrai to popul , like we did in the naiv partit subroutin .
all we do is repeat swap .
correct of the subroutin follow by induct , so in particular the best wai to argu it is by invari .
so i'll state the invari here , but mostli leav it for you to check that inde , everi iter of the for loop maintain thi invari .
so first of all , all of the stuff to the right of the pivot element , to the right of the leftmost entri and up to the index i , is inde less than the pivot element , as suggest by the pictur .
and also suggest by the pictur , everyth begin with the i th entri , lead just up befor the j th entri , is bigger than the pivot .
and i'll leav it as a good exercis for you to check that thi hold by induct .
the invari hold initi , when both i and j ar equal to el <num> , becaus both of these set ar vacuou , okai ?
so , there ar no such element , so thei're trivial satisfi these properti .
and then , everi time we advanc j , well , in on case it's veri easi , where the new element is bigger than the pivot .
it's clear that , if the invari held befor , it also hold at , at the next iter .
and then , if you think about it carefulli , thi swap in thi increment of i that we do , in the case where the new element is less than the pivot .
after the swap , onc the fold is complet , again if thi invari wa true at the begin of it , it's also true at the end .
so what good is that ?
well , by thi claim , at the conclus of the linear scan at which point j ha fallen off the end of the arrai , the arrai must look like thi .
at the end of the for loop , the question mark part of the arrai ha vanish , so everyth other than the pivot ha been organ so that all thi stuff less than the pivot come befor everyth after the pivot , and that mean onc you do the final swap , onc you swap the pivot element from it first and left most entri , with the right most entri less than the pivot , you're done .
okai ?
you've got the desir properti that everyth to the left of the pivot is less than , and everyth to the right of the pivot is bigger than .
so now that given a pivot element we understand how to veri quickli rearrang the arrai so that it's partit around that pivot element , let's move on to understand how that pivot element should be chosen and how , given suitabl choic of that pivot element , we can implement the quicksort algorithm , to run veri quickli , in particular , on averag in o n log time .
i just got the number of divid and conquer algorithm and , so far , i've been short shrift to proof of correct .
thi ha been a conscienc decis on my part .
come up with the right divid and conquer algorithm for a problem can definit be difficult , but onc you have that eureka moment and you figur out the right algorithm you tend to , also , have a good understand of why it's correct , why it actual solv the problem on everi possibl input .
similarli when i present to you a divid and conquer algorithm like , sai , merg sort or quicksort , i expect that mani of you have a good and accur intuit about why the algorithm is correct .
in contrast the run time of these develop algorithm is often highli non obviou .
so , correct proof for divid and conquer algorithm tend to simpli formal the intuit that you have via a proof by induct .
that's why i haven't been spend much time on them .
but nevertheless , i do feel like i ow you at least on rigor correct proof for a divid and conquer algorithm , and we mai as well do it for quicksort .
so in thi option video , we'll briefli review proof by induct , and then we'll show how such a proof can be us to rigor establish the correct of quicksort .
the correct proof for most of the other divid and conquer algorithm that we discuss can be formal in a similar wai .
so let's begin by review the format for proof by induct .
so , the canon proof by induct and the kind that we'll be us here , is when you want to establish an assert for all of the posit integ in .
so now it's some assert which is parameter by n , where n is a posit integ .
i know thi is a littl abstract , so let me just be concret about the assert that we actual care about for quicksort .
so for us , the assert p n is the statement that cor , quicksort is alwai correct on input of length n , arrai that have n element .
so an induct proof ha two part .
the first part is a base case and the second part is an induct step .
for the base case you have to get start so you show that at the veri least your assert is true when n equal on .
thi is often a trivial matter and that'll be the case when we establish the correct of quick sort .
just on our rai with onli on element .
so , the non trivial part of a proof by induct is usual the induct step .
and in the induct step , you look at a valu of n not cover by the base case , so a valu of n bigger than on .
and you show that if the assert hold for all smaller valu , small integ , then it also hold for the integ n .
that is , you show that for everi posit integ n that's two or greater , you assum that p of k hold for all k strictli less than n .
and under that assumpt , which is call the induct hypothesi .
under the assumpt that p of k hold for all k strictli less than n , you then establish that p of n hold as well .
so if you manag to complet both of these step , if you prove both the base case that p <num> hold , you argu that directli , and then also you argu that assum the induct hypothesi , that the assert hold for all smaller integ , it also hold for an arbitrari integ n .
then you're done .
then in fact you have proven that the assert p then hold for everi singl posit integ n .
right ?
so for ani given n that you care about , the wai you can deriv that from on and two is you just start from the base case , p of on hold .
then you appli the induct step n minu on time .
and boom , you've got it .
so you know that p hold for the integ n that you care about as well .
and that's true for arbitrarili larg valu of n .
so those ar proof by induct in gener .
now let's instanti thi proof format , thi type of proof for establish the correct of quicksort .
so let me write again what is the assert we care about .
our definit of p n is gonna be that quicksort is alwai correct on arrai of length n .
and of cours what we want to prove is that quicksort is correct no matter what size arrai that you give it , that is , we want to prove that p n hold for everi singl n at least on .
so thi is right in the wheelhous of proof by induct .
? kai , so that's how we're go to establish it .
now depend on the order in which you're watch the video , you mai or mai not have seen our discuss about how you actual choos the pivot , recal that the first thing quick sort doe is choos a pivot , then it partit the arrai around the pivot .
so , we're go establish the correct of quick sort , no matter how the choos pivot sub routin get implement .
okai , so now matter how you choos pivot , you'll alwai have correct .
as we , as we'll see in a differ video , the choic of pivot definit ha an influenc on the run of quick sort , the correct of quick sort , there's no matter how you choos the pivot .
so it's perceiv by a proof by induct .
so for the base case when n equal on , thi is a fairli trivial statement .
right ?
so , then we're just talk about input that have onli on element .
everi such arrai is alreadi sort .
quicksort , in the bai , when n equal on just return the input arrai .
it doesn't do anyth , and that is inde the sort of arrai that it return .
so , by the rather trivial argument we had directli proven that p of on hold .
we've proven the rather unimpress statement that quicksort alwai correctli sort on element arrai .
okai ?
no big deal .
so , let's move on to the induct step .
so in the induct step we have to fix an arbitrari valu of n that's at least two .
a valu of n not cover by the base case .
so let's fix some valu of n , that leav two .
now what ar we try to prove ?
we're try to prove that quick sort alwai correctli sort everi input arrai of length n .
so we also have to fix an arbitrari such input .
so let's make sure we're all clear on what it is we need to show , what do you show in an induct step .
assum that pfk hold .
for all smaller valu , all smaller integ , then p of n hold as well .
and rememb thi is the induct hypothesi .
so in the context of quicksort , we're assum that quicksort never make a mistak on ani input arrai that ha length strictli smaller than n .
and now we just have to show it never make a mistak on arrai , input arrai that have size exactli n .
so thi is the point in the proof where we actual delv into how quick sort is implement to argu correct .
so recal what the first step of quick sort is , it pick some pivot arbitrarili , we don't know how , we don't care how .
and then it partit the arrai around thi pivot element p .
now as we argu in the video where we discuss the partit sub routin , at the conclus of that sub routin , the arrai ha been rearrang into the follow format .
the pipit is wherev it is , everyth to the left of the pipit is less than the pipit , and everyth bigger than the pipit is greater than the pipit .
all right , thi is where how thing stand at the conclus of the partit sub routin .
so let's call thi stuff less than the pipit the first part of the partit arrai , and the stuff bigger than the pipit , the second part of the partit arrai .
and recal our observ from the overview video that the pivot wind up in it correct posit .
right , where would the pivot be ?
where is ani element suppos to be in the final sort arrai ?
what's suppos to be to the right of everyth less than it , and to the left of everyth bigger than it ?
and that's exactli where thi partit subroutin deposit the pivot element peak .
so now to impli the induct hypothesi , which you'll recal is a hypothesi about how quick sort oper on smaller sub arrai .
let's call the length of the first part in the second part of the partit k1 and k2 respect .
now , crucial , both k1 and k2 ar strictli less than n .
both of these two part have length strictli less than that of the given input arrai a .
that's becaus the pivot in particular is exclud from both of those two part .
so , their gonna have , at most n minu on that mean that we can appli the induct hypothesi , which sai that the quicksort never make a mistak on an arrai that ha size strictli less than n .
that impli that our two recurs call to quickstart , the on to the first part and the on to the second part don't make mistak .
thei're guarante to sort those sub arrai correctli by the induct hypothesi .
and to be veri precis , what we're us to argu that the ar correct , ar p of k1 and p of k2 .
or p is the assert that is alwai correct on a k1 and k2 .
and we know that both of these statement ar true becaus k1 and k2 ar less th , ar both less than n and becaus of the induct hypothesi .
so what's the upshot ?
the upshot is , quicksort's gonna be correct .
and so the first recurs call put all of the element that ar less than the pivot in the correct rel order .
next come the pivot , which is bigger than all of that stuff in the first part and less than all the stuff in the second part , and then the second recurs call correctli order all of the element in the second part .
so with those three thing past togeth , we have a sort version of the input arrai and sinc thi arrai wa an arbitrari on , of link n .
that establish the assert p of n and sinc n wa arbitrari , that establish the induct and complet the proof of correct of quick sort for an arbitrari method of choos the pivot element .
so let's review thi stori so far .
we've been discuss the quick sort algorithm , here again is it high level descript .
so quick sort you call two subroutin first and then you make two recurs call .
so the first subroutin , choosepivot , we haven't discuss yet at all .
that will be on of the main topic of thi video .
but the job of the choos pivot subroutin is to somehow select on of the n element in the input arrai to act as a pivot element .
now , what doe it mean to be a pivot ?
well , that come into plai in the second subroutin .
the partit subroutin , which we did discuss quit a bit in a previou video .
so what partit doe is it rearrang the element in the input arrai , so that it ha the follow properti .
so that the pivot p wind up in it right posit .
that is , it's to the right of all of the element less than it .
and it's to the left of all of the element bigger than it .
the stuff less than it's to the left in some jumbl order .
the stuff bigger than it is to the right in some jumbl order .
that's what's list here as the first part and the second part of the partit arrai .
now , onc you've done thi partit you're good to go .
you just recurs solv recurs sort the first part .
after you get them in the right order , you call quick sort again and recurs sort the right part .
and , bingo , the entir arrai is sort .
you don't need to combin step .
you don't need a step .
moreov , recal in a previou video we saw that the partit arrai can be implement in linear time and , moreov , it work in place with essenti , no addit storag .
we , also , in an option video formal prove the correct of quick sort .
and rememb , quicksort is independ of how you implement the choosepivot subroutin .
so what we're go to do now is discuss the run time of the quicksort algorithm .
and thi is where the choic of the pivot is veri import .
so what everybodi should be wonder about at thi point is , is quick sort a good algorithm ?
doe it run fast ? the bar is pretti high .
we alreadi have mergesort , which is a veri excel , practic , n log n algorithm .
the kei point to realiz at thi junctur is that we ar not current in a posit to discuss the run time of the quick sort algorithm , the reason is we do not have enough inform .
the run time of the quick sort depend crucial on how you choos the pivot .
it depend crucial on the qualiti of the pivot chosen .
you'd be right to wonder what i mean by a pivot's qualiti .
and basiclali what i mean is pivot is good if it split the partit arrai into roughli two equal size subproblem .
and it's bad , it's of low qualiti if we get veri unbalanc subproblem .
so to understand both what i mean and the ramif of have good qualiti and bad qualiti pivot , let's walk through a coupl of quiz question .
thi first quiz , quiz question is meant to explor a sort of worst case execut of the quick sort algorithm .
what happen when you choos pivot that ar veri poorli suit for the particular input arrai .
let me be more specif .
suppos we us the most naiv choos pivot implement like we were discuss in the partit video .
so , rememb , here we just pluck out the first element of the arrai and we us that as the pivot .
so suppos that's how we implement the choos pivot subroutin , and moreov , suppos that the input arrai to quicksort is an arrai that's alreadi in sort order .
so , for exampl , if we just have the number on through eight , it would be on , two , three , four , five , six , seven , eight , in order .
my question for you is , what is the run time of thi recurs quicksort algorithm on an alreadi sort arrai if we alwai us the first element of a subarrai as the pivot ?
okai so thi is a slightli tricki , but actual a veri import question .
so the answer is the fourth on .
so it turn out that quick sort , if you pass it an alreadi sort arrai and you're us the first element as pivot element , it run in quadrat time .
and rememb , for a sort algorithm , quadrat is bad .
it's bad in the sens that we can do better .
merg short run in time n log n , which is much better that n squar , and if we ar happi with an n squar run time , we wouldn't have to resort to these sort of rel exot sort algorithm .
we could just us insert sort and we'd be fine , we'd get that same quadrat run time .
okai so now i ow you an explan .
why is it that quicksort can actual run in quadrat time in thi unlucki case of be pass alreadi sort input arrai .
well , to understand , let's think about what pivot get chosen and what ar the ramif of that pivot choic for how the arrai get partit , and then what the recurs look like .
so .
let's just think of the arrai as be the number on through n in sort order .
what is gonna be our pivot ?
well , by definit we're choos the first element of the pivot , so the pivot is just gonna be on .
now we're go to invok the partit subroutin and if you go back to the pseudo code of the partit's subroutin , you'll notic that if you pass in an alreadi sort arrai , it's gonna do essenti noth .
it's just gonna advanc the n and j until it fall off the end of the arrai .
and it's just go to return back to us the same arrai that it wa path as input .
so , partit subroutin is given an alreadi sort arrai , return an alreadi sort arrai .
okai ?
so we have just a pivot on in the first posit and then the number two through n in order in the remaind of the posit .
so if we draw our usual pictur of what a , partit arrai look like , with everyth , less than the pivot , to the left , everyth bigger than the pivot , to the right .
well , sinc noth is less than the pivot , thi stuff is go to be empti .
thi will not exist , and to the right of the pivot , thi will have length , n minu on , and moreov it will still be sort .
so onc partit complet , we go back to the outer call of quick sort , which then call itself recurs twice .
now in thi case , on of the recurs call is just vacuou .
there's just an empti arrai .
there's noth to do .
so realli there's onli on recurs call , and that happen on a problem of size onli on less .
so thi is about the most unbalanc split we could possibl see .
right ?
where on side ha zero element , on side's n minu on .
that's not realli good ani wors than that .
and thi is go to keep happen over and over and over again .
we're gonna recurs on the number two through n .
we're gonna choos the first element , the two as the pivot .
again , we'll feed it to partit , we'll get back the exact same subarrai that we hand it in , we get to the number two through n in sort order .
we exclud the pivot two .
we recours on the number three through n , a subarrai of length n two .
the next recurs level , we recurs on an arrai of size , of length n three , then n four , then n five , and so on , until final , after a recurs depth of n roughli , we got down to just the last element .
n , the base case kick in and we return that and quick sort complet .
so that's how quick sort is gonna execut on thi particular input with these particular pivot choic , so what run time doe that give to us ?
well .
the first observ is that , you know in each recurs call we do have to invok the partit at sub routin , and the partit sub routin doe look at , everi element in the arrai it is pass as input .
so if we pass partit in arrai of like k , it's gonna do at least k oper cuz it look at each element at least onc .
so the run time is go to be bound below , by the work we do on the outermost call , which is on an arrai of like m .
plu the amount we do in the second level of recurs , which is on a subarrai of length n on , plu n two , plu blah , blah , blah , blah , blah , all the wai down to plu on for the veri last level of recurs .
so thi is a lower ban on our run time and thi is alreadi theta of n squar .
so on easi wai to see why thi sum , n n <num> etcetera , etcetera , lead to a bound of n squar , just focu on the first half of the term .
so the first n over two term in the sum ar all of magnitud at least n over two .
so the sum at least n squar over four .
it's also evid that thi sum is at most , n squar .
so , overal , the run time of quick sort on thi bad input is go to be quadrat .
now have understood what the worst case perform the quick sort algorithm is , let's move on to discuss it best case run time .
now we don't gener care about the best case perform of algorithm for it own case , the reason that we wanna think about quick sort in the best case , first of all it'll give us better intuit for how the algorithm work , second of all it will draw a line in the sand .
it averag case run time certainli can't be better than the best case so thi will give us a target for what we're shoot for in our subsequ mathemat analysi .
so what with the best case , what is the highest qualiti pivot we could hope for , well again .
we think of the qualiti of a pivot as the amount of balanc that it provid between the two subproblem .
so ideal , we choos a pivot which gave us two subproblem , both of size n <num> or less .
and there's a name for the .
element that would give us that perfectli balanc split .
it's the median element of the arrai , okai ?
the element where exactli half of the element ar less than it and half of the element ar bigger than it .
that would give us essenti perfect <num> <num> split of the input arrai .
so here's the question .
suppos we had some input and we ran quicksort and everyth just work out in our favor in the , magic in the best possibl wai .
that is , in everi singl recurs indic of quicksort on ani sub arrai of the origin input arrai , suppos we happen .
to get as our pivot the median element .
that is , suppos in everi singl recurs call we wind up get a perfect <num> , <num> split of the input arrai befor we recurs .
thi question ask you to analyz the run time of thi algorithm in thi magic best case scenario .
so the answer to thi third to thi question is the third option .
the answer is it run in n log n time .
why is that ?
well , the reason is that then the recurr which govern the run time of quicksort is , exactli match the recurr that govern the merg sort run time , which we alreadi know is n log n .
that is , the run time quick sort requir , in thi magic , special case , on an arrai of like n well , as usual you have a recurr in two part .
there is the work that get done by the recurs call and there is the work that get done now .
now by assumpt , we wind up pick the median as the pivot .
so there's go to be two recurs call , each of which will be on input of size of most and over two , and we can write thi .
thi is becaus the pivot equal the mid median .
so thi is not true for quicksort in gener .
it's onli true in thi magic case where the pivot is the median .
so that's what's get done by the two recurs call , and then how much work do we do outsid of the recurs call ?
well , we have to do the choos pivot subroutin , and i guess strictli speak i haven't said how that wa implement , but let's assum that choos pivot doe onli a linear amount of work .
and then as we've seen , the partit subroutin onli doe a linear amount of work as well .
so let's sai oh then , for work outsid of the recurs call .
and what do we know ?
we know thi impli us the master method or just by us the exact same argument as from thi give us a run time bound of n log in .
then again someth i haven't realli been emphas but which is true is that actual we can write .
theta of n log n .
and that's becaus , in the recurr , in fact , we know that the work done outsid of the recurs call is exactli theta of n , okai ?
partit need realli linear time , not just big o of n time .
in fact , the work done outsid of the recurs call is theta of n .
that's becaus the partit subroutin doe inde look at everi entri in the arrai that it's pass .
and as a result , we didn't realli discuss thi so much in the master method .
but as i mention in pass , if you have recurr which ar tight in thi sens , then the , the result of the master method , can also be strengthen to be theta instead of just big o .
but those ar just some extra detail .
the upshot of thi quiz is that even in the best case , even if we magic get perfect pivot throughout the entir trajectori of quicksort , the best we can hope for is an n log n upper bound .
it's not gonna get ani better than that .
so the question is , how do we have a principl wai of choos pivot so that we get thi best case , or someth like it that's best case n log n run time ?
so that's what that problem that we have to solv next .
so the last coupl quizz have identifi a super import question as far as the implement of quick sort , which is how ar we go to choos these pivot , right .
we now know thei have a big influenc on the run time of our algorithm .
it could be as bad as n squar or as good as n log n .
we realli want to be on the n log n side .
so kei question .
how to choos pivot .
and quicksort is the first killer applic we're go to see of the idea of random algorithm , that is , allow your algorithm to flip coin in the code so that you get some kind of good perform on averag .
so the big idea .
is random pivot .
by which i mean .
for everi time we recurs call quick sort and we ar pass some sub arrai of length k .
among the k candid pivot element in the subarrai , we're go to choos each on with eq , equal like .
with probabl of on over k , and we're gonna make a new random choic everi time we have a recurs call .
and then we're just gonna see how the algorithm doe .
so thi is our first exampl of a random algorithm .
thi is an algorithm , where , if you feed it exactli the same input , it will actual run differ on differ execut .
and that ? s ? caus there's random intern to the code of the algorithm .
now , it's not necessarili intuit that random should have ani purpos in comput , in softwar design and algorithm design .
but , in fact , and thi ha been sort of on of the real breakthrough in algorithm design .
mostli in the'70s , realiz how import thi is .
that the us of random can make algorithm more eleg .
simpler , easer to code , more faster .
or just simpli you can solv problem that you could not solv , at least not solv as easili without the us of random so it's realli on thing that should be in your toolbox as an algorithm design , random .
quick sort will be the first kill , killer applic but we'll see a coupl more later in the cours .
now , by the end of thi sequenc of video , i'll have given you a complet , rigor argument about why thi work .
why with random pivot , quicksort alwai run veri quickli on averag .
but , you know , befor we begin to sai anyth too formal , let's develop a littl bit of .
intuit , or at least kind of a daydream , about why on earth could thi possibl work .
why on earth could thi possibl be a good idea , to have random intern to our quicksort implement .
well , so first just , you know , veri high level .
what would be sort of the hope or the dream ?
the hope would be , you know , random pivot ar not gonna be perfect .
i mean , you're not gonna just sort of guess the median .
you onli have a on in n chanc of figur out which on the median is .
but the hope is that most choic of a pivot will be good enough .
so that's pretti fuzzi .
let's drill down a littl bit and develop thi intuit further .
let me describ it in two step .
the first claim is that , you know in our last quiz we said suppos we get lucki and we alwai pick the median in everi singl recurs call .
we observ we'd do great .
we'd get enlogin run time .
so now let ? s observ that actual to get the enlogin run time that it's not import that we magic get the median everi singl recurs call .
if we get ani kind of reason pivot by which a pivot that give us some kind of approxim balanc split of the problem again , we ar gonna be good .
so the last quiz realli wasn't particular to get the exact median .
near median ar also fine .
to be concret , suppos we alwai pick a pivot which guarante us a split of <num> to <num> or better .
that is both recurs call should be call on a rais of size .
and most <num> percent of the on that we start with .
so precis , if we alwai get a <num> <num> split or better in everi recurs call .
i claim that the run time of quick sort in that event will be big o of n log n .
just like it wa in the last quiz where we were , we were actual assum someth much stronger , that we were get the median .
now thi is not so obviou , the fact that <num> <num> split guarante n log n run time .
for those of you that ar feel keen , you might wanna try to prove thi .
you can prove thi us a recurs tree argument .
but becaus you don't have balanc subproblem , you have to work a littl bit harder than you do in the case cover by the master method .
so that's the first part of the intuit .
and thi is what we mean by be good enough .
if we get a <num> <num> split or better , we're good to go .
if we get our desir , our target n log n run time .
so the second part of the intuit is to realiz that , actual , we don't have to get all that lucki to just be get a <num> <num> split .
that's actual a pretti modest goal .
and even thi modest goal is enough to get the n log n run time .
right ?
so suppos our arrai contain the number , the integ between on and <num> , so it's an arrai of length <num> .
think for a second , which of those element is gonna give us a split ?
that's <num> , <num> , or better .
so if we pick ani element between <num> and <num> .
inclus .
will be total good .
right .
we pick someth that's at least <num> .
that mean that the left subproblem is go to have at least the element on through <num> .
that'll even have at least <num> percent of the element .
if we pick someth less than <num> , then the right subproblem will have all of the element <num> through <num> after we partit .
so , we'll also have at least <num> percent of the element .
so anyth between <num> and <num> give us a <num> , <num> split or better .
but that's a full half of the element .
so it's as good as just flip on fair coin and hope we get head .
so with <num> percent probabl , we get a split good enough to get the .
isn't enough to get thi n log inbound .
and so , again , the high level hope is that often enough , you know , half of the time , we get these you know , good enough split .
<num> , <num> split or better .
so , that would soon suggest and end log and run time of averag is a legitim hope .
so that the hell of a intuit , but if i were you i would certainli not be content with thi somewhat hand wavi explan that i've given you so far .
what i've told you is sort of the hope , the dream , why there is at least a chanc , thi might work .
but , but the question remain , and i would encourag such skeptic , which is , doe thi realli work .
and to answer that we're gonna have to do some actual mathemat analysi and that's what i am gonna show you .
i am gonna show you complet vigor analysi of the quick sword algorithm with random pivot and we'll show that ye in fact it doe realli work .
and thi highlight what's gonna be a recur theme in thi cours and a recur theme in thi just studi and understand of algorithm which is quit often there's some fundament problem we ar try to go to the solut and you come up with a novel idea .
it might be brilliant .
and it might suck , and you have no idea .
now obvious you can cut up the idea , run it on some concret instanc , and get a feel for you know , whether it seem like a good idea or not , but if you realli wanna know fundament what make the idea good or what make the idea bad , realli you need to turn to mathemat analysi to give you a complet explan .
and that's exactli what we're go to do with quick sort and then we'll explain in a veri deep wai why it work so well .
specif , in the next sequenc of three video , i'm go to show you an analysi , a proof of the follow theorem about quicksort .
so under no assumpt about the data , that is for everi input arrai of a given length , sai n , the averag run time .
a quick sort implement with random pivot is than log in .
and again , in fact it's theta and then log in , but we'll just focu on the big o of n log in part .
so thi is a veri , veri cool theorem about thi random quick sort algorithm .
on thing i wanna be clear so that you don't undersel thi guarante in your own mind , thi is a worst case guarante with respect to the input .
if you'll notic at the begin of thi theorem , what do we sai , for everi input arrai of link in .
all right so we have absolut no assumpt about the data so thi is a total and gener purpos sort subroutin which you can us whenev you want even if you have no idea where the data's come from and these guarante ar still go to be true .
thi of cours is someth i held forth about at some length back in our guid principl video .
when i argu that , if you can get awai with it , what you realli want is gener purpos algorithm which make no data assumpt so thei can be us over and over again in all kind of differ context and it still have guarante , and quicksort is on of those .
so , basic , if you have a dataset and it fit in the main memori of your machin , again , sort is a for free subroutin .
in particular , quicksort the quicksort implement is for free .
so , thi run so blazingli fast .
it doesn't matter what the arrai is .
mayb you don't even know why you want to sort it but go on ahead .
why not ?
mayb it'll make your life easier like it did , for exampl , in the clo .
payout rhythm for those of you that watch those two option video .
now , the word averag doe appear in thi theorem .
and , you know , as i've been harp on thi , averag is not over ani assumpt on the data .
we're certainli not assum that the input arrai is random in ani sens .
the input arrai can be anyth .
so where is the averag come from ?
the averag is come onli from random , which is intern to our algorithm .
random that we put in the code ourselv , that we're respons for .
so rememb , random algorithm have the interest properti that , even if you run it on the same input over and over again , you're gonna get differ execut .
so the run time of a random algorithm depend , you know , can vari as you run it on the same input over and over again .
the quizz have taught us that the run time of quick sort on a given input fluctuat from , anywher between the best case of n log n to the worst case of n squar .
so what thi theorem is tell us is that , for everi possibl input arrai .
while the run time doe inde fluctuat between an upper bound of n squar , and the lower bound of n log n .
the best case is domin .
on averag , it's n log n .
on averag , it's almost as good as the best case .
that's what's so amaz about quick sort .
n squar that can pop up onc in a while , ha , it doesn't matter .
you're never gonna see it .
you're alwai gonna see thi n log n like behavior in random quick sort .
so , for some of you , i'll see you next in a video on probabl review , that's option .
for the rest of you , i'll see you in the analysi of thi theorem .
so thi is the first video of three in which we'll mathemat analyz the run time of the random implement of quicksort .
so in particular we're gonna prove that the averag run time of quicksort is big o of n log n .
now thi is the first random algorithm that we've seen in the cours and therefor , in it analysi will be the first time that we're gonna need ani kind of probabl theori .
so let me just explain upfront what i ? d like , expect you to know in the follow analysi .
basic i need you to know the first few ingredi of discret probabl theori , so i need you to know about sampl space , that is how to model all of the differ thing that could happen , all of the wai that random choic could resolv themselv .
i need you to know about random variabl , function on sampl space which take on real valu , i need you to know about expect that is averag valu of random variabl , and veri simpl but veri kei properti we're go to need in the analysi of qucksort is linear of expect of expect .
so , if you haven't seen thi befor or if you're too rusti , definit you should review thi stuff befor you watch thi video .
some place you can go to get that necessari review , you can look at the probabl review part on video that's up on the cours .
if you prefer to read someth , like i said at the begin of the cours , i recommend the free onlin lectur note by eric lehman and tom laten .
mathemat for comput scienc .
that cover everyth we'll need to know plu much more .
there's also a wiki book on dist .
probabl with is a perfectli fine obvious free sourc unless you can learn the necessari materi .
okai ?
so after you've got that sorta fresh in your mind and you're readi to watch the rest of thi video , and in particular we're readi to prove the follow theorem state in the previou video .
so , the quick sort of algorithm with a random implement , that is wherein everi singl recurs sub call you pick a pivot uniformli at random restat the follow assert .
but for everi singl input so for a wors case input a rai of length n , the averag run time of quick sort .
with the random pivot .
is o of n login .
and again , to be clear where the random is , the random is not in the data .
we make no assumpt about the data , as per guid principl .
no matter what the input arrai is , averag onli over the random in our own code .
the random intern to the algorithm , we get a run time of n log n .
we saw in the past that the best case behavior of quick sort is n log n .
the worst case behavior is n squar .
so thi theorem is assert that , no matter what the input arrai is , the typic behavior of quick sort is far closer to the best case behavior than it is to the worst case behavior .
okai so that's what we ar go to prove , in the next few video .
so let's go ahead and get start .
so first i'm go to set up the necessari notat and be clear about exactli what is the sampl space what is the random variabl that we care about and so on .
so we're gonna fix an arbitrari arrai of length n , that's gonna be the input to the quick sort algorithm .
and we'll be work with thi fix but arbitrari input arrai for the remaind of the analysi .
okai , so just fix a singl input in your mind .
now what's the relev sampl space .
well recal what a sampl space is .
it's just all the possibl outcom of the random in the world .
so it's all the distinct thing that could happen .
now here the random is of our own devis , it's just a random pivot sequenc .
the random pivot chosen by quick sort .
so omega is just a set of all possibl random pivot that quick sort could choos .
now the whole point of thi theorem prove that the averag , averag run time it put is sort of small , boil down to comput the expect of a singl random variabl .
so here's the random variabl we're go to care about .
four given pipit sequenc , rememb that random variabl ar real valu function defin on the sampl space .
so for a given point in the sampl space , or pipit sequenc sigma , we're go to defin capit c of sigma , as the number of comparison that quick sort make , where by comparison i don't mean someth like within arrai index in a four loop , that's not what i mean by comparison .
i mean , comparison between two differ entri of the input arrai , by compar the third entri in the arrai against the seventh entri in the arrai to see whether the third entri or the seventh entri is smaller .
notic that thi is inde a random variabl that is given knowledg of the pivot sequenc sigma .
the choic of all pivot you can think a quick sort at that point is just a determinist algorithm , with all of the pivot choic predetermin .
so the determinist version of quick sort make some determinist number of comparison .
so , if we're given pivot sequenc sigma , we're just call c of sigma to be whatev , howev mani comparison it make given those choic of pivot .
now the , with the theorem i've state it's not about the number of comparison of quick sort but rather about the run time of quick sort but realli , if you think about it kinda onli real work that the quick sort algorithm doe is make comparison between two , between pair of element in the input arrai .
ye , there's a littl bit of other book keep but that's all nois , that's all second order stuff .
all quick sort realli doe is comparison between the pair of element in the input arrai .
and if you want to know what i mean by that a littl more formal , domin by comparison .
i mean , that there exist a constant c , so that the total number of oper of ani type that quick sort execut is at most a constant factor larger than the number of comparison .
so let's sai that by rt i mean the number of primit oper of ani form that quick sort us and for everi .
pivot sequenc sigma .
the total number of oper is no more than a constant time .
the total number of comparison .
and if you want a proof of thi , it's not that interest , so i'm not gonna talk about it here .
but in the note post on the websit , there is a sketch , of why thi is true .
how you can formal argu that there isn't much work beyond just the comparison .
but i hope most of you find that , to be pretti intuit .
so , given thi , given the , the run time of quick sort boil down just to the number of comparison .
and we want to prove the averag run time is n log n .
all we gotta do , quot unquot , all we have to do , is prove that the averag number of comparison that quick sort make , is o of n log n .
and that's what we're gonna do .
so that's what the , the rest of these lectur ar about .
so that's what we gotta prove .
we gotta prove that the expect of thi random variabl c , which count up the number of comparison quicksort make is for thi arbitrari input arrai a of length n , bound by big o of n log n .
so the high order bit of thi lectur is a decomposit principl .
we've identifi thi random variabl c , the number of comparison , and it's exactli what we care about .
it govern the averag run time of quick sort .
the problem is , it's quit complic .
it's veri hard to understand what thi capit c is .
it's fluctuat between n log n and n squar , and it's hard to know how to get a handl on it .
so how ar we gonna go about prove thi assert that the expect number of comparison that quicksort make is on averag just o n log n ?
at thi point we actual have a fair amount of experi with divid and conquer algorithm .
we've seen a number of exampl .
and , whenev we had to do a run time analysi of such an algorithm , we wrote out a recurr , we appli the master method or , in the worst case , we wrote out a recurs tree to figur out the solut of that recurr .
so you'd be veri right to expect someth similar to happen here .
but as we probe deeper and we think about quicksort , we quickli realiz that the master method just doesn't appli , or at least not in the form that we're us to .
the problem is twofold .
so , first of all , the size of the two subproblem is random .
right ?
as we discuss in the last video the qualiti of the pivot is what determin how balanc a split we get into the two subproblem .
it can be as bad subproblem size zero and on a size n minu on .
or it can be as good as a perfectli balanc split into two subproblem of equal size .
but we don't know .
it's gonna be depend on our rang of choic of the pivot .
moreov the master at least as we discuss it requir solv subproblem to have the same size .
and unless you're extrem lucki that's not gonna happen in the quicksort algorithm .
it is possibl to develop a theori of recurr relat for random algorithm , and to appli it to quick sort in particular .
but i'm not go to go that rout for two reason .
the first on is it realli quit messi .
it get pretti technic , to talk about solut to recurr for random algorithm , or to think about random recurs tree .
both of those get , get pretti complic .
the second reason is , i realli want to introduc you to what i call a decomposit principl , by which you take a random variabl that's complic , but that you care about a lot .
and decompos it in to simpl random variabl which you don't realli care about in their own right but which ar easi to analyz and then you stitch those two thing togeth us linear and expect so that's gonna be the workhors for our analysi of the quick store , of the quick sort algorithm and it's gonna come up again a coupl of time in the rest of the cours , for exampl , when we studi hash .
so , to explain how thi decomposit principl appli to quicksort in particular , i'm gonna need to introduc you to the build block , simpl random variabl which will make up the complic random variabl that we care about , the number of comparison .
so here's some notat .
recal that we fix in the background an arbitrari arrai of length n and that's denot by capit a and some notat which is simpl , but also quit import by z sub i .
what i mean is the ith smallest element in the input arrai capitol a .
also known as the ith order statist .
so let me tell you what zi is not , okai .
what zi is not , in gener , is the element in the posit of the input unsort arrai .
what zi is , is it's the element which is go to wind up in the element of the arrai , onc we sort it , okai .
so if you fast forward to the end of the sort algorithm , in posit i , you're gonna find zi .
so let me give you an exampl .
so suppos we have just a simpl arrai here , unsort .
were the number six , eight , ten and two , then .
z on , well that's the first smallest .
the on smallest or just the minimum .
so z on would be the two z two would be the six z three would be the eight and z four would be the ten .
for thi particular input arrai .
okai ?
so z i is just the smallest number , wherev it mai lie in the origin unsort arrai that's what z i refer to .
so we alreadi defin the sampl space that's just all possibl choic of pivot , the quicksort might make .
i alreadi describ on random variabl with the number of comparison that quicksort make on a particular choic of pivot .
now i am get introduc a famili of much simpler random variabl which count mere the comparison involv a given pair of element in the input wai not all element just a given pair .
so for a given choic of pivot given sigma and given choic of i and j .
both of which ar between on and n .
and so we onli count thing onc .
i'm go to insist that i is less than j alwai .
and now here's a definit .
my xij , and thi is a random variabl , so it's a function of the pivot chosen .
thi is go to be the number of time that zi and zj ar compar in the execut of quick sort .
okai , so thi is gonna be an import definit in our analysi .
it's import you understand it .
so , for someth like the third smallest element and the seventh smallest element , x i j is ask , that's when i equal three and j equal seven , x three seven is ask how mani time those two element get compar as quicksort .
proce .
and thi is a random variabl in the sens that if the pivot choic ar all predetermin , if we think of those be chosen in advanc , then there's just some fix determinist number of time , that zi and zj get compar .
so it's import you understand these random variabl xij .
so the next quiz is gonna ask , a basic question about the rang of valu that a given xij can take on .
so for thi quiz we're consid , as usual , so fix input arrai and now furthermor , fix two specif element of the input arrai .
for exampl , the third smallest element , wherev it mai lie , and the seventh smallest element , wherev it mai lie .
think about just these pair of two element .
what is the rang of valu that the correspond random variabl , xij , can take on ?
that is , what ar the differ number of time that a given pair of element might conceiv get compar in the execut of the quick sort algorithm ?
all right so the correct answer to thi quiz , is the second option .
thi is not a trivial quiz .
thi is a littl tricki to see .
so the assert is that a given pair of element , thei might not be compar at all .
thei might be compar onc .
and thei're not go to get compar more than onc .
okai ?
so here , what i'm gonna discuss is why it's not possibl for a given pair of element to be compar twice dure the execut of quick sort .
it'll be clear later on , if it's not alreadi clear now , that both zero and on ar legitim possibl .
a pair of element might never get compar and thei might get compar onc .
and we'll , and again , we'll go into more detail on that in the next video .
so , but why is it imposs to be compar twice ?
well , think about two element .
sai , the third element and the seventh element .
and let's recal how the partit subroutin work .
observ that in quick sort , the onli place in the code where comparison between pair of the .
input arrai element happen , it onli happen in the partit subroutin .
so that's where we have to drill down .
so what ar the comparison that get made in the partit subroutin ?
well .
go back and look at that code .
the pivot element is compar to each other element in the input arrai exactli onc .
okai , so the pivot just hang out in the first entri of the arrai .
we have thi for loop , thi index j , which march over the rest of the arrai .
and for each valu of j , the j element of the input arrai get compar to the pivot .
okai .
so summar in an invoc of partit , everi singl comparison involv the pivot element , okai .
so two element get compar if and onli if on is the pivot .
all right , so let's go back to the question .
why can't a given pair of element in the get compar two or more time ?
well , think about the first time thei ever get compar in quick sort .
it must be the case that at that moment , we're in a recurs call where either on of those two is the pivot element .
so it's the third smallest element or the seventh smallest element .
the first time those two element ar compar to each other , either the third smallest or the seventh smallest is current the pivot , becaus all comparison involv the pivot element .
therefor .
what's gonna happen in the recurs , well the pit is exclud from both recurs call .
so for exampl if the second smallest element is current the pit that's not gonna be pass on to recurs call which contain the third smallest element .
therefor if you compar onc , on of the element that's pit and it will never be compar again becaus the pit will not even show up in ani futur recurs call .
okai , so that's the reason .
you compar onc , on is the pit , it doesn't get pass to the recurs , you're done , never seen again .
so a random variabl which can onli take on the valu zero or on is often call an indic random variabl , becaus it's in , just indic whether or not a certain thing happen .
so , in that terminolog , each x i j .
is indic whether or not the ith smallest element in the arrai , and the jth smallest element in the arrai , ever get compar .
it can't happen more than onc .
it mai or mai not happen , and xij is on precis when it happen .
so that's the event that it's indic .
have defin the build block i need , these indic random variabl , these x , i , js , now i can introduc you to the decomposit principl as appli to quick sort .
so there's a random variabl that we realli care about which is denot capit c , the number of comparison the quick sort make , that's realli hard to get a handl on in and of itself .
but we can express c as a sum of indic random variabl of these x , i , js and those , we don't care about in their own right , thei're gonna be much easier to understand .
so , let me just rewrit the definit of c m e x and j so we're all clear on them .
so c recal count all of the comparison between pair of input element that whatev make .
wherea an xij onli count the number and it go to be zero or on which compar the ith smallest and the jth smallest element in particular .
now sinc everi comparison involv precis on pair of element some i and some j , with i less the j .
we can write c as the sum of the xij's .
so don't get intimid by thi fanci doubl sum .
all thi is do is it's iter over all of the order pair .
so all of the pair ij , where i and j ar both between on and n , and where i is strictli less than n .
thi doubl sum is just a conveni wai to do that iter .
and , of cours , no matter what the pivot chosen ar , we have thi equal , okai ?
the comparison , ar somehow split up amongst the variou pair of element , the variou i's and j's .
why is it us to express a complic random variabl as a sum of simpl random variabl ?
well , becaus an equat like thi is now right in the wheelhous of linear of expect .
so let's just go ahead and appli that .
rememb , and thi is super , super import , linear of expect sai that the expect of a sum .
equal , the sum , of the expect , and moreov , thi is true , whether or not the reign of variabl ar independ ? kai and i'm not gonna prove it here , but you might wanna think about the fact that the x i j's ar not in fact , independ .
thei were us the fact that we need an expect toward , even for not independ reign of variabl .
and why is thi interest , well the left hand side .
thi is complic , right ?
thi is the , thi is some crazi number of comparison by some algorithm on some arbitrarili long arrai and it fluctuat between two pretti far apart number and log in and then squar .
on the other hand , thi doe not seem as intimid , given x , i , j it's just zero or on , whether or not these two gui get compar or not .
so that is the power of thi decomposit approach , okai .
so it reduc understand of complic random variabl to understand simpl random variabl .
in fact , becaus these ar indic random variabl , we can even clean up thi express some more .
so for ani given xij , be a <num> random variabl , if we expand the definit of expect , just as an averag over the variou valu it's , what is it .
it's , well , it's some probabl it take on the valu zero , that's possibl .
and there's some possibl it take on the valu on .
and of cours thi zero part we can veri satisfyingli delet , cancel .
and so the expect valu of a given xij is just the probabl that xij equal on .
and rememb , it's an indic random variabl .
it's on precis when the i smallest and the j smallest , element get compar .
so put it all togeth .
we find that what we care about , the averag valu of the number of comparison made by quick sort on thi input arrai .
is thi doubl sum , which iter over all order pair .
where each summand is the probabl that the correspond x i j equal on .
that is the probabl that z i and z j get compar .
and thi is essenti the stop point for thi video , for the first part of the analysi .
so let's call thi star and put a nice circl around it .
so what gonna happen next is that in the second video , for the analysi , we're go to drill down on thi probabl .
probabl that a given , pair of get compar and we're gonna nail it , we're gonna get an exact express as a function of i and j , for exactli what thi probabl is .
then in the third video we're go to take that exact express , plug it into the sum , and then , evalu thi sum and it turn out the sum will evalu to the and login .
so that's the plan .
that's how you appli decomposit in term of <num> <num> or indic random variabl .
appli linear of expect .
in the next video we'll understand these simpl random variabl , and then we'll wrap up in the third video .
befor we move on to the next part of the analysi , i do just want to emphas that thi decomposit principl is relev not onli for quicksort , but it's relev for the analysi of lot of random algorithm .
and we will see more applic , at least on more applic later in the cours .
so just to kinda realli hammer the point home , let me spell out the kei step for the gener decomposit principl .
so first you need to figur out what is it you care about ?
so in quick sort we care about the number of comparison , we had thi lemma that said the run time domin our comparison so we understood what we want to know the averag valu for the number of comparison .
the second step is to express thi random variabl y as a sum of simpler random variabl .
ideal indic or <num> random variabl .
now you're in the wheel hous of linear of expect .
you just appli it .
and you find that what it is you care about , the averag valu .
of the random valu , of the random variabl , y .
is just the sum .
the probabl of variou event .
that , given xl random variabl is equal to on .
and so the upshot is , to understand thi seemingli veri complic left hand side , all you have to do is understand someth which , in mani case , is much simpler .
which is , understand the probabl of these variou event .
in the next video , i'll show you exactli how that's done in the case of quick sort .
where these , where we care about the xij , the probabl that two element get compar .
thi is the second video of three , in which we prove that the averag run time of random quicksort is o n log n .
so to remind you of the formal statement , so get into think about quicksort where we implement the choos pivot subroutin to alwai choos a pivot uniformli at random from the subarrai that it get pass .
and we're prove that for a worst case input arrai for an arbitrari input arrai of length n the averag run time of quicksort with the averag over the rang of possibl pivot choic is o n log n .
so let me remind you of the stori so far , thi is where we left thing at the previou video we defin a few random variabl , the sampl space , recal , is just the all the differ thing that can happen that is , all of the random coinflip outcom that quicksort can produc , which is equival to all the pivot choic made by quicksort now the random variabl we care about so first of all there's capit c which is the number of comparison between pair of element in the input arrai that quicksort make for a given pivot sigma and then there ar the xij and so that is just meant to count the number of comparison involv the i th smallest and the j th smallest element in the input arrai .
you'll recal that zi and zj denot the i th smallest and the j th smallest entri in the arrai .
now becaus everi comparison involv some zi and some zj , we can express capit c as a sum over the xij .
so we did that in the last video , we appli linear of expect we us the fact that xij ar <num> or <num> , that is indic random variabl to write the expect of xij just as a probabl that is equal to on and that gave us the follow express .
so the kei insight and realli the heart of the quicksort analysi is to deriv an exact express for thi probabl as a function of i and j .
so for exampl the third smallest element in the arrai , the seventh smallest element in the arrai wherev thei mai be scatter in the input arrai and you want to know exactli what is the probabl thei get compar at some point of the execut of quicksort .
and we're go to get an extrem precis understand of thi probabl in the form of thi kei claim .
so for all pair of element , and again , order pair , we're think of i be less than j , the probabl that zi and zj get compar at some point of the execut in the quicksort is exactli two divid by j minu i plu on .
so for exampl , in thi exampl of third smallest element and the seventh smallest element it would be exactli <num> of the time .
two over five is how often those two element would get compar if you ran quicksort with a random choic of pivot .
that go to be true for everi j and i .
the proof of thi kei claim is the purpos of thi video .
so how do we prove thi kei claim ?
how do we prove that the probabl that zi and zj get compar is exactli two over the quantiti of j minu i plu <num> ?
well fix your favorit order pair .
so fix element zi , zj with i less than j , for exampl the third smallest and the seventh smallest element in the arrai now , what we want to reason about is the set of all element in the input arrai between zi and zj , inclus .
i dont mean between in term of posit in the arrai i mean between in term of their valu .
so consid the set between zi and zj plu on inclus .
so zi , zi <num> dot dot dot zj <num> and zj so for exampl the third , fourth , fifth , sixth and seventh smallest element in the input arrai .
wherev thei mai be , thei ar of cours the initi arrai is not sort , so there's no reason to believ that these j minu i plu on element ar contig , okai ?
thei're scatter throughout the input arrai .
we're go to think about them , okai ?
zi through zj inclus .
now , throughout the execut of quicksort , these j minu i plu on element lead parallel live , at least for a while in the follow sens begin with the outermost call to quicksort and suppos that none of these j minu i plu on element is chosen as a pivot .
where then could the pivot lie ?
well it could onli be a pivot that is greater than all of these or it could be less than all of these for exampl thi is third , fourth , fifth , sixth and seventh smallest element in the arrai .
well the pivot is then either the minimum or the second minimum in which case it is smaller than all the five element or it is eigth or largest or larger element in the arrai , in which case , bigger than all of them .
so there's no wai you're go to have a pivot that somehow is wedg in between thi set becaus thi is a contigu set of order statist , okai ?
now what do i mean by these element lead parallel live ?
well in the case where the pivot is chose to be smaller than all of these element then all of these element will wind up to right of the pivot and thei will all be pass to the recurs call , the second recurs call .
if the pivot is chosen to be bigger , than all of these element , then thei'll all show up on the left side of the partit arrai and thei'll all be pass to the first recurs call .
iter thi or proceed induct , we see that as long as the pivot is not drawn from the set of j minu i plu on element , thi entir set will get pass on to same recurs call .
so thi j minu i plu on element ar live blissfulli togeth in harmoni , until the point in which on of them is chosen as a pivot and of cours that ha to happen at some point the recurs onli stop at some point when the arrai length is either equal to zero or on so if for no other reason , at some point , there will be no other element in the recurs call , other than these j minu i plu on okai ?
so at some point the reverri is interrupt and on of them is chosen as a pivot .
so , let's paus the quicksort algorithm and think about what thing look like , at the time that on of those j minu i plu on element is first chosen as a pivot element .
there ar two case worth distinguish between in the first case the pivot happen to be either zi or zj .
now rememb what is that we ar try to analys .
we're try to analys the frequenc , the probabl that zi and zj get compar well , if zi and zj ar in the same recurs call , and on of them get chosen as the pivot then thei're definit ar go to get compar rememb , when you partit an arrai around it pivot element , the pivot get compar to everyth els .
so , if zi is chosen as a pivot , it certainli get compar to zj .
if zj get chosen as a pivot , it get compar to zi .
so either wai , if either on of these is chosen , thei're definit compar .
if on the other hand , the first of these j minu i plu on element to be chosen as a pivot , is not zi or zj .
if instead it come from the set zi <num> up to zj <num> and so on then the opposit is true , then zi and zj ar not compar now , nor will thei ever be compar in the futur .
so why is that ?
that requir two observ first recal that when we choos an pivot and we partit an arrai all of the comparison involv will pivot .
so two element of which neither of them is a pivot , do not get compar , in the partit subroutin .
so thei don't get compar now .
moreov , sinc zi is the smallest of these and zj is the biggest of these and the pivot come from somewher between them thi choic of pivot is go to split zi and zj in difer recurs call zi get pass to the first recurs call zj get pass to the second recurs call and thei will never meet again .
so there's no comparison in the futur either .
so these two right here , i would sai is the kei insight , in the quicksort analysi .
the fact that for a given pair of element we can veri simpli character exactli when thei get compar , and when thei do not get compar in the quicksort algorithm .
that is , thei get compar exactli when on of them is chosen as the pivot befor ani of the other element with valu in between those two ha had the oportun to be a pivot .
that 's exactli when thei get compar .
so thi ha allow us to prove thi kei claim .
thi exact express on the comparison probabl , that we'll plug into the formula that we had earlier and will give us the deisr valu on the averag number of comparison .
so let's fill in those detail .
so first let me just first rewrit the high order bit from the previou slide .
so now at last , we will us the fact , that our quicksort implement alwai choos a pivot uniformli at ramdom .
that each element of a subarrai is equal like to serv as the pivot element in the correspond partit call so what doe thi bui us ?
thi just sai all of the element ar symetr .
so each of the element zi , zi <num> all the wai up to zj is equal like to be the first on ask to serv as a pivot element .
now the probabl that zi and zj get compar is simpli the probabl that we're in case on as oppos to in case two and sinc each element is equal like to be the pivot that just mean there're just two bad case , two case in which on can occur out of the j minu i plu on possibl differ choic of pivot .
now we're talk about a set of set of j minu i plu on element , each of which of whom is equal like to be ask to be serv first as pivot element and the bad case , the case that lead to a comparison , there's two differ possibl for that .
if zi or zj is first and the other j minu i minu on outcom lead to the good case , where zi and zj never get compar .
so overal becaus everyon is equal like to be the first pivot we have that the probabl that zi and zj get compar is exactli the number of pivot choic that we do comparison divid by the number of pivot choic overal .
and that is exactli the kei claim .
that is exactli what we wa the probabl that a given zi and zj get compar no matter i and j ar .
so wrap up thi video , where doe that leav us ?
we can now plug in thi express for thi comparison probabl into the doubl sum we had befor .
so put it all togeth what we have is that what we realli care about the averag number of comparison that quicksort make on thi particular input of arrai of length n is just thi doubl sum which iter over all the possibl order pair i , j and what we had here befor wa the probabl of compar zi and zj .
we now know exactli what that is .
so we just substitut and thi is where we're gonna stop for thi video .
so thi is gonna be our kei express star which we still neeed to evalu , but that is go to be the third video .
so essenti we done all of the conceptu difficulti in understand where comparison come from in the quicksort algorithm .
all that remain is a littl bit of an algebra manipul to show that thi star express realli is o n log n and that's come up next .
so we're almost at the finish line of our analysi of quick sort .
let me remind you what we're prove .
we're prove that for the random implement of quick sort where we alwai choos the pivot element to partit around uniformli at random , we're show that for everi arrai , everi input of length n , the averag run time of quick sort over the random choic of pivot is of n log n .
so we've done a lot of work in the last coupl of video .
let me just remind you about the stori so far .
in the first video what we did is we identifi the relev random variabl that we care about , capit c , the number of comparison that quicksort make among the pair of element in the input arrai .
then we appli the decomposit approach .
we express capit c , the overal number of comparison , as a sum of indic or <num> <num> random variabl .
for each of those variabl xij , just count the number of comparison involv the ith smallest and jth smallest entri in the arrai , and that's gonna be either zero or on .
then we appli linear of expect to realiz , all we realli need to understand wa the comparison probabl for differ pair of element .
second video we nail what that comparison probabl is , specif , for the i smallest and the j smallest element in the arrai , the probabl that quick sort compar them when you alwai make random choic is exactli .
two divid by the quantiti j minu i .
plu on .
so put that all togeth , yield the follow express , govern the averag number of comparison made by quick sort .
on thing i want you to appreci is , is in the last coupl of video , we've been sort of amazingli exact as algorithm analysi goe .
specif we've done noth sloppi whatsoev .
we've done no estim .
the number of comparison that quick store make on averag is exactli thi doubl sum .
now sure we'll do some inequ to make our live a littl bit easier .
but up to thi point everyth ha been complet exact .
and thi will actual see why there's small constant in the , in the , in quick sort .
it's basic go to be thi factor two .
now the next question to ask is , what ar we shoot for ?
rememb the theorem we want to prove is that the expect number of comparison realli the expect run time is all of n log n , so we're alreadi done .
well not quit we're gonna have to be a littl bit clever , so if we're look at thi doubl sum , and we ask how big ar the sum end and how mani term ar there ?
well the biggest sum end we're ever go to see ar when i and j ar right next to each other when j is on bigger than i , and in that case thi fraction is gonna be on half .
so the term can be as big as on half , how mani term ar there ?
well there's a quadrat number of term .
so it would be veri easi to deriv an upper bound that's quadrat in n , but that's not what we want .
we want on that's n log n .
so to drive that , we're gonna have to be a littl bit more clever about how we evalu thi sum .
so , the idea is , what we're go to do , is to think about a fix valu of i in thi outermost sum .
and then we're gonna ask , how big could the inner sum be ?
so let's fix some valu of i , the valu of the index in the outer sum .
and then let's look at the inner sum , where j rang from i plu on up to n , and the valu of the sum end is on over the quantiti j minu i plu on .
so how big can thi be ?
well , let's first understand what the term actual ar .
so j start at i plu on and then it ascend to n .
and as j get bigger the denomin get bigger .
so the sum end get smaller .
so the biggest sum end is gonna be the veri first on .
and j is as small as possibl .
name i plu on .
when j is i plu on the sum end is on half .
then j get increment in the sum .
and so that's , we're gonna pick up a on third term follow by on fourth term , and so on .
so there's gonna be , for everi inner sum is gonna have a thi form , on half plu on half equal on fourth .
and then it's gonna sort of run out at some point , when j equal n .
and the biggest term we're ever go to see is gonna be a on over n , in the case where i equal on .
so .
let's make our live easier by take thi express we start with .
star , and instead of have a doubl sum , let's just upper bound thi with a singl sum .
so what ar the ingredi of a singl sum ?
well , there's thi two , can't forget the two .
then there's n choic for i , actual , there's n minu on choic for i , but let's just be sloppi and sai n choic .
so that give us a factor n .
and then how big can an inner sum be ?
well , inner sum is just a bunch of these term , on half plu on third and so on .
the biggest of those inner sum is the on occur when i equal on , at w , at which point the last term is on over n .
so , we're gonna just do a chang of variabl and express the inner , upper bound on each inner sum as the sum from k equal two to n of on over k .
so that's look more manag just have the singl sum involv thi index k , and life's gonna get realli good when we prove the next claim , which is that thi sum cannot be veri big , it's onli logarithm in n , even though there's a linear number of sum n's , the overal valu of the sum is onli logarithm .
that , of cours , is gonna complet the proof , 'caus that'll give us an overal bound of two time n time the natur log on n .
so it's an n login bound with realli quit reason constant .
so , why is thi true ?
why is thi sum onli logarithm larg ?
well , let's do a proof by a pictur .
i'm go to write thi sum .
in a geometr fashion .
so on the x axi , let me mark off point correspond to the posit integ .
and on the y axi , let me mark off point correspond to fraction of the form , on over k .
and what i ? m gonna do is gonna draw a bunch of rectangl .
of decreas area , specif thei all have with on , and the height ar gonna be like on over k .
so the area of thi gui's on , the area of thi gui's on half , the area of thi gui's on third , and so on .
and now i'm go to overlai on thi pictur the graph of the function , the continu function , f of x equal on over x .
so notic that is go to go through these three point .
it's gonna kiss all of these rectangl on their upper right corner .
now what is it we're try to prove ?
the claim we're try to prove is that thi sum , on half plu on third and so on , is upper bound by someth , so the sum can be just thought of as the area in these rectangl , the on half , the on third and so on , and we're go to upper bound it by the area under the blue curv , if you notic the area under the blue curv is at least as big as the sum of the area of the rectangl becaus the curv hit each of these rectangl in it north east corner .
so put that into mathemat , the sum from k equal two to n of on over k .
is met in abov by the integr .
and we'll start the area of the curv at on .
and then we need it to go all the wai up to n .
of the function on over x .
the x , so that's the area under the curv .
and if you rememb a littl bit of calculu the integr of on over x is the natur log of x .
so thi equal the natur log of x .
evalu at on .
also known as login minu log on .
and of cours log on would be zero , so that give us our login .
so that complet the proof of the claim .
that inde , the sum of these on over k's is bound abov by the natur log of n , and that in fact complet the proof of the theorem .
you've got to be the expect number of comparison , at most two n time thi sum , which is at most log n .
and altogeth , we find that the expect number of comparison that quick sort make on an arbitrari input of length n .
is two time n time the natur log of n .
so that would be big o of n , log n , with quit reason constant .
now , thi is just the number of comparison , but as we observ earlier , the run time of quicksort on averag is not much more than that , the run time is domin by the number of comparison that it make .
moreov , as we discuss when we were talk about the detail of the implement , it work in place , essenti no extra storag is necessari .
so that is a complet and mathemat rigor explan of just why quicksort .
is so quick .
welcom to part on of our probabl review .
the first time that we need these concept in thi cours is for those that wanna understand the analysi of quick sort .
why it run in big o of end log and time on averag .
and these topic will also come up a coupl of other time in the cours .
for exampl when we studi a random algorithm for the minimum cut problem in graph .
and also when we try to understand the perform of hash .
here ar the topic we're go to cover .
we'll start at the begin with sampl space , and then we'll discuss , event and their probabl .
we'll talk about random variabl , which ar real valu function on a sampl space .
we'll talk about expect , which is basic the averag valu of a random variabl .
we'll ident and prove a veri import properti , call the linear of expect , which will come up over and over again in our analys of random process .
so that's gonna be the topic for part on , that will conclud the video with on exampl ty these concept togeth in load balanc .
and , thi video is by no mean the onli sourc you can turn to , to learn .
about these concept .
a coupl of other sourc i recommend ar the onlin lectur note by eric and tom also , there's a wiki book on discreet probabl , which you could , check out .
and i wanna be clear , thi is realli not meant to be a cours or a tutori on probabl concept .
it's realli onli meant to be a refresh .
so i'm gonna go at a reason fast pace , and it's gonna be a pretti cursori present .
and if you want a more thorough review , i would , i would check out on of these other sourc , or your favorit book on discreet probabl .
and along those same line , i'm think that mani of you have seen some of thi materi befor .
don't feel compel , like , to , to watch thi video straight begin to the end .
feel free to just sort of dip in , and review the concept , that you need a refresh on .
so , let's start at the begin with sampl space .
so , what is a sampl space ?
well we're analyz random process , so ani number of thing can happen .
and the sampl space is just a collect of all of those thing that could happen .
so thi is basic the univers in which we're gonna discuss probabl and averag valu .
so i'll often us annot bigo mega to describ the sampl space .
so on thing that we've got go for us in the design of algorithm is typic we can take omega to be a finit set .
so that's why we're onli deal onli with discreet probabl which is a veri happi thing .
caus that's much more elementari then more gener probabl .
so i realiz thi is sort of a super abstract concept , and the next few definit will also be abstract , so i'm go to us two veri simpl , veri concret run exampl to illustr the next few concept .
so the first exampl is just go to be , we take two six side dice and we roll them .
so then of cours what's the sampl space ?
well , it's just all of the possibl outcom of just two dice .
in addit to defin the outcom ; everyth that could possibl happen , we need to defin what is the probabl of each individu outcom .
so , of cours , the probabl of each outcom should be at least zero , should be non neg and there's also the obviou constraint that the sum of the probabl should be on .
so , exactli on thing's gonna happen .
now i realiz thi is a , a super abstract concept and the next few definit ar also a littl abstract , so throughout them i'm go to us two realli simpl , realli concret exampl to illustr what these concept mean .
so , the first exampl is just gonna be you take two six side dice , and you roll them , and then of cours , the sampl space is just the <num> differ outcom you could have of these two dice .
and assum that each of these two dice is well craft , then we expect each of these <num> outcom to be equal like to occur with a probabl of <num> <num> .
the second run exampl i'm go to us is more directli relat to algorithm and it's motiv by the quick sort algorithm .
recal that we're studi the implement of quick sort that choos a pivot uniformli at random in everi recurs call .
so let's just focu on the veri first outermost call of quick sort and think about the random choic of the pivot just in that call .
so then the sampl space , all of the differ thing that could happen is just all of the n differ choic for a pivot , assum the arrai ha length n .
so we can repres the sampl space just as integ on , two all the wai up to n , correspond to the arrai index , of the randomli chosen pivot .
and again , by definit , by the construct of our code , each of these event , each of these thing is equal like , probabl of on over n .
now let's talk about event .
an event is noth more than a subset of all of the thing that could happen .
that is a subset of the sampl space omega .
the probabl of the event is exactli what you would think it would be .
it's just the sum of the probabl of all of the outcom contain in that event , right ?
so an event is just a bunch of stuff that might happen , we know the probabl of each individu thing that could happen , we add them up to get the probabl of an event .
so the next two quizz ar meant to give you some practic with these concept and in particular , the last few to comput the probabl of event in our two run exampl .
so in the first quiz , thi is our first run exampl where we think about two dice and we have our <num> possibl outcom .
consid the subset of outcom in which the sum of the two dice equal seven .
what is the probabl of that event ?
right , so the correct answer is the third on , the probabl is on over six .
why is that ?
well , first , let's be more precis about what thi event is .
what ar the outcom in which the sum of the dice equal to seven .
well there's exactli six such outcom .
on six .
two five .
three four .
four three .
five two and six on .
each of the <num> outcom is equal like ha a probabl of on over <num> .
so we have six member of the set , usual on over <num> , so the probabl is on sixth .
let's move on to the next quiz , which consid our second run exampl , name the randomli chosen pivot in the outermost call to quick sort on an input arrai of length n .
so , recal that in quick sort , when you choos a pivot , you then partit the arrai around the pivot , and thi split the input arrai into two sub arrai .
a left on , element less that the pivot , and a right on , those bigger than the pivot .
and the more balanc the split into these two sub problem , the better .
so , ideal , we'd like a fifti , fifti split .
so what thi quiz ask you is what fraction of pivot , that is what's probabl that a randomli chosen pivot .
will give you a reason good split , mean both of the sub problem have size at least <num> .
that is you get a split <num> <num> or better .
that's what thi quiz ask about , what's the probabl that the randomli chosen pivot satisfi that properti ?
so the correct answer to thi quiz is again , the third option .
it's a <num> percent probabl you get a <num> <num> split or better .
so to see why , let's , again , be precis about what is the event that we're talk about ?
then we'll comput it probabl .
so , when doe a pivot give you a <num> <num> split or better ?
well , for concret , suppos the arrai contain just the integ between on and <num> .
now , what's the properti we want ?
we want that both of the two sub arrai , have at least <num> percent of the element .
neither on ha more than <num> percent of the element .
well , if we choos an element that's <num> or bigger in valu , then the left sub problem will have at least <num> element .
it's the number on through <num> , and if we choos an index which is , choos an element that's at most <num> , then the right sub arrai's go to have at least <num> element in the number <num> to <num> .
so anyth between <num> and <num> , inclus is go to give us a <num> <num> split , more gener .
ani pivot from the middl <num> percent of the quan , of the quintil is gonna give us the desir split .
so we do badli if we get someth within the first quarter , we do badli if we get someth within the last quarter .
anyth in the middl work .
so more formal we can sai that the event s that we're analyz is among the possibl pivot choic .
we're interest in the on that is not in the first quarter or not in the last quarter .
now the coordin of ; the number of pivot in thi set is essenti on half of the overal pivot choic .
i'm ignor , fraction here for simplic .
so the probabl of thi event is the , coordin of thi .
divid by or time the probabl of each of the individu outcom .
and sinc we choos the pivot unit from them at random each on ha the probabl of on over n so you get n over two divid by n or on half .
okai , now that we've explor the concept of event in our run two exampl , we see that the probabl that the sum of two dice is equal to on sixth .
a us fact to know if you're everi plai crap .
and we know that a pivot give us a <num> , <num> split or better in random quick sort with fifteen percent probabl , a us fact if you want to develop intuit for why quick sort is in fact quick .
so that's event .
let's move on to random variabl .
so random variabl ar basic some statist measur what happen in the random outcom .
so formal if we want to defin it , it's a real valu function defin on the sampl space omega .
so given an outcom , given a realiz of the random thi give you back a number .
now the random variabl that we most often care about in algorithm design is the run time of a random algorithm .
that's the case for exampl with the quick sort algorithm .
so notic that is in fact a random variabl .
if we know the state of the world , if we know the outcom of all of the coin flip that our code is go to make , then there is just some run time of our algorithm .
so in that sens it's a random variabl .
given the outcom of the coin flip , out pop a number , the run time , sai in millisecond of the algorithm .
here i'm gonna give you a coupl more modest exampl of random variabl in our two run exampl .
if we're roll two dice , on veri simpl random variabl take as input the outcom , so that the result of the two dice , and spit out , the sum .
so that's certainli a reign of variabl .
in ani given outcom it's gonna take on some integ valu between two at the minimum and twelv at the maximum .
our second run exampl is the randomli chosen pivot made by the outermost quick sort .
so let's think about the random variabl which is the size , mean the sub arrai length past to the first recurs call .
equival thi reign of variabl is the number of element of the input arrai smaller than the randomli chosen pivot .
so , thi is the random variabl that take on some integr valu between zero at the smallest , that's if we happen to pick the pivot equal to the minimum of the arrai and n minu on at the largest .
that's if we happen to pick the maximum element as the pivot element .
next let's talk about the expect of a random variabl .
thi is realli noth more than the averag , and of cours when you take the averag of some statist , you want to do it weight by the probabl of it variou valu .
so let's just make that precis real quick .
so consid some random variabl , capit x , the expect , thi is also call the expect valu .
and the notat is capit e squar bracket , then of the random variabl .
and again in english the expect is just the averag valu , natur weight by the probabl of the variou possibl outcom .
or more mathemat we sum , over everyth that could happen so that littl i denot on possibl outcom .
we look at the valu of thi random variabl , when that outcom occur and then we weight it time the probabl of that outcom occur .
so the next two quizz ask you to comput the expect of the two random variabl that we identifi on the previou slide .
so the first quiz is about two dice and the random variabl , which is the sum of the valu of those two dice .
what is the averag valu of that random variabl ?
what is it expect ?
okai , so the answer to thi question is the second option .
the averag valu is seven .
there's a bunch of differ wai to see that .
in my opinion , the best wai to comput thi is us linear of expect , which is the next concept we're gonna cover .
if you want to , you can just comput thi by brute forc by which i mean you could iter over all <num> possibl outcom , look at the valu of the two dice in each and just evalu that sum we had in the definit on the last slide .
a slightli sneakier wai to do it if you don't know linear of expect would be to pair up the variou outcom .
so , it's equal like that sum of the two dice is two .
or twelv .
it's equal like to be three .
or eleven .
four and ten and so on .
each wai of pare up these valu of the two dice result in fourteen .
and when you averag you get seven .
but again , thi , the right wai to do thi is linear of expect which we'll cover next .
so , the second quiz cover the second random variabl we identifi .
so now we're back to quicksort and the random pivot show in the outermost and the question is how big , on averag , an expect is the sub arrai in the first recurs call ?
equival , on averag how mani element ar go to be less than the randomli chosen pivot ?
so the correct answer to thi quiz is the third option .
in fact , it's actual quantiti n minu on over two , not n over two , but basic half of the element .
again , there's sort of a sneaki wai to see thi , if you want , which is that clearli the two recurs call ar symmetr .
the expect valu of the left recurs call is go to be the same as the size of the right recurs call .
the two recurs call alwai compris n minu on of the element , so becaus thei're symmetr , you expect half in each , so n minu on over two in each .
though for thi problem , i think it's perfectli fine just to comput thi us the definit of expect .
so if we let x denot the random variabl that we care about ; the sub arrai size .
then we can just comput directli by sum over all of the possibl outcom , all of the possibl choic of the pivot .
so with probabl on over n we chose the minimum of the pivot result in zero element be pass to the first recurs call .
with probabl on over n we pick the second smallest element result in on element be pass to the first recurs call .
if probabl on over n we pick the third smallest given us a summari size of two .
and so on .
and then , with probabl on over n , we pick the maximum element , give us the sub arrai size of n <num> .
so if you just sort of comput thi sum out , you will get , as expect , n <num> over two .
so expect is the last definit that i'm go to give you in thi part on of the probabl review .
next if our fifth and final concept for thi video , which is linear of expect .
and that's not a definit , that's more of a theorem .
so what is linear of expect ?
well thi is a veri simpl properti of random variabl that's super , super import .
thi come up all the time when we analyz random algorithm and random process more gener .
so what is linear of expect ?
well it's the follow veri simpl claim .
which i'll sometim denot just by line x for short .
suppos you've got a bunch of random of variabl defin on the same sampl space .
then if you want to think of the expect valu of the sum of these random of variabl .
it doesn't matter if you take the sum first and then take the expect .
or if you take the expect first and then the sum .
that is the expect valu of a sum of random variabl .
is equal to , the sum of the expect of the individu random variabl .
and on of the reason lean , linear of expect is so ubiquit us is becaus it alwai work no matter what these random variabl ar , in particular , even when the random variabl ar not independ .
now , i haven't defin independ random variabl yet .
that will come in part two of the probabl review , but hopefulli you have an intuit sens of what independ mean .
so thing ar independ if know someth about on of the random variabl doesn't influenc what you expect from the other random variabl .
now i realiz the first time you see linear of expect , it's a littl hard to appreci .
so first of all as far as applic , we'll see plenti throughout thi cours .
pretti much everi singl applic of probabl that we'll see , the analysi will involv linear of expect .
but it mai be hard to appreci why thi is not a tautolog .
just symbol it mai look like it ha to be true .
but to point out that there is content here .
if i replac the sum by product , then thi equat would in gener be fals .
if the reign of variabl ar not independ .
so the same thing is not true about product .
it's realli about sum .
so let me just give you a trivial illustr of linear of expect , point out how it realli easili allow us to evalu the sum of two dice .
do our first run exampl let's introduc the first random variabl x on and x two for the result of the first and second die , respect .
now comput the expect valu of a singl die is easi .
there's onli six valu to enumer over .
contrast that with the <num> outcom to numer over when we evalu the sum of the two di .
so the averag valu of a singl die , you won't be surpris to hear , is <num> , right ?
so it rang , integ between on and six uniformli , so <num> on averag and now , us expect , the sum of two dice is simpli doubl the averag valu of a singl on .
so in the next slide , i'm go to prove thi properti , prove linear of expect .
but frankli , the proof is , is pretti trivial .
so if you don't care about the proof , that's fine .
you can skip it without loss .
i'm includ it just for complet .
and i gotta sai , i don't know of anoth mathemat statement which is simultan so trivial to prove , and so unbeliev us .
there's realli someth remark about linear of expect .
so how doe the proof go ?
well , honestli , we just write out the sum , the definit of an expect .
we revers the sum , and we're done .
so , let me start with the right hand side of the equat .
so that's , that wa the sum of the expect of the random variabl .
so now , let's just appli the definit of expect .
so , it's just a weight averag over the possibl outcom .
and now , instead of sum first over the , the reign of variabl j and then over the realiz outcom i , i am go to , to it in a revers order .
i'm go to sum first over the outcom i and then over the random variabl j .
now the probabl of outcom i is independ of j .
so we can yank the p of i outsid of that inter sum .
but now what have we got ?
so insid the parenthes , we simpli have the valu of the sum of the x i's , x x j's on the outcom i .
and over here , we're just averag the sum of the x j's with respect to the probabl , the p i's .
so thi is just the definit of the expect of the sum of the reign of variabl .
so that's it .
so linear of expect is realli just a revers of the doubl sum .
now for those of you who ar rusti on these kind of manipul , i just want to point out , you know thi revers of the doubl sum itself , is there's noth complic at all , about what's go on .
so if you want a realli pedestrian wai to think about what's happen , just imagin that we take these sum add , these and we just write them out in a grid , where on , or let's just sai the column ar index by the random variabl j and the row ar index by the outcom i .
and in a given cell of thi grid , we'd just write the sum in xji time pi  and the point is both of the doubl sum , both the first on and the second on , ar just the sum of all of the entri in thi grid .
in the first sum we first group thing accord to the row and then sum each of them up and the second sum , we group thing accord to the column .
we do column sum and then sum up the column sum .
but either wai it's just some of the entri in the grid , so it doesn't matter which wai you sum .
okai ?
so revers of doubl sum is just a trivial thing .
so if you get lost in the notat with these doubl sum , the point is , you can just interpret each of them in term of thi grid .
both of these doubl sum ar noth more than the sum of the valu in all of the cell of thi grid .
on order of summat just sai you group first accord to row sum , and then sum those up .
that's the first summat .
the second summat , you first take column sum , and then sum those up .
but of cours it doesn't matter .
you just get the result of everyth in the grid , okai ?
so there's no , , no trick up my sleev when i revers these sum .
it's a total elementari trivial thing , okai ?
so , again , linear expect .
trivial to prove .
incred us , don't forget it .
so i want to conclud thi video with on final exampl in order to tie togeth all of the concept that we just learn or just review .
and , it's go to be an exampl of load balanc .
a sign in process us to server .
but thi in fact is quit import for the analysi of hash that we're go to see toward the end of the cours as well .
but for now let's just think about the , the follow simpl problem .
for some integ n you have n comput process would have to be assign to n server in some wai .
now you're feel realli lazi .
okai .
so you're just gonna take each of the process and you're just gonna assign it to a total random server .
okai ?
with each server equal like to get a given process .
and the question i want to studi is , doe thi lazi cost you , at least on averag ?
so if you look at a server , what's the expect load ?
so let's proce to the solut , the answer to thi question .
so , befor you start talk about expect , on ha to be clear about the sampl space and what ar the probabl of the variou outcom ?
so rememb the sampl space omega just denot everi possibl thing that can happen .
so what ar we do ?
for each process , we're assign it to a random server , so all of the thing that can happen ar all of the differ assign of these end process to these end server and if you think about it , there ar n rais to the n possibl outcom , becaus you have n choic for each of the n process .
moreov , becaus each process is assign to on of the server uniformli at random , each of these assign is equal like .
probabl on over n to the n .
now that we have a sampl space , we're in a posit to defin a random variabl .
and we alreadi know what random variabl we care about .
we care about the averag load of a server .
now , all of the server ar exactli the same , so we just have to focu on on server , let's sai the first server .
and look at the number of process assign to it .
and if you go back to the problem statement what we're ask is to comput the expect valu of y .
the expect number of process assign to a server .
now , of cours , in principl we could go to the definit of expect and just comput by brute forc the sum over all possibl outcom of the valu of y and take the averag .
unfortun there ar end of the end differ outcom .
and that's a lot .
so , what can we do other than thi brute forc comput ?
well , recal our exampl of linear of expect and the sum of two dice .
we observ that instead of com .
comput the sum by enumer all thirti six outcom .
it wa much better to focu on a singl dye , comput it expect and then conclud with linear of expect .
so we'll do the same thing here , instead of focus on the sum y , we'll focu on constitu part of y .
so whether or not a singl process get assign to the first server and then we'll get awai with that with linear of expect .
so , more precis , for a given process , j , let's defin xj to be whether to be on if and onli if the j process get assign to the first server , zero otherwis .
<num> random variabl like xj ar often call indic random variabl .
that's becaus thei , in effect , indic whether or not a certain event occur .
in thi case , whether or not the process get assign to the first server .
why did i make thi definit ?
well , observ that the total number of process that get assign to the first server is simpli the sum from j equal on to n of xj .
xj sai whether or not a given process ; the jade process is on the first server .
the total number is the sum of these over all j .
now the benefit from thi maneuv is we onli to comput the expect of a extrem simpl indic random variabl xj .
thi is like the win that we got when we're sum up two dice by instead of have to comput the sum , the expect valu of the sum , we just had to focu on the expect of a singl die .
that wa realli easi .
similarli here the expect of a singl xj is realli easi .
specif , let's write it out just us the definit of the expect .
so the expect valu of an x j is .
well .
let's group togeth all the outcom in which it take on the valu zero .
so , the contribut of the expect is zero for all of those outcom .
and then there's the rest of the outcom where x j take on the valu on .
and in those case it contribut on to the expect .
now , obvious , we get some happi cancel happen here , with the zero part , and all we have to worri about is the probabl that xj take on the valu on .
okai , what wa xj again ?
how did we defin it ?
rememb it's the event that , it's on exactli when the jth process get assign to the first server .
how ar process assign .
or , rememb the propos solut assign to each process , to each of the end server , equal like , with uniform probabl .
so , the probabl that the jth process get assign to the first server , is on over n .
so thi leav us with just the sum , from j equal on to n , of on over n .
that is , we just sum up on over n with itself n time .
thi of cours is equal to on .
so summar , the expect number of process assign to a given server is exactli on .
so realli on averag we don't lose much for our lazi of just randomli sprai the process to the server .
on averag for ani given server we do fine .
and that's a good illustr of the role that random plai in a lot of algorithm design in comput scienc more gener .
often us random choic you can do surprisingli well with veri littl work and that's inde part of the crux of quick short , if you choos pivot at random you get an averag run time of n log n just as good as .
so at the end of the dai what we find is that the expect number of process assign to a given server ; sai the first server , is just on .
so , at least if we onli care about averag , we lose veri littl from thi trivial process of randomli sprai the process through the server .
on averag ani given server ha just on process on it .
thi is characterist of the role that random plai in algorithm design and comput scienc more gener .
often we can get awai with realli simpl heurist just by make random choic .
of cours , quicksort is on exampl of that , where we get an extrem preval us practic sort algorithm just by make randomli chosen pivot in everi recurs call .
so welcom to part two of our probabl review .
thi video assum you've alreadi watch part on or at least ar familiar with concept cover in part on .
name sampl space , event , random variabl , expect and linear of expect .
in thi part of the review we're go to be cover just two topic .
condit probabl and the closer relat topic of independ .
both between event and between random variabl .
i want to remind you that thi is by no mean the onli sourc you can or should us to learn thi materi .
a coupl of other sourc free that i recommend ar lectur note that you can find onlin by eric .
and also there's a wiki book on discret probabl .
so , condit probabl , i hope you're not surpris to hear , is fundament to understand random algorithm .
that said , in the five week we have here , we'll probabl onli us it onc .
and that's in analyz the correct of the random contract algorithm for comput the minimum cut of an undirect graph .
so , just to make sure we're all on the same page , here's some stuff you should have learn , from part on of the probabl review .
you should know what a sampl space is .
thi repres all of the differ outcom of the random coin flip , all of the differ thing that could happen .
often in random algorithm analysi , thi is just all of the possibl random choic that the algorithm might make .
each outcom ha some known probabl by and , of cours , the sum of the probabl equal on and rememb that event is noth more than a subset of omega .
omega is everyth that could possibl happen .
s is some subset of thing that might happen and , of cours , the probabl of event is just the probabl of , of all the outcom that the event contain .
so , let's talk about condit probabl .
so on discuss the condit probabl of on event given a second event .
so , let x and y denot two event , subset of the same sampl space .
you might want to think about these two event x and y in term of an event diagram .
so we could draw a box , repres everyth that could conceiv happen .
so that's omega .
then we can draw a blob correspond to the event x .
so that's some stuff .
might or might not happen , who know .
and then the other event y is some other stuff which might or might not happen .
and in gener these two event could be disjoint , that is thei could have no intersect .
or thei might have a non trivial intersect .
x intersect y .
similarli thei need not cover omega .
it's possibl that noth x nor y happen .
so what's we're look to defin is the probabl of the event x given the even y .
so we write probabl of x bar y , phrase x given y .
and the definit is , i think , pretti intuit .
so given y mean we assum that someth in y happen .
origin anyth in omega could have happen .
we didn't know what .
now we're be told that whatev happen that li somewher in y .
so we zoom in on the part of the pictur that , in which contain y .
so that's gonna be our denomin .
so , our new world is the stuff in y .
that's what we know happen .
and now we're interest in the proport of y that is fill up with x .
so , we're interest in what fraction of y's area is occupi by stuff in x .
so x intersect y , divid by the probabl of y .
that is by definit the condit probabl of x given y .
let ? s turn to a quiz , us our familiar exampl of roll two dice .
to make sure that the definit of condit probabl make sens to you .
okai , so the correct answer to thi quiz is the third answer .
so let's see why that is .
so what ar the two event that we care about ?
we want to know the probabl of x given y , where x is the event that at least on die is a on .
and y is the event that the sum of the two dice is seven .
now , the easiest wai to explain thi is let's zoom in , let's drill down on the y .
let's figur out exactli which outcom y compris .
so the sum of the two dice , be seven , we saw in the first part of the review , there's exactli six outcom which give rise to the sum seven , name the order pair on , six .
two five , three four , four three , five two , and six on .
now , rememb that the probabl .
of x given y is by definit the probabl of x intersect y divid by the probabl of y .
now , what you notic from thi formula is we actual don't care about the probabl of x per se or even about the event x per se , just about x intersect y .
so , let's just fig , so , now we know why there ha to be six outcom .
which of those also belong to x ?
well , x is those where at least on die is on .
so , x intersect y is just go to be the on , six and the six , on .
now the probabl of each of the <num> possibl outcom is equal like .
so each on is on over <num> .
so sinc x intersect y , ha onli two outcom .
that's gonna give us two over <num> in the numer .
sinc y ha six outcom , that give us a six over <num> in the denomin .
when you cancel everyth out , you're left with a on third .
so just appli the definit of condit probabl to the correct definit of the two relev event , we find that inde a third of the time is when you have a on condit on the sum of the two be seven .
let's move on to the independ of two event .
so .
again we consid two event , x and y .
by definit , the event ar depend if and onli if the follow equat hold .
the probabl that both of them happen .
that is the probabl of x intersect y is exactli equal to the probabl that x happen time the probabl that y happen .
so that's a simpl innocu look definit .
let me re phrase it in a wai that it's even more intuit .
so i'll you check thi , it's just a some trivial algebra .
thi equat hold , for the event x and y , if and onli if , thi is just us the definit of condit probabl we had on the last slide , if and onli if the probabl of x given y , is exactli the same thing as the probabl of x .
so , intuit , know that y happen , give you no inform about the probabl that x happen .
that's the sens in which x and y ar independ .
and , you should also check that thi hold if and onli if , the probabl of y , given x , equal the probabl of y .
so , symmetr , know that x ha occur give you no inform , no new inform about whether or not y ha occur .
the probabl of y is unaffect by condit on x .
so at thi junctur i feel compel to issu a warn .
which is , you mai feel like you have a good grasp of independ .
but , in all likelihood , you do not .
for exampl i rare feel confid that i have a keen grasp on independ .
of cours i us it all the time in my own research and my own work , but it's a veri subtl concept .
your intuit about independ is veri often wrong , even if you do thi for a live .
i know of no other sourc that's creat so mani bug in proof by profession mathematician and profession comput scienc research as misunderstand of independ and us intuit instead of the formal definit .
so , for those of you without much practic with independ , here's my rule of thumb for whether or not you treat random variabl as independ .
if thing ar independ by construct , like , for exampl , you defin it in your algorithm , so the two differ thing ar independ .
then you can proce with the analysi under the assumpt that thei're independ .
if there's ani doubt , if it's not obviou the two thing ar independ , you might want to , as a rule of thumb , assum that thei're depend until further notic .
so the slide after next will give you a new exampl show you thing which ar independ and thing which ar not independ .
but befor i do that i wanna talk about independ of random variabl rather than just independ of event .
so you'll recal a random variabl is from the first video on probabl review .
it's just a real valu function from the sampl space to the real number .
so onc you know what happen you have some number .
the random variabl evalu to some real number .
now , what doe it mean for two random variabl to be independ ?
it mean the event of the two variabl take on ani given pair of valu ar independ event .
so inform , know the valu taken on by on of the random variabl tell you noth about what valu is taken on by the other random variabl .
recal the definit of what it mean for two event to be independ , thi just mean that , the probabl that a take on valu littl a , b take on valu littl b .
the probabl that both of those happen is just the product of the probabl that each happen individu .
so what's us about independ of event is that probabl just multipli .
what's us about independ of random variabl is that expect just multipli .
so , we're go to get an analog of linear expect where we can take , we can interchang an expect in the product freeli , but i want to emphas thi , thi interchang of the expect of the product is valid onli for independ random variabl and not in gener , unlik linear expect .
and we'll see a non exampl .
we'll see how thi fail on the next slide for non independ random variabl .
so , i'll just state it for two random variabl , but the same thing hold by induct for ani number of random variabl .
if two random variabl ar independ , then the expect of their product .
equal the product of their expect .
and again , do not forget that we need a hypothesi .
rememb , linear of expect did not have a hypothesi for thi statement about product .
we do have a hypothesi of them be independ .
so why is thi true ?
well , it's just a straight forward deriv where you follow your nose or write it out here for complet , but , but i realli don't think it's that import .
so you start with the expect of the product .
thi is just the averag valu of a time b , of cours weight by the probabl of , of ani particular valu .
so the wai we're gonna group that sum is we're go to sum over all possibl combin of valu , a and b , that capit a and capit b might take on , so that's gonna give us a valu of a time b .
time the probabl of that big a take on the valu of littl a and capit b take on the valu of littl b .
so that's just by definit where thi is the valu of the random variabl , capit a time capit b and thi is the probabl that it take on that valu with the valu a and b .
now becaus a and b ar independ , thi probabl factor into the product of the two probabl .
thi would not be true if thei were not independ .
it's true becaus thei're independ .
so same sum where all possibl joint valu of all a and b .
you still have a time b .
but now we have time the probabl that a take on the valu of a time the probabl that b take on the valu of b .
so now we just need to regroup these term .
so let's first sum over a .
let's yank out all the term that depend on littl a .
notic none of those depend on littl b .
so we can yank it out in front of the sum over littl b .
so i have an a time the probabl that big a take on the valu of littl a .
and then the stuff that we haven't yank out is the sum over b , of b time , littl b time the probabl that capit b take on the valu littl b .
and what's here insid the quantiti ?
thi is just the definit of the expect of b .
and then what remain after we have factor out the expect of b ?
just thi other sum which is the definit of the expect of a .
so , inde four independ random variabl , the expect valu of the product is equal to the product of the expect .
let's now wrap up by ty these concept togeth in an exampl , a simpl exampl that nevertheless illustr how it can be tricki to figur out what's independ and what's not .
so here's the set up .
we're go to consid three random variabl .
x1 , x2 and x3 .
x1 and x2 we choos randomli , so thei're equal like to be zero or on .
but x3 is complet determin by x1 and x2 .
so it's gonna be the xor of x1 and x2 .
so xor stand for exclus or .
so what that mean is that if both of the operand ar zero , or if both of them ar on , then the output is zero .
and if exactli on of them is on , exactli on of them is zero , then the output is on .
so it's like the logic or function , except that both of the input ar true , then you output fals , okai ?
so that's exclus or .
now thi is a littl hand wavi , when we start talk about probabl , if we want to be honest about it , we should be explicit about the sampl space .
so what i mean by thi , is that x1 and x2 take on all valu , thei're equal like .
so we could have a zero , zero or a on zero or a zero on or a on , on and in each of these four case , x3 is determin by the first two , as the x or , so you get a zero here , a on here , a on here and a zero there .
and each of these four outcom is equal like .
so let me now give you an exampl of two random variabl , which ar independ , and a non exampl .
i'll give you two random variabl which ar not independ .
so first , i claim that , if you think that x1 and x3 , then thei're independ random variabl .
i'll leav thi for you to check thi mai or mai not seem counter intuit to you .
rememb x3 is deriv in part from x1 .
never the less , x1 and x3 , ar inde independ .
and why is that true ?
well , if you innumer over the four possibl outcom , you'll notic that all four possibl two byte string occur as valu for on and three .
so here thei're both zero , here thei're both on , here you have a zero and on , and here you have a on and zero .
so you've got all four of the combin of probabl on over four .
so it's just as if x1 and x3 were independ fair coin flip .
so that's basic why the claim is true .
now .
that's a perhap counterintuit exampl of independ random variabl .
let me give you a perhap counterintuit exampl of depend random variabl .
needless to sai , thi exampl just scratch the surfac and you can find much more deviou exampl of both independ and non independ if you look in , sai , ani good book on discret probabl .
so now let ? s consid the random variabl x1 product x3 .
and x two and the claim is these ar not independ .
so thi'll give you a formal proof for .
the wai i'm go to prove thi could be slightli sneaki .
i'm not go to go back to the definit .
i'm not gonna contradict the consequ of the definit .
so it's prove that thei're not independ all i need to do , is show that the product of the expect is not the same as the expect to the product .
rememb if thei were independ , then we would have that equal .
product of expect will equal the expect to product .
so if that's fals than there's no wai these random variabl ar independ .
so the expect of the product of these two random variabl is just the expect valu of the product of all three .
and then on the other side , we look at the product of the expect valu of x1 and x3 .
and the expect valu of x2 .
so let's start with the expect valu of x2 .
that's pretti easi to see .
that is zero half the time and that is on half the time .
so the expect valu of x2 is go to be on half .
how about the expect valu of x1 and x3 ?
well , from the first claim , we know that x1 and x3 ar independ random variabl .
therefor , the expect valu of their product is just the product of their expect .
equal thi expect equal to the expect valu of x1 time the expect valu of x2 , excus me , of x3 .
and again , x1 is equal like to be zero or on .
so it expect valu is a half .
x3 is equal like to be zero or on so it expect valu is a half .
so the product of their expect is on fourth .
so the right hand side here is on eighth ; on half time on fourth , so that's an eighth .
what about the left hand side , the expect valu of x1 time x3 time x2 ?
well , let's go back to the sampl space .
what is the valu of the product in the first outcom ?
zero .
what is the valu of the product in the second outcom ?
zero .
third outcom ?
zero .
forth outcom ?
zero .
the product of all three random variabl is alwai zero with probabl on .
therefor , the expect valu , of cours , is gonna be zero .
so inde , the expect valu of the product of x1 , x3 and x2 zero doe not equal to the product of the correspond expect .
so thi show that x1 , x3 and x2 ar not independ .
i said pretti much everyth i wanna sai about sort at thi point but i do wanna cover on more relat topic , name the select problem .
thi is a problem of comput order statist of an arrai .
with comput the median of an arrai be a special case .
analog to our coverag of quick sort the goal is go to be the design and analysi of a super practic random algorithm that solv the problem .
and thi time we'll even achiev an expect run time that is linear in the length of the input arrai .
that is bigger of n for input arrai of length n as suppos to the o of n login time that we had for the expect run time of quick sort .
like quick sort the our analysi is also go to be , quit eleg .
so in addit these two requir video's on thi veri practic algorithm will motiv two option video's that ar on veri cool topic but of a similar more theoret natur .
the first option video is go to be on how you solv the select problem in determinist linear time .
that is without us random .
and the second option video will be a sort lower bound .
that is why no comparison sort can be better than mersort .
can have better run time .
and login .
so a few word about what you should have fresh in your mind befor you watch thi video .
i'm definit assum that you've watch the quick sort video .
and not just watch them , but that you have that materi pretti fresh in your mind .
so in particular the video of quick sort about the partit subroutin , so thi is where you take an input arrai , you choos a pivot , and you do by repeat swap .
you rearrang the arrai so that everyth less than the pivot's to the left of it , everyth bigger than the pivot is to the right of it .
you should rememb that sub routin .
you should also rememb the previou discuss about pivot choic , the idea that the qualiti of a pivot depend on how balanc a split into two differ sub problem it give you .
those ar both go to be import .
for the analysi of thi random linear time select algorithm , i need you to be , rememb the concept from probabl review part on , in particular , random variabl , their expect , and linear of expect .
that said let ? s move on and formal defin what the select problem is .
the input is the same as for the sort problem ; just you're given an arrai .
of indistinct entri but in addit , you're told what order statist you're look for .
so that's gonna be a number i , which is an integ between on and n .
and the goal is to output just a singl number , mainli the i th order statist .
that is the i th smallest entri in thi input arrai .
so just to be clear , we have an arrai entri of , let's just , sai four element .
continu the number ten , eight , two , and four , and you were look for sai the third or statist , that would be thi eight .
the first order statist is just the minimum elent , element of the arrai .
that's easi to find with a linear scan .
the nth order statist is just the maximum .
again easier , easi to find with a linear scan .
the middl element is the median and you should think of that as the canon version of the select problem .
now , when n is odd , it's obviou what the median is .
that's just the middl element , so the n on over <num> th order statist .
if the arrai ha even link , there's two possibl medium , so let's just take the smaller of them .
that's the end over truth order statist .
you might wonder why you'd ever want to comput the median of an arrai rather than the mean , that is , the averag .
it's easier to see that you can comput the averag with a simpl linear scan .
and the median you can , on motiv is it's a more robust version of the mean .
so if you just have a data entri problem and it corrupt on element of an input arrai , it can total screw up the averag valu of the arrai .
but it ha gener veri littl impact on the median .
a final comment about the problem is i am go to assum that the arrai entri ar distinct .
that is , there's no repeat element .
but just like in our discuss of sort , thi is not a big assumpt .
i can encourag you to think about how to adapt these algorithm to work even if the arrai do have duplic .
you can inde still get the same veri practic , veri fast algorithm with duplic element .
now , if you think about it , we alreadi have a pretti darn good algorithm that solv the select problem .
here's the algorithm , it two simpl step and it run in o of n login time .
step on , sort the input arrai .
we have variou subroutin to do that .
let's sai we pick merg sort .
now what is it we're try to do ?
we're try to find the ith smallest element in the input arrai .
well , onc we've sort it , we certainli know where the ith smallest element is .
it's in the ith posit of the sort arrai .
so that's pretti cool .
we've just done what a comput scientist would call a reduct , and that's a super us and super fundament concept .
it's when you realiz that you can solv on problem .
by reduc it to anoth problem that you alreadi know how to solv .
so what we just show is that the select problem reduc easili to the sort problem .
we alreadi know how to solv the sort problem n log n time , so that give us an n log n time solut to the select problem .
but , again , rememb the mantra of ani algorithm design worth their salt is can we do better .
we should avoid contin .
just becaus we got n log n doesn't mean we can stop there .
mayb we can be even faster .
now certainli we gonna have to look at all the element in the input arrai in the worst case .
we shouldn't expect to do better than linear , but , hei why not linear time ?
actual , if you think about it , we probabl should have ask that question back when we were studi the sort problem .
why were we so content with the n log n time bound for merg short ?
and the o of n log n time on averag bound for quick sort .
well , it turn out we have a realli good reason to be happi with our n log n upper bound for the sort problem .
it turn out , and thi is not obviou , and will be the subject of an option video .
you actual can't sort an input arrai of length n better than n log n time .
either in the wors case , or on averag .
so , in other word , if we insist on solv the select problem via a reduct to the sort problem then we're stuck with thi n log n time amount .
okai , strictli speak that's for someth call comparison sort ; see the video for more detail .
but the upshot is if we want a gener purpos algorithm and we wanna do better than n log n for select , we have to do it us ingenu beyond thi reduct .
we have to prove that select is a strictli easier problem .
then sort it .
that's the onli wai we're gonna have an algorithm that beat n log n .
it's the onli wai we can conceiv get a linear time algorithm .
and , that is exactli what is up next on our plate .
we're go to show select is inde fundament easier than sort .
we can have a linear time algorithm for it , even though we can't get a linear time algorithm for sort .
you can think of the algorithm we're gonna discuss as a modif of quick sort , and in the same spirit of quick sort , it will be a random algorithm , and the run time will be an expect run time that will hold for ani input arrai .
now , for the sort problem , we know that quick sort , that n log n time on averag for the averag is over the coin flip done by the code .
but we also know that if we want to we could get a sort algorithm in n log n time that doesn't us random .
the merg sort algorithm is on such solut .
so , if you were give a liner time solut for a select for find order statist that us random .
and we'd be natur to wonder is there an analog to merg sort ?
is there an algorithm which doe not us random and get thi exact same linear time ?
in fact , there is .
the algorithm's a littl more complic and there .
for not quit as practic as thi random algorithm .
but , it's still veri cool .
it's a realli fun algorithm to learn and to teach .
so , i will have an option video about linear time select without random .
so , for those of you who aren't gonna watch that video or wanna know what's the kei idea the idea is to choos the pivot determinist in a veri care wai us a trick call the median of median .
that's all i'm gonna sai about it now .
you should watch the option video if you want more detail .
i do feel compel to warn you that if you're go to actual implement a select algorithm , you should do the on that we discuss in thi video and not the linear time on becaus the on we'll discuss in thi video ha both smaller constant and work in place .
so , what i want to do next is develop the idea that we can modifi the quick sort paradigm in order to directli solv the select problem .
so , to get an idea of how that work , let me review the partit subroutin .
like in quick sort .
thi subroutin will be our workhors for the select algorithm .
so what the partit subroutin doe is it take and input some jumbl up arrai and it's go to .
solv a problem which is much more modest than sort .
so , in partit , it's go to first choos a pivot element , somehow .
we'll have to discuss what's a good strategi for choos a pivot element .
but suppos , you know , in thi particular input arrai , it choos the first element , thi three , as the pivot element .
the respons of the partit subroutin , then , is to rearrang the element in thi arrai so that the follow properti ar satisfi .
anyth less than the pivot is to the left of it .
it can be in jumbl order but if you're less than the pivot , you'd better be to the left like thi two and on is less than the three .
if you're bigger than the pivot , then again you can be in jumbl order amongst those element but all of them have to be to the right of the pivot , and that's true for the number four through eight , thei all ar to the right of the pivot three in a jumbl order .
so thi in particular put the pivot in it right posit where we belong in the final sort arrai and at least for quick sort , it enabl .
us to recurs sort to smaller sub problem , so thi is where i want you to think a littl bit about how we should adapt thi paradigm .
so suppos i told you the first step of our select algorithm is go to be to choos a pivot and partit the arrai , now the question is .
how ar we go to recur ?
we need to understand how to find the ith order statist of the origin input arrai .
it suffic to recur on just on sub problem .
of smaller size .
and find a suitabl order statist in it .
so how should we do that ?
let me ask you that in , with some veri concret exampl , about what pivot we'd choos , and what order statist we're look for , and see what you think .
so the correct answer to thi quiz is the second answer .
so we can get awai with recurs just onc , and in thi particular exampl we're go to recur on the right side of the arrai .
and instead of look for the fifth order statist like we were origin .
we're go to recurs search for the second order statist .
so , why is that ?
well , first , why do we recur on the right side of the arrai ?
so , by assumpt we have an arrai with ten element .
we choos the pivot , we do partit .
rememb the pivot wind up in it right posit .
that's what partion doe .
so , if the pivot wind up in the third posit , we know it's the third smallest element in the arrai .
now that's not what we were look for .
we were look for the fifth smallest element in the arrai .
that of cours is bigger .
than the third smallest element of the arrai , so by partit , where is the fifth element gonna be , it's gotta be to the right , of thi third smallest element , to the right of the pivot , so we know for sure .
that the fifth order statist of the origin arrai li to the right of the pivot .
that is guarante .
so we know where to recur on the right hand side .
now .
what ar we look for ?
we ar no longer look for the fifth order statist , the fifth smallest element .
why ?
well , we've thrown out both the pivot , and everyth smaller than it .
rememb , we're onli recurs on the right hand side .
so we've thrown out the pivot , the third element , and everyth less that it .
the minimum and the second minimum .
have delet the three smallest element , and origin look for the fifth smallest of what remain of what we're recurs on .
we're look for the second smallest element .
so , the select algorithm , in gener , is just the gener of thi idea , so arbitrari arrai .
and arbitrari situat of whether the pivot come back , equal to less than or bigger than the element you're look for .
so let me be more precis .
i'm gonna call thi algorithm r select for random select .
and accord to the problem definit it take as input as usual an arrai a of some length n .
but then also the order statist that we're look for .
so we're gonna call that i .
and of cours we assum that i is some integ between on and n inclus .
so for the base case that's gonna be if the arrai ha size on .
then the onli element we could be look for is the on quarter statist and we just return the sole element of the arrai .
now we have to partit the arrai around the pivot element .
and just like in quick sort we're not go to be , we're go to be veri lazi about choos the pivot .
we're gonna choos it uniformli at random from the possibl and hope thing work out .
and that'll be the crup of the analysi prove that random pivot ar good enough suffici often .
have chosen the pivot we now just invok that standard partit sub routin .
as usual that's gonna give us the partit's arrai .
you'll have the pivot element .
you'll have everyth less than the pivot to the left , everyth bigger than the pivot to the right .
as usual , i'll call everyth to the left the first part of the partit arrai .
everyth bigger the second part .
now we have a coupl of case , depend on whether the pivot is bigger or less than the element we're look for .
so i need notat to , to talk about that .
so let's let j be , the order statist that p is .
so if p wind up be the third smallest element , like in the quiz , then j's gonna be equal to three .
equival , we can think of j as defin as the posit of the pivot in the partit version of the arrai .
now there's on case which is veri unlik to occur but we should includ it just for complet .
if we're realli lucki then in fact our random pivot just happen to be the older statist we were look for .
that's when i equal j .
we're look for i's smallest element if by dumb luck the pivot wind up be i's smallest element , we're done .
we can just return it we don't have to recur .
now in gener of cours , we don't randomli choos the element we're look for .
we choos someth that well , it could be bigger or could be smaller than it .
in the quiz , we chose a pivot that wa smaller than what we're look for .
actual , that's the harder case .
so let's first start with a case where the pivot wind up be bigger than the element we're look for .
so that mean that j is bigger than i .
we're look for the ith smallest .
we randomli chose the jth smallest for j bigger than i .
so thi is opposit case of the quiz .
thi is where we know what we're look for ha to be to the left of the pivot .
the pivot's the j smallest .
everyth less than it is to the left .
we're look for the i smallest .
i is less than j , so that's gotta be on the left .
that's where we recur .
moreov , it's clear we're look for exactli the same order statist .
if we're look for the third smallest element , we're onli throw out stuff which is bigger than someth even bigger than the third smallest element .
so we're still look for the third smallest of what remain .
and natur the new arrai size is j minu on , becaus that's what's is to the left of the pivot .
is when the random element that we choos is less than what we ar look for and then we're just like the quiz .
so name what we ar look for is bigger than the pivot , it's got to be on the right hand side , we know we've got recur on the right hand side .
into the right hand side ha n j element we threw out everyth up to the pivot .
we threw out j thing that's n j left and of all of the j thing we threw out ar less than what we ar look for .
so what we us to be look for is i's smallest element now we ar look for the i j's smallest element .
so that is the whole algorithm .
that is how we adopt the approach we took toward the sort problem in quick sort , and adapt it to the problem of select .
so , is thi algorithm ani good ?
let's start studi it properti , and understand how well it work .
so let's begin with correct .
so the claim is that , no matter how the algorithm's coin flip come up , no matter what random pivot we choos , the algorithm is correct , in the sens that it's guarante to output the order statist .
the proof is by induct .
it preced veri similarli to quick sort , so i'm not gonna give it here .
if you're curiou about how these proof go , there's an option video about the correct of quick sort .
if you watch that and understand it , it should be clear how to adapt that induct argument , to appli to thi select algorithm as well .
so as usual for divid and conquer algorithm , the interest part is not so much know , understand why the algorithm work , but rather understand how fast it run .
so the big question is , what is the run time of thi select algorithm ?
now to understand thi , we have to understand the ramif of pivot choic on the run time .
so , you've seen the quick sort video , thei're fresh in your mind .
so what should be clear is that , just like in quick sort , how fast is algorithm on is go to depend on how good the pivot ar , and what good pivot mean is pivot that guarante a balanc split .
so in the next quiz , we'll make sure that you understand thi point , and ask you to think about just how bad thi select algorithm could be if you get extrem unlucki in your pivot choic .
so the correct answer to thi question is exactli the same as the answer for quick sort .
the worst case run time , if the pivot ar chosen , just , in a realli unlucki wai , is actual quadrat in the arrai length .
rememb , we're shoot for linear time .
so thi quadrat is a total disast .
so how could thi happen ?
well , suppos you're look for the median .
and suppos you choos the minimum element as the pivot everi singl time .
so if thi is what happen if everi time you choos the pivot to be the minimum , just like in quick sort thi mean everi time you recurc all you succe do is peel off a singl element from the inputt arrai , now you're not go to find the medium element until you've done roughli n over two recurs call , each on an arrai that ha size at least a constant fraction of the origin on , so that it's a linear number of recurs call , each on an arrai of size at least some constant time n , so that give you a total run time of quadrat overal .
of cours , thi is an absurdli unlik event .
frankli , your comput is more like to be struck by a meteor than it is for the pivot to be chosen as the minimum element in everi recurs call .
but , if you realli had an absolut worst case choic of pivot , it would give thi quadrat run time bound .
so the upshot then is that the run time of thi random select algorithm depend on how good our pivot ar and for worst case choic of pivot , the run time can be as larg as n squar .
now hopefulli , most of the time we're gonna have much better pivot and so the analysi proce on make that idea precis .
so the kei to a fast run time is go to be the , the usual properti that we want to see in divid and conquer algorithm .
mainli everi time recours , everi time thei recours , the problem size better not just be smaller , but it better be smaller by a signific factor .
how would that happen in thi select approach base on the partit subroutin ; well if both of the sub problem ar not too big , then we're guarante that when we recours we make a lot of progress .
so let's think about what the best possibl pivot would be in the sens of give a balanc split .
right ?
so , of cours , in some sens the best pivot is you just choos the order statist you're look for and that , then you're done in constant time but that's extrem unlik and it's not worth worri about .
so ignor the fact that we might guess the pivot .
what's the best pivot if we want to guarante an aggress decreas in the input size befor the next gener .
well , the best pivot's the on that give us as most balanc split as possibl .
so what's the pivot that give us the most balanc split , a <num> <num> split ?
well , if you think about it , it's exactli the median .
of cours , thi is not super help , becaus the median might well be what we're look for in the first place .
so thi is , sort of , a circular idea .
but for intuit , it's still worth explor what kind of run time we would get in the best case , right .
if we're not go to get linear time even in thi magic best case , we certainli wouldn't expect to get it on averag over random choic of the pivot .
so what would happen if we actual did luckili choos the median as the pivot everi singl time ?
well , we get the recurr that the run time that the algorithm requir on an arrai of length n .
well , there's onli gonna be on recurs call .
so thi is the big differ from quick sort , where we had to recur on both side , and we had two recurs call .
so here , we're gonna have onli on recurs call .
in the magic case where our pivot ar alwai equal to the median , both sub problem size contain , ar onli half as larg as the origin on .
so when we recur , it's not a problem size guarante to be , at most , n over two .
and then outsid the recurs call , pretti much all we do is a partit indic .
and we know that that's linear time .
so the .
recurr we get is t of n is at most t of n over two , plu big o of n .
thi is total readi to get plug into the master method .
it wind up be case two of the master method .
and inde , we get exactli what we want , linear time .
to reiter , thi is not interest in it own right , thi is just for intuit .
thi wa a saniti check that , at least for a best case choic of pivot , we'd get what we want , a linear time algorithm , and we do .
now the question is , how well do we do with random pivot ?
now the intuit , the hope is exactli as it wa for quick sort , which is that random pivot ar perfectli good surrog for the median , the perfect pivot .
so have the analysi of quick sort under our belt we're inde random pivot do approxim veri close with the perform you'll get with best case pivot .
mayb , now we have reason to believ thi is hopefulli true .
that said as a mathemat statement thi is total not obviou and it's go to take proof , that's the next subject for next video .
but let me just be clear exactli what we're claim .
here is the run time guarante the random select provid .
for an arbitrari input arrai of length n , the averag run time of thi random collect algorithm is linear , big o of n .
let me reiter a coupl point i made for the analog guarante for the quick sort algorithm .
the first is that , we're make no assumpt about the data whatsoev , in particular we're not assum that the data is random .
thi guarante hold not matter what input arrai that you feed into thi random algorithm , in that sens thi is a total gener purpos subroutin .
so , where then doe thi averag come from ?
where doe the expect come from ?
the random is not in the data , rather the random is in the code and we put it there ourselv .
now let's proce to the analysi .
in thi video i'll explain the mathemat analysi of the random linear time select algorithm that we studi in the previou video .
specif , i'm go to prove to you the follow guarante for that algorithm .
for everi singl input arrai of length n the run time of thi random select algorithm on averag will be linear .
pretti amaz if you think about it becaus that's bare more what the time it take just to read the input .
and in particular thi linear time algorithm is even faster than sort .
so thi show that select is a fundament easier problem than sort .
you don't need to reduc to sort .
you can solv it directli in o n time .
i want to reiter the same point i made about quick sort .
the guarante is the same .
it is a gener purpos subroutin .
we make no assumpt about data .
thi theorem hold no matter what the input arrai is .
the expect , the averag that's in the theorem statement is onli over the coin flip made by the algorithm made insid it's code of our own devis .
befor we plung into the analysi , let me just make sure you rememb what the algorithm is .
so it's like quick sort .
we partit around a pivot except we onli recurs onc , not twice .
so we're given an arrai with some length n .
we're look for the ith order statist , the ith smallest element .
the base case is obviou .
you're not in the base case ; you choos a pivot p , uniformli at random from the input arrai just like we did in quick sort .
we partit around the pivot just like we did in pic , in quick sort .
that split the arrai into a first part of those element less than the pivot and the second part of those element which ar bigger than the pivot .
now , we have a coupl of case .
the case which is veri unlik so we don't realli worri about , if we're lucki enough to guess the pivot as the ith order statist what we're look for .
that's when the new posit j .
of the pivot element happen to equal i .
what we're look for .
then , of cours , we just return it .
that wa exactli what we want .
in the gener case , the pivot is go to be in the posit j , which is either bigger than what we're look for i , that's when the pivot is too big or j .
it's posit will be less than the ith order statist we're look for .
that's when the pivot is too small .
so if the pivot's too big , if j is bigger than i that when we're look for is on the left hand side amongst the element less than the pivot .
so that's where we recurs .
we've thrown out both the pivot and everyth to the right of it .
that leav us with an arrai of j minu i element and we're still look for the ith smallest among these j minu1 smallest element .
and then the final case , thi is what we went through in the quiz and last video , is if we choos a pivot who's smaller than what we're look for , that's when j is less than i , then it mean we're safe to throw out the pivot and everyth less than it .
we're safe recurs on the second part of those element bigger than the pivot .
have thrown out the j's smallest element , we're recurs on an element of length of n j and we're look for the i j smallest element in those that remain , have alreadi thrown out the j smallest from the input arrai .
so that's random select .
let's discuss why it's linear time on averag .
the first thought that you might have , and thi would be a good thought , would be that we should proce exactli the same wai that we did in quick sort .
you recal that when we analyz quick sort , we set up these indic random variabl , x , i , j determin whether or not a given , pair of element got compar at ani point in the algorithm .
and then we just realiz the sum of the comparison is just the sum , overal , of these x , i , js .
we appli linear of expect and it boil down to just figur out the probabl that a given pair of element get compar .
you can analyz thi random select algorithm in exactli the same wai .
and it doe give you a linear time bound on averag .
but it's a littl messi .
it wind up be not quit as clean as in the quick sort analysi .
moreov , becaus of the special structur of the select problem , we can proce in an even more slick wai here than the wai we did with quick sort .
so , again we'll have some constitu random variabl .
we'll again appli linear of expect but the definit of those random variabl is go to be a littl bit differ than it wa in quick sort .
so , first a preliminari observ .
which is that the workhors for thi random select procedur is exactli the same as it wa in quick sort .
name it's the partit subroutin .
essenti all of the work that get done outsid of the recurs call just partit the input arrai around some pivot element as we discuss in detail in a separ video that take linear time .
so usual when we sai someth's linear time we just us big o notat .
i'm gonna go ahead and explicitli us a constant c here for the oper outsid the recurs call .
that'll make it clear that i'm not hide anyth up my sleev when we do the rest of the analysi .
now what i wanna do on thi slide is introduc some vocabulari , some notat which will allow us to cleanli track the progress of thi recurs select algorithm .
and by progress i mean .
the length of the arrai on which is current oper .
rememb we're hope for a big win over quick sort , cuz here we onli do on recurs call , not two .
we don't have to recurs on both side of the pivot just on on of them .
so it stand to reason , that we can think about the argument make more and more progress as a singl recurs call oper on arrai of smaller and smaller length .
so the notion that will be import for thi proof is that of a phase .
thi quantifi how much progress we've made so far , with higher number phase correspond to more and more progress .
we'll sai that the r select algorithm at some midpoint of it execut is in the middl of phase j .
if the arrai size that the current recurs call is work on ha length between <num> 4th rais to the j time n and the smaller number <num> 4th j <num> time n .
for exampl think about the case where j equal zero .
that sai phase zero recurs call , oper on arrai with size of n and <num> percent of n .
so , certainli , the outermost recurs call is go to be in phase zero .
becaus the input arrai ha size n .
and then , depend on the choic of the pivot , you mai or mai not get out of phase zero in the next recurs call .
if you choos a good pivot , and you wind up recurs on someth , that ha , at most , <num> percent of the origin element , you will no longer be in phase zero .
if you recurs on someth that ha more than <num> percent of what you start with , of the .
input arrai , then you're still gonna be in phase zero even in the second recurs call .
so overal the phase number j , quantifi the number of time we've made <num> percent progress , rel to the origin input arrai .
and the other piec of notat that's go to be import is what i'm go to call xj .
so for a phase j , xj simpli count the number of recurs call in which a random select algorithm is in phase j .
so thi is gonna be some integ .
it could be as small as zero , if you think about it , for some of the phase .
or it could be larger .
so why am i do thi ?
why am i make these definit of phase and of these xj ?
what's the point ?
we're gonna rememb the point wa we wanna be abl to cleanli talk about the progress that the random select algorithm make through it recurs , and what i wanna now show you is that in term of these xj , count the number of iter in each phase , we can deriv a rel simpl upper bound on the number of oper that our algorithm requir .
specif the run time of our algorithm , can be bound abov by the run time in a given phase , and then sum those quantiti over all of the possibl phase .
so we're gonna start with a big sum , over all the phase j .
we want to look at the number of recurs call that we have to endur in phase j , so that's xj by definit .
and then we look at the work that we do outsid of the recurs call in each recurs call dure phase j .
now , in a given recurs call , outsid of it recurs call , we do c time m oper where m is the length of the input arrai and dure phase j we have an upper bound on the link of the input arrai .
by definit it's at most three quarter rais to the j time n .
so that is , we multipli the run time time thi constant c thi , we inherit from the partit subroutin and then we can , for the input length , we can put an upper bound of three quarter rais to the j time n .
so just to review where all of these term come from , there's three quarter j time n is an upper bound on the arrai size .
dure phase j , thi by the definit of the phase .
then , if we multipli that time c , that's the amount of work that we do on each phase j sub problem .
how much work do we do in phase j overal or we just take the work per sub problem that's what's circl in yellow and we multipli it time the number of such sub problem we have .
and , of cours , we don't wanna forget ani of our sub problem so we just make sure we sum all of our phase , j , to insur that at everi point we count the work done in each of the sub problem .
okai ?
so , that's the upshot of thi slide .
we can upper bound the run time of our random algorithm veri simpli in term of phase and the xj's , the number of sub problem that we have to endur dure phase j .
so , thi upper bound on our run time is import enough to give it notat , we'll call thi star , thi will be the start point of our final deriv when we complet the proof of thi theorem .
now don't forget , we're analyz a random algorithm so therefor the left hand side of thi inequ the run time of r select , that's a random variabl .
so that's a differ number depend on the outcom of the random coin flip of the algorithm .
depend on the random pick it ha chosen , you will get differ random run time .
similarli the right hand side of thi inequ .
is also a random variabl .
that's becaus the x j's ar random variabl .
the number of sub problem in phase j depend on which pivot get chosen .
so .
to analyz , what we care about is the expect of these quantiti , their averag valu .
so we're gonna start modestli and as usual , thi will extend our modest accomplish to much more impress on us linear of expect , but our first modest goal is just to , to understand the averag valu .
of an xj , the expect valu of xj .
we're gonna do that in two step .
on the next slide , i'm go to argu that to analyz the expect of xj , it's suffici to understand the expect of a veri simpl coin flip experi .
then , we'll analyz that coin flip experi .
then we'll have the domino all set up in a row .
and on the final slide , we'll knock'em down and finish the proof .
so let's try to understand the averag number of recurs call we expect to see in a given phase .
so , again , just so we don't forget .
xj is defin as the number of recurs call dure phase j .
where a recurs call is in phase j , if and onli if the current sub arrai length li between three fourth rais to the j <num> time n .
and then , the larger number of three fourth rais to the j time n .
so again , for exampl , phase zero is just the recurs call under which the arrai length is between <num> percent of the origin element and <num> percent of the origin element .
so what i wanna do next is point out that a veri simpl suffici condit guarante that we'll proce from a given phase onto the next phase .
so it's a condit guarante termin of the current phase .
and it's an event that we've discuss in previou video .
mainli that the pivot that we choos give a reason balanc split .
<num> <num> or better .
so recal how partit work , we choos a pivot p .
it wind up wherev it wind up .
and the stuff to the left of it's less than p .
the stuff to the right of it is bigger than p .
so <num> to <num> split or better , what i mean is that each of these , each , the first part and the second part ha , at most , <num> percent of the element in the input arrai .
both have twen , both have at least <num> , and , at most , <num> .
and the kei point is , that if we wind up choos a pivot that give us a split that's at least as good the current phase must end .
why must the current phase end ?
well , to get a <num> , <num> split or better than no matter which case we wind up in , in the algorithm we're guarante to recurs on a sub problem that ha at most <num> percent of what we start with .
that guarante that whatev phase we're in now , we're go to be in an even bigger phase when we recurs .
now , i want you to rememb someth that we talk about befor , which is that you've got a decent chanc when you pick a random pivot of get someth that give you a <num> , <num> split or better .
in fact , the probabl is <num> percent .
right ?
if you have an arrai that ha the integ from on to <num> inclus , anyth from <num> to s , <num> to <num> will do the trick .
that'll insur that at least the first <num> element ar exclud from the rightmost call and at least rightmost <num> element ar exclud from the left recurs call .
so thi is why we can reduc our analysi of the number of recurs call dure a given phase , to a simpl experi involv flip coin .
specif , the expect number of recurs call .
now we ar gonna see in a given phase j , is no more than the expect number of coin flip in the follow experi .
okai , so you've got a fair coin , <num> percent head , <num> percent tail .
you commit to flip it until you see the head and the question is , how mani coin flip doe it take up to and includ the first head that you see ?
so the minimum it's gonna be on coin flip if you hit a head the first time it's on .
if you get a tail and then a head , then it's two .
if it's tail , tail , head it's three and so on , and you alwai stop when you hit that first head .
so what's the correspond ?
well , think of head as be you're in phase j , and if you get a good pivot , it give you a <num> <num> split .
call that head .
and it guarante that you exit thi phase j .
just like it guarante that you get to termin the coin flip experi , experi .
now , if you get a pivot which doesn't give you a <num> <num> split , you mai or mai not pass to a higher phase j , but in the worst case , you don't .
you stick to phase j is you get a bad split , and that's like get a tail in the coin flip experi , and you have to try again .
thi correspond give us a veri elementari wai to think about the progress that , that our random select algorithm is make .
so , there's on recurs call in everi step in our algorithm , and each time we either choos a good pivot or a bad pivot , both could happen , <num> <num> probabl .
a good pivot mean we get a <num> <num> split or better .
a bad pivot mean , by definit , we get a split wors than <num> <num> .
so what have we accomplish ?
we've reduc the task of upper bound the expect number of recurs call in a phase j to understand the expect number of time you have to flip a fair coin befor you get on hit .
so on the next slide we'll give you the classic and precis answer to thi coin flip experi .
so , let me us capit n to denot the random variabl , which we were just talk about , the number of coin flip you need to do befor you see the first head .
and , it's not veri import , but you should know that these random variabl have their own name .
thi would be a geometr random variabl with paramet on half .
so you can us a few differ method to comput the expect valu of a geometr random variabl such as thi , and brute forc us the definit of expect work fine as long as you know how to manipul infinit sum .
but for the sake of varieti , let me give you a veri sneaki proof of what it's expect is .
so the sneaki approach is to write to the expect valu of thi random variabl in term of itself and then solv for the unknown , solv for the expect .
so let's think about it .
so how mani coin flip do you need ?
well for sure you're gonna need on .
that's the best case scenario .
and now two thing can happen , either you get head and that ha <num> percent probabl you stop or you get tail that happen with <num> percent probabl and now you start all over again .
again you just put point until you get first head .
on averag how mani time doe that take .
well by the definit of capit n you expect .
the expect of n coin flip , in the case where you get tail , and you have to start all over .
so thi on repres the first coin flip , the on half is the probabl that you can't stop , that you have to start all over probabl of tail , and then becaus it's a memori less process , becaus when you start anew on the second coin flip have gotten the tail , it's as if you're back at time on all over again .
so now we have a trivial equat , in term of the unknown expect valu of n and the uniqu solut , the uniqu valu , that the expect valu of capit n could have , in light of thi equat , is two .
so , on averag if you flip a fair coin and stop when you get head , you're go to see two coin flip on averag .
to make sure you haven't sort of lost the forest for the tree , let me remind you why we were talk about thi coin flip analysi in the first place .
so recal in the previou slide we show that xj , and rememb xj is the number of recurs call you'd expect to see in a given phase j , and we argu that the number of recurs call you're gonna see is bound abov .
by the expect number of coin flip until the head .
so thi exact calcul of two for the coin flip give us an upper bound of two for the number of recurs call on averag in ani given phase j .
so now that we've got all our duck line up in a row , let's wrap up the proof on thi final slide .
so , inherit from part on of the proof , we have an upper bound .
on the expect run time .
of the r select algorithm .
thi is what we were call star on the first brief slide in star , it look a littl messi , but we had the sum over the phase j .
but we had two thing that were independ of j the constant c and the origin input length n , so let me just yank the c and the n out front .
and then we have thi residu sum over the phase j .
of three quarter rais to the j rememb that come from our upper bound on the sub problem size dure phase j and then of cours we have to keep track of how mani phase j sub problem we have solv that by definit is xj .
now star wa written as a rand in accord term to the random variabl .
now we're gonna go ahead and take the expect and again i have said thi over and over but don't forget where's the expect come from .
thi is over the random pivot choic that our code make .
so the expect run time of the algorithm is most the expect of thi start quantiti .
so like i said earlier , pretti much everi time we're gonna do ani analysi of process , we're gonna wind up us linear of expect at some point .
here is where we do it .
linear expect sai the expect of a sum is just the sum of the expect .
so we yank the c and the n outsid of the expect .
we yank thi sum over phase .
outsid of the expect .
we yank thi three fourth rais to the j outsid of the expect and then we just have the expect valu of xj , the averag number of recurs call we expect to see in hj .
now on the previou two slide , we figur out an upper bound on how mani recurs call we expect to see in each phase .
so first by the coin flip analysi , by the reduct of the coin flip analysi , thi is the most expect number of coin flip n , which on the previou slide , we argu wa exactli two .
so bring that two out in front of the sum , that no longer depend on j .
so we get a most 2cn .
time the sum over phase j , of three quarter rais to the j .
now thi kind of sum we have seen previous in the cours .
it came up when we were analyz the master method and we sum up our run time upper bound over the level of our recursin tree .
and if we're not in case on if we're in case two or three we had geometr sum that were nontrivi .
thei requir a certain formula to calcul , so let me remind you of that formula here , when the three quarter ar be power up to the j .
so thi ha valu at most , on over on minu , the number that's get power , so in thi case it's three quarter , on minu three quarter is a quarter check reciproc , you got four .
and the upshot is that the expect number of oper that thi random select algorithm us to find the order statist in a given input arrai , is eight time c time n .
where c is the , hidden constant in the linear run time of partit .
and so that complet the proof .
the input arrai wa arbitrari .
we show the expect run time over the random choic of the algorithm is linear in n .
that is , onli a constant factor larger than what is requir to read the input .
pretti amaz .
previou video cover an outstand algorithm for the select problem , the problem of comput the ith statist of a given arrai .
that algorithm which we call the r select algorithm wa excel in two sens .
first it super practic , run blazingli fast in practic .
but also it enjoi a satisfi theoret guarante .
for everi input arrai of length n at the expect run time of r select is big o of n , where the expect is over the random choic of the pivot that r select make dure execut , now in thi option video i'm go to teach you yet anoth algorithm for the select problem .
well why bother given that our select is so good ?
well frankli , i just can't help myself .
the idea of thi algorithm ar just too cool not to tell you about , at least in option video like thi on .
the select algorithm , we cover here is determinist .
that is , it us no random whatsoev .
and it's still gonna run in linear time , big o of n time .
but now , in the worst case for everi singl input arrai .
thu , the same wai merg short get the same asymptot run time , big o of n log n , as quick sort get on averag .
thi determinist algorithm will get the same run time o of n , as the r select algorithm doe on averag .
that said , the algorithm we're gonna cover here , well , it's not slow .
it's not as fast as r select in practic , both becaus the hidden constant in it ar larger .
and also becaus it doesn't' oper in place .
for those of you who ar feel keen , you might wanna try code up both the random and the determinist select algorithm , and make your own measur about how much better the random on seem to be .
but if you have an appreci for boolean algorithm , i think you'll enjoi these lectur nonetheless .
so let me remind of the problem .
thi is the i th order statist problem .
so we're given an arrai , it ha n distinct entri .
again , the distinct is for simplic .
and you're given a number i between on and n .
you're respons for find the i th smallest number , which we call the i th order statist .
for exampl , if i is someth like n over two , then we're look for the median .
so let's briefli review the random select algorithm .
we can think of the determinist algorithm cover here as a modif of the random algorithm , the r select algorithm .
so when that algorithm is pass in arrai with length n , and when you're look for the i th order statist , as usual , there's a trivial base case .
but when you're not in the base case , just like in quick sort , what you do is you're gonna partit the arrai around pivot element p .
now , how ar you gonna choos p ?
well , just like quick sort , in the random algorithm , you choos it uniformli at random .
so each of the n element of the input arrai ar equal like to be chosen .
as the pivot .
so , call that pivot p .
now , do the partit .
rememb partit put all of the element less than the pivot to the left of the pivot .
we call that the first part of the partit arrai .
anyth big , bigger than the pivot get move to the right of the pivot .
we call that the second part of the arrai .
and let j denot the posit of the pivot in thi partit arrai .
equival , let j be what order statist that the pivot wind up happen to be .
right ?
so , we happen to choos the minimum element then j's gonna be equal to on .
if we happen to choos the maximum element , j's gonna be equal to n .
and so on .
so , there's alwai the lucki case , chanc on in n , that we happen to choos the ith order statist as our pivot .
so , we're go to find that out when we notic that j equal i .
in that super lucki case , we just return the pivot and we're done .
that's what we're look for in the first place .
of cours , that's so rare it's not worth worri about , so realli the two main case depend on whether the pivot that we randomli choos is bigger than what we ar look for or if it's less than what we ar look for .
so , if it's bigger than what we ar look for , that mean j is bigger than i , we're look for the ith smallest , we randomli chose the j'th smallest .
then rememb we know that the ith smallest element ha to lie to the left of the pivot .
good element in that first part of the partit arrai .
so we recur there .
it's an arrai that ha j <num> element in it , everyth less than the pivot .
and we're still look for the ith smallest among them .
in the other case , thi wa the case cover in a quiz a coupl video back , if we guess a pivot element that is less than what we're look for , well then we should discard everyth less than the pivot and the pivot itself .
so we should recur on the second part of a , stuff bigger than the pivot .
we know that's where what we're look for li .
and have thrown awai j element , the smallest on at that .
we're rehears on a rai of and minu j , i'm look for the smallest element in that second part .
so , that wa the random select algorithm , and you'll recal the intuit for why thi work is random pivot should usual give pretti good split .
so the wai the analysi went is we should .
each iter , each recurs call , with <num> percent probabl , we get a <num> <num> split or better .
therefor , on averag , everi two recurs call , we ar pretti aggress shrink the size of the recurs call .
and for that reason , we should get , someth like a linear time bound .
we do almost as well as if we pick the median in everi singl call , just becaus random pivot ar a good enough proxi of best case pivot , of .
the median .
so now the big question is suppos we weren't permit to make us of random .
suppos thi choos a random pivot trick wa not in our tool box .
what could we do ?
how ar we go to determinist choos a good pivot ?
let's just rememb quickli what it mean to be a good pivot .
a good pivot is on that give us a balanc split , after we do the partit of the arrai .
that is , we want as close to a <num> <num> split between the first and the second part of the partit arrai as possibl .
now , what pivot would give us the perfect <num> <num> split ?
well , in fact , that would be the median .
well , that seem like a total ridicul observ , becaus we canon , ar try to find the median .
so previous we were abl to be lazi , and we just pick a random pivot , and us that as a pretti good proxi for the best case pivot .
but now , we have to have some subroutin that determinist find us a pretti good approxim of the median .
and the big idea in thi linear time select algorithm , is to us what's call the median of median as a proxi for the true mean of the input arrai .
so when i sai median of median , you're not suppos to know what i'm talk about .
you're just suppos to be intrigu .
now , let me explain a littl bit further .
here's the plan , we're gonna have our new implement of chose pivot and it's gonna be determinist .
you will see no random on thi slide , i promis .
so the high level strategi is gonna be we're gonna think about the element of thi arrai like sport team , and we're gonna run a tournament , a <num> round .
knockout tournament , and the winner of thi tournament is go to be who we return as the propos pivot element .
then we'll have to prove that thi is a pretti good pivot element .
so there's gonna be two round in thi tournament .
each element , each team is gonna first particip in a world group , if you will .
so thei'll be , small group of five team each , five element each .
and to win your first round , you have to be the middl element out of those five .
so that'll give us n over five first round winner .
and then the winner of that second round is go to the med , be the median of those n over five winner from the first round .
here ar the detail .
the first step isn't realli someth you actual do in the program , it's just conceptu .
so logic , we're go to take thi arrai capit a , which ha n element , and we're gonna think of it as compris n over five group with five element each .
so if n is not a multipl of five , obvious , there'll be on extra group that ha size between on and four .
now for each of these group of five , we're go to comput the median , so the middl element of those five .
now for five element , we mai as well just invok a reduct to sort ; we're just gonna sort each group separ , and then us the middl element , which is the median .
it doesn't realli how you do the sort .
becaus after all , there's onli five element .
but you know , let's us sort , what the heck .
now what we're go to do is we're go to take our first round winner and we're gonna copi them over into their own privat arrai .
now thi next step is the on that's go to seem danger like cheat , danger like i'm do someth circular and not actual defin a proper algorithm , so c you'll notic ha link over n over five .
we start with an arrai of link n .
thi is a smaller input .
so let's recurs comput the median of thi arrai capit c .
that is the second round of our tournament amongst the n over five first round winner , the n over five middl element of the sort group .
we recurs comput the median , that's our final winner , and that's what we return as the pivot element from thi subroutin .
now i realiz it's veri hard to keep track of both what's happen intern to thi juic pivot subroutin and what's happen in the call function of our determinist select algorithm .
so let me put them both togeth and show them to you , clean up , on a singl slide .
so , here is the propos determinist , select algorithm .
so , thi algorithm us no random .
previous , the onli random wa in choos the pivot .
now we have a determinist subroutin for choos the pivot , and so there's no random at all .
i've taken the liberti of in line true's pivot subroutin .
so that is exactli what line on , two , and three ar .
i haven't written down the base case just to save space i'm sure you can rememb it , so if you're not in the base case .
what did we do befor ?
the first thing we do is choos a random pivot .
what do we do now ?
well , we have step on through three .
we do someth much more clever to choos a pivot .
and thi is exactli what we said on the last slide .
we break the arrai into group of five .
we sort each group , for exampl , us merg sort .
we copi over the middl element of each of the n over five group into their own arrai capit c .
and , then , we recurs comput the median of c .
so , when we recur on select that we pass the input c .
c ha n over five element so that's the new link .
that's a smaller link than what we start with so it's a legitim recurs call refin the median of n over five element .
so , that's gonna be the n over tenth order statist .
as usual .
well to keep thing clear i'm ignor stuff like fraction , in your real implement you'd just round it up or down .
as appropri .
so step on through three ar the new step routin that replac the random select that we had befor .
step four through seven ar exactli the same as befor .
we've chang noth .
all we have done is rip out that on line where we chose the pivot randomli and past in these line on through three .
that is the onli chang to the random select algorithm .
so , the next quiz is a standard check that you understand thi algorithm , at least , not necessarili why it ? s fast ; but , at least , just how it actual work .
and i onli ask you to identifi how mani recurs call there ar , each time .
so , for exampl in there's two recurs call , in quick sort there's two recurs call , in r select there's on recurs call .
how mani recurs call do you have each time , outsid of the base case in the d select algorithm ?
all right , so the correct answer is two .
there ar two recurs call in deselect , and mayb the easiest wai to answer thi question is not to think too hard about it and liter just inspect the code and count , right name there's on recurs call in line three , and there's on recurs call in either six or seven , so quit liter , you know there's seven line of code , and two of the on that get execut have a recurs call so the answer is two .
now what's confus is that in the random , a coupl thing , first in the random select algorithm , we onli have on recurs call .
we have the recurs call .
in line six or seven , we didn't have thi in line three .
that on in line three is new compar to the random procedur .
so we're kind of us to think of on recurs call us the divid and conquer approach to select , here we have two .
moreov .
conceptu .
the roll of these two recurs call ar differ .
so the on in line six or seven is the on we're us to .
that's after you've done the partit so you have a smaller sub problem and then you just recurs find the residu or statist in the residu arrai .
that's sort of the standard divid and conquer approach .
what's sort all crazi .
is thi second us of a recurs call which is part of identifi a good pivot element for thi outer recurs call and thi is so counter intuit , mani student in my experi don't even think that thi algorithm will hold , sort of , thei sort of expect it to go into an infinit loop .
but again , that sort of over think it .
so let's just compar thi to an algorithm like merg sort .
what doe merg sort do ?
well it doe two recurs call and it doe some other stuff .
okai .
it doe linear work .
that's what it doe to merg .
and then there ar two recurs call on smaller sub problem , right ?
no issu .
we definit feel confid that merg is gonna termin becaus the sub problem keep get smaller .
what doe deselect do , if you squint ?
so don't think about the detail just high level .
what is the work done in deselect ?
well .
there ar two recurs call , there's on's in line three , on's in line six or seven , but there's two recurs call on sm , smaller sub problem size .
and there's some other stuff .
there's some stuff in step on and two and four , but whatev .
those ar recurs call .
it doe some work .
two recur have caus the smaller sub problem , ti's got to termin .
we don't know what the run time is , but it's got to termin , okai ?
so if you're worri about thi termin , forget about the fact that the two recur of caus have differ semant and just rememb , if ever , you onli recur on smaller sub problem , you're definit go to termin .
now , of cours who know what the run time is ?
i ow you an argument on why it would be anyth reason , that's go to come later .
in fact what i'm gonna prove to you is not onli doe thi select algorithm termin , run in finit time , it actual run in linear time .
no matter what the input arrai is .
so where as with r select , we could onli discuss it expect run time be linear .
we show that with disastr bad choic for pivot , r select can actual take quadrat time .
under no circumst will deselect ever take , ever take quadrat time .
so for everi input arrai it's big o of n time .
there's no random becaus we don't randomli do anyth in choos pivot , so there's no need to talk about averag run time ; just the worst case run time over all input is o of n .
that said , i want to reiter the warn i gave you at the veri begin of thi video which is , if you actual need to implement a select algorithm , you know , thi on wouldn't be a disast .
but it is not the method of choic , so i don't want you to be misl .
as i said there ar two reason for thi .
the first is that the constant hidden in the begon notat ar larger for v select than for r select .
that will be somewhat evid from the analys that we give for the two algorithm .
the second reason is , recal we made a big deal about how partit work in place and therefor quicksort and r select also work in place , that is , with no real addit memori storag .
but in thi deselect algorithm we do need thi extra arrai c to copi over the middl element , the first round winner .
and so the extra memori , as usual , slow down the practic perform .
on final comment .
so for mani of the algorithm that we cover , i hope i explain them clearli enough that their eleg shine through and that for mani of them you feel like you could have up with it yourself , if onli you'd been in the right place at the right time .
i think that's a great wai to feel and a great wai to appreci some of these veri cool algorithm .
that said , linear time select , i don't blame you if it , if you feel like you never might have come up with thi algorithm .
i think that's a total reason wai to feel after you see thi code .
if it make you feel better , let me tell you about who came up with thi algorithm .
it's quit old at thi point , about <num> year , from <num> .
and the author , there ar five of them and at the time thi wa veri unusu .
so , manuel blum .
bob floyd .
vaughn pratt .
ron rivest .
and bob targen .
and thi is a pretti heavi weight line up , so as we've discuss in the past , the highest award in comput scienc is the acm ture award given onc each year .
and i like to ask my algorithm class how mani of these author do thei think , have been award a ture award .
i've ask him mani time .
the favorit answer anyon's ever given me ha been .
six , which i think is in spirit should be correct .
strictli speak the answer is four .
so , the onli on of these five author that doesn't have a tour award is von pratt , although he's done remark thing span the gambit from co found sun system to have veri famou theorem about , for exampl , test primal .
but the other four have all been award the tour award at some point .
so in chronolog order , so the late bob floyd who wa a professor here at stanford .
wa award the <num> award , both for contribut to algorithm but also to program languag and compil .
so bob targen who , as we speak , is here as a visitor at stanford and ha spent hi ph .
d here and ha been here as a faculti at time , wa award it for contribut to graph algorithm and data structur .
we'll talk some more about some of hi other contribut in futur cours .
manuel blum wa award the ture award in'<num> larg for contribut in cryptographi , and mani of you probabl know ron rivest as the r in the rsa cryptosystem .
so he , won the , ture award along with shamir and adleman back in'<num> .
so in summari , if thi algorithm seem like on that might have allud you even on your most creativ dai , i wouldn't feel bad about it .
thi is a , thi is a quit clever algorithm .
now let's turn to the analysi of the determinist select algorithm that we discuss in the last slide by blum , floyd , pratt , rivest , and tarjan .
in particular , let's prove that it run in linear time on everi possibl input .
let's , remind you what the algorithm is .
so the idea is , we just take the r select algorithm .
but instead of choos a pivot at random , we do quit a bit more work to choos what we hope is go to be a guarante pretti good pivot .
so again , line on through three ar the new choos pivot subroutin .
and it's essenti implement a two round knockout tournament .
so first , we do the first round match .
so what doe that mean ?
that mean we take a we think of it as compris these group of five element .
so the first five element on through five and the element six through ten and point eleven through fifteen and there again and so on .
if we sort each of those five us , let's sai , merg sort although it doesn't matter much , then the winner in each of these five first round match is the median of those five .
that is the third highest element , third largest element out of the five .
so we take those in over five first round winner the middl element of each of the five and the sort group , we copi those over into a new arrai of capit and in it for five .
and then we .
second round of our tournament at which we elect the medium of these n over five , first round winner as our final pivot , as our final winner .
so , we do that by recurs call deselect on c .
it ha a length n over five for the medium .
so that's the n over tenth statist in that arrai .
so , we call the pivot p and then we just proce exactli like we did .
and in the random case .
that is , we partit a around the pivot , we get a first part , a second part , and we recur on the left side or the right side as appropri , depend on whether the pivot is less than or bigger than the element that we're look for .
so the claim is , believ it or not , that thi algorithm run in linear time .
now , you'd be right to be a littl skeptic of that claim .
certainli , you should be demand from me some kind of mathemat argument about thi linear time claim .
it's not at all clear that that's true .
on reason for skeptic is that thi is an unusu extravag algorithm .
in two sens for someth that's gotta run in linear time .
first is , first is it's extravag us of recurs .
there ar two differ recurs call , as discuss in the previou video .
we have not yet seen ani algorithm that make two recurs call and run in linear time .
the best case scenario wa alwai for our two recurs call algorithm like merg sort or quick sort .
the second reason is that , outsid the recurs call , it seem like it ? s just kind of a lot of work , as well .
so , to drill down on that point , and get a better understand for how much work thi algorithm is do , the next quiz ask you to focu just on line on .
so when we sort group of five in the input arrai how long doe that take .
so the correct answer to thi quiz is the third answer .
mayb you would have guess that given that i'm claim that the whole algorithm take linear time , you could have guess that thi sub routin is go to be wors than linear time .
but you should also be wonder you know , isn't sort alwai n log n so , aren't we do sort here .
why isn't the n log n thing kick in ?
the reason is we're do someth much , much more modest than sort the link n input arrai , all we're sort ar these puni littl sub arrai that have onli five element and that's just not that hard , that can be done in constant time so let me be a littl more precis about it .
the claim is that sort an element , an arrai with five element take onli some constant number of oper .
let's sai <num> .
where did thi number , <num> come from ?
well , you know , for exampl , suppos we us merg sort .
if you go back to those veri earli lectur , we actual count up the number of oper that merg sort need to sort an arrai length of m .
for some gener m , here m is five , so we can just plug five into our previou formula that we comput from merg sort .
right if we plug amicl five into thi formula , what do we get , we get six time five time log base <num> <num> .
who know what log base <num> is , that's some weird member but it's gonna be a most three right .
so that's the most three of three <num> is four multipli that by five and again time six and will get you <num> .
so it's constant time to sort just on of these group of five .
now of cours , we have to do a bunch of group of five becaus there's onli a linear number of group .
constant for each , so it's gonna be linear time overal .
so to be realli pedant .
we do <num> oper at most per group .
there's n over five differ group .
we multipli those , we get <num> n oper .
so do all the sort and that's obvious a big o event .
so linear time for step on .
so have warm up with step on .
let's look now at the whole seven line algorithm , and see what's go on .
now i hope you haven't forgotten the paradigm that we discuss for analyz the run time of determinist divid and conquer algorithm like thi on .
so name we're gonna develop a recurr and rememb a recurr express the run time , the number of oper perform , in two part .
first of all , there's the work done by the recurs call on smaller sub problem .
and secondli , there's the work done local , not in the recurs call .
so let's just go through these line on at a time , and just do a run talli of how much work is done by thi algorithm , both local and by the recurs call .
so the quiz wa about , step number on .
we just argu that sinc it's constant time to sort each group , and there's a linear number of group , we're gonna do linear work , theta of n .
for step on .
so copi these first round winner over in to their special arrai c is obvious linear time .
now , when we get to the third line , we have a recurs call , but it's a quit easi recurs call to understand .
it's just , recurs on a , a rai that ha size twenti percent as larg as the on we start with , on the n over five element .
so thi , rememb the notat we us for recurr .
gener , we denot by capit t the run time of an algorithm on of a given length .
so thi is go to be the run time that our algorithm ha in the worst case on input of length n over five .
cuz n over five is the length of the arrai that we're pass to thi recurs call .
good .
step four , partit .
well we had .
video about how thei were go to partit the y to linear time .
we knew that all the wai back from quick sort , so that's definit theta of n .
step five is constant time , i'm not go to worri about it .
and final we get to line six and seven so at most on of these will execut so in either case there's on recurs call .
so that's fine , we know in recurr when there's recurs call we'll just write capit t of whatev the input length is .
so we just have to figur out what the input length here is .
it wa n over five in step , in line three so we just have to figur out what it is in line six or seven .
oh yeah , now we're rememb why we didn't us recurr when we discuss random quick sort and .
the random select algorithm .
it's becaus we don't actual know how big the recurs call is , how big the input pass to thi recurs call in line six or seven is .
line three , no problem .
it's guarante to be twenti percent of the input arrai cuz that's how we defin it .
but for line six or seven , the size of the input arrai that get pass to the , to the recurs call depend on how good the pivot is .
it depend on the split of the arrai a into two part , which depend on the choic of the pivot p .
so at the moment all we can write is t .
of question mark .
we don't know .
we don't know how much work get done in that recurs , caus we don't know what the input size is .
let me summar the result of thi discuss .
so write down a recurr for the d select algorithm .
so with t of n denot the maximum number of oper the d select ever requir to termin an arrai of input it's just the usual definit of t of n when us recurr .
what we establish in our talli on the last slide is that deselect doe linear stuff outsid the recurs call .
it doe the sort of group of five .
it doe the copi , and it doe the partit .
each of those is linear , so all of them togeth is also linear .
and then it doe two recurs call .
on whose size we understand , on whose size we don't understand .
so , for onc i'm not go to be sloppi and i'm go to write out an explicit constant about the work done outsid of the recurs caus .
i'm go to write , i'm go to actual write c time n for some constant c .
so of cours no on ever care about base case , but for complet thi let me write it down anywai .
when d select get an input of onli on element it return it , what's call that on oper for simplic .
and then in the gener case and thi is what's interest .
when you're not in the base case and you have to recur , what happen ?
well you do a linear work outsid of the recurs call .
so that's c time n for some constant c .
c is just the constant on all of our big theta on the previou slide .
plu the recurs call in line three , and we know that happen on an arrai of size five .
as usual , i'm not gonna worri about round up or round down , it doesn't matter .
plu our mysteri recurs call on an arrai of unknown size .
so that's where we stand and we seem stuck becaus of thi peski question mark .
so , let's prove lemma which is gonna replac thi question mark with someth we can reason with , with an actual number that we can then analyz .
so the upshot of thi kei lemma is that all of our hard work in our choos pivot subroutin in line on through three bear fruit in the sens that we're guarante to have a pretti good pivot .
it mai not be the median , it mai not give us a <num> <num> split .
then we could replac the question mark with , on half time n .
but it's gonna let us replac the question mark by seven tenth time n .
now , i don't wanna lie to you , i'm gonna be honest , it's not quit <num> 10n , it's more like <num> 10n minu five , there's a littl bit of addit error , so , take care of the addit error add noth to your conceptu understand of thi algorithm or why it work .
for those of you who want a truli rigor proof , there ar some post lectur note which go through all the gori detail .
but in lectur i'm just gonna tell you what's sort of moral true and ignor the fact that we're gonna be off by three here and four there .
and then we'll be clear when i show you the proof of thi limit , where i'm be a littl bit sloppi and why it realli shouldn't matter , and it doesn't .
so to explain why thi kei limit is true why we get a <num> <num> split or better guarante , let me set up a littl notat .
i'm get sick of write n over five over and over again , so let's just give that a synonym , let's sai , k .
so thi is the number of differ sort of first round match that we have , the number of group .
i also want some notat to talk about the first round winner , that is the median of these group of five , the k first round winner .
so , were gonna call xi the smallest of those who win their first round match and make it to the second round .
so just to make sure the notat is clear , we can express the pivot element in term of these x ? s .
rememb , the pivot is the final winner .
it win not onli it first round tournament , but it also the second round tournament .
it's not onli the middl element of the first group of five .
it's actual the median of the n over five middl element .
it's the median of the median .
that is , of the k middl element , it's the k over two order statist , k over two smallest .
i'm sai thi , assum that k is even .
if k wa odd , it would be some slightli differ formula as you know .
so let's rememb what we're try to prove .
we're try to prove that for our propos pivot , which is exactli thi element x sub k over two , it's exactli the winner of thi <num> round knockout tournament .
we're try to argu that for thi propos pivot , we definit get a <num> <num> split or better .
so what that mean is , there better be at least <num> percent of the element that ar bigger than the pivot .
that wai if you recur on the left side on the first part , we don't have to deal with more that more than <num> percent of the origin element .
similarli , there better be at least <num> percent of the element that ar smaller than the pivot .
that wai if we recur on the right hand side we know we don't have to deal with more than <num> percent of the origin input element .
so if we achiev thi goal , we prove that there's at least <num> percent on each side of xk over two , then we're done .
that prove the kei lemma that would get a <num> <num> split or better .
so i'm gonna show you why thi goal is true .
i'm gonna introduc a thought experi .
and i'm gonna lai out it abstractli .
then we'll sorta do an exampl to make it more clear .
and then we'll go back to the gener discuss and finish the proof .
so what we're gonna do is a thought experi , for the purpos of count how mani element of the input arrai ar bigger than our pivot choic , and how mani ar smaller .
so in our mind we're go to imagin that we're take element in a and rearrang them in a 2d grid .
so here ar the semant of thi grid .
each column will have exactli five element that will correspond to on of the group of five .
so we'll have n over five column correspond to our n over five group in our first round of our tournament .
is not a multipl of five then on of these group ha size between on and four but i'm just not gonna worri about it , that some of the addit loss , which i'm ignor .
moreov were go to arrang each column in a certain wai so that go from bottom to top the entri of that go from smallest to largest .
so thi mean that in thi grid we have five row .
and the middl row , the third row , correspond exactli to the middl element , to the winner of the first round match .
so becaus these middl element these first round winner ar treat special , i'm go to denot them with big squar , the other four element of the group two of which ar smaller two of which ar bigger ar just go to be littl circl .
furthermor , in thi thought experi , in our mind , we're go to arrang the column from left to right in order of increas valu of the middl element .
now rememb , i introduc thi notat x of i is the smallest amongst the middl element .
so a differ wai of what i'm try to sai is that the leftmost column is the group that ha x1 as it middl element .
so among the n over five middl element , on of the group ha the smallest middl element .
we put that all the wai on the left .
so thi is gonna be x1 in the first column , the smallest of the first round winner .
x2 is the second smallest of the first round winner , x3 is the third smallest and so on .
at some point we get to the median of the first round winner , xk over two .
and then , wai at the right is the largest of the first round winner .
and i'm sure that you rememb that the median of median which is xk over two is exactli our pivot .
so thi is our lucki winner .
i know thi is a lot to absorb , so i'm gonna go ahead and go through an exampl .
if what i've said so far make perfect sens , you should feel free to skip the follow exampl .
but if there's still some detail you're wonder about , and hope thi exampl will make everyth crystal clear .
so let's suppos we have an input arrai .
i need a , a slightli big on to grid make sens .
let's sai there's an input arrai of twenti element .
so there's go to be the input arrai , which is in a total arbitrari order .
there's gonna be the vertic after we sort each group of five .
and then i'm gonna show you the grid .
so thi is the input we're all gonna us .
let's now go ahead and delin the variou group of five .
so after sort thi group , you get the follow .
from each group there's a singl winner mainli the middl element so that would be the twelv , and the six , and the nine , and the fourteen , those ar the four survivor from the first round of the tournament .
and the median of these four element which , at the end of the dai is gonna to be our pivot is the second smallest of the four , that's how we defin the median from an even number of element , so that's gonna be the nine .
so , thi first transform from the input arrai , to thi vagu mini sort version of the input arrai with the group of five sort , thi we actual do in the code .
thi happen in the algorithm .
now , thi grid we're just do in our mind .
okai ?
we're just in the middl of prove why the algorithm is fast .
why the fit bit guarante to give us close to a , a <num> <num> split or better .
so , let me show you an exampl of thi grid in our mind , what it look like for thi particular input .
so the grid alwai ha five row .
the column alwai have five element caus the column correspond to the group .
here becaus n equal twenti and over five is four .
so there's gonna be , four column and five row .
and moreov we arrang the column from left to right so that these middl element go from smallest , to largest .
so the middl element ar six nine twelv and fourteen and we're gonna draw the column in that order from left to right .
so first we'll write down the middl element , the middl row from decreas to increas , six , nine , twelv , fourteen .
again the median of thi is our pivot , which is the nine .
and then each column is just the other four element that goe along with thi middl element from decreas to increas as we go from bottom to top .
so thi is the grid that we're been talk about on the other slide , in thi particular exampl .
so i hope that make what we're talk about clear , what these x ? s mean , and what worri we have amongst the row , amongst the column and so on .
so let ? s go back to the gener argument .
here is the kei point , here is why were do thi entir thought experi , it ? s go to let us prove our kei limit .
we're go to get a <num> <num> split or better .
<num> percent of the stuff at least is less than the pivot ; <num> percent at least is bigger than the pivot .
so why is there at least <num> percent of the stuff below the pivot ?
why is the pivot bigger then at least <num> ?
well , it's bigger then everyth to the left and everyth below the stuff to the left .
that is we know that xk over two is bigger than the k over two minu on element .
that is to the left of it , those other middl element that it's bigger then .
that's becaus it's the median of the median .
so , if we just go straight west from the pivot we onli see stuff which is less .
furthermor , these column ar arrang from decreas to increas order as we go from south to north , from bottom to top .
so if you travel south from ani of these smaller xmi we onli see stuff which is still smaller .
so all we're us in here is transit of the less than relat .
if you go straight west you see stuff which is onli smaller from ani of those point if you go southward you'll see stuff which is even smaller than that .
so thi entir yellow region , everyth southwest of the pivot element , is smaller than it .
and that's a good chunk of the grid .
right ?
so for all of these column , it's basic three out of the five , or <num> percent of them ar smaller than the pivot , and half of the column , essenti , ar in thi part of the grid .
so if the pivot bigger than <num> percent of the stuff in <num> percent of the group that mean it's bigger than <num> percent of the element overal .
and if we reason in an exactli symmetr wai , we find that the pivot is also smaller than at least <num> percent of the arrai .
so to find thing bigger than the pivot , what do we do ?
first we travel eastward .
that give us middl element that ar onli bigger than it and then we stop wherev you want on our eastward journei and we head north , and we're gonna see stuff which is still bigger .
so thi entir north eastern corner .
is bigger than the pivot element , and again that's <num> , that's at <num> percent of roughli <num> percent of the group .
return to our exampl , the southwest region of the nine .
is thi stuff , on , three , four , five , six .
certainli , all of that is smaller than the nine .
you'll notic there's other thing smaller than the nine as well .
there's the eight , there's the two , there's the seven , which we're not count .
but it depend on the exact arrai .
whether or not , in those posit , you're gonna have stuff smaller than the pivot or not .
so it's thi yellow region we're guarante to be smaller than the pivot .
similarli , everyth northeast of the pivot is bigger than it .
those ar all doubl digit number and our pivot is nine .
again there's some other stuff in other region bigger than the pivot , the twenti , the ten , the eleven , but again those ar posit where we can't be guarante that it will be bigger than the pivot .
so it's the yellow region that ar guarante to be bigger and smaller than the pivot , and that give us the guarante <num> <num> split .
okai , so that proof wa hard work , show that thi determinist choos pivot subroutin guarante a <num> <num> split or better .
and you probabl feel a littl exhaust and like we deserv a qed at thi point .
but we haven't earn it .
we have not at all prove that thi determinist select algorithm run in linear time .
why doesn't a guarante <num> <num> split guarante us linear time automat ?
well , we had to work pretti hard to figur out thi element guarante thi <num> <num> split .
in particular we had to invok anoth recurs call .
so mayb thi wa a pyrrhic victori .
mayb we had to work so hard to comput the pivot that it outweigh the benefit we'd get from thi guarante .
<num> <num> split .
so , we still have to prove that's not the case even in conjunct do both of these thing , we still have our linear time bound .
we'll finish the analysi in the next video .
so the time ha arriv for us to finish the proof of the fact that thi determinist algorithm base on the median of median idea , doe inde run in linear time .
we've done realli all the difficult work .
we've discuss the algorithm ingenu requir .
to choos a pivot determinist that's guarante to be pretti good .
so rememb the idea wa you take the input arrai , you logic break it into group of five , you sort each group .
that's like the first round of a two round knockout tournament .
the winner of the first round ar the middl element of each group of five .
that's the initi set of median .
and then the second around we take a median of these n over five first round winner , and that's what we return as the pivot .
and we prove thi kei lemma which is the <num> <num> lemma , which sai that if you choos the pivot by thi two round knockout tournament , you're guarante to get a <num> <num> split or better .
so your recurs call in line six or seven .
of have a de select is guarante to be on an arrai that ha at most <num> percent of the element that you start with .
in other word you're guarante to prune at least <num> percent of the arrai befor you recur again .
but what remain to understand is whether or not we've done a sensibl trade off .
have we kept the work requir to comput thi <num> <num> split small enough .
that we get the desir linear run time .
or have we , is the cost of find a pretti good pivot outweigh the benefit of have guarante good split ?
that's what we gotta prove .
that's the next subject .
here's the stori so far .
you'll recal that , as usual , we defin t of n to be the worst case run time of an algorithm .
in thi case , d select on input of arrai length .
i didn't put arrai of length n .
and we discuss , okai , there's the base case as usual .
but in the gener case , we discuss how , outsid of the two recurs call .
the deselect algorithm , there's a linear number of oper .
what doe it have to do ?
it ha to do the sort , but each sort is on a group of size constant , size five , so it take constant time for a group .
there's a linear number of group , so step on take linear time , the copi take linear time , and the partit take linear time .
so , there's some constant c , which is gonna be bigger than on , but it's gonna be constant .
so , then outsid of a recurs caus .
deselect alwai doe at most c time n oper .
now what's up with the recurs call .
well , rememb there's two of them .
first , there's on on line three that's just respons for help choos the pivot .
thi on we understand .
it's alwai on twenti percent of the imput rate of like the first round winner , so we can veri safe write t of n over five for the work done , in the worst case , by that first recurs call .
what we didn't understand until we prove the kei lemma wa what's up with the second recurs call , which happen on either line six or line seven .
the size of the imput rate on which we recurs depend on the qualiti of the pivot , and it wa onli when we prove the kei lemma that we had a guarante on the .
<num> <num> split or better what doe that mean ?
that mean the largest sub arrai we could possibl recur on ha seven tenth n element .
so what remain is to find the solut for thi recurr and hopefulli prove that it is inde big o event .
so i'm go to go ahead and rewrit the occurr at the to of the slide .
we're not realli go to worri about the t to on equal on .
what we're interest in is the fact that the run time on an input of length n is at most c time n .
where again c is some constant which is gonna have to be at least on , given all the work that we do outsid of the recurs call .
plu the recurs call on line three on an arrai of size n over five .
plu the second recurs call , which is on some arrai that ha size in the worst case seven tenth n .
so that's cool .
thi is exactli how we handl the over determinist divid and conquer algorithm that we studi in earlier video .
we just wrote down a recurr and then we solv the recurr , but now , here's the trick .
and all of the other recurr that came up .
for exampl , merg short , strassner's matrix multipl algorithm , multipl , you name it .
we just plug the paramet into the master method .
and becaus of the power of the master method , boom !
out pop up an answer .
it just told us what the recurr evalu to .
now , the master method , as power as it is , it did have an assumpt , you might recal .
the assumpt wa that everi sub problem solv had the same size .
and that assumpt is violat by thi linear time select algorithm .
there ar two recurs call .
on of 'em on twenti percent of the origin arrai .
the other on is probabl on much more than twenti percent of the origin arrai .
it could be as much as <num> percent of the origin arrai .
so becaus we have two recurs call , and sub problem of differ size , thi doe not fit into the situat that the master method cover .
it's a veri rare algorithm in that regard .
now , there ar more gener version of the master method , of the master theorem which can accommod a wider class of recurr includ thi on here .
altern we could push the recurs tree proof so that we could get a solut for thi recurr .
some of you might want to try that at home .
but i want to highlight a differ wai you can solv recurr just for varieti , just to give you yet anoth tool .
now the good new of the , about thi approach that i'm gonna show you is that it's veri flexibl .
it can be us to solv sort of arbitrarili crazi recurr .
it's certainli go to be power enough to evalu thi on .
the bad new is that it's veri out of hock .
it's not veri necessarili veri easi to us .
it's kind of a dark art figur out how to appli it .
so it's often just guess and check , is what it's call .
you guess what the answer to the recurr is and then you verifi it by induct .
here , becaus we have such a specif target in mind , the whole point of thi exercis is to prove a linear is not bound , i'm gonna call it just hope and check .
so we're gonna hope there's linear of time and then we're gonna try to produc a proof of that just that verifi the linear time bound us induct .
specif what ar we hope for , we're cross our finger that there's a constant , i'm go to call it a , a can be big but it's got to be constant , and again rememb constant mean it doe not depend on n in ani wai .
such that our recurr at the top of thi slide t n is bound abov by a time n for all and at least on .
why is thi what we're hope ?
well suppos thi were true .
by definit t of n is a upper bound of the run time of our algorithm .
so it's bound and by a time n then it's obvious an o event .
it's obvious a linear time algorithm .
it's obvious a get that get suppress in the big rotat .
so that's the hope , now let's check it .
and again , check mean just verifi by induct on n .
so the precis claim that i'm go to prove is the follow .
i'm gonna go ahead and choos the constant a .
rememb all we need is some constant a , no matter how big as long as it's independ of n .
that'll give us the big o of n time .
so i'm actual gonna tell you what a i'm gonna us for conveni .
i'm gonna choos a to be 10c .
now what is c ?
c is just a constant that we inherit from the recurr that we're given .
now rememb what thi recurr mean is thi is what the run time is of the deselect algorithm and the c time n repres the work that's outsid of the recurs call .
so thi is just a constant multipl on the amount of linear work that deselect doe for sort the group , for do the partit and for do the copi .
and so there's gonna be some small task at a reason cost and , and for the proof i'm just gonna multipli that by ten and us that as my a .
and the claim is if i defin a in that wai then inde , it is true that for all and at least on , t of n is band abov by a time n .
now , i realiz i just , i pull thi constant a out of nowher , right ?
y10 time c .
well , if you recal our discuss when we prove that thing were big o of someth els , there again , there wa some constant .
so to formal prove that someth is big o of someth els , you have to sai what the constant is .
and in the proof , you alwai wonder how do you know what constant to us ?
so , in practic , when you're actual , if you have to actual do on of these proof yourself , you revers engin what kind of constant would work .
so you just go through the argument with a gener constant .
and then you're like , oh , well , if i set the constant to be thi , i can complet the proof .
so we'll see , that's exactli what's gonna happen in the proof of thi claim .
it'll be obviou .
the veri last line you'll see why it show a equal 10c .
so i just revers engin what i need for the proof .
but to keep the proof easi to follow line by line i decid to just full disclosur tell you the cost right at the begin .
now no prize for guess that the wai thi proof proce is by induct on n .
induct's the obviou thing to us , we're try to prove an assert for everi singl posit number n and moreov we're given thi recurr which relat solut of smaller sub problem to that of bigger problem .
so that set thing up for us of the induct hypothesi .
if you want a longer review of what proof by induct ar , i suggest that you go back and re watch the option video where we prove the correct of quicksort .
that is , is a fairli formal discuss of what the templat is like for a proof by induct .
and that's the on we're gonna appli here .
so , there's two ingredi in ani proof by induct is , is a usual trivial on in the form of a base case .
that's also gonna be trivial here .
in the base case you just directli establish the assert when n equal on .
so , we're try to prove that t of n is the most a time n for everi n when n equal on we could just substitut .
but what we're try to prove is that t of on is at most a time on also known as a .
and we're given that t of on is on .
right that's the base case of the recurr that we're given .
so that's what we're us here .
what we want to be true is that thi isn't the most a time on , but it is .
so the constant c we're assum is at least on , so it certainli can multipli c time ten to get a .
it's definit at least on .
so the right hand side here is unquestion bigger than the left hand side .
a in fact is bigger than ten , let alon bigger than ten .
so the interest ingredi is gener the induct step so rememb what you do is here is you assum you've alreadi proven the assert that , in thi case the t of n is at most an for all smaller integ , and now you just mere have to prove it again for the current integ .
so we're now interest in the case where n is bigger than on and the assumpt that we've alreadi proven to everyth smaller is call induct hypothes .
so what doe it mean that we alreadi prove it for all smaller number , that mean we can us in the proof of our induct step the fact that p of k is the most a time k for all k strictli less than n .
all we gotta do is enlarg the rang of n's to which thi hold to on more to the current valu n .
and all we have to do is follow our nose .
so pretti much , we , we have to start on the left hand side with t of n , and we have to wind up on the right hand side with a time n .
and pretti much , at everi step of the proof , there's just gonna be on conceiv thing we could do .
so we just follow our nose .
we start with what we wanna upper bound , t of n .
well , what do we got go for us ?
the onli thing we can do at thi point is invok the recurr that we were given up here .
so we have an upper bound on t of n in term of the t valu of smaller integ .
so we ar given that t of n is at most c time n , plu t of n over five , plu t of seven tenth n .
of cours ignor fraction , you would round up or round down , if you want to be precis , and the auxiliari lectur note ar more precis , if you want to see what the gori detail look like .
but it's realli just exactli the same argument .
on just ha to be a littl bit more anal about it .
so now that we've invok the recurr , what can we possibl do , right ?
we can't realli do ani direct manipul on ani of these three term .
but fortun , we have thi induct hypothesi .
that appli to ani valu , ani integ which is less than n .
so we have her , n <num> , that's certainli less than n .
we have <num> percent of n .
that's certainli less than n .
so we can appli the induct hypothesi twice .
we alreadi know that these t valu ar bound abov by a time their argument .
so t of n over <num>'s at most a , time n over five .
t of seven tenth n is at most a , time seven tenth n .
now we can group term togeth , not we're compar appl to appl .
so we have n , time quantiti c , plu a <num> , plu seven tenth a .
let me just go ahead and group the two a turn togeth .
and that's gonna be nine tenth a .
no , don't forget where we're go , what the end goal is .
we want a upper bound t of n by an .
so we wanna write that thi is bound abov by a time n .
and now you see exactli how i revers engin our choic of a , as a function of the given constant c .
sinc a is ten time as big as c , if i take <num> percent of a and add c , i just get a back .
so by our choic of a .
thi equal an .
and that pretti much wrap thing up .
so don't forget what all thi stuff stand for .
so what did we just prove ?
what did we just prove by induct ?
we prove t of n is , at most , a constant time n for everi n .
that is , t of n is big o of n .
what wa t of n ?
that wa the run time of our algorithm .
that's all we care about .
so becaus t of n is big o of n , inde , deselect run in o of n time .
thi option video will be , more or less , the last word that we have on sort for the purpos of thi cours .
and it'll answer the question , can we do better ?
rememb , that's the mantra of ani good algorithm design .
i've shown you n log n algorithm for sort , merg short in the worst case , quick sort , on averag .
can we do better than n log n ?
inde , for the select problem , we saw we could do better than n log n .
we could linear time .
mayb we can do linear time for sort as well .
the purpos of thi video is to explain to you why we cannot , do sort , in linear time .
so thi is a rare problem where we understand quit precis how well it can be solv at least for a particular class of call comparison base sort which i'll explain in a moment .
so here's the form of the theorem , i want to give you the gist of in thi video .
so in addit to restrict to comparison base sort which is necessari as we'll see in a second , i'm go to make a second assumpt which is not necessari but is conveni for the lectur which is that i'm onli go to think about determinist algorithm for the moment .
i encourag you to think about why the same style of argument give an n log and lower bound on the expect run time of ani random algorithm .
mayb i'll put that on the cours site as an option theori problem .
so , in particular , a quick sort is optim in the random sens .
it have averag and long end time and then again claim that no comparison base sort can be better than that , even on averag .
so , i need to tell you what i mean by a comparison base sort algorithm .
what it mean , it's a sort algorithm that access the element of the input arrai .
onli via comparison , it doe not do ani kind of direct manipul on a singl arrai element .
all it doe , is it pick pair of element and ask the question is the left on bigger or is the right on bigger .
i like to think of comparison base sort as gener purpos sort routin .
thei make no assumpt about what the data is other than that it's from some total order set .
i like to think of it realli as a function that take as an argument a function pointer that allow it to do comparison between abstract data type .
there's no wai to access the gut of the element .
all you can do is go through thi api , which allow you to make comparison .
and inde if you look at the sort routin and sai the unit's oper system , that's exactli how it's set up .
you just patch in a function pointer to a comparison oper .
i know thi sound super abstract so , i think it becom clear onc we talk about some exampl .
there's famou exampl of comparison base sort includ everyth we've discuss in the class so far .
there's also famou exampl of non comparison base sort which we're not gonna cover , but perhap some of you have heard of or at the veri least thei're veri easi to look up on wikipedia or wherev .
so exampl includ the two sort algorithm we discuss so far , mergesort .
the onli wai that mergesort interact with the element in the input arrai is by compar them and by copi them .
similarli , the onli think quick sort doe with the input arrai element is compar them and swap them in place .
for those of you that know about the heap data structur which we'll be review later in the class .
heap sort .
where you just , heapifi a bunch of element , and then extract the minimum n time .
that also us onli comparison .
so what ar some famou non exampl ?
i think thi will make it even more clear what we're talk about .
so bucket sort is on veri us on .
so , bucket sort's us most frequent when you have some kind of distribut assumpt on the data that you're sort .
rememb that's exactli what i'm focus on avoid in thi class .
i'm focus on gener purpos subroutin where you don't know anyth about the data .
if you do know stuff about the data , bucket sort can sometim be a realli us method .
for exampl , suppos you can model your data as i i d sampl from the uniform distribut on zero on .
so thei're all ration number , bigger than zero , less than on , and you expect them to be evenli spread through that interv .
then what you can do in bucket sort is you can just .
prealloc end bucket where you're gonna collect these element .
each on is gonna have the same width , width on over n .
the first bucket you just do linear pass with the input arrai .
everyth that's between zero and on over n you stick in the first bucket .
everyth in between on over n and two over n you stick in the second bucket .
two over end and three over n you sick in the third bucket and so on .
so with the singl pass .
you've classifi the input element accord to which bucket thei belong in , now becaus the data is assum to be uniform at random , that mean you expect each of the bucket to have a veri small popul , just a few element in it .
so rememb if it .
element ar draw uniform from the interv zero on , then it's equal like to be in each of the n avail bucket .
and sinc there's n element that mean you onli expect on element per bucket .
so that each on is gonna have a veri small popul .
have bucket the data , you can now just us , sai , insert sort on each bucket independ .
you're gonna be do insert sort on a tini number of element , so that'll run in constant time , and then there's gonna be linear number of bucket , so it's linear time overal .
so the upshot is .
if you're will to make realli strong assumpt about your data like it's drawn uniformli at random from the interv zero on then there's not an n log in lower bound in fact you can allud the lower bound and sort them in your time .
so , just to be clear .
in what sens is bucket sort not comparison base ?
in what sens doe it look at the gut of it element and do someth other than access them by pair of comparison ?
well , it actual look at an element at input arrai and it sai what is it valu , and it check if it valu is . <num> versu . <num> versu . <num> , and accord to what valu it see insid thi element , it make the decis of which bucket to alloc it to .
so , it actual stare at the gut of an element to decid how , what to do next .
anoth non exampl , which eh , can be quit us is count and sort .
so thi sort algorithm is good when your data again we're gonna make an assumpt on the data , when their integ , and their small integ , so thei're between zero and k where k is sai ideal at most linear in n .
so then what you do , is you do a singl pass through the input arrai .
again , you just bucket the element accord to what their valu is .
it's somewher between zero and k , and it's an integ by assumpt .
so you need k bucket .
and then you do a pass , and you sort of depopul the bucket and copi them into an output arrai .
and that give you a , a sort algorithm which run in time , o of n plu k .
where k is the size of the biggest integ .
so the upshot with count sort is that , if you're will to assum that data ar integ bound abov by some factor linear in n , proport to n , then you can sort them in linear time .
again counti sort doe not access the rail and it's mere through comparison .
it actual stare at an element , figur out what it's valu is , and us that valu to determin what bucket to put the element in .
so in that sens it's not a comparison case sort and it can under compar it's assumpt to beat the end log and lower it down .
so a final exampl is the on that would them rate sort .
i think that thi is sort of an extens of count sort , although you don't have to us count sort as the interloop you can us other so call stabl sort as well .
it's the stuff you can read about in mani program book or on the web .
and up shot at rate sort .
you , you again you assum that the date ar integ .
you think of them in digit represent , sai binari represent .
and now you just sort on bit at time , start from the least signific bit and go all the wai out to the most signific bit .
and so the upsid of rate sort , it's an extens of count sort is the sens that if your data is integ that ar not too big , polynomi bound in n .
then it let you sort in linear time .
so , summar , a comparison base sort is on that can onli access the input arrai through thi api , that let you do comparison between two element .
you cannot access the valu of an element , so in particular you cannot do ani kind of bucket techniqu .
bucket sort , count sort , and rate sort all fundament ar do some kind of bucket and that's why when you're will to make assumpt about what the data is and how you ar permit to access that data , that's when you can bypass in all of those case , thi analog and lower valu .
but if you're stuck with a comparison base sort , if you wanna have someth .
gener purpos .
you're gonna be do n log n comparison in the worst case .
let's see why .
so we have to prove a lower band for everi singl comparison base sort method , so a fix on .
and let's focu on a particular input length .
call it n .
okai , so now , let's simplifi our live .
now that we're focus on a comparison base sort method , on that doesn't look at the valu of the arrai element just in the rel order .
we mai as well think of the arrai as just contain the element . . .
on , two , three , all the wai up to n , in some jumbl order .
now , some other algorithm could make us of the fact that everyth is small integ .
but a comparison base sort method cannot .
so there's no loss in just think about an unsort arrai contain the integ n inclus .
now , depsit seemingli restrict the space of input that we're think about , even here , there's kind of a lot of differ input we've gotta worri about , right ?
so n element can , can show up , and n factori differ order , right ?
there's n choic for who the first element is , then n <num> choic for the second element , m minu two choic for the third element , and so on .
so , there's n factori for how these element ar , ar arrang in the input arrai .
so i don't wanna prove thi super formal , but i wanna give you , the gist , i think , the good intuit .
now , we're interest in lower bound the number of comparison that , thi method make in the worst case .
so let's introduc a paramet k , which is it worst case number of comparison .
that is , for everi input , each of these end factori input , by assumpt , thi method make no more than k comparison .
the idea behind the proof is that , becaus we have n factori fundament differ input , the sort method ha to execut in a fundament differ wai on each of those input .
but sinc the onli thing that caus a branch in the execut of the sort method is the resolut of the comparison , and we have onli comparison , it can onli have two to the k differ execut path .
so that forc two to the k to be at least n factori .
and a calcul then show that , that forc k to be at least omega n log n .
so let me just quickli fill in the detail .
so cross all in factori possibl input just as a thought experi .
we can imagin run thi method in factori time just look at the pattern of how the comparison is resolv .
right ?
for each of these in factori input , we run it through thi sort method , it make comparison number on , then comparison number two , then comparison number three , then comparison number four , then comparison number five , and you know it get back a zero , then a on , then a on , then a zero .
give in some other input and it get back a on , then a on , then a zero , then a zero and so on .
the point is , for each of these in factori input , it make at most k comparison , we can associ that with a k bit string , and becaus it .
is there's onli k bit we're onli go to see two to the k differ k bit string two to the k differ wai that a sequenc of comparison result .
now to finish the proof we ar gonna appli someth which i don't get to us as much as i'd like in an evid class but it's alwai fun when it come up , which is the pigeon hole principl .
the principl you recal is the essenti obviou fact that if you try to stuff k plu on pigeon into just k cubbi hole , on of those k cubbi hole ha got to get two of the pigeon .
okai at least on of the cubbi hole get at least two pigeon .
so for us what ar the pigeon and what ar the hole ?
so our pigeon ar these in factori differ input .
the differ wai you can scrambl the imag on through .
and , what ar our hole ?
those ar the two indic differ execut that the sort method can possibl take on .
now if .
the number of comparison k us is so small , that two to the k , the number of distinct execut , number of distinct wai comparison can resolv themselv , is less than the number of differ input that have to be correctli sort .
then by the pivot princip .
on color get two hole .
that is , two differ input get treat in exactli the same wai , by the sort method .
thei ar ask , exactli the same k comparison and the comparison resolv ident .
on .
jumbl of on through n , then you get a <num> then it's a total differ jumbl of n and then again you get a <num> and if thi happen the algorithm is toast , in the sens that it's definit not correct , right , cuz we've fed it two differ input .
and it is unabl to resolv which of the two it is .
right ?
so , it mai be on premut of on through n , or thi total differ premut of on through n .
the algorithm ha tri to learn about what the input is through these k comparison , but it ha exactli the same data about the input in two , the two case .
so , if it output the correct sort version in on case , it's gonna get the other on wrong .
so , you can't have a common execut of a sort algorithm unscrambl total differ premut .
it can't be done .
so what have we learn ?
we've learn that by correct , two to the k is in fact at least in the factori .
so how doe that help us ?
well , we wanna lower bound k .
k is the number of comparison thi arbitrari store method is us .
thei wanna show that's at least n log n .
so we , to lower bound k , we better lower bound n factori .
so , you know , you could us stirl's approxim or someth fanci .
but we don't need anyth fanci here .
we'll just do someth super crude .
we'll just sai , well , look .
thi is the product of n thing , right ?
n time n minu on time n minu two , blah , blah , blah , blah .
and the largest of those , the n over two largest of those n term ar all at least n over two .
the rest we'll just ignor .
pretti sloppi , but it give us a lower bound of n divid by two rais to the n divid by two .
now we'll just take log base two of both side , and we get the k is at least n over two , log base two of n over two , also known as omega of n log n .
and that my friend is why a heret determinist sort algorithm that's comparison base ha gotta us n log n comparison in the worst case .
so in thi set of lectur we'll be discuss the minimum cut problem in graph and we'll be discuss the random contract algorithm .
a random algorithm is just so simpl and eleg , it's almost imposs to believ that it could possibl work , but that's exactli what we'll be prove .
so on wai you can think about thi set of lectur is as a segu of sort between our discuss of random and our discuss of graph .
so we just finish talk about random in the context of sort and search .
we'll pick it up again toward the end of the class when we discuss hash , but while we're in the middl of random probabl review , i want to give you anoth applic of random in a total differ domain .
in particular to the domain of graph rather than to sort and search .
so that's on high level goal of these lectur .
a second on is we'll get our feet wet talk about graph .
and a lot of the next coupl week , that's what we're gonna be talk about fundament graph primit .
so thi will give us an excus to start warm up with the vocabulari , some of the basic concept of the graph , and what a graph algorithm look like .
anoth perk , although it's not on of the main goal , but i wanna do , i do wanna point out thi fact .
is that , at least , compar to most of the stuff that we're discuss in thi class , thi is a rel recent algorithm , the contract algorithm .
by rel recent , i mean , okai , it's twenti year old .
well at least that mean most of us , i know not all of us , but most of us at least were born at the time that thi algorithm wa invent .
and so just on quick digress .
you know , in the intro cours like thi most of what we're gonna cover will be oldi but goodi , so from as much as <num> year ago .
and while it's kind of amaz given how much the world and how much technolog ha chang over the past <num> year that idea in comput scienc from that long ago is still us , thei ar .
okai .
so it's just sort of an amaz thing about the stuff that the first gener of comput scientist figur out .
okai .
it's still ; it's still relev to thi dai .
that .
said , algorithm is still a vibrant field with lot of open question and when i have an opportun i'll try and give you glimps of that fact .
so i do wanna point out here that thi is somewhat more reason algorithm that most of the other on will see which date back to the 90s .
so let's talk about graph .
fundament , what a graph doe is repres pair wise relationship amongst a set of object .
so as such , a graph is go to have two ingredi .
so first of all there's the object that you're talk about and these have two veri common name and you're just gonna have to know both of the name even though thei're complet synonym .
the first name is vertic .
so vertex is the singular , vertic is the plural .
also known interchang as node .
i'll be us the notat , capit v for the set of , of vertic .
so those ar the object .
now , we wanna repres pair wise relationship .
so these pair ar gonna be call edg .
it'll be note by denud by capit e and there's two flavor of graph and both ar realli import , both come up all the time in applic .
so you should be awar of both kind .
so there is undirect graph and direct graph and that just depend on whether the edg themselv ar undirect or direct .
so edg can be undirect by which i mean thi pair is unord .
there ar just two vers of edg .
the two n point sai u and v and you don't distinguish on as the first and on as the second .
or edg can be direct , in which case you have a direct graph and here a pair is order .
so you do have a notion of a first vertex or a first end point and the second vertex or second end point of an edg .
those ar often call the tail and the head , respect .
and onc in a while although i'll , i'll try to us , i'll not us thi terminolog , you hear direct edg call arc .
now i think all of thi is much clearer if i just draw some pictur inde on us to call graph dot and line , the dot would refer to the vertic .
so here's four dot or four vertic .
and , the edg would be line .
so the wai you denot on of these edg , is , you just draw a line between the two , end point of that edg , the two vertic that it correspond to .
so thi is an undirect graph with four vertic and five edg .
we could equal well have a direct version of thi graph .
so let's , still have four vertic and five edg .
but to indic that thi is a direct graph , and that each edg ha a first vertex and a second vertex , we're gonna add arrow to the line .
so the arrow point to the second vertex , or to the head of the , of the edg .
so the first vertex is often call the of the edg .
so , graph ar complet fundament .
thei show up not just in comput scienc but in all kind of differ disciplin social scienc and , and biologi be two promin on .
so , let me just mention a coupl of reason you might us them just off the top of my head .
but liter there's hundr or thousand of other .
so , a veri liter exampl would be road network .
so , imagin you type in ask for drive direct from point a to point b and some web applic or softwar or whatev comput a , a road , a rout for you .
what it's do is it's manipul some represent of a road network , which inevit is go to be store as a graph , where the vertic correspond to intersect , and the edg correspond to individu road .
the web is often fruitfulli thought of as a graph , a direct graph .
so here the vertic ar the individu web page and edg correspond to hyperlink .
so the first vertex of an edg , the tail , is go to be the page that contain the hyperlink .
the second vertex or the head of the edg is go to be what the hyperlink point to .
so that's the web as a direct graph .
social network ar , quit natur , repres as graph .
so here , the vertic correspond to the individu .
in the social network and the edg correspond to relationship , sai friendship link .
i encourag to think about amongst the popular social network these dai , which on ar undirect graph and which on ar direct graph .
we have some interest exampl of each of those .
and often graph ar us even when there isn't such an obviou network structur .
so , just to mention on exampl let me just write down preced constraint .
so , to sai what i mean you might think about you know , sai you're a freshman in colleg and you're look at your , your major is a comput scienc major .
now , you wanna know what cours to take in what order .
and you could think the follow graph , where each of the cours in your major correspond to a vertex and you draw a direct edg from on , from cours a to cours b , if cours a is a prerequisit for cours b , that is , it ha to be complet befor you begin cours b .
'kai , so that's a wai to repres depend .
sort of , a tempor order between pair of , pair of object .
us a direct graph .
so that's the basic languag of graph .
let me now talk about cut in graph , cuz again thi set of lectur is gonna be about the so call minimum cut problem .
so the definit of a cut of a graph is veri simpl .
it's just a group , a partit , of the vertic of the graph , into two group , a and b .
and both of those two group should be non empti .
so to describ thi in pictur , let me give you a cartoon of a cut in both the undirect and direct case .
so for an undirect graph , you can imagin draw your two set a and b .
and onc you've defin the two set , a and b , the edg then fall into on of three categori .
you've got edg with both of the end point in a .
you've got edg with both of the end point in b .
and then you've got edg with on end point in a , and on endpoint in b .
so that's gener what the pictur graph look like view through the len of a particular cut ab .
the pictur for direct graph is similar , you would again have an a , and you'd again have a b , you have direct edg with both end point in a , direct edg with both end point in b .
and now you have actual two further categori .
so , you have edg who cross the cut from left to right , that is , whose tail vertex is in a , and whose head vertex in b .
and you can also have edg which cross the cut in the opposit direct that is their tail is in b and their head is in a .
usual when we talk about cut we're gonna be concern with how mani edg cross a given cut .
and by that i mean the follow the cross edg of a cut a , b .
ar those that satisfi the follow properti .
so in the case it's exactli what you think it would be .
on of the endpoint is in a , the other endpoint is in b , that's what it mean to cross the cut .
now in the direct case there's a number of reason definit you could propos about which edg cross the cut .
typic and in thi cours we're go to focu on the case were we onli think about edg that cross the cut from the left to the right .
and we ignor edg that cross from the right to the left .
so that is the edg that cross the cut ar those with tail in a and head in b .
so refer to our two pictur , our two cartoon of cut , for the undirect on , all three of these blue edg would be edg cross the cut ab , cuz thei're the on that have on end point on the left side , and on end point on the right side .
now for the direct on , we onli have two cross edg .
so the two that cross from left to right , with tail in a and head in b .
the on that's cross backward doe not contribut .
we don't count it as a cross edg of the cut .
so the next quiz is just a saniti check that you've absorb the definit of a cut of a graph .
right , so the answer to thi quiz is the third option .
recal what is the definit of a cut ?
it's just the wai the group , the vertic into two set a and b both should also be non empti .
so we have n vertic and essenti we have on binari degre of freedom for each .
for each vertex we can decid whether or not it goe in set a or goe in set b .
so two choic for each of the n vertic .
that give us a two to the n possibl choic .
two to the n possibl cut overal .
now that's slightli incorrect becaus recal that a cut can't have a non empti set a or a non empti set b .
so those ar two of the two to the n option which were disallow .
so strictli speak , the number is two to the end , minu two .
but , two to the end is certainli the closest answer of the four provid .
now the minimum cut problem is exactli what you'd think it would be .
i'd give you as input a graph , and amongst these exponenti mani cut , i want you to identifi on for me with the fewest number of cross edg .
so a few quick comment so first of all the name for thi cut is a min cut a min cut is on with the fewest number of cross edg .
secondli to clarifi i am go to allow in the input what is call parallel edg .
there will be lot of applic where parallel edg ar sort of pointless but for minimum cut it's natur to allow parallel edg that mean you have two edg that correspond final , the more season programm among you ar probabl wonder what i mean by , you're given a graph as input .
you might be wonder about how , exactli , that's repres .
so the next video's gonna discuss exactli that .
the popular wai of repres graph , and , and how we're gonna usual do it in thi cours , specif via what's call an adjac list .
okai so i want to make sure that everybodi understand exactli what the minimum cut problem is ask , so let me draw for you a particular graph .
with a vertic .
and quit a few edg .
and what i want you to answer is what is the min cut valu in thi graph , that is how mani edg cross the man on cut , the cut with the fewest number of cross edg .
all right , so the correct answer is the second choic .
the cut valu is two , and the cut which show that is just to break it , basic , in half , into the two halv .
in thi case , there ar onli two cross edg .
thi on and thi on .
and i'll leav it for you to check that there's no other edg that ha as few as two edg .
now in thi case , we've got a veri balanc split when we took the minimum cut .
in gener , that need not be true .
sometim even a singl vertex can defin the minimum cut of a graph .
and i encourag you to think about a concret exampl that prove that .
so why should you care about comput the minimum cut ?
well thi is on problem amongst a genr call graph partit where you're given a graph and you wanna break it into two or more piec .
and these kind of graph partit problem come up all the time in a surprisingli divers arrai of applic .
so let me just mention a coupl at a high level .
so on veri obviou on when you have a , when your graph is repres a physic network .
what identifi someth like a allow you to do is identifi weak in your network .
then perhap it's your own network and you want to understand where you want to soup up the infrastructur .
becaus it's in some sens a hot spot of your network or a weak point .
or mayb there's someon els's network and you want to know where the weak spot is in their network .
in fact there is some declassifi document about mm , about fifteen year ago or so which show that the unit state and soviet union militari , back dure the cold war , were actual quit interest in comput minimum cut .
cuz thei were look for thing like , for exampl , what's the most effici wai to disrupt the other countri's transport network .
anoth applic which is a big deal in social network analyst these dai , is the idea of commun detect .
so the question is amongst a huge graph , like sai the graph of everybodi whose on facebook or someth like that , how can you identifi small pocket of peopl that seem tightli knit .
that seem close relat from which you like to infer that thei're a commun of some sort .
you know mayb thei all go to the same school , mayb thei all have the same interest , mayb thei're part of the same biolog famili whatev .
now , it's , to some degre , still an open question , how to best defin commun and social network .
but as a quick and dirti sort of first order heurist , you can imagin look for small region , which , on the on hand , ar highli interconnect amongst themselv .
but quit weakli connect .
to the rest of the graph , so subroutin like the minimum cut problem can be us for identifi these small dens interconnect , but then weakli connect to everybodi els pocket of the graph .
final cut problem ar also us a lot in , in vision .
so for exampl , on wai you can us them is in what's call imag segment .
so here what's go on is your given as input two d arrai where each entri is a pixel from some imag .
and there's a graph which is veri natur to defin given a two d arrai of pixel , name you have an edg between two pixel if their neighbor .
so for two pixel that ar immedi next to each other , left to right or top to bottom .
you put an edg there .
so that give you , what's call a grid graph .
and now unlik the basic minimum cut problem that we're talk about here .
in imag segment it's most natur to us edg weight , where the weight of an edg is basic how like you expect those two pixel to be come from a common object .
why not ?
you expect two neighbor pixel to come from the same object .
well , perhap their color map ar almost exactli the same , and you just expect that thei're part of the same thing .
so onc you've defin thi grid graph with suitabl edg weight , now you run a graph partit or mid cut type sub routin and the hope is that the cut that it identifi rip off on of the contigu object in the pictur .
and you do that a few time and you get the major object in the given pictur .
so thi list is by no mean exhaust of the applic of ming cut and graph partit sub routin , but i hope it serv as suffici motiv to watch the rest of the lectur in thi sequenc .
okai , so thi video's not about ani particular graph problem , not about a , ani particular graph algorithm .
just , sort of , the preliminari we need to discuss algorithm on graph .
how do we measur their size ?
how do we repres them , and so on .
rememb what a graph is , it realli ha two ingredi .
first of all , there's thi set of object we're talk about .
those might be call vertic .
synonym , we might call them node .
we repres pair wise relationship us edg .
these can be either un direct in which case , thei're order pair or an edg can be direct from <num> to anoth .
in that case , thei're order pair , and we have a direct graph .
now , when we talk about sai , the size of a graph , or the run time of an algorithm that oper on a graph .
we need to think about what we mean by input size .
in particular , for a graph , there's realli two differ paramet that control how big it is , unlik an arrai .
for arrai , we just had a singl number , the length .
for graph , we have the number of vertic , and we have the number of edg .
usual we'll us the notat n for the number vertic , m for the number of edg .
so the next quiz will ask you to think about how the number of edg m , can depend on the number of vertic , n .
so , in particular , i want you to think about in thi quiz , an un direct graph it ha n vertic .
there's no parallel edg .
'kai , so for a given pair of vertic , there's either zero or on edg between them .
moreov , let's assum that the graph is unconnect .
'kai ?
so i don't want you to think about graph that have zero edg .
now , i haven't defin what a graph is .
what it mean for a graph to be connect formal , yet , but i hope you get the idea .
it mean it's in on piec , you can't break it into two part that have no edg cross between them .
so , for such a graph , no parallel edg , in on piec , n vertic , think about what is the minimum number of edg it could possibl have , and what is the maximum number of edg es , as a function of n , that it could possibl have .
all right , so the correct option is the first on the fewest number of edg that a connect undirect graph we can have is n minu <num> , and the maximum number of edg that an undirect graph with no parallel edg can have is n time n minu <num> over <num> , better known as n choos <num> .
so why doe it need at least n minu <num> edg , if it's go to be in on piec .
well think about at , ad the edg on at a time .
okai , on each of the edg of the graph .
now , initi , you just have a graph with zero edg , the graph ha indiffer piec and isol vertic ha no edg at all .
now each time you add on edg , what you do is you take two of the exist piec , at best , and fuse them into on .
so , the maximum decreas you can have in the number of differ piec of a graph is it can decreas by <num> each time you add an edg .
so from a start point of n differ piec , you've got to get down to <num> piec .
so that requir the addit of n minu <num> edg .
you can also convinc yourself of thi best , by draw a few pictur and notic that tree achiev thi bound exactli , so for exampl here is a <num> vertex tree that ha <num> edg .
so thi is a case where m is inde , n minu <num> .
now , for the upper bound , why can't you have more than n choos <num> ?
well , it's clear that the largest number of edg you can have is for the complet graph .
where everi singl pair of edg ha <num> between them .
again , there's no parallel arc and edg ar unord .
so , there's at most , n choos <num> possibl of where to put an edg .
so again , if n equal <num> , here would be an exampl with a maximum possibl number , <num> edg .
so , now that i've got you think about how the number of edg can vari with the number of vertic .
let me talk about the distinct between spars and dens graph .
it's import to distinguish between these two concept becaus some data structur and algorithm ar better suit for spars graph .
other data structur and algorithm ar better suit for dens graph .
so , to make thi precis , let me just put down thi veri common notat n is the number of vertic of the graph under discuss , m is the number of inch .
thi is quit standard notat .
pleas get us to it and us it yourself .
if you revers these , you will confus a lot of peopl who have familiar with graph algorithm and data structur .
now on thing we learn from the previou quiz is the follow .
so in most applic , not all applic , but most applic , m is at least linear in n .
rememb in the quiz we saw is at least n minu <num> if you want the graph to be connect , and it's also big o of n squar .
thi is under the assumpt that there's no parallel arc .
now , there ar case where we want to allow parallel arc .
in fact we'll do that in the contract algorithm for the min cut problem .
there ar case where we want to allow the number of edg to drop so low , that the graph break into multipl piec .
for exampl , when we talk about connect compon but more often than not , we're think about a connect graph with no parallel edg .
and then we can pin down the number of edg m to be somewher between the linear and the number of node , linear and n and quadrat in it .
now i'm not go to give you a super formal definit of what a spars or a dens graph is , and peopl ar a littl loos with thi , thi terminolog in practic .
but basic , spars mean you're closer to the lower bound , closer to linear .
dens mean , you're closer to the upper bound , closer to quadrat .
now i know thi leav ambigu when the number of edg is someth you know like n to the <num> halv .
usual in that case you'd think of that as a dens graph .
so usual anyth which is more than n time logarythm term , you'd think of that as a dens graph .
but again , peopl ar a littl bit sloppi with thi when thei talk about graph .
next i want to discuss two represent of graph and we're mostli go to be us the s econd on in thi cours , but thi first on , the adjac matrix , i do want to mention just briefli , just on thi slide .
thi is the supernatur idea where you repres the edg in a graph us a matrix .
let me describ it first for undirect graph .
so , the matrix is go to be denot by capit a , and it sai squar n by n matrix where n is the number of vertic of the graph .
and the semant ar the i jth entri of the matrix is <num> .
if and onli if there's an edg between the vertic i and j in the graph .
i'm assum here that the vertic ar name <num> , <num> , <num> , <num> , et cetera all the wai up to n .
it's easi to add bell and whistl to the adjac matrix to accommod parallel edg to accommod edg weight , which is accommod direct arc , direct edg .
if you want to have parallel arc , you could just have aij denot the number of arc that ar between i and j .
if edg have differ weight , you could just have aij be the weight of the ij edg .
and for the direct graph you could us plu on and minu on .
so if the arc is direct from i to j , you'd set i , aij to be plu <num> .
if the arc is direct from j to i , you'd set aij to minu <num> .
there ar mani metric by which you can evalu a data structur , or a represent .
two import on i want to discuss here .
first of all , the number of resourc it requir and in thi context , that's the amount of space that the data structur need .
the second thing is what ar the oper of the data structur support .
so let's just begin with space requir .
what ar thei for the adjac matrix ?
all right , so the answer at least with the sort of straight forward wai of store a matrix is n squar .
and thi is n depend of the number of edg .
so you could try to beat thi down for spars graph us spars matrix trick .
but for the basic idea of just actual repres an n by n matrix , you got n squar entri , you gotta store on bit in each whether the edg is there or not .
so that's go to give yo u n squar space .
the constant ar , of cours , veri small , becaus you're just store on bit per entri .
but nonetheless thi is quadrat in the number of vertic .
now that's go to be fine if you have a dens graph , the number of edg is as high as n squar , then you're not realli wast anyth in thi represent .
but in a spars graph , if m is much closer to linear , then thi is a super wast represent .
let's talk about the ajac list represent , thi is the , the domin on we'll be us in thi class .
thi ha sever ingredi .
so , first you keep track of both the vertic and the edg as independ entiti .
so you're go to have an arrai , or a list of each .
and then we want these two arrai to cross refer each other in the obviou wai .
so given a vertex , you want to know which edg it's involv in .
given an edg , you want to know what it endpoint ar .
so , let's sai first , most simpli , each edg is go to have two pointer , on for each of the two endpoint .
and in direct graph , of cours , it would keep track of which on is the head and which on is the tail .
now , each vertex is go to point to all of the edg of which it's a member .
now in an undirect graph , it's clear what i mean by that .
in a direct graph , you could do it in a coupl wai .
gener you'd have a vertex , keep track of all of the edg , for which it is the tail .
that is , all of the edg which you can follow on hop out from the edg .
if you want to , you can also have a second arrai , at a more expens of storag , where the vertex also keep track of the edg point to it .
the edg for which it's the head .
so , let me ask you the same question i did with an adjac matrix .
what is the space requir of an adjac list , as a function of the number of edg m , and the number of vertic n , of the graph ?
so , the correct answer to thi question is the third option , theta of m plu n , which we're go to think of as linear space in the size of the gra ph .
so , thi quiz is , is a littl tricki .
so , it's explain the answer when we return to the slide with the ingredi of adjac list .
and let's comput the space for each of these four ingredi separ .
most of them ar straightforward .
for exampl , consid the first ingredi .
thi is just an arrai , or a list of the n vertic .
and we just need constant space per vertex to keep track of it exist .
so thi is go to be theta of n , linear in the number of vertic .
similarli , for the m edg , we just need linear space in the number of edg to rememb their exist .
so that's go to be theta of m .
now , each edg ha to keep track of both of it endpoint .
so that's two pointer , but two is a constant .
for each of the m edg , we have a constant space to keep track of endpoint .
so that's go to give us anoth theta of m constant per edg .
now , thi fourth case , you might be feel kind of nervou , becaus a vertex , in principl could have edg involv all n minu <num> of the vertic .
so the number of point or is it a singl vertex could be theta of n .
also you could have you know , you do have n vertic that could be theta of n squar .
and certainli in someth like a complet graph you realli would have that function .
but the point is in spars graph n , n squar is wai overkil to the space need by thi fourth set of pointer .
actual , if you think about it for each pointer in the fourth categori , a vertex point to a given edg , there is a pointer in the third categori point in the opposit direct , from that edg back to that vertex .
so , there's actual a on to on correspond .
between pointer in the third categori , and pointer in the fourth categori .
sinc the third categori ha space theta of m , so doe all of the pointer in the fourth categori .
so ad up over the four ingredi , we have on theta of n , and three theta of ms , so that's go to give us overal a theta of m plu n .
if you prefer , anoth wai you could think about thi would be theta of the max of m and n .
these ar the same up to a constant factor .
now , as we discuss in a previou slide .
often , m is go to be bigger than n , but i want to do a gener analysi here , which appli even if the graph is not connect , even , even if it is in multipl piec .
so the space of the adjac list is within a constant factor the same as the number of ingredi in the graph , the number of vertic plu the number of edg .
so in that sens , that's exactli what you want .
now be confront with these two graph represent that i've shown you i'm sure you're ask , well , which on should you rememb ?
which on should you us ?
and the answer , as it so often is , is it depend .
it depend on two thing .
it depend on the densiti of your graph .
it depend on how m compar to n .
and it also depend on what kind of oper that you support , want to support .
now given what we're cover in thi class , and also the motiv applic i have in mind i can give you basic a clean answer to thi question for the purpos of these five week .
which is we're go to be focus on adjac list .
the reason we're go to focu on adjac list in thi class , is both , is for both of these reason , both becaus of the oper we want and both becaus of the graph densiti and motiv applic .
so , first of all , most of the graph primit , not all , but most , will be deal with graph search and adjac list ar perfect for do graph search .
you get to a node .
you follow an outgo arc .
you go to anoth node .
you follow an outgo arc and so on .
and so , adjac list ar the perfect thing to do graph search .
adjac matric ar definit good for certain kind of graph oper .
but thei're not thing we're realli go to be cover in thi class .
so that's reason on .
reason two is , a lot of the motiv for graph primit these dai come from massiv , massiv network .
i mention earlier how the web ca n be fruitfulli thought of as a direct graph .
where the vertic ar individu web page .
and direct arc correspond to hyperlink , go from the page with the hyperlink , point to the on that the hyperlink goe to .
now , it's hard to get an exact measur of the web graph , but a conserv lower bound on the number of vertic is someth like <num> billion .
so that's <num> to the <num> .
now that's push the limit of what comput can do , but it's within the limit .
so if you work hard , you can actual oper on graph with <num> to the <num> node .
now , suppos we us an adjac matrix represent .
so if n is <num> to the <num> , then n squar is go to be like <num> to the <num> .
and now we're get close to the estim number of atom in the known univers .
so that is clearli not feasibl now and it's not go to be feasibl ever .
so the adjac matrix represent is total out for , huge spars graph like the web graph .
adjac list , well , the degre , on averag , in the web , is thought to be someth like <num> .
so , the number of edg is onli go to be someth like <num> to the <num> .
and then the adjac of thi represent will be proport to that .
and again , that's realli push what we can do with current technolog , but it is within the limit , so us that represent we can do non trivial comput on graph , even at the scale of the web graph .
so now i get to tell you about the veri cool random contract algorithm for comput the minimum cut of a graph .
let's just recal what the minimum cut problem is .
we're given as input an undirect graph .
and the parallel edg ar allow .
in fact , thei will aris natur throughout the cours of the algorithm .
that is , we're given pair of vertic , which have multipl edg which have that pair as endpoint .
now , i do sort of assum you've watch the other video on how graph ar actual repres , although that's not go to plai a major role in the descript of thi particular algorithm .
and , again , the goal is to comput the cut .
so , a cut is a partit of the graph vertic into two group , a and b .
the number of edg cross the cut is simpli those that have on endpoint on each side .
and amongst all the exponenti possibl cut , we want to identifi on that ha the fewest number of cross edg , or a min cut .
so , here's the random contract algorithm .
so , thi algorithm wa devis by david karger back when he wa an earli ph . d student here at stanford , and thi wa in the earli 90s .
so like i said , quot unquot onli about twenti year ago .
and the basic idea is to us random sampl .
now , we'd known forev , right , ever sinc quicksort , that random sampl could be a good idea in certain context , in particular when you're sort and search .
now on of the thing that wa such a breakthrough about karger's contract algorithm is , it show that random sampl can be extrem effect for fundament graph problem .
so here's how it work .
we're just gonna have on main loop .
each iter of thi while loop is go to decreas the number of vertic in the graph by <num> , and we're gonna termin when we get down to just two vertic remain .
now , in a given iter , here's the random sampl amongst all of the edg that remain in the graph to thi point , we're go to choos on of those edg uniformli at random .
each edg is equal like .
onc you've chosen an edg , that's when we do the contract .
so we take the two endpoint of the edg , call them the vertex u and the vertex v , and we fuse them into a singl vertex that repres both of them .
thi mai becom more clear when i go through a coupl exampl on the next coupl of slide .
thi merg mai creat parallel edg , even if you didn't have them befor .
that's okai .
we're gonna leav the parallel edg .
and it mai creat a self loop edg pointer that both of the endpoint is the same .
and self loop ar stupid , so we're just gonna delet as thei aris .
each gener decreas the number of vertic that remain .
we start with n vertic .
we end up with <num> .
so after n <num> gener , that's when we stop and at that point we return the cut repres by those two final vertic .
you might well be wonder what i mean by the cut repres by the final two vertic .
but i think that will becom clear in the exampl , which i'll proce to now .
so suppos the input graph is the follow four node , four edg graph .
there's a squar plu on diagon .
so , how would the contract algorithm work on thi graph ?
well , of cours , it's a random algorithm so it could work in differ wai .
and so , we're gonna look at two differ trajectori .
in the first iter each of these five edg is equal like .
each is chosen for contract with twenti percent probabl .
for concret , let's sai that the algorithm happen to choos thi edg to contract , to fuse the two endpoint .
after the fusion these two vertic on the left have becom on , wherea the two vertic on the right ar still hang around like thei alwai were .
so , the edg between the two origin vertic is unchang .
the contract edg between the two vertic on the left ha gotten suck up , so that's gone .
and so what remain ar these two edg here .
the edg on top , and the diagon .
and those ar now parallel edg , between the fuse node and the upper right node .
and then i also shouldn't forget the bottom edg , which is edg from the lower right node to the super node .
so that's what we mean by take a pair of the vertic and contract them .
the edg that wa previous connect with them vanish , and then all the other edg just get pull into the fusion .
so that's the first iter of karger's algorithm of on possibl execut .
so now we proce to the second iter of the contract algorithm , and the same thing happen all over again .
we pick an edg , uniformli at random .
now there's onli four edg that remain , each of which is equal like to be chosen , so the <num> probabl .
for concret , let's sai that in the second iter , we wind up choos on of the two parallel edg , sai thi on here .
so what happen ?
well , now , instead of three vertic , we go down to <num> .
we have the origin bottom right vertex that hasn't particip in ani contract at all , so that's as it wa .
and then we have the second vertex , which actual repres diffus of all of the other three vertic .
so two of them were fuse , the leftmost vertic were fuse in iter <num> .
and now the upper right vertex got fuse into with them to creat thi super node repres three origin vertic .
so , what happen to the four edg ?
well , the contract on disappear .
that just get suck into the super node , and we never see it again .
again , and then the other three go , and where there's , go where thei're suppos to go .
so there's the edg that us to be the right most edg .
that ha no hash mark .
there's the edg with two hash mark .
that goe between the , the same two node that it did befor .
just the super node is now an even bigger node repres three node .
and then the edg which wa parallel to the on that we contract , the other on with a hash mark becom a self loop .
and rememb what the , what the algorithm doe is , whenev self loop like thi appear , thei get delet automat .
and now that we've done our n <num> iter , we're down to just two node .
we return the correspond cut .
by correspond cut , what i mean is , on group of the cut is the vertic that got fuse into each other , and wound up correspond to the super node .
in thi case , everyth but the bottom right node , and then the other group is the origin node correspond to the other super node of the contract graph , which , in thi case , in just the bottom right node by itself .
so thi set a is go to be these three node here , which all got fuse into each other , contract into each other .
and b is go to be thi node over here which never particip in ani contract at all .
and what's cool is , you'll notic , thi doe , in fact , defin a min cut .
there ar two edg cross thi cut .
thi on , the rightmost on and the bottommost on .
and i'll leav it for you to check that there is no cut in thi graph with fewer than two cross edg , so thi is in fact a min cut .
of cours , thi is a random algorithm , and random algorithm can behav differ on differ execut .
so let's look at a second possibl execut of the contract algorithm on thi exact same input .
let's even suppos the first iter goe about in exactli the same wai .
so , in particular , thi leftmost edg is gonna get chosen in the first iter .
then instead of choos on of the two parallel edg , which suppos that we choos the rightmost edg to contract in the second iter .
total possibl , <num> chanc that it's gonna happen .
now what happen after the contract ?
well , again , we're gonna be left with two node , no surpris there .
the contract node get suck into oblivion and vanish .
but the other three edg , the on with the hash mark , all stick around , and becom parallel edg between these two final node .
thi , again , correspond to a cut a , b , where a is the left two vertic , and b is the right two vertic .
now , thi cut you'll notic ha three cross edg , and we've alreadi seen that there is a cut with two cross edg .
therefor , thi is <i>not< i> a min cut .
so what have we learn ?
we've learn that , the contractu algorithm sometim identifi the min cut , and sometim it doe not .
it depend on the random choic that it make .
it depend on which edg it choos to randomli contract .
so the obviou question is , you know , is thi a us algorithm .
so in particular , what is the probabl that it get the right answer ?
we know it's bigger than <num> , and we know it's less than <num> .
is it close to <num> , or is it close to <num> ?
so we find ourselv in a familiar posit .
we have what seem like a quit sweet algorithm , thi random contract algorithm .
and we don't realli know if it's good or not .
we don't realli know how often it work , and we're go to need to do a littl bit of math to answer that question .
so in particular , we'll need some condit probabl .
so for those of you , who need a refresh , go to your favorit sourc , or you can watch the probabl review part ii , to get a refresh on condit probabl and independ .
onc you have that in your mathemat toolbox , we'll be abl to total nail thi question .
get a veri precis answer to exactli how frequent the contract algorithm successfulli comput the minimum cut .
so in the last video i left you with a cliffhang .
i introduc you to the minimum cut problem .
i introduc you to a veri simpl algorithm , random algorithm , in the form of contract algorithm .
we observ that sometim it find the main cut and sometim it doesn't .
and so the <num> question is just how frequent doe it succe and just how frequent doe it fail .
so now that i hope you've brush up the condit probabl and independ , we ar gonna give a veri precis answer to that question in thi lectur .
so recal thi problem we ar given as input in undirect graph , possibl with parallel edg , and that the goal is to comput among the exponenti number of possibl differ cut , that's with the fewest number of cross edg .
so , for exampl in thi graph here , which you've seen in a previou video , the goal is to comput the cut a , b .
here , cuz there ar onli two cross edg , and that's as small as it get .
that's the minimum cut problem and karger propos the follow random contract algorithm base on random sampl , so we have n <num> iter , and the number of vertic get decrement by <num> in each iter .
so we start with n vertic , we get down to <num> .
and how do we decreas the number of vertic ?
we do it by contract or fuse two vertic togeth .
how do we pick which pair of edg , which pair of vertic to fuse ?
well we pick on of the remain edg , uniformli at random .
so there's mani edg there ar remain .
we pick each on , equal like .
what , if the endpoint of that edg ar u , v , then we collaps u and v togeth into a singl super node .
so that's what we mean by contract two node into a singl vertex and then if that caus ani self loop , and as we saw the exampl , we will in gener have self loop , then we delet them befor proceed .
after the n <num> gener , onli two vertic remain .
you'll recal that two vertic natur correspond to a cut .
the first group of the cut a correspond to the vertic that were fuse into on of the super vertic remain at the end .
the other super vertex correspond to the set b the other origin vertic of the graph .
so the goal of thi lec , of thi video is to give an answer to the follow question what is the probabl of success ?
where by success , we mean output a particular min cut a , b .
so let's set up the basic notat .
we're gonna fix ani with input graph , undirect graph .
as usual we us n to denot the number of vertic and m to denot the number of edg .
we're also go to fix a minimum cut a , b .
if a graph ha onli on minimum cut , then it's clear what i'm talk about here .
if a graph ha multipl minimum cut , i'm actual select just on of them .
becaus i'm gonna focu on a distinguish minimum cut a , b , and we're onli gonna defin the algorithm as success if it output thi particular minimum cut a , b .
if it output some other minimum cut , we don't count it .
we don't count it as success .
okai .
so , we realli want thi distinguish minimum cut a , b .
in addit to n and m , we're gonna have a paramet k , which is the size of the minimum cut .
that is , it's the number of cross edg of a minimum cut .
for exampl , that cross a , b .
the k edg that cross the minimum cut a , b ; we're go to call capit f .
so the pictur you wanna have in mind is , there is , out there in the world , thi minimum cut a , b .
there's lot of edg with both end point in a , lot of edg possibl with both endpoint in b .
but , there's not a whole lot of edg with on endpoint in a and on in endpoint in b .
so the edg f , would be precis , these three cross edg here .
so our next step is to get a veri clear understand of exactli when the execut of the contract algorithm can go wrong , and exactli when it's gonna work , exactli when we're go to succe .
so let me redraw the same pictur from the previou slide .
so given thei were hope that the contract algorithm output thi cut a , b at the end of the dai , what could possibl go wrong ?
well , to see what could go wrong , suppos , , at some iter , on of the edg in capit f , rememb f ar the edg cross the min cut a , b , so it's these three magenta edg in the pictur .
suppos at some iter on of the edg of f get chosen for contract .
well becaus thi edg of f ha on endpoint in a and on endpoint in b , when it get chosen for contract , it caus thi node from a and thi node from b to be fuse togeth .
what doe that mean ?
that mean , in the cut that the contract algorithm final output , thi node from a and thi node from b will be on the same side of the output cut .
okai , so the cut output by the contract algorithm will have on on side both the node from a and the node from b .
therefor , the output of the contract algorithm if thi happen will be a differ cut than a , b , okai ?
it will not output a , b if some edg of f is contract .
and if you think about it , the convers is also true .
so let's assum now , that in each of the n <num> iter , the contract algorithm never contract an edg from capit f .
rememb capit f ar exactli the edg with on endpoint in a and on endpoint in b .
so if it never contract ani edg of f , then it onli contract edg where both endpoint lie in capit a or both endpoint lie in capit b .
well , if thi is the case then , vertic from a alwai stick togeth in the fuse node , and vertic from b alwai stick togeth in the fuse node .
there is never a iter where a node from a and a node from b ar fuse togeth .
what doe that mean ?
that mean that when the algorithm output <i>cut< i> all of the node in a have been group togeth , all of the node in b have been group togeth , in each of the two super node , which mean that the output of the algorithm is inde the desir cut a , b .
summar , the contract algorithm will do what we want .
it will succe and output the cut a , b , if and onli if it never choos an edg from capit f for contract .
therefor , the probabl of success , that is , the probabl that the output is the distinguish min cut a , b , is exactli the probabl that never contract an edg of capit f .
so , thi is what we're gonna be interest in here .
thi realli is the object of our mathemat analysi , the probabl that in all of the n <num> iter we never contact an edg of capit f .
so , to think about that , let's think about each iter in isol , and actual defin some event describ that .
so for an iter i , let si denot the event , that we screw up an iter i .
with thi notat , we can succinctli sai what our goal is , so , to comput the probabl of success .
what we wanna do is we wanna comput the probabl that <i>none< i> of the event , s1 , s<num> up to n minu , s n <num> never occur .
so , i'm gonna us thi not symbol to sai that s1 doe not happen .
so we don't screw up in iter <num> , we don't screw up in iter <num> , we don't screw up in iter <num> , and so on .
all the wai up to , we don't screw up , we don't contract anyth from capit f , in the final iter , either .
so summar , analyz the success probabl of the contract algorithm boil down to analyz the probabl of thi event , the intersect of the not si with i rang from iter <num> to iter n <num> .
so we're gonna take thi in babi step , and the next quiz will lead you through the first on , which is , let's have a more modest goal .
let's just think about iter <num> .
let's try and understand , what's the chanc we screw up , what's the chanc we don't screw up , just in the first iter ?
so the answer to thi quiz is the second option .
the probabl is k m , where k is the number edg cross the cut a , b , and m is the total number of edg .
and that's just becaus the probabl of s1 , the probabl we screw up , is just the number of cross edg .
that's the number of outcom which ar bad , which caus which trigger s1 , divid by the number of edg .
that's the total number of thing that could happen .
and sinc all edg ar equal like , it just boil down to thi .
and by the definit of our notat , thi is exactli k m .
so thi give us an exact calcul of the failur probabl in the first iter , as a function of the number of cross edg , and the number of overal edg .
now , it turn out it's gonna be more us for us to have a bound not quit as exact , an inequ .
that's in term of the number of vertic n , rather than the number of edg , m .
the reason for that is , it's a littl hard to understand how the number of edg is chang in each iter .
it's certainli go down by <num> in each iter , becaus we contract that in each iter , but it might go down by more than <num> when we delet self loop .
by contrast the number of vertic is thi veri steadi obviou process .
on less vertex with each success iter .
so , let's rewrit thi bound in term of the number of vertic n .
to do that in a us wai , we make the follow kei observ .
i claim that , in the origin graph g , we ar given as input , everi vertex ha at least k edg incid on it , that is in graph theoret terminolog , everi edg ha degre at least k .
where , recal , k is the number of edg cross our favorit min cut a , b .
so why is that true ?
why must everi vertex have a decent number of neighbor , a decent number of edg incid to it .
well , it's becaus , if you think about it , each vertex defin a cut by itself .
rememb , a cut is just ani group into other vertic into two group , that ar not empti , that don't overlap .
so on cut is to take a singl vertex , and make that the first group , a , and take the other n <num> vertic , and make that the second group , capit b .
so how mani edg cross thi cut ?
well , it's exactli the edg that ar incid on the first note , on the note on the left side .
so everi singl cut , fall exponenti mani cut , have at least k cross edg , then certainli the n cut defin by singl vertic have at least k cross edg , so therefor , the degre of a vertex is at least k .
so our assumpt that everi singl cut in the graph ha at least k cross edg becaus it's a lower bound on the number edg incid on each possibl vertex .
so , why is that usual ?
well let's recal the follow gener fact about ani graph ; which is that if you sum up over the degre of the node , so if you go node by node , look at how mani edg ar insid on that node , that's the degre of v , and then sum them up over all vertic .
what will you get ?
you'll get exactli twice the number of edg , okai ?
so thi is true for ani undirect graph , that the sum of the degre of the vertic is exactli doubl the number of edg .
to see thi , you might think about take a graph , start with the empti set of edg , and then ad the edg of the graph on at a time .
each time you add a new edg to a graph , obvious the number of edg goe up by <num> , and the degre of each of the endpoint of that edg also go up by <num> , and there ar , of cours , two endpoint .
so everi time you add an edg , the number of edg goe up by <num> , the sum of those degre goe up by <num> .
therefor , when you've ad all the edg , the sum of the degre is doubl the number of edg that you've ad .
that's why thi is true .
now , in thi graph , at that we have a hand here , everi degre is at least k , and there's n node .
so thi left hand side , of cours , is at least kn for us .
so therefor if we just divid through by <num> , and flip the inequ around , we have the number of edg ha to be at least the size of the cross cut , so the degre of everi vertex time the number of vertic divid by <num> .
so thi is just the primit inequ rearrang .
put thi togeth with your answer on the quiz , sinc the probabl of s1 is exactli k m , and m is at least kn <num> , if we substitut , we get that the probabl of s1 is at worst <num> n , <num> over the number of vertic , and the k cancel out .
so that's , sort of , our first mileston .
we've figur out the chanc that we screw up in the first iter , that we pick some edg from the cross the cut a , b .
and thing look good .
thi is a , thi is a small number , right ?
so , in gener , the number of vertic might be quit big .
and thi sai that the probabl we screw up is onli <num> over the number of vertic .
so , so far , so good .
of cours , thi wa onli the first iter .
who know what happen later ?
so now that we understand the chanc of screw up in the first iter , let's take our next babi step , and understand the probabl that we don't screw up in either of the first two iter .
that is , we're gonna be interest .
and the follow probabl .
the probabl that we don't screw up in the first iter nor in the second iter .
now , as you go back to the definit of a condit probabl , to realiz that we can rewrit an intersect like thi in term of condit probabl .
name , as the probabl that we don't screw up in the second iter , given that we didn't do it alreadi , time the probabl that we didn't screw up in the first iter .
okai ?
so the probabl that we miss all of these k vulner edg and in the second iter given that we didn't contract ani of them in the first iter .
now notic thi , we alreadi have a good understand on the previou slide .
we ar given a nice lower bound of thi .
we sai there's a good chanc that we don't screw up , probabl at least <num> <num> n .
and in some sens we also have a veri good understand of thi probabl .
we know thi is <num> minu the chanc that we do screw up .
and what's the chanc that we do screw up ?
well , these k edg ar still hang out in the graph .
rememb we didn't contract ani , in the first iter that's what's given .
so there ar k wai to screw up , and we choos an edg to contract uniformli at random , so the total number of choic is the number of remain edg .
now the problem is , what's nice is we have an exact understand of thi probabl .
thi is an equal .
the problem is we don't have a good understand of thi denomin .
how mani remain edg ar there ?
we have an upper bound on thi .
we know thi is at most n <num> , assum we got rid of on edg in the previou iter , but actual what , if you think about it , what we need of thi quantiti is a lower bound and that's a littl unclear becaus in addit to contract the on edg and get that out of the graph , we might have creat a bunch of self loop and delet all event .
so it's hard to understand exactli what thi quantiti is .
so instead we're gonna rewrit thi bound in term of the number of the remain vertic , and of cours we know it's exactli n <num> vertic remain .
we took two of the last iter and contract down to <num> .
so how do we relat the number of edg to the number of vertic ?
well we do it just in exactli the same wai as in the first iter .
we'll make some more gener observ .
in the first iter , we observ that everi node in the origin graph induc a cut .
okai , with that node wa on on side , the other n <num> edg were on the other side .
but the fact that's a more gener statement , even after we've done a bunch of contract , ani singl node in the contract graph , even if it repres a union of a bunch of node in the origin graph , we can still think of that as a cut in the origin graph .
right ?
so if there's some super node in the contract graph , which is the result of fuse twelv differ thing togeth , that correspond to a cut where those twelv node in the origin graph ar on the on side a , and the other n <num> vertic ar on the other side of the cut , b .
so , even after contract , as long as we have at least two node in our contract graph , you can take ani node and think of it as half of a cut , on side of a cut in the origin graph .
now rememb , k is the number of edg cross our minimum cut a , b , so ani cut in the origin graph g ha to have k cross edg .
so , sinc everi node in the contract graph natur map over to a cut in the origin graph with at least k edg cross it , that mean , in the contract graph , all of the degre have to be at least k .
if you ever had a node in the contract graph that had onli sai k <num> incid edg , well then you'd have a cut in the origin graph with onli k <num> edg contradict .
so just like in the first iter , now that we have a lower bound on the degre of everi singl vertex , we can deriv a lower bound on the number of edg that remain in the graph .
the number of remain edg is at least <num> <num> , that's becaus when you sum over the degre of the vertic , you doubl count the edg , time the degre of each vertex , that we just argu that that's at least k in thi contract graph , time the number of vertic , that we know there's exactli n <num> vertic left in the graph at thi point .
so now what we do is to plug thi inequ , to plug thi lower bound of the number of remain edg , on , as we'll substitut that for thi denomin , so in lower bound the denomin , we upper bound thi fraction , which give us a lower bound on <num> minu that fraction , and that's what we want .
so what we find is that the probabl that we don't screw up in the second iter given that we didn't screw up in the first iter .
where again , by screw up mean pick on of these k edg cross a , b to contract is at least <num> <num> n <num> .
so , that's pretti cool .
we took the first iter , we analyz it , we show the probabl that we screw up is pretti low , we succe with probabl of at least <num> <num> n .
in the second iter , our success probabl ha drop a littl bit , but it's still look pretti good for reason valu of n , <num> <num> n <num> .
now , as i hope you've pick up , we can gener thi pattern to ani number of iter , so that the degre of everi node of the contract graph remain at least k .
the onli thing which is chang is the number of vertic is drop by <num> .
so , extend thi pattern to it logic conclus , we get the follow lower bound on the probabl that the contract algorithm succe .
the probabl that the contract algorithm output the cut a , b , you recal we argu , is exactli the same thing as the probabl that it doesn't contract anyth , ani of the k cross edg , ani of the set f in the first iter , nor in the second iter , nor in the third iter , and then so on , all the wai up to the final n <num> th iter .
us the definit of condit probabl , thi is just the probabl that we don't screw up in the first iter , time the probabl that we don't screw up in the second iter given that we didn't screw up in the first iter , and so on .
in the previou two slide , we show that , we don't screw up in the first iter , with probabl of at least <num> <num> n .
in the second iter , with probabl at least <num> <num> n <num> .
and of cours , you can guess what that pattern look like .
and that result in the follow product .
now , becaus we stop when we get down to two node remain , the last iter in which we actual make a contract , there ar three node .
and then , the second to last iter in which we make a contract , there ar four node .
so that's where these last two term come from .
rewrit , thi is just n <num> n time n <num> n <num> , and so on .
and now someth veri cool happen , which is massiv cancel , and to thi dai , thi is alwai just incred satisfi to be abl to cross out so mani term .
so you get n <num> , cross it out here and now here , there's go to be a pair of n 3s that get cross out , and n 4s , and so on .
on the other side , there's go to be a pair of 4s that get cross out , and a pair of 3s that get cross out .
and we'll be left with onli the two largest term on the denomin , and the two smallest term in the numer , which is exactli <num> n n <num> .
and to keep thing simpl among friend , let's just be sloppi and lower bound thi by <num> n <num> .
so that's it .
that's our analysi of the success probabl of karger's contract algorithm .
pretti cool , pretti slick , huh ?
okai , i'll conced , probabl you're think hei , wait a minut .
we're analyz the probabl that the algorithm succe , and we're think of the number of vertic n as be big , so we'll see here as a success probabl of onli <num> n <num> , and that kinda suck . so that's a good point .
let me address that problem .
thi is a low success probabl .
so that's disappoint .
so why ar we talk about thi algorithm , or thi analysi ?
well , here's someth i want to point out .
mayb thi is not so good , <num> n <num> you're go to succe , but thi is still actual shockingli high for an brute forth algorithm which honestli seem to be do almost noth .
thi is a nontrivi lower bound and non trivial success probabl , becaus don't forget , there's an exponenti number of cut in the graph .
so if you try to just pick a random cut i . e you put everi vertex <num> <num> left or right , you'll be do wai wors than thi .
you'll have a success probabl of like <num> <num> n .
so thi is wai , wai better than that .
and the fact that it invers polynomi mean is that us repeat trial , we can turn a success probabl that's incred small into a failur probabl that's incred small .
so lemm show you how to do that next .
so , we're gonna boost the success probabl of the contract algorithm in , if you think about it a total obviou wai .
we're gonna run it a bunch of time , each on independ us a fresh batch of random coin .
and we're just go to rememb the smallest cut that we ever see , and that's what we're gonna return , the best of a bunch of repeat trial .
now the question is , how mani trial ar we gonna need befor we're pretti confid that we actual find the meant cut that we're look for ?
to answer thi question vigor , let's defin some suitabl event .
so by ti , i mean the event at the ith trail succe , that is the ith time we run the contract algorithm which doe output that desir meant cut a , b .
for those of you that watch the part ii of the probabl review , i said a rule of thumb for deal with independ is that , you should mayb , as a work hypothesi , assum grant variabl ar depend , unless thei're explicitli construct to be independ .
so here's a case where we're just gonna defin the random variabl to be independ .
we're just gonna sai that we run the contract algorithm over and over again with fresh random so that thei're gonna be independ trial .
now , we know that the , probabl that a singl trial fail can be pretti big , could be as big as <num> <num> n <num> .
but , here , now , with repeat trial , we're onli in troubl if everi singl trial fail .
if even on succe , then we find the meant cut .
so a differ wai of sai that is we're interest in the intersect of t1 and t2 and so on , that's the event that everi singl trial fail .
and now we us the fact that the trial ar independ .
so , the probabl that all of these thing happen is just the product of the relev probabl .
so , the product from i <num> to capit n of the probabl of not ti .
recal that we argu that the success probabl of a singl trial wa bound below by <num> n <num> .
so the failur probabl is bound abov by <num> <num> n <num> .
so sinc that's true for each of the capit n term , you get an overal failur probabl for all capit n trial of <num> minu <num> n <num> rais to the capit of n .
all right , so that's a littl calcul .
don't lose sight of why we're do the calcul .
we want to answer thi question , how mani trial do we need ?
how big doe capit n need to be befor ar confid that we get the answer that we want ?
okai , so to answer that question i need a quick calculu fact , which is both veri simpl and veri us .
so for all real number x , we have the follow inequ , <num> x is bound abov by e x .
so i'll just give you a quick proof via pictur .
so first think about the line <num> x .
what doe that cross through ?
well , that cross through the point when x is <num> , y is <num> , and when x is <num> , y is <num> .
and it's a line , so thi look like thi blue line here .
what doe e x look like ?
well , if you substitut x <num> , it's gonna be <num> .
so in fact two curv kiss each other at x <num> .
but exponenti grow realli quickli , so as you jack up x to higher posit number , it becom veri , veri steep .
and for x neg number it stai non neg the whole wai .
so thi sort of flatten out for the neg number .
so , pictori , and i encourag you to , you know , type thi into your own favorit graph program .
you see the e x bound abov everywher , the line , the <num> x .
for those of you who want someth more rigor , there's a bunch of wai to do it .
for exampl , you can look at the expans of e x at the point <num> .
what's the point ?
the point is thi allow us to do some veri simpl calcul on our previou upper bound on the failur probabl by work with exponenti instead of work with these ugli on minu whatev rais to the whatev term .
so , let's combin our upper bound from the previou slide with the upper bound provid by the calculu fact .
and to be concret , let's substitut some particular number of capit n .
so , let's us littl n <num> trial , where littl n is the number of vertic of the graph .
in which case , the probabl that everi singl trial fail to recov the cut a , b is bound abov by e to the <num> n <num> .
that's us the calculu fact appli with x <num> n <num> .
and then we inherit the capit n and the expon which we just substanti to littl n <num> .
so of cours the n <num> ar gonna cancel , thi is gonna give us e <num> , also known as <num> e .
so if we're will to do littl n <num> trial , then our failur probabl ha gone from someth veri close to <num> , to someth which is more like , sai , <num> some more percent .
now , onc you get to a constant success probabl , it's veri easi to boost it further by just do a few more trial .
so if we just add a natur log factor , so instead of a littl n <num> trial , we do littl n <num> time the natur log of the littl n .
now , the probabl that everyth fail is bound and abov by the <num> e that we had last time , but still with the residu natur log of n up top .
and thi is now , mere <num> n .
so i hope it's clear what happen .
we took a veri simpl , veri eleg algorithm , that almost alwai didn't do what we want .
it almost alwai fail to output the cut a , b .
it did it with onli probabl <num> n <num> .
but , <num> n <num> is still big enough that we can boost it , so that it almost alwai succe just by do repeat trial .
and the number of repeat trial that we need is the reciproc of it origin success probabl boost by , for the logarithm factor .
so that transform thi almost alwai fail algorithm into an almost alwai succeed algorithm .
and that's a more gener less , more gener algorithm techniqu , which is certainli worth rememb .
let me conclud with a coupl comment about the run time .
thi is probabl the first algorithm of a cours , of the cours where we haven't obsess over just what the run time is .
and i said , it's simpl enough .
it's not hard to figur out what it is , but it's actual not that impress .
and that's why i haven't been obsess over it .
thi is not almost linear .
thi is not a for free primit as i've describ it here .
so it's certainli a polynomi time algorithm ; it run time is bound abov by some polynomi in n and m .
so it's wai better than the exponenti time you get from brute forc search through all <num> n possibl cut .
but it is certainli , the wai i've describ it , we gotta to n <num> trial , plu a log factor , which i'm not even go to bother write down .
and also , each trial , while at the veri least , you look at all the edg , so that's go to be anoth factor of m .
so thi is a bigger polynomi than in ani , almost ani of the algorithm that we're go to see .
now , i don't wanna undersel thi applic of random sampl in comput cut becaus i've just shown you the simplest , most eleg , most basic , but therefor also the slowest implement of us contract to comput cut .
there's been follow up work with a lot of extra optim , in particular , do stuff much more clever than just repeat trial , so basic us work that you did in previou trial to inform how you look for cut in subsequ trial .
and you can shave larg factor off of the run time .
so there ar much better implement of thi random contract algorithm than what i'm show you here .
those ar , howev , outsid the cours , scope of thi cours .
so thi is short option video , realli just for fun , i want to point out an interest consequ track algorithm ha about a problem that is in pure graph theori .
so , to motiv the question , i want to remind you of someth we discuss in pass , which is that a graph mai have more than on minimum cut .
so , there mai be distinct cut which ar ti for the fewest number of cross edg .
for a concret exampl , you could think about a tree .
so , if you just look at a star graph , that is hub and spoke , it's evid that if you isol ani leaf by itself , then you get a minimum cut with exactli on cross edg .
in fact , if you think about it for a littl while , you'll see that in ani tree you'll have n <num> differ minimum cut , each with exactli on cross edg .
the question concern count the number of minimum cut .
name , given that a graph mai have more than on minimum cut , what is the largest number of minimum cut that a graph with n vertic can have ?
we know the answer is at least n <num> .
we alreadi discuss how tree have n <num> distinct minimum cut .
we know the answer at most someth like <num> n , becaus a graph onli ha roughli <num> n cut .
in fact , the answer is both veri nice and wedg in between .
so the answer is exactli n choos <num> , where n is the number of vertic .
thi is also known as n n <num> <num> .
so it can be bigger than it is in tree , but not a lot bigger .
in particular , graph have onli ; undirect graph have onli polynomi mani minimum cut .
and that's been a us fact in a number of differ applic .
so , i'm go to prove thi back to you .
all i need is on short slide on the lower bound and then on slide for the upper bound , which follow from properti of the random contract algorithm .
so for the lower bound , we don't have to look much beyond our tree exampl .
we're just gonna look at cycl .
so for ani valu of n , consid the n cycl .
so here , for exampl , is the n cycl with n <num> .
that would be an octagon .
and the kei observ is that , just like in the tree , how move each of the n <num> edg break the tree into two piec and defin the cut .
with a cycl , if you remov just on edg , you don't get a cut .
the thing remain connect , but if you remov ani pair of edg , then that induc a cut of the graph , correspond to the two piec that will remain .
no matter which pair of edg you remov , you get a distinct pair of group , distinct cut .
so rang overal n choos <num> choic of pair of edg , you gener n choos <num> differ cut .
each of these cut ha exactli two cross edg , and it's easi to see that's the fewest possibl .
so that's the lower bound , which wa simpl enough .
let's now move on to the upper bound , which , a pure count all fact will follow from an algorithm .
so consid ani graph that ha n vertic , and let's think about the differ minimum cut of that graph .
what we're go to us is that the analysi of the contract algorithm proceed in a fairli curiou wai .
so rememb how we defin the success probabl of a contract algorithm .
we fix up front , some min cut a , b .
and we defin the contract algorithm , the basic contract algorithm , befor the repeat trial .
we defin the contract algorithm as success , if and onli if it output the minimum cut a , b that we design upfront .
if it output some other minimum cut , we didn't count it .
we said nope , that's a failur .
so we actual analyz a stronger properti than what we were try to solv , which is output a given min cut a , b rather than just ani all min cut .
so how is that us ?
well , let's appli it here .
for each of these t minimum cut of thi graph , we can think about the probabl that the contract algorithm output that particular min cut .
so we're gonna instanti the analysi with a particular minimum cut ai , bi .
and what we prove in the analysi is that the probabl that the algorithm output the cut ai , bi , not just ani all min cut .
but , in fact , thi exact cut ai , bi is bound below by .
we , in the end , we made a sloppi inequ .
we said it's at least <num> n <num> .
but if you go back to the analysi , you'll see that it wa , in fact , <num> n n <num> , also known as <num> n choos <num> .
so instanti the contract algorithm success probabl analysi without all of the repeat trial busi , we show that for each of these t cut , for each fix cut ai , bi , the probabl that thi algorithm output that particular cut is at least <num> n choos <num> .
let's introduc a name for thi event , the event that the contract algorithm output the ith min cut .
let's call thi si .
the kei observ is that the si ar disjoint event .
rememb an event is just a subset of stuff that could happen .
so on thing that could happen is that the algorithm output the ith main cut , and by thi joint , we just mean that there is no outcom that in a given pair of event .
and that's becaus the contract algorithm at n to the , onc it make it conflict , it output a singl cut is a distinct cut .
it can onli output a dest on of them .
why is it import that these si ar disjoint event ?
well , with disjoint event , the probabl <i>add< i> the probabl of the union of a bunch of disjoint event is the sum of the probabl of constitu event .
if you want to think about thi pictori , and just draw a big box , denot everyth that could happen omega , and then these si just these that don't overlap .
so s1 , s2 , s3 , and so on .
now the sum or probabl of thi joint event can sum to , at most , <num> , right ?
the probabl of all of omega is <num> , and these si have not overlap and ar pack into omega , so the sum of their probabl is gonna be smaller .
we're ad up formal .
we have that the sum of the probabl .
which we can lower bound by the number of differ event .
and rememb there ar t differ min cut for some paramet t .
for each min cut ai , bi , a lower bound of the probabl that , that could spit out as output is <num> n choos <num> .
so a lower bound on the sum of all of these probabl is the number of them , t time the probabl lower bound , <num> n choos <num> , and thi ha got to be at most <num> .
rearrang , what do we find ?
t , the number of differ mid cut , is bound abov by n choos <num> .
exactli the lower bound provid by the n cycl .
the n cycl ha n choos <num> distinct minimum cut .
no other graph ha more .
everi graph ha onli a polynomi number inde , at most a quadrat number of minimum cut .
