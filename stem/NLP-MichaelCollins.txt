so in thi next segment of the class , we ar go to talk about lexic pcfg , lexic probabilist context free grammar .
and , these fix mani of the problem i describ earlier with pcfg and lead to much better pars perform .
so , we'll first talk about how to lexic a treebank .
we'll then give definit of lexic pcfg .
we'll talk about paramet estim in these model , and we'll actual make direct us of the smooth techniqu we saw for the languag model in the first lectur from thi class .
we'll talk also about pars with these model .
and then final , i'll talk a bit about accuraci of these model compar , for exampl , to pcfg and go into some depth about how we actual evalu differ pars model .
okai , so the first kei idea , idea in lexac pcfg is to add annot specifi what's call the head of each rule in a context free grammar .
so , i've shown you a simpl grammar here .
and i'm go to assum that the rule fall into two type .
we either have some part of speech rewrit as a word or we have some non termin rewrit as a , a sequenc of non termin , which could be non termin like a preposit phrase or part of speech like vi or vt .
the kei idea here is go to be for each rule in the grammar to identifi on of the children of that rule , as what's call the head of that rule .
so , i've us red to denot the head of each rule .
so , vp is the head of s , vi is the head of vp , vt is the head of vp and so on .
so , we ar choos on of the children to be the head of the rule , and so thi is in some sens an addit piec of inform in our context free grammar .
we don't just have the rule , we also have annot specifi the head .
so , that's the abstract idea .
let me talk a littl bit more about where it come from .
it's actual a core idea in linguist .
thi idea of head and rule goe back to veri , veri earli work in linguist .
some intuit in some sens , the head is sort of the core of the rule , is the most import part .
so , a verb , a verb phrase , for exampl , will alwai have some verb on the right hand side of the rule , whether it's an intransit verb or transit or ditransit verb .
and thi verb is , in some sens , the semant center of that rule .
it's the most import part of the rule .
similarli for an s , you can argu that the vp is sort of the semant center of that rule , is the predic in that rule .
and also for these noun phrase , the rightmost noun is again the most import part of the rule .
so , these head annot ar actual often not present in treebank .
and thei certainli weren't present in the pan wall street journal treebank , which wa the treebank us for , for mani of the origin experi in statist pars .
and so , these annot have to be recov with a set of rule , so let me give you some exampl of how we might do thi .
thi is an exampl for noun phrase of a set of determinist rule which will recov the head for each noun phrase .
for exampl , thei would make these annot i've just shown you down here .
so , the first rule sai , that if the rule contain a singular noun , a plural noun , or a proper noun , then choos the rightmost of those noun , so that would appli to these two rule here .
thei both contain on of these three categori , in which case , we just return the rightmost categori as be the head of that rule .
if thi fail howev , we go on to the next rule , which sai that if the rule contain an np , choos the leftmost np .
so , thi is go to appli to case like the follow .
if i have a structur like that , it doesn't contain on of these three categori .
it doe howev , contain np .
and in that case , we take the leftmost np as the head .
that's thi rule down here .
those ar , by far , the most common case .
most noun phrase , at least in the wall street journal treebank have either on of these three categori or a noun phrase on the right hand side of the rule .
but there ar some funni corner case .
and so , we go through a few of these case here .
if the rule contain a jj , thi is an adject , choos the rightmost jj , thei're awesom , rather strang noun phrase which compos which don't contain noun , but do contain adject .
cd is a number .
thi is someth like a <num> , or a <num> , <num> , or someth like thi .
and then final , we just have some default rule sai we choos the rightmost child under the noun , noun phrase .
here's a second exampl .
thi is an exampl for verb phrase specifi a set of rule which recov the head of verb phrase , and thei're awfulli similar to what i show you befor .
the first rule sai that if the rule contain an intransit verb or transit verb , choos the leftmost vi or vt .
so , that would be thi here .
in fact , we'd probabl be care to specifi all verb categori here .
there might be sever differ subcategori of verb .
on the other hand , if the rule contain a vp , choos the left most vp .
okai , so , if we have a structur like the follow , then thi rule fail .
the vp doesn't domin vi or vt .
and instead , in thi case , we just choos the leftmost vp as the head of the rule .
and final , we have a default case just sai if we don't find ani of these categori , just choos the leftmost child .
so , why ar we do thi ?
the main motiv is go to be the follow .
in a moment , i'll show how onc we have these anot of head and rule , we can us them to propag lexic inform up through the tree .
so , we will actual transform our tree bank from these kind of structur which ar kind of pcfg structur with rule such as , np s goe to np vp .
and we're go to add lexic inform in the tree , in particular , for each non termin in the tree , for exampl , here we have vp , we'll now add a lexic item question which is drawn from somewher below it in the subtre .
so , the question came from thi word down here .
and the head of annot will be us to defin how headword flow up through the tree .
so , in on sens , you can just view these new structur as entir as a new set of non termin .
so , wherea befor , i had non termin like s or vp .
now , i have non termin like s question or np lawyer .
so , there might be around <num> non termin in my origin grammar .
in thi new grammar , i have <num> non termin time the vocabulari size so it could easili be in the thousand of non termin .
but in a formal sens , noth ha chang , we've just vastli increas the number of non termin in the grammar .
you can see that by ad thi lexicon inform to these non termin higher in the tree , it will allow us to be more sensit to lexic inform at higher level in the tree .
and that's go to be the kei , kei idea in these lexic pcfg .
thi is how we make the , the pcfg more sensit to lexic inform .
so , how is thi process perform ?
let's again look at thi exampl .
the critic idea is to propag lexic item bottom up through the tree where each constitu receiv it headword from it head child .
so , let's look at thi tree in a littl bit more detail .
the first case is simpl .
whenev i have a part of speech , for exampl , dt , we simpli get it lexic head , it lexic item from the word below it .
so , thi just goe straight up .
and so , each of these part of speech just immedi receiv the headword from the word below .
i should have said , by the wai , these word ar often refer to as headword , the headword of a constitu .
now , let's go a littl higher in the tree .
let's take thi noun phrase here .
and so , for thi noun phrase , we have the follow rule .
np goe to determin nn .
and let's assum that our head rule have identifi nn as the head of that particular rule .
well , our rule for propag lex , lexic item up through tree , sai , that word ar propag from the head to the parent .
and so , wit , for exampl , is propag up through the tree like thi .
so , thi noun phrase receiv it head word from it head child in the rule .
similarli , if we look at vp goe to vt np , let's sai , for the sake of argument vt ha been identifi as the head of thi rule , that mean that thi vp get the word , question from there .
we have a similar step here .
and final , if we have s goe to np vp .
let's assum that the vp is the head until thi lexic item get propag up through here .
so , it's , it's realli veri simpl .
we propag lexic item , bottom up through these tree us these head annot , where each non termin receiv it head word from it head child .
and the final result is , we've transform our tree in a wai that each non termin now includ lexic inform taken from the subtre below it .
so we've now seen how to lexic the treebank , how to add these lexic annot to each nontermin in a treebank .
next , let's talk about lexic probabilist context free grammar .
i'll give some basic definit .
okai , so firstli to recap let's remind ourselv of context free grammar and chomski normal form .
so a cfg in chomski normal form consist of the follow n is a set of non termin symbol in the grammar .
sigma is a set of termin symbol or word .
we have s which is a distinguish start symbol , which alwai occur at the root of the tree .
and then final we have a set of rule r .
and each rule in r must take on of two differ form .
so , it either ha the form x goe to y on y two .
where all three element x y on and y two ar non termin .
so an exampl would be s goe to n p v p .
or it consist of x goe to y , where x is a non termin and y is a word .
so , for exampl , dt goe to there .
and we saw that , given a pcfg , in chomski normal form .
we can us dynam program to find the highest probabl pars in order cubic time in the length of the sentenc .
so n is length of the sentenc .
and cubic time in the number of non termin .
and the grammar .
so here's a definit of lexic context free grammar in chomski normal form .
it close relat to the definit i gave you on the previou slide .
and so with the lexic cfg we again assum we have some set of non termin symbol .
we have a set of termin symbol and we have a start symbol but the rule in the grammar take a slightli differ form .
so there ar three differ case .
let's go through the simplest case first .
so thi sai we can have a rule where x is non termin and h is a word where x h we write as h so for exampl d t the goe to the as on exampl of thi rule , where x equal d t .
so these ar the rule seen at the veri leav of the tree for a part of speech with some lexic item simpli rewrit as that lexic item .
now let's look at thi two remain case , let's look at thi on first .
thi sai i can take ani three non termin x , y1 and y2 and ani two word h and w and i can have a lexic rule where xh rewrit as y1hy2w let me give you an exampl of such a rule .
we could for exampl have vp saw goe to vt saw mp dog .
now critic , i'm go to subscript these arrow .
thi ha a on here to specifi which of the two children is the head of thi rule .
so becaus saw is on the left hand side of the rule .
and saw is here on the first non termin .
i subscript the arrow on to sai that , that's where thi head word came from .
that thi is the head of the rule .
so , in thi particular exampl , we have x equal vp .
y1 is equal to vt .
y2 is equal to np .
h is equal to saw , and w is equal to dog .
thi case is similar , so then we can give an exampl of thi rule , thi for exampl could be s saw .
goe to two n p man v p saw .
notic in thi case the head word come from the second child and that's why the arrow's annot in thi wai .
so the , thi annot on the arrow is sort of a peski detail .
but it's import .
and it's import becaus it void ambigu .
if , for exampl , i have np dog goe to nn dog , nn dog veri unlik , but possibl .
where the h is equal to dog and w is equal to dog .
which can be care to specifi , which of these two children the head word came from .
so here's an exampl of a lexic , context free grammar , and thi trumpski the normal form , and let me show you a poli tree under thi .
so we have s saw goe to , for exampl , np man , vp saw , and we'll just mark thi to sai that thi is where the , the head came from .
thi is the head of the phrase .
and then n p man could go to d t there .
n , n man , for exampl , so i've us thi rule and now i'm us thi rule .
again let's the head .
and we have the , man .
and we could go on and on .
so we have deriv with these lexic context free grammar which look veri similar to deriv under regular context free grammar , we just have he , head word associ with each of these non termin in the tree .
so we will introduc paramet or probabl in these lexic pcfg .
it's import to emphas the similar and differ from regular pcfg .
so in a regular pcfg we saw paramet like the follow .
for exampl , q of s goe to np vp .
thi is the basic the condit probabl given that i have an s there ar go to be multipl wai of rewrit that s .
np vp is go to be on of them and thi is go to be the probabl that we choos thi as oppos to all those other altern .
we have paramet in lexic pcfg which again ar associ with entir rule so here's an exampl paramet .
and now , thi paramet is for the rule , s saw goe to np vp saw .
so thi , again , is go to have an interpret as a condit probabl .
in thi case , the interpret is .
given that i have s saw on the left hand side of the rule , there ar mani , mani differ possibl .
on is to choos the second non termin as the head .
and to choos np man , vp saw , as the two children .
and there ar go to be mani , mani other poss , possibl .
so there ar definit similar between these two model .
in fact technic speak , a lexic pcfg realli is just a special case of pcfg .
but qualit someth ha realli chang which is that we've gone from a rel small number of paramet in thi origin model to a veri , veri larg number of paramet .
we have on paramet for everi possibl lexic rule .
and so we're go to have to be veri care in how we estim the paramet of these model , but we'll see that the smooth techniqu you saw for languag model in the veri first segment of thi cours can be appli veri , veri directli to thi problem .
next let's talk a littl bit about pars with lexic cfg .
i don't want to go into tremend detail about pars algorithm for lexic pcfg , but i want to give you a sketch here .
the main point be that you can us a veri , veri similar dynam program algorithm to the dynam program algorithm we saw for regular p f g's so the new form of grammar i've just shown you , is veri similar to a chomski normal form cfg .
but wherea befor we had rule like s goe to npvp .
so , there ar order n cube possibl rule where n is the number of noon termin , so n time n time n in thi new grammar i have thing like s saw goe to np dog vp saw .
and so if i think of the number of choic well i have three non termin here s , np , and vp .
so i have n cube choic there .
and also in each rule i have two word , the head word saw and the modifi word dog .
and so i'm go to have sigma squar .
and so there ar mani , mani possibl rule , okai .
so i've gone from a grammar with order n cube rule .
to grammar with thi mani rule .
so naiv you can just pick up the dynam program algorithm we saw for probabilist context free grammar and appli it to thi new grammar .
these rule ar realli just like regular context free rule .
we can just treat these as non termin , we just have a much larger set of non termin than we had befor .
but if you do thi naiv you'll end up with .
the follow run time .
rememb befor we had order n cube time number of nontermin cube as the run time for , for dynam program .
thi is our new run time .
and we have sigma squar come in here .
thi is a terribl idea becaus the capillari size can be huge .
sigma could easili be on the order of 100s , and much more like thousand or even ten of thousand .
but there's a simpl observ which give us at least a plausibl effici algorithm .
if i have a particular sentenc like the dog saw the cat .
i can immedi discard rule such as the follow .
so s question goe to two mp dog vp question .
i can immedi discard thi rule becaus it ha a word question in on of these non termin , which is seen nowher in the sentenc .
so we know that rule can never be us in paus the sentenc .
and so we can actual restrict ourselv to a much smaller set of rule , where we now have order n squar time n cube , becaus again we have capit n choic for each of these three non termin .
and then , these two word , rather than be drawn from a full vocabulari , have to be drawn from on of the n word in the sentenc .
okai ?
so we end up with a much smaller set of rule just by restrict the grammar in thi veri simpl wai .
and thi is actual go to lead to a run time .
of order n to the fifth time n cube .
but where did thi come from ?
well , we have order n cube .
it's the depend of the dynam program algorithm on the sentenc length .
and then we have to multipli in the number of non termin or rule in the grammar .
and so thi is where we get order n cube time n squar .
it's the number of possibl rule , and we sai , so thi is the run time we get .
so , that's realli more of a sketch than a precis definit of the algorithm .
but hopefulli , you'll get the idea .
and the import point is that you can end up paus with these lexic pcfg rel effici .
we now have gone from an n cube depend on the sentenc length to an n to the fifth .
and so that's expens , but it's still manag , if you're care about how you do thing .
so next let's talk about how to estim the paramet of a lexic pcfg .
so firstli , i want to emphas where we've , what we've end up with , and why thi is go to be a much better pars model of languag than raw pcfg .
and to do thi , i've pick an exampl sentenc and , and an exampl pars tree .
with a case of proposit phrase ambigu so thi sentenc is the man saw the dog with the telescop and thi is the full pal tree with all it's lexic glori where each of these nontermin ha an associ lexic item .
so , the probabl of thi tree again is go to be a product of term with on q paramet for each rule that ha been seen in the tree , and i've list a few import on here , there ar a few other term .
and so for exampl , the root of the tree , i have s saw goe to np man vp saw and i have paramet q for that rule .
and so on and so on in the tree .
so if you recal if we had been appli just regular pcfg without lyscol we would have had these simpl rule such as g goe to npvp or vp goe to vp preposit phrase and these rule would have had associ probabl but as i had argu befor , these rule ar rather insensit to lexic inform .
now , we see that these rule actual incorpor rich sourc of lexic inform .
and so , take , for exampl , the rule involv the preposit .
so befor , we had vp goe to vp preposit phrase now we have v p saw goe to v p saw p , p with .
what we see here is that we now have paramet that explicitli model depend within , between lexic item .
for exampl , the depend between saw and with .
and these depend ar link to particular grammat relat .
so for exampl , we have the abil to sai how like is a preposit phrase we preposit with to modifi a verb phrase with , with head saw .
that's basic what thi rule is sai .
there's anoth exampl at the top of the tree .
we now have the probabl of saw be associ with man in the relationship where .
man is basic the subject of saw .
and so on and so on through the tree .
so you can see how our , the paramet of our model , now have direct access to import lexic inform .
the challeng of cours , is go to be that we have a veri larg number of rule .
and paramet not model .
so we go to have to be quit care about how we estim them from a set of train sampl .
so let's now talk how that can be done .
thi is a model from eugen charniak , it's a famou model form late 90s , on of the first lexic pcfg model , and , on of the first model to show thi veri much improv perform of irregular pcfg's .
so here's exampl of paramet which i'll us as an exampl throughout thi section .
the paramet associ with s saw go to in np man vp saw .
so the first step is go to be actual decompos thi .
into a product of two term , two paramet , which will make our job sim , sim , simpler .
so thi first paramet correspond to the probabl , given that i have s saw the probabl of that wri rewrit as np v p .
okai , so if you think we're r , rewrit s saw , there ar mani possibl wai of rewrit it .
here i have paramet correspond to just the choic of the rule .
and for now ignor the second lexic item man associ with thi rule .
the second paramet can be in , interpret as follow .
so , sai we have , s saw and we've actual chosen thi rule .
so we've chosen np , vp , and saw is the head of the vp .
so i have a two under thi arrow .
then we have a choic of which lexic item is go to fill in thi posit in the rule .
so , potenti ani possibl word could fall into thi posit .
thi paramet is , is correspond to the probabl that man is chosen in thi posit within the rule .
and these kind of paramet ar crucial becaus thei basic model , in thi case , the probabl for a particular word man to plai a particular role , name the subject of the verb saw .
so we see a veri direct model of these depend .
between lexic item saw and man .
and again those depend can be veri us for disambigu .
so that's step on .
first step is to take thi paramet and actual decompos it into those product of two differ term .
so now , let's look at the second step in deriv thi estim .
so , the first step wa to break thi paramet down into the product of two differ term essenti us the chain rrule of probabl , we first predict the rule and then we predict the lexic condit on the rule togeth with the head word .
the second step is to us smooth estim for the paramet estim of these two paramet we're left with and so , we'll appli exactli the same idea we saw for languag model , for exampl .
so , in thi exampl , i'm go to us linear interpol .
so we're go to have two paramet , lambda <num> , lambda <num> greater than or equal to <num> , and lambda <num> plu lambda <num> is equal to <num> .
and what do i have here ?
i have two maximum likelihood estim .
so , if we look at thi first on , thi is go to be estim as the count of s saw , so thi is s head by saw , rewrit as np vp with the second child , vp , as , as the head divid by the count of s saw .
so , that maximum likelihood estim is basic an estim of the probabl of s saw , rewrit with thi particular rule .
again , it's a veri natur symbol , intuit estim .
the second on is basic a back off estim that complet ignor the lexic inform .
so now , we're go to back awai from condit on the word , saw , and so we're go to have count of s goe to np vp divid by count of s .
notic thi is almost ident to the estim you'd see in a regular pcfg , so we're essenti interpol two differ estim , on , which condit on lexicon inform which isn't present in the pcfg and the other , which throw awai the lexicon inform .
and by the usual argument , thi estim will tend to be more robust in that the count will be larger , we'll have fewer problem with count be zero and so on wherea , thi estim will be more detail and that it take into account lexic inform , which is veri us .
and by interpol these two estim , we get a bounc .
we get , in some sens , the strength of both of these two estim , robust and also some sensit to lexic inform .
so now , let's look at the second paramet .
thi paramet here , and we'll us a veri similar method , again a smooth estim .
i'm us paramet lambda <num> , lambda <num> , lambda <num> , to distinguish these from lambda <num> , lambda <num> .
so again , we have lambda <num> , lambda <num> , lambda <num> greater or equal to <num> , lambda <num> , plu lambda <num> , plu lambda <num> is equal to <num> , the usual constraint .
and now , let's look at these , these differ maximum likelihood estim .
so , the first on is basic if we think about thi schemat , if i have s saw , i have np vp saw .
thi maximum likelihood estim is go to be the ratio of two count .
the first count is just go to be the number of time i've seen thi entir configur .
and the , the on thi , on the numer , i'm go to count the number of time i've seen man , in thi particular posit .
okai , so thi estim is basic condit on the entir structur , the entir rule , and the head word saw .
at the second level , i throw awai the word saw and simpli sai , given i have thi structur and i have some head wood here , ani head wood , what's the probabl of see man in thi posit ?
the final level down here is , given i have an np , what is the probabl that i have man as the head word of that np , okai ?
so , we've gone from a veri detail estim , thi is basic sai what's the probabl of man be the subject of the saw , thi on is sai what's the probabl of man just be a subject irrespect of the head word , and final , thi is just sai , what's the probabl of man be the head of an np ?
and , you , again , by the usual argument , thi estim will be more robust , but it's much less detail .
thi will be less robust , robust , but more detail , and we inter attempt to gain the strength of all these estim by interpol between these three thing .
so , if we put thi altogeth , we can see that , you know , thi wa our goal , to estim the paramet for an entir rule .
we first decompos thi into product of two differ paramet and we've estim these us the smooth techniqu .
and if you look back at all of the count that we've us in thi model , we've us everyth from extrem detail account , the number of time man and saw ar , ar seen togeth in , in thi particular configur to veri , veri coars account , thing like number of time i've seen s goe to np vp .
so , we essenti have everyth from the count and see in , in estim for regular pcfg right up to these veri detail lexic count .
so , that's basic it .
that is basic how we can estim the paramet of a lexic pcfg .
so , if we have a tree bank , we can just read off these count and estim the paramet in the wai i've shown here .
there ar some other detail which i'm not go to go into but which ar import in get these algorithm to work realli well .
so , let me just briefli mention them and give you a refer if you want to feed read further .
on is that although i've shown you the case for lexic pcfg in chomski normal form , it is import to deal with rule which have more than on child .
so here , i have an exampl rule from the tree bank , which ha on , two , three , four children on the right hand side of thi rule .
there ar variou wai of do thi .
a veri simpl wai is to bi binar the rule , so we have a flat rule like thi , their , their head word here for breviti .
you can convert thi to a structur like follow , where we have intermedi case here .
so , thi might be vp underbar or someth , if we us vp underbar to signifi an intermedi nontermin within a vp .
and so , what we've essenti done here is binar the grammar by ad these intermedi rule to , to build the structur .
that's a veri simpl wai of deal with thi , there ar other wai which ar more sophist and work slightli better .
it turn out that in addit to associ a headword , for exampl , told , with each nontermin , it's us also to propag up the part of speech inform for that headword .
for exampl a v or a preposit , or a prp , thi is a person pronoun such as him , or a compliment , that give you a littl bit more inform in these lexic rule .
so , you just don't have the lexic item , you also have the part of speech , and that can help .
you can build more refin estim us these smooth estim techniqu that can start to reli on these part of speech that can be veri us .
anoth thing is that you need to modifi these grammar to encod prefer for close attach .
so , go back to thi exampl , john wa believ to have been shot by bill , it's much more like for by to attach to shot rather than believ .
and we still haven't realli address that problem .
but there ar modif to lexic pcfg which will allow us to learn those kind of prefer , and thei ar also veri us .
if you want to read more about thi i'll post a link to a paper i wrote .
thi is actual my thesi work , which origin from about <num> , when i build , built on of these lexic pcfg paus .
and thi goe through the detail of drive a model that take into account all of these differ step .
okai , to finish up thi segment on lexic pcfg , i want to talk about how we evalu differ paus model and give some insight into how accur these lexic pcfg can be for paus .
so i'll actual describ two wai of evalu a pauser .
here's the first .
or here's the basi of the first , and it's a veri wide method .
so , the kei insight is that if we take some pars tree , for exampl the tree here , we can repres thi as a set of constitu .
where each constitut consist of a label , a start point , and an end point .
so , if i look at thi np for exampl , if i number the word in the sentenc on , two , three , four , five , , thi end piec band word on through two inclus and so thi constitut is on , two .
similarli i have an np <num> , <num> i have a vp span word three to five , and i have an s span word on through five .
so thi particular path tree ha four differ constitu .
note that we do not includ part of speech , within thi definit , so we just look at level higher in the tree , than the part of speech .
so and the short stori is we can take ani pars tree and map thi to a set of constitu we can do thi both for human annot pars tree , which will , act as our gold standard in test data .
and we can also do it for the output from the parser that we've built .
in which we're try to , to evalu .
so , let's now see how we can us thi idea to evalu a parser .
and thi is through basic calcul precis and recal .
on these constitu that i've just defin .
so let's illustr thi with a specif exampl .
so in gener i'll be in the situat where i have some gold standard tree for some test data sentenc .
okai , so i have some test sentenc and i have the human annot tree , thi is taken as be correct .
and then i have some parser output which is also a tree .
which mayb exactli the same as the gold standard tree or mayb partial correct and that it mai includ some substructur which you've also seen on the gold standard tree .
so , both of these tree can be match .
to the represent i just describ , so thei can be repres as a set of constitu .
where again , a constitu consist of a label , a start point , and an end point .
now we can defin variou number .
so i'll defin g to be the number of constitu seen in the gold standard tree and in thi case it's nd i'll defin p to be the number of constitu in the output from the parser , in thi case it's on , two , three , four , five , six .
and final i'll defin c to be the number of constitu that ar actual correct .
in thi case thi is also six .
what do i mean by correct ?
that mean that if i look at thi constitu s18 for exampl , i can see that that's also in the gold tree so that's a correct constitu .
and assum if i go through each of these in turn , i'll find thei're actual a correct constitu .
it's worth note that thi method is gener enough that it can allow case where the number of constitu in the two tree ar differ .
and that can be import .
becaus in realiti , that often happen .
just as an asid .
if all of our rule were binari branch .
so if we realli had restrict to binari branch rule .
the number of constiut in both of these tree would actual be ident .
but , in realiti in these tree bank , we allow rule which ar non bianari branch .
and becaus of thi , we mai end up with differ number of constitu in the two pars tree .
okai , so we've calcul these three number g , p , and c so now the recal is defin as a <num> time c over g , so thi is basic sai all of the gold standard constitu , how mani of them do i recov correctli , in thi case is <num> time <num> over <num> .
the precis is a <num> time c over p .
in thi case , <num> time <num> over <num> .
so thi is sai , of the constitu you recov , what percentag of them ar correct ?
so in thi particular exampl , <num> of the constitu i recov ar actual correct .
but , i've onli recov six out of seven .
i think that's around <num> of these gold standard constitu .
so , thi will be eight , you should do the calcul , i'm just do thi on the top of my head but thi is equal to <num> .
okai , so we've have some error in term of a recal we have <num> in term of precis so let me give you some indic of how well differ polici work under thi particular metric .
and thi is us a , dataset that we've talk about a lot , the penn wall street journal treebank , us about <num> , <num> sentenc , that's i think on the order of <num> million word , or someth around that , as train data .
and then we have <num> , <num> sentenc which ar test data exampl .
and again , to emphas each of these is go to consist of some sentenc , some sequenc of word with a human annot tree and we ar go to be abl run a parser on these two and a half thousand sentenc and compar the parser output of these tree , us recal and precis , precis , we'll see all of the constitu we , propos on to the parser .
what proport ar correct , that's precis .
and also what proport of the gold standard constitu we recov correctli .
that's the recal .
okai so first result for pcfg , is the follow .
and you can realli see how poorli a pcfg perform .
as i said there were , thei were realli a disappoint when thei were first appli .
low 70s in term of recal and precis .
and these pars tree look realli quit dread , actual , if you look at the output pcfg .
thi paper , in <num> , wa , i think , the first realli accur tree bank parser .
it wa due to david madigan .
it wa creat in collabor with a group at .
ibm research who produc a huge amount of semin work in the earli '90s on statist and natur languag process .
on of the thing thei look at wa statist pars .
so thi is about <num> percent recal and precis .
that wa realli a breakthrough result .
wai higher than these low '70s number .
thi is a veri , veri differ model .
from the model i've the model i've describ so far , these , thi is a model base on decis tree , and thei're sort of bottom up pauser .
so we're not go into the detail of thi , but the main import point is that thi wa realli the first seriou treatment parser and it realli work quit well .
the lexic p c f g's have describ the result from my own work my case thesi wa on develop lexic p c f g .
thei give result that ar , again signifigantli higher .
so we're get up to about <num> real convers .
so thi is realli the number you can think of in term of the accuraci of these lexic pcfg .
there ar more recent result which have push perform , further , up into mayb the <num> , <num> rang .
i've list a few differ case here .
these two method make us of discrimit , discrimin estim method .
and we'll see some of these method later in the cours , when we see log linear model .
and condit random field and so on .
and petro method make us of what ar call latent variabl , pcfg .
but in both case there ar close , connect to lexic pcfg .
or certainli to , to pcfg's in thi later case .
so , that's the first evalu method i want to describ , thi idea of look at precis and recal on recov constitu .
but , i want to look at a second type of evalu which is base on accuraci in recov what ar call depend .
and thi can actual be a veri us wai of get some insight into how accur a parser is , and what construct it's do well on and what construct it's not do so well on .
so let me again take thi tree .
the man saw the dog with the telescop .
thi could be a gold standard tree or it could be the output from a local gpc pauser and again we see that the non termial ar all lexic and the kei idea is go to ab that we can convert a tree to a set of depend .
and let's see how we do thi , okai .
so we basic have , we have a , a grammar in chomski normal form here .
so for everi binari rule we're go to have a singl depend .
let's skip over thi on for now .
we'll go to the next , second on .
so the first depend sai , basic i , i should have said thi column show the head word and thi is go to show the modifi word .
rememb our rule in thi grammar can take the follow form x h goe to <num> y1 h , y<num> w or x h goe to <num> y<num> w , let's write y1 w .
y two of h .
okai , so i essenti have some rule .
x <num> goe to y w for exampl .
and i have some head word h .
and i have some word w .
so here i have h and w , and here i have the rule .
so thi first depend here is sai that i have saw as the head .
i have man as it modifi .
and i have the rule as go to np vp and that depend is extract from thi rule at the top of the tree .
so what thi is basic sai is i have a depend relationship between saw and man , these two word in the sentenc , and we can think off thi rule product as a label of the type of the depend which is involv .
thi is basic signifi that we have a subject verb depend .
thi all goe back to what i show you veri earli on in the pars slide , which is that whenev i see a configur of thi kind of form , i have a subject , and i have , i'm sorri , i have a verb .
and i have a subject .
and so now we're see that these depend reveal exactli these kind of grammat relat .
i've been slightli care here to not just give the word involv but also it's posit in the sentenc .
thi is on , two , three , four , five , six , seven , eight .
so , thi is saw at posit three .
just in case we have the same word .
in fact , we do have the same word repeat multipl .
the word , the , is repeat multipl time .
okai .
so we can convert a tree to a set of depend .
which basic repres the critic grammat relat within thi particular .
pars tree .
and we can again calcul precis and recal .
so we can convert both our gold standard tree to these depend represent .
and we can also convert our , parser output to these depend relat .
and then we can see how accur we ar .
in recov these kind of depend .
so on thing to note , is the follow .
i should just go back to the slide for a second .
you'll notic .
that i have on , two , three , four , five , six , seven , eight depend and i have on , two , three , four , five , six , seven , eight word and that's alwai go to be the case , i'm alwai go to have the same number of depend as i do have word okai so there none of thi busi of differ power structur have differ number of depend , and what i should have said is , i had thi special depend for the word at the veri root of the tree .
thi is saw , and i'll just specifi the rule in thi case to be root and the head word to be root , okai ?
so to make sure all , that we also take account that thi , thi word at the veri root of the tree .
okai , so , everi tree , whether from a gold standard tree .
or from the parser output is go to have a number of depend equal to the number of word in the sentenc .
and so we can actual just report accuraci in term of how accur we ar in recov these depend .
so to get a depend correct , you have to get the two word in the depend .
you also have to get the label , i . e .
the rule , correct as well .
if we look back at the result from my phd thesi parser , again around <num> depend accuraci .
okai , now on veri nice properti of these depend is that we can delv a littl deeper by look at the accuraci .
or more specif the precis in recal , in recov differ recoveri type . so for exampl , i could look at all subject verb depend .
more precis i could look at all deped with thi label .
so whenev i see thi label , i know that the depend is involv subject and a verb , and i can again calcul precis and recal in term of recov .
depend with thi particular label .
so the recal for subject verb would be the number of subject verb depend i have correct .
divid by the number of subject verb depend in the gold standard .
and the precis would be the number of subject verb depend i get correct , out of the number of subject verb depend i have in the parser's output .
and notic these <num> number mai not be equal , becaus we mai av differ number of subject verb depend in the <num> parser and so we go back to the idea of , of calcul recal and precis .
the nice thing about thi measur is that we can go depend type by depend type .
and we can figur out which depend recov with high accuraci y and which dependencyi ar actual more problemat .
and here ar some number .
so , in fact , if we look at subject verb pair , so thi is depend of the follow form .
we do pretti well on these , we get over <num> recal and precis .
if we look at object verb pair , so these ar basic go to be of thi type , vp1 goe to vt np .
so whenev i see a depend like thi , i have a relationship between a verb and it object .
for exampl if i sai , the dog saw the man .
i'm go to have a depend between soar and man label vp1 , vt np , becaus man is the object of soar .
and again these ar recov with pretti high accuraci over <num> recal and precis .
if we look at other argument to verb , so in gener you can look at all of the other product involv verb .
so rule of thi form where we can add y <num> and y <num> .
each of these is go to have a depend where the verb is modifi in some wai .
again we perform pretti well here , about <num> percent recal and precis .
some problem , problem case though , if we look at preposit phrase attach .
so these ar go to be rule of the follow form .
xh goe to y1h , ppw .
so , x could be in a noun phrase , or it could be a verb phrase .
or ani case where we have a preposit phrase modifi someth els .
we see that we score low 80s in term of recal and precis .
so we can see that's a pretti stark drop from subject verb accuraci or object verb accuraci .
and it reflect the fact that pp attach ar a challeng problem , in that , in realiti , to get , veri high accuraci .
you realli need full world knowledg of what is more like , seman , semant speak .
am i more like to be us a telescop to see or am i more like to be look at someth hold a telescop ?
and those kind of decis ar difficult .
and that's reflect in , in the drop in accuraci here .
if we look at coordin , so these ar thing like dog dog in hous .
and cat , where cat could modifi the hous , or dog .
these were even lower accuraci .
these ar veri , veri difficult .
so the take home from thi is , i think .
that the core structur of these polici is quit good .
basic relat like subject verb , object verb .
other argument to verb ar recov with high accuraci .
it's when we have modifi by preposit phrase attach call nation ambigu and so on .
those ar the case which ar caus difficulti .
these ar old result .
actual , thi parser , in spite of thi date , wa more like <num> .
although i think if you look at multipl parser , you'd still see a similar split between hard sorri , easi , of high accuraci depend , and lower accuraci depend .
so , to summar , we saw that a kei weak of pcfg wa the lack of sensit to lexic inform .
and in thi lectur , i've describ lexic pcfg as <num> wai .
of get around thi problem .
some kei step were , firstli to lexic a tree bank us head rule , thi allow us to go from rule like s goe to npvp to b's lexic rule like s sore goe to np dog , vp sore .
so we us thi trick .
to vastli increas the number of non termin and also the number of rule in our grammar .
the second step wa to estim the paramet of a lexic pcfg us smooth estim .
so we have a veri larg number of rule of thi form that pose challeng to paramet estim but we can us smooth estim techniqu to come up with a robust estim for these kind of rule paramet .
final in term of accuraci of these pcfg throughout <num> in recov constitu or depend .
and as we saw the caus and structur .
core grammat relationship like subject verb and verb object ar recov with quit high accuraci .
case such as preposit phrase attach ar still fairli challeng .
okai , so over the next few lectur we ar go to talk about machin translat .
machin translat is the problem of , automat translat from on languag to anoth languag , it's on of the oldest problem in artifici intellig , and comput scienc .
i think it's a fascin , it's a veri challeng problem , and it's a problem that clearli ha huge impact and implic .
so , here's an exampl from a translat system that mani of you would have seen , which is googl translat .
in thi case , translat from arab into english .
and over the next few lectur , we will go through the main step in build a modern translat system .
in particular , describ the kind of statist method that googl and other peopl us to build translat system .
so , in thi first lectur on translat , i want to go over a number of introductori topic .
i first want to talk about challeng in machin translat .
what make translat a difficult problem ?
i then want to describ what i'll call the classic approach to machin translat .
these ar rule base approach which were us in the earli dai of translat and is still quit wide , wide us but thi will give us a ground of some of the histori of machin translat .
and approach that peopl thought about in the first few decad of machin translat .
in the final part of thi lectur , i want to give a brief introduct to statist translat system .
statist translat system learn translat model from veri larg number of exampl translat .
thei go back to the earli 1990s .
there wa semin , semin work by research at ibm who develop the first statist translat system .
and over the last two decad , there's been consider interest in these system and statist method form the basi of mani state of the art system includ , for exampl , googl translat .
so , in thi part of the lectur , i'll give a brief introduct to , to statist mt .
and then , over the next few lectur , we'll go into quit some detail in describ the model actual us in statist mt system .
okai .
so let's talk about some of the challeng in machin translat .
not surprisingli , all of the problem i've describ concern ambigu in the last few lectur have direct consequ for the machin translat problem .
and , in fact , ambigu is on of the main problem or main challeng to translat system .
so , here's the first exampl .
these ar exampl of lexic ambigu .
and these exampl ar taken from the articl by bonni dorr and other from <num> .
and sever of the exampl on the subsequ slide ar taken from thi same paper .
so , if we look at a word like book in english , it ha two quit distinct mean , as illustr by these exampl .
so , if we sai book the flight , it is a verb that ha on mean .
if i sai read the book , it is a noun and ha quit a differ mean .
and so , for mani languag , if we try to translat book into anoth languag , we will end up with differ lexic choic depend on which sens of book is actual be us .
thi particular exampl is spanish .
so , on the book , the flight will get on translat .
read the book will get anoth translat .
and so , translat is essenti go to requir us to resolv thi lexic ambigu between which of these two us of book is actual be us in a particular context .
that is well known to be a difficult problem .
there ar mani other exampl .
let's look at thi third exampl here .
so here i have kill , and actual in both of these case , kill is actual a verb .
and so , we have kill a man versu kill a process .
thei're somewhat differ sens in english .
you could argu thei're similar .
but thei're realli rather distinct sens .
and certainli , again , if you go to spanish , you'll end up with two quit differ translat of these two differ sens .
so , the bottom line is , in mani case , to translat accur , you will need to resolv these kind of lexic ambigu on the sourc languag side .
anoth challeng , which we've describ actual earlier in the cours , is that languag differ languag have differ word order .
and so , i'll just give a brief recap of that argument .
thi argument that we saw a few lectur ago .
so if we consid translat to english and japanes , english is systemat subject verb object .
so i can sai for exampl , the dog saw the cat .
wherea japanes is predominantli subject , object , verb .
so i would sai , the dog the cat saw .
so these ar simpl exampl .
here's anoth on .
but when we get to longer sentenc , we see that these kind of word order differ lead to realli quit complex differ in word order between these two languag .
so again , for accur translat , we're go to have to model these differ in word order and that can again be a challeng problem .
here's anoth exampl from the articl by dorr et al .
thi is an interest properti where you can see that syntact structur is not necessarili preserv across translat .
differ languag can express concept in fundament differ wai .
so here , we have an english sentenc , the bottl float into the cave .
and here is the spanish translat .
and here's actual a paraphras of that translat .
so , you'll see the translat liter is the bottl enter the cave float .
and so , what's go on here is that the verb float ha in some sens becom thi essenti adverbi modifi .
so , float is an adverb modifi enter .
and thi into preposit is , in some sens , be translat as enter or it certainli ha a strong influenc in the choic of verb in thi case .
so , you see kind of the verb becom an adverb and the preposit have a strong impact on the choic of main verb with the claus .
so , there's not necessarili a direct correspond between the syntact structur of these two translat .
that again , can pose major challeng for translat system .
syntact ambigu , not surprisingli , caus problem .
so , thi is anoth exampl from dorr et al .
so , that littl bit of a grim exampl .
but it's john hit the dog with the stick .
there ar two possibl analysi here .
john could either be us the stick to do the hit , or the dog could actual have the stick .
so , there's a preposit phrase ambigu here .
we could have either of these two attach .
and on the spanish side , we'll actual get two quit differ translat depend on thi attach .
so in the first case , if we were attach to here , we'll get thi sentenc here .
and if with attach to dog , we end up with a differ translat .
so , short stori is that we will , in thi particular case , to get an accur translat most like have to resolv thi preposit phrase attach ambigu correctli .
so , here's anoth exampl .
thi is an exampl where we need to resolv pronoun to their refer befor we do translat .
so , pronoun resolut wa a problem i spoke about , i think , in the veri first introductori lectur for thi class .
let's go over thi exampl .
so , in thi particular case , i have an english sentenc , the comput output the data ; it is fast .
and thi pronoun it could potenti refer to either the data or the comput , two possibl in thi context .
it's veri clear that it refer to the comput in thi particular exampl becaus it make a lot of sens for the comput to be fast .
it make much less sens for the data to be fast , and so pronoun resolut is a problem of take a pronoun like it and decid which noun phrase in the context it refer to .
in thi particular translat into spanish , it is then translat as es .
okai , so now let's look at a second exampl .
so , it's the comput output the data ; it is store in ascii .
again , there ar two potenti noun phrase that it could refer to .
and in thi case , it's veri clear from the context that it refer to the data , becaus it make sens for the data to be store in ascii , doesn't make sens for the comput to be store in ascii , okai ?
now , notic in the second case , it is actual translat as a differ pronoun in spanish , estan .
so , why is that ?
well , in thi first case , es refer to a singl noun phrase .
and so it's translat in thi , thi sens .
in the second case , it refer to a plural noun phrase .
and so , we get a differ form .
so basic , to be abl to translat thi word , it , correctli into spanish , we're actual go to have to first resolv which noun phrase in english it refer to .
to decid whether the noun phrase that it refer to is plural or singular , and therebi to choos on of these two differ pronoun form .
so again , thi is a problem with ambigu .
these sentenc ar ambigu .
at least , we have to resolv where these pronoun refer .
and depend on these disambigu decis , we end up with differ translat .
okai , so in thi next segment we're go to go over classic machin translat .
so i'm go to give it veri high level descript of variou system that peopl develop in the first few decad of machin translat .
and thi'll give us some veri us context in think about how we might design differ machin translat system .
system i'm go to describ ar all rule base method .
these ar method where human would try to hand build translat system .
for exampl compil dictionari or bilingu dictionari .
specifi how word in lang , in on languag would translat into word in anoth languag .
so earli machin translat system made us of a method call direct machin translat .
these method essenti perform the translat process word by word .
so , if you have a sentenc in on languag , you'd essenti , word by word , try to map it to word in the other languag .
there wa veri littl analysi of the sourc languag text .
okai , so you can imagin that it might be us to mayb paus or at least tag the sourc languag set text .
to get some idea of the structur of the underli sentenc you're try to translat .
but these direct machin translat system realli did awai with ani analysi of thi form .
thei reli on veri larg bilingu dictionari .
so i'll give an exampl in the next slide .
where for each word in the sourc languag you would specifi a set of rule .
for how to translat that word into the target languag .
in term of reorder .
and so there's alwai thi problem that if we have word or sentenc in on languag .
that we mai want to reorder thing .
we mai want to move thing around .
in direct translat system , usual a simpl set of reorder rule wa appli .
after the word for word translat had been perform .
so , for exampl , in , english you see the adject like blue befor the noun dog , wherea in french , sorri my french is terribl but i think thi is correct , you would see dog translat as thi word , blue translat as thi word , and notic the order is revers .
and so you would need a set of rule to cover these kind of simpl reorder phenomena .
here is an exampl of a set of direct translat rule , thi is taken from the jurafski and martin textbook and it origin from i think a russian system from <num> .
that's actual i think fascin to look at thi .
so thi is a set of rule for translat the word much or mani in english into russian .
and so you can imagin that somebodi some human ha look at a lot of exampl translat or mayb just us their intuit about the two languag to deriv a set of rule of how to form thi translat base on the surround context .
so these rule sai that if the proceed word is how .
so if you have , how much , then you would translat on wai .
on the other hand , if the preced word is as , so as much , you might translat a differ wai .
otherwis if the word is much rather than mani , you would go into some other rule , and so on and so on .
so , deriv these kind of rule is a pretti painstak task .
you can imagin for ani reason translat system , you might have at least a few thousand word in the languag that you need to translat .
and you would have to compil a set of rule for each differ word in the languag .
so direct machin translat system ar rather limit .
let me go over some kei problem that thei face .
on is the follow , it's difficult or nearli imposs to captur the kind of word order differ we saw diff , earlier between differ languag .
so here i have thi english japanes exampl again .
so you could imagin try to write a set of rule that , given the english string , predict the word order in the target side , in the japanes side , but that's veri hard to do when you have no analysi .
of thi english string you don't know which word ar verb , versu , versu noun and you certainli have no syntact structur .
so that's on challeng .
the second on is that word ar translat without ani knowledg of the role , the syntact role thei plai in the sentenc .
so if we take the word , that , for exampl , in english , it can take two quit differ syntact role .
on is the complement of a claus .
so if i sai , thei said that i like ic cream .
thi is on veri particular sens of the word , that .
on the other hand if thei sai thei , thei like that ic cream that is actual a determin .
so thi would be a complement and thi would be a determin , and in mani case where you translat into a differ languag you will end up with differ translat in these two differ context .
again with no analysi of the english sourc languag .
there's realli no wai to be abl to distinguish these two case and to translat correctli .
so the problem with direct translat system led peopl to consid what ar call transfer base approach .
so here's a sketch of how these work out .
we have some sentenc sai , in english , and we're try to map thi to some sentenc in french , for the sake of exampl .
so in transfer base system , you would form thi translat in three differ step .
the first thing you would do on the english side is do some analysi .
and , so for exampl , you might find a pars tree of the english or some other represent of the syntact or semant structur of that english .
so that's the analysi stage .
in the next stage , we perform what is call transfer .
and here , we transform , so thi is an english pars tree we somehow transform thi to a french .
thi is english .
thi is a french pars tree .
and to perform thi transfer step , you us a set of rule mayb rather similar to the exampl i show you for russian earlier .
but these rule critic can now refer to the syntact structur of the english side , so thei have more inform , and so thei can hopefulli perform a better job of transfer across to the french , french side .
final , there is a stage call gener .
where we take thi french pars tree and we produc the french string .
thi is usual at least , if thi is just a syntact structur , thi is a simpl step where we just remov the tree and produc the string .
so , these ar transfer base approach .
we might us variou level of represent .
at thi intermedi stage , we might us syntact structur .
we might us someth which is closer to a symant structur .
closur to some represent of the mean of the two languag .
so , on us diagram that peopl often us to character differ translat system is call the translat pyramid .
and it look like the follow .
so , i have english over here .
let's sai , i'm try to translat into french over here .
so , in direct translat system , we sort of directli translat from the english to french .
so we just draw and arrow here .
in transfer base system , we do the follow .
we do some analysi .
so schemat , we get to some point a littl bit further up the pyramid .
and then , we have a transfer stage .
and so , thi is the english pars tree , for exampl .
thi is the french pars tree .
and final , we have a gener stage .
and the intuit behind the pyramid is that if you do a bit of analysi , then you have less far to go in thi transfer state .
you made the , you made your transfer prompt a lot easier than in pure direct case .
and , of cours , there might be variou level of analysi so we might have some method of go a bit higher up the pyramid .
in which case , it becom even easier to translat .
of cours , it mai be harder to form a deeper analysi , so there's some trade off there .
now , the final type of translat system take thi idea to the extrem .
thi is what's call the interlingua base translat .
and in thi case , there ar just two phase .
so , in the first step , in the analysi step , we're go to analyz the sourc languag sentenc into what we hope is a languag independ represent of it mean .
so , we actual think of some point at the apex of thi pyramid as be the languag independ represent of mean .
so , we actual , the analysi step , we go all the wai up to the languag independ represent .
and then , secondli , in the gener step , we take thi languag independ represent and gener the foreign languag , okai ?
and critic , thi is go to depend on some definit of interlingua which is some represent of the meet of languag which is independ of languag .
there ar some potenti advantag of thi method .
so , on is the follow .
in a transfer base system , let's sai we have n differ languag , n equal <num> or n equal <num> .
and we want to build translat system between all these differ pair of languag .
so , in transfer base system , for everi differ pair of languag , we're go to have to build a differ transfer lexicon .
so , we're go to have to rebuild the transfer compon for the system .
so , in thi case , n squar is approxim equal to <num> .
in thi case , n squar is approxim equal to <num> , <num> okai ?
so , that's a lot of work , build the differ transfer compon for the differ languag pair .
the appeal thing with a , an interlingua base system is that we just need to build n analysi compon .
so , for each of the n languag , we have to build a compon which take it , sai english , and resolv it to some languag independ represent .
and we need n gener compon .
let's rememb the gener compon would take the languag independ represent and gener , sai a french string or an english string , okai ?
so , becaus we've done awai with the transfer compon in theori , we just need these n analysi compon and these n gener compon .
so that can be a save .
the downsid to interlingua base approach is realli it is veri , veri difficult to come up with a truli languag independ represent of mean .
it might even from a philosoph point of view be an imposs problem .
here's on exampl of why it is difficult .
how do we present differ concept in an interlingua ?
so , differ languag tend to break down concept of the world in quit differ wai .
so , let me give you an exampl .
in english , we have a singl word for wall , but in german there're actual two word for wall .
on for a wall that is intern to a hous , so thi is the wall which is within a hous .
and on for an extern wall , wall on the outsid of a hous .
so , two differ word , two differ concept .
japanes ha two word for brother .
it ha on word for an elder brother , and on word for a younger brother .
spanish ha two word for the english word leg .
you us on word for a leg of a human and anoth word for leg of an anim , or the leg of a piec of furnitur , of a tabl for exampl .
and as you go through languag by languag , you find that each languag ha it own wai of break up the world into differ concept .
if you're not care , the interlingua is simpli go to be an intersect of the wai , of these differ wai of break thing down .
we'll put anoth wai .
it's veri difficult to imagin , a priori , all the wai , differ wai you could break down concept in the world .
everi time you add a new languag to your translat system , the chanc ar you will be surpris to find some distinct made in that languag , but which you hadn't thought of when you design the interlingua .
so , bottom line is , design an interligua might be , might be a veri , veri difficult task becaus of these kind of conceptu problem .
so in thi final segment , i want to make a brief introduct to statist method to machin translat .
and statist method will be the focu of the next few lectur of thi class .
so , here's the basic idea .
a kei observ is that so call parallel corpora ar avail in mani languag pair .
so , a parallel corpu , let's sai for english and french , consist of a set of exampl translat .
so , each element of the corpu is an english sentenc pair with some french sentenc .
so , we have sentenc pair where each of these sentenc in english ha the associ translat in french .
so , we're basic go to treat translat as a supervis learn problem where we have e , f exampl .
here's an english sentenc , and f is a french sentenc .
assum we're try to build a translat system from english to french or from french to english and we might have mani thousand of exampl translat .
so , it turn that these kind of corpora exist across mani languag pair .
so , on classic exampl is from earli work at ibm .
realli dissemin work in thi area of translat .
thei us corpu call the canadian hansard .
so , these ar proceed of the canadian parliament .
so , anyth which is said in the canadian parliament is written down in both english and french .
and so , actual , we have million of exampl translat between english and french .
but mani other languag pair have these resourc .
for exampl , the europarl corpu is a corpu drawn from a european parliament and it contain , i think , <num> or <num> european languag where again , we have translat into all of these differ languag simultan .
so , the first statist machin translat system came about in the earli 1990s , larg due to these research at ibm .
and as i said , the basic idea is to take some exampl that ar translat and then to try learn a statist model which given a new sentenc in sai , french will translat it into english .
thi , on the surfac , is sort of a radic and rather preposter view .
it's complet differ from the rule base approach which have been develop prior to the earli 1990s .
but it actual ha been a highli success approach .
and over the last <num> or <num> year , there's been huge amount of research on statist machin translat and it's gotten to the point where commerci system such as googl translat us precis thi technolog .
the idea actual is an old on .
it goe back to warren weaver just after the second world war , who suggest appli statist and method from cryptographi to translat .
actual , if we go to the next slide , here is a quot .
so , here's a quot from warren weaver in <num> in a letter to norbert wiener .
so , he wa spur on by great success in the second world war where statist method had been us to decrypt encrypt messag .
and so he sai the follow .
on natur wonder if the problem of translat could conceiv be treat as a problem in , in cryptographi .
when i look at an articl in russian , i sai , thi is realli written in english , but it's been code in some strang symbol .
i will now proce to decod .
and so , warren weaver sort of had thi preposter idea of us statist cryptograph method to try to perform translat .
so , let's talk a littl bit about how the ibm research actual envis thi translat process .
and their idea wa to appli the noisi channel model , which we saw in the lectur on tag model .
so , their idea wa to appli the noisi channel model to translat .
so by convent , let's just assum that our goal is to build a translat system from french into english .
so , we want someth that map some french sentenc s , to some english sentenc e .
and , you know , a natur wai to try to do thi is to try to build a condit model pe given f .
so , we have , with some french sentenc f .
if we consid everi possibl sentenc in english in turn , a veri larg set of cours , an infinit set of sentenc , we assum that each of these sentenc ha a probabl e of f , which is the condit probabl given that i'm translat f that e will be the output .
now , in a noisi channel model , we're again go to us a gener model .
and we're go to us bay' rule to kind of flip thi problem around .
so , here's how thi goe .
a noisi channel model in for translat is go to have two compon .
so , p of e is a languag model .
so , thi will assign probabl to sentenc .
for exampl , like the dog laugh .
stop .
and thi , for exampl , might veri well be a trigram languag model exactli as we saw in the veri first week of the class .
okai , so thi is a model that we can learn from veri larg amount of english data alon .
the second part of the model is what's call the translat model .
and thi is a model of p of f given e .
notic that thi is revers from the direct here .
thi is what we often see with the noisi channel model .
so , pf given e for ani english sentenc , for exampl , the dog laugh .
we consid all possibl french sentenc .
so gain , an infinit set .
we're go to have model pf given e for each french sentenc condit on the english sentenc .
okai , thi model is go to be estim from a set of translat exampl .
okai , so thi is go to be estim from our bilingu corpu , parallel corpu .
okai , as we saw befor with the noisi channel model , if we have these two model , we can actual deriv p given f us bay' rule .
so , by the usual rule of probabl , thi is pef of f , and thi is pe time p of f given , divid by , and here i have a sum of all english sentenc p time p of f given e .
okai , so given a new french sentenc f , we ar go to output the most like translat under the model .
and so , that mean we're go to search for sentenc e that maxim thi condit probabl .
and now , i can plug thi formula in here .
and notic that thi denomin , it's constant with respect to the english sentenc we're search over .
and so , thi constant term isn't need when we're consid the argmax .
we saw exactli the same trick when we saw noisi channel model for hidden alcohol earlier in thi class .
so , we end up with the follow problem .
so rememb , again , our problem is to take some french sentenc f and to produc some translat e .
to do thi , we're go to search for the sentenc e that maxim the product of two term .
thi term is a languag model term .
it's the prior probabl of the english sentenc e , and the second term is a condit probabl , the probabl of the french sentenc f given the english sentenc e .
of cours , we've seen how we can build a languag model us a trigram model .
of the next few lectur , we'll see how we can build model pf given e , these translat model .
that's go to be the critic new compon of these translat system .
so , a few more note about the noisi channel model .
so as i said , thi languag model can be estim from ani data .
and actual , we don't need a parallel corpu to estim the paramet of the languag model .
so that mean we can actual leverag potenti veri , veri larg quantiti of text in english alon .
thi translat model is train from a parallel corpu consist of french , english sentenc pair .
notic that even though our goal is to translat from f french to english , becaus we've us the noisi channel model , we estim p of f given e .
okai , so thing ar flip .
the translat model is in a sens backward .
on big advantag of thi is that we now have a model that make us of a languag model of english sentenc .
and thi give us a veri strong sourc of inform about which sentenc ar like or grammat or fluent .
on the english side , and that can make up for a lot of diffici in the translat model .
next in thi class , we'll talk about how to build thi model of f given e .
and final , we're go to be left with thi problem of , for a given french sentenc f , find the english sentenc that maxim the product of these term .
thi is also a veri challeng problem .
we're set , search over a veri larg set of possibl translat .
and so , we'll talk a lot about thi problem a littl later in the class .
so , here's an exampl of the noisi channel approach from a tutori by philip cohen and kevin knight .
and so , imagin we're try to translat in thi case from spanish into english .
there ar mani , mani possibl translat , mani possibl english sentenc .
and what we've shown here is a few possibl .
and in each case , we've shown the condit probabl of the spanish given the english under the translat model .
okai , so we might get these variou term here , and notic again that we have ps given e here .
we do not have pe given s .
okai , so thi part of the , the translat model is actual backward from what we realli need .
and so that's on compon of the model .
when i evalu the probabl of these differ translat , thi is on compon that i take into account , p of s given e .
and notic that if we just us thi part of the model alon , we actual come out with thi translat which is a pretti bad translat .
but here's the full model .
and notic that now to evalu the probabl of each of these endless altern .
i actual take the product of two term p of sq and e time p of e .
so , thi is now the model under a trigram languag model , of the english sentenc .
so , for each of these english sentenc , i can calcul it probabl under a trigram languag model .
and some of these sentenc , of cours , ar much more like than other in particular .
thi sentenc here is much more like than these other sentenc .
and when we multipli togeth these two term , you'll find out the highest probabl translat is actual thi on .
so the , at a high level , the import part here is to evalu the plausibl of each of these exampl as a translat of thi input .
we multipli two thing , firstli p of e , which is the prior probabl under a languag model of see thi sentenc in english .
and secondli , p of s given e , which is the probabl of see thi spanish sentenc given that we have thi english sentenc underli the process .
okai , so in thi next segment of the class we're go to talk about the ibm translat model .
the ibm model go back to the late <num> , 1980s , earli 1990s .
thei were semin in realli start the whole statist approach to the translat problem .
thei form a central part of modern statist translat system , and so we'll cover these model in some detail in the segment of the class .
so just to recap , thi is a slide from the last segment of thi class .
thi is a recap of a noisi channel model .
which wa introduc by the ibm research to translat .
so a noisi channel model , as we said , ha two compon .
firstli , p of e , thi is a languag model , thi is a model that assign a probabl to ani sentenc in english .
and secondli , p of f given e .
so thi is what's call the translat model .
thi assign the model for a french sentenc given an english sentenc .
on note .
throughout thi lectur , i'll follow convent which is , i'll alwai assum that we're translat from a french sentenc to an english sentenc .
so that is our goal .
okai .
of cours , we might have other languag for translat between .
but for the purpos of thi lectur , we'll alwai assum we're translat from french to english .
okai , so we have a languag model for string in english .
we have a translat model which , as i said , is in a sens backward .
it sai what's the condit probabl of ani french sentenc , given an english sentenc .
when we translat on to thi model we search for the english sentenc that maxim the product of p of e and p of f given e .
and so when we evalu for particular sentenc f .
differ english translat , we take both of these term into account .
so , the roadmap for the next few lectur in thi cours on translat is as follow .
i'm first go to describ ibm model <num> and <num> .
so ibm went through a seri of model , i think <num> through <num> , and we're just go to do the first two becaus thei will essenti introduc mani of the import idea .
and actual model <num> is a pretti decent model for the problem we're go to be look at .
we'll then go on to describ phrase base model .
so phrase base model were invent around the late 1990s , and thei're in some sens a second gener of s , school translat system .
thi is the first gener .
thei make direct us of idea from these ibm model , and so these ibm model will , in some sens , form a basi of phrase base model , but phrase base model work much , much better than ibm model .
thei do , in fact form the basi of mani modern statist translat system , so for exampl , googl translat is heavili built on thi technolog .
so , we'll first talk about ibm model <num> and then we'll talk about ibm model <num> and then final we'll talk about paramet estim in model <num> and <num> .
how do we actual learn the paramet of these model ?
from data consist of exampl translat .
okai .
so now , let's talk about ibm model on .
the critic question is how do we defin a model p of f given e .
so , we might , for exampl , have an english sentenc like the dog bark .
so , thi is e .
and thi is a french sentenc sai , thi sentenc here which i believ is a reason translat to french , but my french is terribl .
and so , we want to assign thi pair of sentenc a condit probabl or more gener , thi is on of mani possibl translat for the english .
we want to find distribut of all possibl french sentenc pair with thi english sentenc .
i've seen throughout that each english sentenc we're look at ha l word , and the french sentenc we're look at ha m word .
so , m and l , the length of two sentenc .
so , you might try to model thi probabl directli with no intermedi structur .
but that turn out to be a veri difficult thing to do .
and so , an absolut critic idea in the ibm model wa to defin the idea of what's call an align between these <num> sentenc .
okai .
so , in thi case i have a number of french word , m equal <num> and the number of english word l equal <num> .
and an align is just go to be a sequenc of valu .
a1 , a2 , up to an .
a is <num> in thi case .
and then , basic , for each french word , specifi which english word is align to ?
so , here's on possibl align .
we can sai le is align to the , thi word is align to thi and thi line , word is align to thi , okai ?
so , more formal , ani of these align variabl can take ani valu in <num> to l where l is the length of the english sentenc .
talk in a second about why we have <num> includ here , but for thi particular line we have a<num> equal <num> , a2 equal <num> , a3 equal <num> .
why do i have <num> as a possibl align point ?
so , if i sai , for exampl , a1 is equal to <num> .
that mean we have to actual assum that thi word is align to what's call a null word .
and so , the ibm research found it wa us to includ thi extra word null which could be us to gener some french word in a sentenc .
intuit thi correspond to french word which there is no natur word to align to it in english .
okai .
so , how mani possibl align ar there ?
so , if i have m possibl word , and sorri , l word on the english side .
e is over here .
and i have m possibl word on the french side .
each of these french word can be align to <num> , <num> , <num> up to l .
so , there ar <num> plu l , l plu <num> possibl align for each word .
and becaus i'm choos an align for each of the m word , we have l plu <num> is m .
as the number of possibl align .
so , the align can be ani structur where each french word is align to a singl english word .
so , here's an exampl an exampl english french sentenc .
in thi case , the number of english word is <num> .
on , two , three , four , five , six .
and so , l equal <num> .
number of french word , on , two , three , four , five , six , seven .
so , m equal seven .
and on align is the follow .
so , let's number of these english word , <num> , <num> , <num> , <num> , <num> , <num> .
and the french word , <num> , <num> , <num> , <num> , <num> , <num> , <num> .
and so , basic , an align is go to be a sequenc of number , <num> , <num> , <num> , <num> , <num> , <num> , for exampl .
specifi word by word in french which english word were align to .
so , thi is sai that word <num> is align to word <num> in english .
word <num> in the french is align to word <num> .
word <num> in the french is align to word <num> .
word <num> in the french is align to word <num> .
and then , the next <num> word ar all align to english word <num> .
okai .
so , that is on possibl set for these align variabl .
and that's probabl a pretti good set for the align variabl .
okai .
it doe a reason job of sai what the variou word in french correspond to on the english side .
so , that's on possibl align .
let me show you anoth possibl align .
so , thi on down here is just sai that everi french word is align to english word <num> .
so , we can draw like follow .
okai ?
so again , each french word is align to a singl english word .
in thi case , thei're all align to word on on the english side .
thi is clearli a veri bad explan of the translat in thi case .
so , it's a veri bad align .
so , the next idea will be to defin a model which assign a condit probabl to ani align a pair with a translat f .
condit on two thing , condit on the english sentenc and on the length of the french translat .
so , you can visual thi as follow , we might have some english sentenc for exampl the dog laugh .
and let's sai , m is equal to <num> .
and so , we know there ar three french word , f1 , f2 and f3 , in thi case , okai ?
so , each of these word could take ani valu into the so , we could have all of these differ word in french .
and so , that's on choic .
for each of these posit , we're go to choos a french word .
and in addit , we're go to choos an align from the space of possibl align .
so , for exampl , we might have , as i show you befor , thi align with the word le and thi , and then thi .
and that would be on particular choic .
and so , that would have probabl of the align in thi case will be <num> , <num> , <num> .
sorri , <num> .
and it's be condit on their dog laugh .
and on the fact that the french translat is also of length <num> .
okai .
we're actual go to decompos thi into a product of two model .
and we'll describ soon , how we can defin these two model .
the first is go to be a distribut of a possibl align .
so , condit on just the english sentenc and on the length of the french , we're go to defin a distribut over all possibl align .
rememb , there ar l plu <num> pair m possibl valu for a .
and then , secondli , we're go to defin a second model which condit on align , an english sentenc and an english sentenc length .
assign a probabl to each possibl french translat of that english sentenc .
onc we've done thi , we can recov our , sort of end goal , which is to defin a probabl of ani french sentenc given in english sentenc .
and we can do thi just by sum out over all possibl align .
thi follow by the usual rule of probabl .
so , thi entir product here is pf , a given e , m .
and by the usual rule of probabl , you can margin l , thi align variabl .
you can sum it out to get a model of p of f given e .
okai .
so , that's the basic trick in introduc align in thi model .
it's a veri import thing to do .
it allow us to introduc these intermedi variabl align which give us an explan of the , the translat process .
and as we'll see , we end up with veri natur parameter of these two model .
and we can then , sum out over the space of all possibl align to get thi final distribut over f if we need to do so .
so , the bottom line is , estim the probabl of f given e directli is veri difficult .
so , instead , we come up with the model of p of f , a given e .
sorri , thi should be condit on length , i guess .
and then , we sai , p of f given em is equal to the sum over a , p of f , a given e , m .
now , a veri import byproduct of thi process is that onc we have a model , of thi form , we can actual , given an english sentenc and a french sentenc , we can find the most like align .
so , we have , if we have some english sentenc here and we have some french sentenc here , we can , for everi possibl align between these two sentenc , evalu their probabl and pick the most like align .
and so , we do thi by , again , us usual rule of probabl .
we can sai that p of a given f , e , m is thi model term on the numer , and then , on the denomin , i have a sum over all possibl align , thi is p of f given e , m .
and again , thi equat follow by the usual rule of probabl .
and then , for a given f , e pair we can find the most like align amongst the space of all possibl align .
i'm not go to go into detail about how thi is done , it's describ in the note that ar provid to accompani , accompani thi class .
but you can actual , in the ibm model on and two , comput thi most like align veri , veri effici .
in fact , nowadai the ibm model ar rare , if ever , us for an actual translat .
even though thei were origin design for translat , thei're rare us for translat .
but thei do plai a critic role in allow us to recov align between sentenc .
so , onc we've train the paramet of an ibm model , we can then , sentenc by sentenc pair in our train set , find the most like align for those sentenc pair .
so , here's an actual exampl .
thi is for a french english translat us some of the ibm align model or translat model .
so , here is the french , here is the english .
and here , what i've shown , is the most like align .
and so , what we actual have here is , in fact , for each english word , we have it's align to a french word .
so , in fact , thi model ha been train in the opposit direct p , e give f and we have the sum of the align p , e , a given f .
thi kind of model .
so , it's train in the revers direct and becaus of thi i have a an align now , where each english word is align to a singl french word .
so , let's look at thi .
it's sai that the is align to le , which if you know french and english is pretti much correct , council is align to thi word and so on and so on .
so , these align actual ar quit good and thei becom veri import in the phrase base translat system we'll see in the next coupl of lectur of the class .
becaus thei essenti given us an anchor on how word in the english ar align to word in the french .
okai ?
so , bottom line , onc we've train ibm model on or two , we can look at our train data sentenc , which consist of these sentenc pair and actual find these most like align .
and at that point , we have a much better handl on how these two sentenc correspond to each other .
okai , so let's look at how ibm model <num> now defin these two probabl .
rememb we have two part to the model .
condit probabl of an align given an english sentenc and the length of the french sentenc m , and then secondli p of f given e a m .
so i'm go to go over these two .
side of the model in turn .
so , firstli , for the align model , ibm model <num> is the simplest model you can think of , which is just assum that everi possibl align is equal like .
so , rememb we have some english sentenc .
we have , the null word , i'll call that e <num> , and then e <num> , e <num> , e <num> , up to e l , and then i have some french sentenc length , so i have m posit on the french side .
so we're just go to assum that for each of these posit .
each of these l plu <num> align .
so thi word for exampl can be align to ani of these differ thing .
each of these ha probabl <num> over l plu <num> becaus there ar l plu <num> possibl .
the null word plu each of the english posit .
and i have m french word to choos .
so the overal probabl is just go to be <num> over l plu <num> to the n .
so , again , thi is just assum that everi possibl align is equal like .
it's the dumbest possibl model , it's a major simplifi assumpt , but it get thing start .
it will get us off the ground .
so now let's talk about the second part of the model .
thi is the model of p of f .
given a , e , m .
so what's go on here ?
we have some english sentenc e .
so for exampl we might again have the dog bark .
and i'll write null as the 0th word in the sentenc .
we have some length .
of the french sentenc , sai m equal <num> .
so we have <num> , <num> , <num> posit here .
and we have some align , so for the sake of argument let's assum we have thi align here .
the first word in french is align to the first in english , second word in french , second word in english .
third word in french .
third word in english .
okai ?
so how do we calcul the distribut of the possibl french sentenc ?
and let me give a particular exampl , let's assum that we have , thi is f <num> .
thi is f <num> and then final , we have f <num> .
so , how do we calcul the condit probabl of thi particular french sentenc ?
we have a condit on the entir english sentenc , the align and the length of the french sentenc .
so in thi particular case it's go to be the product of three term .
we'll have t of le given the , and intuit thi is a paramet correspond to the condit probabl it's in the french word le given that we're align to the english word the .
it's the probabl of thi english word the emit the french word le as it translat .
the second term is t of the second word given dog .
and then we have a final translat of thi third word given bark .
so on t paramet for each of the french word .
where we condit the ident of the french word on the english word that we're align to .
the abstract form of thi is as follow .
so we have a product from j equal <num> to m , rememb m is the length of the french sentenc , so we're go posit by posit in the french sentenc , and at each point we have t of f sub j , that's the jth word .
condit on e of a sub j , so thi is the ident of the english word , which the jth french word is align to .
so we're essenti assum that each of these french word is fill in , in turn , independ from all of the other french word , actual condit onli on the ident of the english word that we're align to .
so here's anoth exampl of how thi goe , thi is come back to the exampl i show you earlier .
assum again we have thi english sentenc , thi french sentenc , we have the english length six , french word french length seven .
and let's sai , sai we have thi align so i'll draw thi here so thi align look like .
then in thi particular case , sorri thi should also be condit on m which is <num> in thi case .
so in thi particular case the probabl of see thi particular french sentenc condit on e and the align is a product of t term .
notic i have on t term for each french word .
so we have on t term for each of these seven french word , and at each point we condit on the english word we ar align to , so le is line to the , for exampl , programm is line to program and so on and so on .
so intuit , ibm model <num> correspond to the follow gener process .
so , we have some english string .
so for exampl we might have , the dog bark .
and we have some length of the english string for exampl m equal <num> .
so we have three possibl french word .
and so here's how we proce .
so firstli we pick some align a , uniformli random from the space of all possibl align .
so we might , for exampl , pick some align like thi .
okai , have done that we then fill in the french word with a product of probabl .
so we might , for exampl , pick these three word .
and , thi part of the express would have probabl t of le given dog , time t of chien given the , time t of aboi given the .
of cours , thi is a veri unlik align , given these two sentenc , but it's nevertheless possibl .
and so intuit in the second step , the gener process for each french posit , we choos a french word at random from the distribut defin by t .
so for exampl thi word is chosen from the distribut t of the word given dog .
and thi word is chosen from the distribut t of the word , given the , and so on .
so first step , choos an align uniform at random , second word second step , fill in the french word on by on , condit onli on the english word which we're align to .
the final result is a model of the follow form , we have condit probabl f and a , given e m .
we have product of two term .
firstli , p of a can be m , secondli , f , given a e m .
that's what just an applic of the chain rule to thi .
and given thi particular form , for ibm model <num> , the final express is the follow .
so thi correspond to the uniform probabl of the align , thi correspond to pick each french word base just on the english word which we're align to .
so , later in thi class we'll see how to estim these t paramet okai .
so we're go to have some parallel corpu as the input to our model and from thi we're go to get t f e for all english word e , and french word f , so like f should get a condit probabl .
what i've shown you here is on such entri from a model which is actual train on a real data .
and thi is for an english word posit .
a list of the most probabl word in french onc we train the paramet of thi model .
and you can see it actual done a reason job , if you know some french , thi seem like a reason set of translat probabl .
so posit can be translat as thi word .
it can be translat as posit , in french , there ar other possibl like thi word and so on and so on .
okai , so thi is a typic kind of lexicon that you might get from the output of thi process .
for each english word you get a probabl for everi possibl french word .
so now let's move onto ibm model <num> .
ibm model <num> is , as we'll see , a fairli straightforward extens of model <num> .
so , the onli differ is go to come in the align model .
so , ibm model <num> introduc what ar call align , or sometim call distort paramet , okai ?
so , i'm go to us q for these paramet and it's go to have a few differ indic here .
so , m , as befor , is the length of the french sentenc , and i is the length of the english sentenc .
and j is the index of the french word , and i is the index of the english word .
so , let me give you an exampl .
so again , if we come back to that exampl , the dog bark , and sai we have m is equal to <num> .
we have three french word , and let's sai sort of that , some of these <num> , <num> , and <num> , so the second french word , let's sai we align thi to the third english word .
the probabl of thi happen is q of so that's <num> , <num> , <num> , the english posit three condit on the french posit two and the length of the two sentenc , so l , m ar both equal to three in thi case , okai ?
so , condit on the length of the english and the length of the french sentenc , and condit on the posit of the french word we're consid posit two in thi particular exampl .
we have a distribut over all possibl align .
again , zero is taken to be null word , and that is on possibl .
we'll come to a concret exampl in a moment to thi .
but here abstractli is how we write thi out .
so , an align is again go to be a sequenc of variabl , a1 through am , specifi for each of the french word what it's align to .
and to calcul the probabl of an align , we just multipli togeth the q variabl .
so , i have a product from j equal <num> to m and add q .
a sub j is go to be the posit in the align for the j th word , a condit on the posit j , the length l , and the length m of the english and french sentenc .
the final form of the model is then go to be the joint probabl of f and a condit on e .
and thi length m is a product from j equal <num> to m of q time t .
and these t term ar exactli the same as we saw befor .
so , thi is t of fj given e of a sub j .
sorri , thi is a j down here .
so , let's work through an exampl .
so again , consid thi exampl where i have thi english sentenc and thi french sentenc , and the length of the english and french ar <num> and <num> respect .
and let's consid thi particular align i show you befor , i just draw here , it's thi particular align .
so , what's the probabl of thi align condit on the english word and the length of the french sentenc be <num> .
it's go to be a product of the q term .
so , the first term is the probabl of posit on in the french be align to posit two in the english , condit on these two sentenc length .
and you'll see that i have a similar term for each posit in the french , <num> , <num> , <num> , <num> , <num> , <num> , <num> .
each of these posit is align <num> , <num> , <num> , <num> , <num> , <num> respect .
and so , thi product to q term give us the probabl of thi align condit on the english sentenc , and on the length of the french sentenc .
so , thi model is incred naiv .
it miss all kind of import linguist problem .
from a linguist point of view , it's realli not a realist model of translat .
but the import point is that , in spite of thi , it get us off the ground and it will actual be quit success in recov good align between differ sentenc in our train sampl .
and those align will form a critic compon of the phrase base translat system we'll see a littl later in thi class .
so , a naiv model , but in practic it doe quit a good job .
here's the second part of thi model .
rememb , the second part is to defin the condit probabl of the french sentenc given the recondit on the english , the align , and the two length .
and in thi case , it's exactli the same as model <num> .
so again , rememb thi align is the follow .
and so , we have on term for each french word .
and at each point , we condit on the english word which it's be align to .
and we have these t paramet reflect the condit probabl , for exampl , of le be translat from the and so on and so on .
so , thi is the second part of the model .
it's the same as ibm model <num> .
so final , to get some intuit , let's consid the gener process underli the model i've just shown you .
it's basic assum that a french string f is gener from an english string e us the follow process .
so again , we're condit on e be some sentenc , for exampl , the dog bark .
and we're go to , we kind of condit on the length of the french sentenc m , for exampl , m equal <num> .
so , in the first step , we choos an align from the set of possibl align with thi probabl .
so , for each word in the french sentenc , we choos an align to some english word , okai ?
and , and thi is dictat by these align paramet , these q paramet .
in the second step , we fill in the french word on by on , condit on the english word we're align to .
so someth like thi .
and so that's the second step of the process .
the final result of all thi is that the joint probabl of f and a condit on the english sentenc e , and the length of sentenc n is the product of these two model term .
and so , i end up with thi model form .
i have a product from j equal <num> to n .
and for each j , i have both a distort paramet and also a translat paramet .
so , if q of a sub j condit on the posit of the <num> length , and then secondli i have the translat paramet , the probabl of fj be emit from e sub aj .
thi is the probabl of emit a french word from the english word that is actual align to .
and that is it .
that is the final model form for ibm model <num> .
so , final point i want to make about ibm model <num> is the follow .
onc we have the paramet q and t , and we'll actual talk next about how to estim these paramet from train exampl .
so , onc we have these paramet , it is veri straightforward to find the most like align for a given sentenc pair .
so , what i'm go to describ here is a process which take in an english sentenc pair with a french sentenc as the input , and then as output will produc some align .
so , for exampl , it might produc the follow align .
so , for each word in the french sentenc , we've chosen on of the english word to align to .
and if we have the paramet by ibm model <num> , it's straightforward to find the most like align under the , the model .
let me show how thi work .
so , the kei to kei definit is the follow .
so , given a sentenc pair e1 through el , f1 through fm , then for each aj , for j equal <num> to m , you simpli find the align that maxim the product of two term .
firstli , q of a , given j , l , m , and secondli , t of fj given ea .
so you find the align that maxim thi product .
let me illustr thi with a particular exampl .
okai , so sai i want to align thi word a in french .
so , it could potenti be align to ani on of the english word , or it could be align to null , okai ?
so , ani on of these possibl align is possibl .
and we have to choos between them .
let's go through each of them in turn and see how thi express is score .
so , for the null word , we evalu q of <num> given , and then we have the third word in the french sentenc .
and we have <num> is the length of the english and <num> is the length of the french , so thi is the distort paramet .
basic , specifi the probabl that posit three in french is reli into posit zero .
and then , we have a translat paramet t of a given null .
so , look at the second word , and .
in thi case , we're go to evalu q of <num> given <num> , <num> , <num> becaus now we ar align to the first posit in english , and then we have t of a given and .
next word is there .
we have q of <num> given <num> , <num> , <num> , and then t of a given thi word there in the second posit , and so on and so on .
we can go right to the end 'till the final word is implement , and that is q of <num> given <num> , <num> , <num> time t of a given implement .
so , for each of these altern word that we could align to , we calcul two firm , two term .
firstli , q reflect the likelihood of that posit , and then t reflect how like it is that thi french word wa gener from that english word .
and we take the max , so we find the maximum of all these valu , and then take the singl align that maxim thi product .
in thi case , you'd hope it wa thi word , ha , which is the natur translat .
okai , so to recap the main messag here is onc we have these paramet , it's veri easi to recov align .
so , it's veri easi to fill in these align on train exampl where an align specifi for each french word , an align to on of the english word where an english could come could includ the null posit .
so in the final segment of thi lectur i want to talk about how we estim these q and pr , q and t paramet in ibm model <num> .
and in particular we're go to focu on someth call an em algorithm for estim of paramet in thi model .
so here is a definit of the paramet estim problem .
as input to the paramet estim algorithm we assum a set of sentenc pair .
so i'm go to us e k , f k .
to refer to our kth train exampl .
and that exampl consist of an english sentenc , so thi could for exampl could be , the dog .
and thi is a french sentenc which is assum to be a translat of the english sentenc .
here's anoth exampl .
so , we might for exampl find that a hundredth train sampl , consist of thi english sentenc , togeth with thi frame sentenc .
as output from the model we want paramet of the form t , f , e or q , i , j , l , m .
so for exampl , we might have an estim of t of le , given the , or we might have an estim of q of <num> given <num> <num> <num> , just rememb it is a distort or align paramet .
now , a kei challeng .
in thi problem is that we ar assum that we do not have align on our train sampl .
okai , so in an ideal situat somebodi would have annot these align , so these align would have been present in our train sampl .
but that's realli an unrealist assumpt .
it's an extrem labori task to annot these align .
and you know , typic train set size can easili be you know in the order of hundr of thousand of sentenc .
we might have <num> , <num> sentenc of train data .
and it's just too much to expect human to annot all of those align on all of our train exampl .
or readi for ani signific subset train sampl .
so that's go to be the chee , the kei challeng , and that is go to mean that we need to us someth call the em algorithm , which is a realli a rather remark algorithm .
thi can be us for estim , in thi kind of scenario , where part of the data is hidden , hidden or miss .
these align ar unobserv .
so let's , as a warm up for the em algorithm , first consid the case where the align ar observ .
okai , so let's just for now as humor in thi ideal set where we do actual have train exampl with align .
so each train exampl would now be a tripl , e f a .
for exampl , thi might be a hundredth train sampl .
specifi an english sentenc .
a french sentenc .
and an align between these two sentenc .
so thi particular line would sai that the first word in french align to a two .
second word is align to a three .
and so on , and so on .
okai ?
so , as i said , each train exampl is a tripl e f a .
we're now see we have align .
so , in thi case , deriv maximum likelihood paramet estim is basic trivial .
and it look veri much like the other .
paramet estim we've seen earlier in thi cours .
the ml estim ar just ratio of count taken from our train data .
so , for exampl , if i want to estim the probabl of le given the .
i just set thi as the ratio of two term .
so on the numer is number of time that these two word have been align to each other , ar seen , ar seen align to each other in the train data .
and on the denomin , i have just the number of time i see the english word .
so that's a particular exampl of thi definit .
so again , numer , number of time i've seen e align to f .
denomin the number of time i've seen e .
similarli the maximum length estim distort paramet ar a simpl ratio of count .
thi numer count is the number of time i've seen the word i in french align to word j in english .
given that the two sentenc length ar l and m respect , and the denomin is basic the number of time i've seen the ith posit align to anyth given that we have the length l and m of these two sentenc .
so , that's the warm up .
thi is the case where the align ar observ .
i'm actual go to now write some pseudocod to be absolut explicit about how these count ar calcul .
the main reason for thi , though , is that we'll see that the em algorithm is in some sens , well it's certainli veri close relat to the pseudocod i'm about to show you to the algorithm i'm go to shown you , to show you .
there's just a coupl of twist .
okai , so here , i'm go to give the algorithm which explicitli calcul those count i wa just refer to .
and again , the main motiv for thi is go to be that the em algorithm is essenti go to be a variant of thi algorithm .
so , as input we have , a train corpu .
and again , i'm assum thi ideal scenario where i do have the align on each train sampl .
so , each train sampl consist of an f , e , a tripl .
so , fk is a sentenc in french .
and i'm go to assum m sub k is the length of the french sentenc .
ek is an english sentenc .
i'll assum l sub k is the length of that sentenc .
and then , final , we have align variabl a1 through amk .
so , as on exampl we might have , e100 is equal to the dog , f100 is equal to thi .
and then , our align a100 is equal to sai <num> , <num> .
which basic correspond to thi align here , okai .
so , the algorithm is go to iter over the corpu or run over the corpu and calcul these count .
and then , final the maximum likelihood estim ar just go to be simpl count ratio .
so tmll of f given e , is thi ratio count , and similarli , the distort paramet ar ratio of count .
okai , so let's talk about thi inner loop where the count ar actual calcul .
and so , i'm go to pass over each train sentenc in turn and for each train sentenc , i'm go to consid everi possibl french posit , right , <num> through mk .
and everi possibl english posit <num> through lk .
i'm , assum that the null word is also possibl as usual .
and i'm go to increment some count .
critic definit is these delta variabl .
so , k is the exampl number .
i is a french posit .
and j is an english posit .
we're go to see these delta variabl appear in a , in a veri central wai in the em algorithm which we'll introduc next .
but for now , these delta ar just defin , basic , as indic function indic which align ar present .
so , delta k , i , j is <num> if the ith french word is align to the jth english word on thi kth exampl .
so let's , let me illustr thi with thi particular exampl .
for thi , we have delta <num> , <num> , <num> is equal to <num> becaus french word <num> is align to french word <num> in english .
similarli , i have delta , <num> , <num> , <num> is equal to <num> .
and then , final , delta <num> i , j is equal to <num> for all other i , j .
okai .
so these delta ar actual quit simpl .
thei just indic which align ar actual present , present .
so , thi inner loop is basic go to iter over all possibl i , j posit and it's go to increment some count .
so , let me actual , if you trace it through , you can trace through which count ar actual in increment on thi particular exampl .
so , if you go through thing , you'll find that c of the le is equal to c of the le plu <num> , that's <num> increment .
and similar from thi line , we're go to get c of the is equal to c of the plu <num> .
down here , we're go to get c of <num> given <num> , <num> , <num> .
so , that's a length of two sentenc .
it's go to be c <num> , given <num> , <num> , <num> , plu <num> .
and final , c <num> , <num> , <num> equal c <num> , <num> , <num> plu <num> .
those ar all the count which ar increment base on delta , <num> , <num> , <num> be equal to <num> .
and then , we have a second set of count .
you're go to find that c of dog , chien equal c of dog , chien plu <num> , c of dog equal c of dog plu <num> .
and there's a coupl of more count here .
so , actual , eight count in total will be increment for thi particular train exampl .
and you can see that all thi is basic do is in a rather labori wai , make sure the count for the correct item , for exampl , for the align to le ar increment on thi particular exampl .
becaus the is a line to le .
okai .
so , just to recap , thi is meant to be sort of a veri explicit descript of the maximum likelihood estim paramet estim method and the maximum likelihood paramet estim under thi veri strong assumpt that we know the align on each of the train sampl .
so , the em algorithm is actual go to be close relat to what i've just shown you , but recal that we ar now go to assum that the train exampl do not have align .
so , each train exampl is an english sentenc , pair with a french sentenc .
we gain of n train sampl .
and critic , the align ar omit .
so , the em algorithm is go to take thi input , and again , output t and q paramet as it final output .
but it's go to proce in a slightli differ wai .
so , the first major differ from the algorithm that i just show you is that the algorithm is go to be iter .
we're go to start off with some q and t paramet .
for exampl , the first iter , thei might just be random paramet .
okai , so we start off with some random initi .
and then , at each step we're go to us these q and t paramet to calcul count .
in a moment , i'll describ how we do that .
and so , we're go to calcul count .
actual , base on , we have the data .
thi is the rk , fk pair .
and that , togeth , which our current guess at the paramet will give us some count .
and from those count , we'll reestim the q and t paramet .
so , we're go to have an iter method where at each , at each iter , we start with some valu for q and t and calcul some new valu for q and t .
so , the next step again , we're go to take these new q , t paramet and take our data .
and from those calcul count .
and from these , we calcul our q and t variabl .
so , we'll keep iter like thi until , in some sens , we've reach converg .
it's typic with these kind of model to run for mayb , i don't know , <num> to <num> iter .
it's fairli common for these ibm model .
okai .
so that's the basic idea .
we're go to have thi iter algorithm , random , random initi , and then , we recalcul the q and t paramet at each step go through thi process .
and the onli thing that's realli go to chang in how these count ar calcul is that , rememb , our data doesn't includ align variabl .
so , instead , we're go to us our q and t paramet to actual calcul these delta .
rememb , we had delta k , i , j is equal to <num> if a , k , i is equal to j .
okai , so we , these were indic function make us of the align variabl we had .
now , we're go to replac these delta with valu that ar actual calcul base on our current paramet on the data .
and i'll in , in a moment , talk much more about thi wai of calcul the delta , how it's actual done , what intuit behind it is .
okai , so now let's describ the em algorithm .
the input to the algorithm is a train corpu consist of sentenc pair f k is the kth french sentenc e k is the kth english sentenc .
and in the initi step , we're go to initi our t and q paramet , so some initi valu .
you might for exampl , choos random initi valu for these paramet .
then the algorithm proce as follow .
so , we take s , capit s iter over the data .
and as i said befor , s might be typic <num> to <num> for train these ibm model .
okai ?
at each iter , we first set all of our count equal to <num> .
at the end of the iter , we're go to recalcul .
the t and q paramet , base on the count that we calcul .
and in thi middl portion i'm go to describ how the count ar actual calcul .
okai , so we initi set the count equal to <num> .
and then again , we pass over everi train exampl .
so we might find exampl like the on i show you befor .
and we consid everi posit in the french string , and everi posit pair , sorri , everi , we consid everi posit in the french string pair with everi possibl posit in english string .
we have a loop over i and j .
and then again we have these updat to our account .
which ar actual ident to the algorithm i show you previous .
so we have updat where we have c or c plu delta .
the critic differ in how these delta ar calcul .
so again , rememb that befor , we had delta , k , i , j is equal to <num> if a , k , i equal j , <num> otherwis .
so in the veri lucki scenario where we have align , we can fill in these delta as either <num> or <num> depend on whether the align is in our train data or not .
of cours in the em algorithm we're not go to assum we have align .
an so instead , we calcul these delta valu base on our current paramet .
so i have an express here base on the qs and ts form our previou iter .
okai , so the qs and ts ar our common paramet valu .
we're go to be abl to calcul these delta , us thi express here , which i'll describ on the next slide , and we can us these delta to increment these count .
okai , so let's describ how to calcul these delta , us the express i show you on the previou slide .
and to illustr thi express i'm go to us a particular exampl so let consid the third french word a .
so let number these <num> , <num> , <num> , <num> , <num> , <num> , <num> and we have <num> , <num> <num> , <num> , <num> , <num> on the english side .
and , let's first consid delta <num> so thi is the 100th train exampl that's what thi <num> here mean .
and i'm go to consid posit <num> .
and let's first posi , consid , valu <num> .
which intuit correspond to , thi word a be align to the null word , which is the word zero .
so what doe thi express sai .
well , on the numer , i multipli two thing togeth .
firstli i have q of <num> given <num> , <num> , <num> .
that's becaus i'm align word <num> in the french to word <num> in english .
that's what i'm consid .
and then i have a t term , which is t of a given null .
so that's the numer .
so , basic i'm consid thi align to null and i'm multipli to get the associ distort paramet .
and also the translat paramet .
so now look at the denomin .
so thi is , thi whole thing is go to be divid by some express star .
let me write , star over here .
so start is actual go to be a sum of term where we're go to consid each of these possibl english posit .
and so thi is actual go to be q of <num> , given <num> , <num> , <num> time t of a given null .
plu q of <num> given <num> , <num> , <num> time t of a given and .
so thi is posit on of the english distort paramet and the translat paramet .
plu <num> of <num> given <num> , <num> , <num> time t of a given the .
and i basic go to through all possibl english word multipli in a q and a t paramet and that's how i calcul thi denomin .
so denomin is essenti ar go to be a normal constant which we'll see veri soon .
similarli delta <num> .
<num> , <num> is go to be equal to q of <num> given <num> , <num> , <num> time t of a given and .
and again we divid by star , the same sum .
delta <num> <num> , <num> is go to be q of <num> given <num> , <num> , <num> time t of a given , now we have a third word , sorri , we have the second word there , divid by star .
and so on and so on .
so we can fill in these valu .
delta , <num> , <num> , <num> .
delta , <num> , <num> , <num> .
delta , <num> , <num> , <num> .
and so on .
right the wai up to delta , <num> <num> <num> , and you can verifi that these differ delta sum to <num> .
so in some sens , thei're a fraction count .
thei sai thei , thei sum to <num> , and thei defin a probabl distribut .
over the differ possibl align for thi particular french word a .
so these delta actual have a veri , direct , probabilist interpret , which is follow , as follow .
delta k i j is the probabl that the ith french word is align to the jth english word , condit on the english sentenc e k , and the french sentenc k under my current paramet t and q .
so i'll write a semicolon here follow by t and q to mean that we're talk about probabl under the model defin by a common paramet .
so basic what we've done in thi algorithm is we hallucin these delta valu by calcul probabl of the align under the common paramet .
and recal we then us these delta in calcul the count which ar us to re estim these q and t paramet .
so to recap if we go back to thi algorithm again i'm go to show you thi diagram to remind you of thi .
we start off with some initi valu of the q and t variabl and we also have our data .
so thi consist of e k , f k for k equal <num> to n .
from these two input we calcul count and these ar often refer to as expect count .
and thei're expect count under thi previou model , under the previou q and t valu .
we do that by us thi definit of delta which make direct us of q and t in the previou iter and we calcul the count .
from thi we calcul new valu for the q and t variabl us these simpl express down here .
so that's the first iter .
and then in the second iter , we repeat thi .
so again , we have two input .
we have our data , togeth with our current paramet , paramet .
we calcul our expect count , and from those expect count , we re estim q and t .
and we just keep go with thi process .
and thi is refer to as the em algorithm .
or at least thi is an instanc of the em algorithm appli to these ibm translat model .
so let me briefli talk about the justif for thi algorithm .
there's actual much more detail about the algorithm and it justif in the note which ar post with thi class .
but let me give you a sketch .
so , as we said the train sampl ar e k , f k pair for k equal <num> to n .
and we can defin someth call the log likelihood function which is a function of our t paramet and our q paramet .
and thi is basic go to be a measur of how well our paramet fit our train exampl .
so higher valu for l mean that l , or that t and q valu do a better job of model the train exampl we're look at .
so how is thi defin ?
thi is simpli defin as the log probabl of the data .
or more precis a sum over all of my n exampl .
and for each exampl , i calcul the condit probabl of f k given e k under the model .
when i take it's log .
so thi is a log likelihood , it's basic log probabl , sum of log probabl .
and recal that under the ibm model , thi probabl ha the follow form .
where i have a sum over all possibl align .
i have p of f k comma a given e of k .
so i have the sum within the log .
and of cours , thi p thi probabl p is go to be some product of q and t term for thi particular exampl .
reflect e k , f k and the particular line that we're look at .
and so you can see that thi is clearli veri directli a function of our q and t paramet .
as we vari the q and t paramet , these probabl will vari .
and thi likelihood function mai go up or down .
the maximum likelihood estim ar then defin as the t and q pair that maxim thi function .
so , on the maximum likelihood estim , we would like to find the paramet that make the data as probabl as possibl .
in some sens fit the data as well as possibl .
where a measur of fit is thi log likelihood .
more formal , you can deriv mani formal guarante for maximum likelihood estim .
for exampl , show that thei .
converg closer and closer to the true upper length paramet as we get more and more train data .
now as it happen , thi likelihood function is quit nasti .
it's quit difficult to optim .
and you know , nice look function , will have a singl maximum .
thi is a convex function .
imagin we onli had a singl paramet , t here , and i have some function l of t .
thi is nice and well behav becaus it onli ha a singl maximum , and essenti ani hill climb method will in gener be guarante to get to the maximum of that function .
unfortun thi log likelihood function is not nearli so nice , so schemat it look rather like thi , where you might have mani differ point which ar local optimum .
so hill climb method is in gener veri like to get stuck .
at on of these non optim point .
again these ar schemat , where i'm show function where there's just a singl paramet .
in realiti these ar multi dimension function .
where i have t and q , we might have thousand of paramet .
and so these kind of surfac can becom extrem complex with mani , mani local optima .
okai .
thi make optim of the likelihood function veri hard .
in fact it's almost certainli provabl hard .
it's mp hard or , or someth similar .
the guarante for the em algorithm is that , you know , it defin a sequenc of paramet , we start q t and at each point we calcul a new q t and we keep go like thi for mayb a few ten or hundr of iter .
the em algorithm is guarante in the limit , to converg to on of these local optima of the likelihood function .
and so while that is in ideal , ideal we'd like to get to the global maximum .
in practic it work quit well and is a quit en effect algorithm .
on thing to note is that becaus of thi properti , the point you covert to is go to depend on your initi paramet valu , and so you have to sometim be care about how you initi your paramet .
there's a lot more discuss of that in the note which ar post along with thi class .
so final to summar some kei idea we saw in the ibm translat model .
realli the singl kei idea wa to introduc these align variabl , specifi how word in on languag ar align with word in anoth languag .
and second to make us of translat paramet .
for exampl the probabl that dog is translat to the word chien .
and also distort paramet .
for exampl the probabl of the posit on in french is align to posit two in english .
we saw thi paramet estim algorithm , the em algorithm .
so thi is an iter algorithm for train the q and t paramet .
it start off with some initi valu for q and t , and then recalcul them us the method i describ .
and we typic run thi for sever iter , or until converg .
and critic , onc i have recov these q and t paramet us the em algorithm .
i can go back to my train exampl and fill in align .
so earlier i show you how we can recov the most like align for a sentenc .
onc we have the q and t paramet .
and thi is actual how the idea model current us the machin translat system .
thei ar a critic compon in that thei allow us to recov these align in our train exampl .
and in the next lectur of thi cours , we'll talk about phrase base system .
and phrase base system ar go to make veri direct us of the align which ar recov us the ibm model .
okai , so in thi next segment of the class , we ar go to look at phrase base translat system .
phrase base translat system first came around in the late 1990s , and in some sens , thei're a second gener of statist machin translat system .
the first gener wa the ibm model that we saw in the last lectur .
phrase base system , build on the idea form the ibm translat model , thei make direct us of the align that we saw last lectur , but thei repres a signific step forward .
thei also ar wide us in modern research and also industri base system for translat .
okai , so here's a road map for what's come next in the class .
we'll first describ how to learn phrase , for what is often call a phrase base lexicon , from translat exampl , with the align we cover us the ibm model i show you the last time .
we'll then describ a basic phrase base model .
and then final , we'll spend some time talk about the decod problem in phrase base model and describ the decod algorithm for phrase base model .
so the critic idea in phrase base model is to automat learn a veri larg lexicon from data where the lexicon consist of phrase in on languag .
pair with phrase in anoth languag .
so here ar some exampl phrase .
on the left hand side , i have german , and on the right hand side i have english .
and each of these entri in the lexicon .
pair some sequenc of german word with some sequenc of english word .
for exampl , thi first entri sai that these two word in german can be translat it as these two word in english .
and notic in gener .
i can have on or more word on the german side and i can have on or more word on the english side .
and the number of word can differ .
so , for exampl , i have two word here in german correspond to three word here in english .
so these lexic entri ar go to form the basi of the translat system .
and , most importantli , thei go beyond the simpl word to word translat we saw in the ibm model .
so rememb in the ibm model we had paramet such as the probabl of thi word in french be a translat of thi word in english .
now were go to have probabl for exampl , of thi entir phrase .
of be a translat of thi english phrase .
so now were go to have translat entri or lexicon entri with multi word phrase on either side .
and thei will have associ paramet , which we'll also learn from data .
so , what we're first go to describ is how we actual learn these lexicon from a set of translat exampl .
so , as in the ibm model i'm go to assum that there is some veri larg set of train exampl .
for exampl , we might have german versu english train data .
where each exampl in the train data consist of a german sentenc pair with an english sentenc .
we might have sever ten of thousand or sever hundr of thousand of exampl translat like thi .
and given thi corpu , thi what we call a parallel corpu , or what is sometim call , in literatur , a bitext .
given thi input , we're go to describ method for learn a lexicon .
these lexicon can often be veri , veri larg .
thei could often have mani hundr of thousand or even million of entri in thi form .
so here's an exampl which illustr the basic idea of how phrase ar extract from translat exampl .
thi is taken from a tut , tutori by philip koehn and kevin knight .
so our train sampl in thi instanc is thi spanish sentenc .
align with thi english sentenc .
so these two sentenc in translat of each other .
and we can run the ibm model and get some kind of align .
so we might have for exampl mayb someth like thi .
okai we'll talk a lot in a moment about how we us these align in drive phrase , but onc we've deriv the align , we can start extract phrase pair from thi exampl .
and so , here ar some exampl .
so maria is identifi as a translat of mari .
we have thi correspond .
we have thi word correspond to the word witch , thi word correspond to green .
so these ar just singl word phrase .
we might for exampl , line the spanish phrase no to the english sequenc did not .
and then we might have longer phrase .
so , for exampl , we might have , the , the fact that thi sequenc of word , which is seen here , and the spanish , is align to thi sequenc of word , did not slap , in the english .
and here's anoth exampl .
so each phrase phrasal entri , each lexic entri , is go to pair some sub sequenc of spanish word .
with some subsequ sequenc of english word , we might have everyth , anyth from word to word correspond , just a singl word on each side , to much longer phrase , where we have multipl word in on languag align with multipl word in the other languag .
and we're basic go to run some algorithm over each of our train exampl in turn , and from each of these extract a set of lexic entri of thi form .
so you can imagin if we have a few hundr thousand train exampl , each train exampl consist of a pair of sentenc , we might end up with a quit larg set of possibl phrase .
so here's a quick recap of how we can us the ibm translat model to deriv align in our train exampl specif how ibm model <num> can be us in thi wai .
so rememb that ibm model <num> defin a distribut so m is the length of the french sentenc .
e is an english sentenc , so it's e <num> , e <num> up to e l where l is the length of the english sentenc .
for exampl we might have thi english sentenc here .
f is a french sentenc , consist of word f <num> , f <num> , up to f m .
and a is a set of align variabl .
so a <num> , a <num> , up to a m .
so condit on an english sentenc and a french sentenc length we have a distribut of all possibl choic of align variabl and choic of french translat .
now a veri us by product is that onc we've train the model .
right , onc we have these q and t paramet i show you last time , we can calcul the most like align for ani train exampl .
so given a english sentenc , a french or other foreign sentenc , in thi case a spanish sentenc .
and the length of the foreign sentenc , m .
we can search over all possibl align and find the align that is most like under the model .
and what will , thi will actual do is , for each foreign word we will find the most like align , where the align just specifi each foreign word which word in the english align to .
so in thi particular exampl we mai in thi exampl recov in the align i'll show you here as a reason align of these two sentenc .
okai ?
and so we can do thi to everi on of our train exampl .
onc we've learn these q and t paramet us the em algorithm , so we deriv q and t us em , we can go through our train sentenc on by on and recov these align .
notic that under these ibm model these align have the follow properti .
thei ar , for each foreign word , we have an align to a singl english word , so for each foreign word , we pick out a singl english word to which we're align .
it's go to be us to repres these align as what ar call align matric , and so here's an exampl , again , for thi english spanish pair that i just show you .
and so i have the english word down the column , of thi first column , and i have the spanish word across the top of thi tabl , and then i show a dot if there's an align , ookai ?
so thi dot mean that mari and mari ar align .
thi dot mean that no and not ar align .
thi dot mean that slap and daba ar align , and similarli , okai , una is align to slap , bof is align to slap and so on .
so we place an align , on of these dot in a cell in thi matrix if these two word ar align .
notic onc again that for each spanish word we have an align to a singl english word .
so for each spanish word , you see exactli on symbol , on dot in thi column .
so we have on dot here , on dot here , and no dot in ani other posit .
on dot here , on dot here , on dot here , and so on .
okai so again , we're obei thi constraint that foreign word is align to a singl english word .
so that wa a recap of how we can us the ibm model to deriv these align on train data sentenc .
in realiti , though , there ar two problem .
firstli , these align ar often rather noisi .
so , over much of these align look good , thei ar , thei often contain nois or erron align .
and secondli , these align ar mani to on .
what i mean by that is that , each word in the foreign languag is align to exactli on word in the english languag .
okai ?
so we mai have , we think about the english and the foreign languag , each word on the foreign side is align to exactli on word on the english side .
so mani word on thi side mai be align to on word on thi side , but we certainli can't , for exampl , have a particular english word .
or we can't have align like thi where multipl english word ar align to the same foreign word .
thi is rule out .
we have to constrain that each foreign word is like exactli on english word .
and thi isn't necessarili realist .
in mani exampl , you'll find that thi constraint is not realist .
so , a number of heurist have been develop to try to get around these kind of problem .
first is to try and make these align more robust , somehow reduc the nois , and secondli , to somehow get around thi constraint that the , the , the , align ar mani to on and , in particular , to try to move two align , which ar mani to mani .
so we might , for exampl , have align like thi where some foreign word ar align to multipl english word .
and similarli , some english word ar align to multipl french word .
so the , here's how thi is gener done .
the trick is that we can make the observ that the ibm model can be train in both direct .
so we can train a model for the condit of probabl of a french sentenc given the english , thi is what we've seen so far , but i can also just revers the two languag and train a model for english given french .
and we can find the most like align under each of these model in each of these two direct .
and now , we're go to have two differ align for each train exampl .
and the intersect of these align turn out to be a veri reliabl start point for the align process .
let me illustr thi with an exampl .
so here is the sentenc pair we've seen earlier , the spanish sentenc and the english sentenc , so f in thi case is spanish .
it's a foreign string and e is english .
and thi is the align i show you earlier deriv from ibm <num> model of f given e .
so notic that thi satisfi the constraint that each foreign word is align to a singl english word .
it's a singl point in each of these column specifi the align for each of foreign word .
here's the align from the revers model , so i can train an ibm model , <num> model for thi languag pair , but go in the opposit direct where i model the condit probabl of the english given the spanish .
and from that , i can again deriv the most like align .
and now , notic that thi align ha the constraint that each english word is align to a singl spanish word , so if i look at ani row of thi matrix , there's a singl dot in each row specifi the spanish word that thi english word wa align to .
so these ar the two align .
now , we can consid the intersect of these two align .
so at certain align point , for exampl thi on , ar seen in both align and thei constitut the intersect .
so there's thi on here , thi on here .
if we look further , we see thi on is also in both align .
thi on is in both align .
thi on is and thi on is .
so some set , subset of the point in these two align ar go to be seen in both , and that subset is the quot intersect align .
okai ?
now , in practic , what ha been found is that these intersect align tend to be quit reliabl , and so thei're veri us thei're a veri us start point for a process , which is then go to grow the align .
okai , so what we'll see next is that we'll see a process that consid , we put a cross by all the point which ar not seen in both align .
i'm go to just circl these .
okai , so these circl point ar in both align .
and then , there ar some point which ar in neither align .
these on , two , three , four point , sorri , ar in on align , but not the other align .
okai ?
so , what we'll see is that there ar method , which basic start with these intersect align and start ad back in some of these point .
so i don't want to go into detail about how thi process work .
but let me give you a sketch of thi .
we onli explor align point which occur in on align or the other align .
we add on align point at a time .
we onli add align point , which align a word that current ha no align .
and at first , we restrict ourselv to align point that ar neighbor , thei're adjac or diagon of current align point .
i'll try to post as , link to some paper on thi process if you want to read about it in more , in more detail in the class .
but here's an exampl of the kind of final align you might get us these heurist , which start with the intersect align and then add in some point in the union of these two align .
so thi is our final align matrix .
and notic , that the align is no longer mani to on , okai ?
there ar some english word which ar align to multipl spanish word .
and similarli , there ar some spanish word that ar align to multipl english word .
so to recap , the motiv for thi is these two problem .
on , that align ar often noisi , and two , that there ar mani to on .
and what we now have , hopefulli , is a set of mani to mani align which ar more reliabl than take align from simpli on direct or on model pfe or pef alon .
okai .
so , have deriv these align on each of the train exampl , the next step is to extract phrase from each train exampl .
rememb a phrase or a phrasal entri look like a sequenc of english word pair with a sequenc of foreign word .
so , thi would be an exampl .
so , the basic idea is go to be to extract , all possibl phrase pair , which ar consist with the align on thi particular exampl .
so , i could potenti take ani sequenc of word on the foreign side .
so , mayb these <num> word here and ani sequenc of word on the english side .
sai , these <num> word here and extract them as a phrase pair , as long as thei're consist with the align .
we'll talk in a second about what it mean to be consist .
so abstractli we're go to consid all possibl sub sequenc of word on the foreign side and all possibl subsequ of word name the side and consid all possibl parent of those two choic .
okai .
so , how do we check if a potenti phrasal entri is consist with the align .
so , let me show you on align which is consist .
and , thi is the phrase pair that pair maria no with mari did not .
and it's go to be us to visual thi by draw a rectangl in the figur correspond to thi particular pair of these two phrase .
okai .
so , a phrase pair is consist if three condit ar satisfi .
so , on , there ha to be at least on word in the english phrase e align to a word in f .
so , on of these three word ha to be align to on of these two foreign word , and actual we have multipl align .
okai ?
so , we definit satisfi thi first condit .
we actual have three case where a word on the , in the english side is align to a word on the foreign side .
the second condit is that there ar no word in f align to word outsid e .
okai ?
so , let's check thi on by on .
so , if we look at maria , we can see that all of it align ar to word which ar within thi phrase we're propos .
so , it's a line actual onli to mari in thi case , and that's within these three word that i'm consid .
and similarli , if we look at the word no , it's onli align to english word within thi phrase , and so thi second condit is satisfi .
it would be broken if there ar ani align point in these on , two , three , four , five , six , seven , eight cell down here .
so , for exampl , if we also had an align here , then no would now be align to an english word which fall outsid the subsequ , mari did not , in which case thi second constraint would be violat .
but of cours , that isn't present , so we're fine .
thirdli , it is a similar constraint sai that there ar no word in the english side align , align to word outsid f .
so , if we could check these english word on by on , so mari is onli align to maria , that's within the phrase we're consid .
did is onli align to no , it's within the phrase we're consid .
not is align to no , so it's within the phrase we're consid .
and so , thi phrase pair , mari did not , maria no is certainli consist .
and so , thi would actual be extract , thi phrase pair would be extract from thi train exampl .
let me show you on exampl of a phrase pair which isn't consist , so mari did , and mari no , so that's the pair here maria no and then mari did .
and we can fill in thi rectangl here .
and thi actual violat constraint <num> becaus thi foreign wood is align to not and not is outsid thi sequenc of english word mari , mari did .
and so , thi is not a good phrase pair and it would not be extract from , from thi exampl .
so , notic that a veri conveni wai of visual these phrase pair , or the consist on , is thei correspond to rectangl i can draw which surround these align and for which we have no align point in the row or column outsid of these rectangl .
so there ar no align point in these cell here , that correspond to constrain three , and there ar no align point in these column here that correspond to constraint number two .
so , thi is actual anoth perfectli well form phrase , did not slap , can be no daba una bofetada , and there ar mani other phrase .
okai ?
so , we can have verd align to green .
so , we can go through some singl word align on maria to mari .
bruja to witch .
okai , so those ar some singl word phrase or translat .
we can translat the as a la , that's anoth good on .
and then , there ar larger phrase .
so , if we look at thi rectangl here , we have bruja verd be align to green witch .
and if we look at thi case here , we have a la bruja verd .
i'm sorri for my spanish , i'm sure it's terribl .
and align to the green witch .
and so on and so on .
so , we get everyth from singl word translat to much longer sequenc .
for exampl , thi , these four spanish word align to these three english word .
and we do thi on everi train exampl , okai ?
so , from each train exampl , we're go to get an english sentenc pair with some foreign sentenc .
and we're go to extract a set of phrase from each train sampl .
and you can see how what we're go to end up with thi realli pretti larg lexicon of phrase , where the phrase consist of everyth from singl word correspond like bruja to witch , to larger string such as thi align here .
onc we've extract these phrase , we can also calcul paramet estim base on the phrase we've extract .
and thi is veri simpl , or at least thi is the simplest wai of do thi .
so for ani pr , ph , phrase pair fe , for exampl , f might be thi sequenc , e might be a singl word , slap .
we can deriv the condit probabl of f given e , as the ratio of these two count .
thi look just like the maximum likelihood estim we've seen elsewher in the cours .
so , i can count the number of time i've seen thi sequenc correspond to the word slap .
count the number of time i've seen the english word slap and take the ratio of these two term .
so , thi is the simplest possibl wai for estim paramet of these model .
it's a littl dubiou from a probabilist point of view .
i don't want to get into the detail of that but the fact that we have multipl overlap phrase on each train sampl make thi somewhat suspect .
but empir , when we actual perform translat , these paramet ar veri us , actual work quit well when us in a translat context .
here's actual an exampl from a tutori by phillip kern , from eacl <num> , which is a confer in natur languag process .
here's an exampl he gave for the french string den vorschlag , which mean roughli speak , the propos in english .
and what i'm show here , sorri , thi should be t not p .
these ar translat paramet .
is a list of phrase that ar extract .
so for exampl , on phrase is thi german sequenc of word pair with the propos .
and the probabl for thi , the paramet is about <num> . <num> .
we have thi funni look phrase here , but thi come from someth like their govern's propos .
in that case , thi ha been extract as a phrase .
that's where thi apostroph s come from .
we have a propos , we have the idea , we have thi propos , which would have the word propos , of the propos .
so , you can see we have a whole bunch of phrase with vari probabl .
these ar the probabl of each of these phrase and thei look quit reason .
thi is a real exampl extract from german english data .
so in the next segment of the class , we'll talk extens about what's call the decod problem in phrase base model .
but first thing , i just want to give you a sketch of how the phrase base model can be us in translat .
so for thi , i'm go to us a particular exampl where we have some german string , and we're go to see that we produc the english translat of thi , basic left to right .
we're build up the translat left to right on the english side .
and as we build up the translat left to right , we're go to incorpor variou school indic how plausibl thi particular translat is .
and in particular , there're go to be three type of school .
firstli , a languag model score .
secondli , a phrase model score , us these paramet t that i show you earlier .
and final , a distort model , which i will describ in more detail as we go .
so the basic idea in translat thi german sentenc is go to be to try to find a sequenc of phrase where we can translat , at each point , on or more german word into on or more english word , and therebi , build a translat left to right .
okai ?
so the first step we might sai oh , let's translat thi word , heut as todai , becaus i have some phrase or entri , which sai that these two thing ar a possibl translat of each other .
now , thi particular move , thi particular choic is go to have an associ probabl or cost .
we're go to have some languag model score q .
so thi is the languag model and thi is go to allow us to give a prior probabl for how like the english string that we're build .
thi would typic be a trigram languag model , and then we're go to have some translat model score , which ar go to be the t paramet i show you earlier .
and thei're go to reflect for exampl how like todai is to be translat as heut .
and then , final , we're go to have some paramet , which ar go to be what ar call distort paramet and which ar go to penal us from skip over larg sequenc of english of german word .
thei're go to try to enforc the constraint that the word order is in the two languag ar somewhat relat to each other .
we'll talk about about thi as we go forward in the class .
so i could multipli these q and t paramet togeth , and i could search over all possibl translat , and try to find the maximum translat under thi score .
but actual , it's a littl more conveni to sum the log paramet , we're go to take the sum of log q plu log t , plu , then we'll see thi is term .
okai .
so thi particular move is go to have a languag model score .
firstli we have a probabl of todai under a trigram languag model , so thi q paramet is go to be a trigram paramet .
what's the probabl of todai given that we have these two star symbol ?
these ar just the same old star symbol we've seen from languag model in the veri first lectur of thi class .
secondli , we have the probabl of gener the word heut , given todai .
and final , thi is go to be some cost penal the number of word we skip over .
now , becaus both of these phrase ar at veri , veri start of the sentenc , we defin the number of word we've skip over to be <num> .
is go to be some paramet that's typic neg .
it might be minu <num> for exampl , which is go to penal case where we skip over larg number of word .
and thi is go to be the onli control we have over differ word order of these two languag .
it's extrem crude and make littl sens linguist , but it doe give you some control over word order .
as i said , we'll talk later in the class about how to improv upon thi , but thi is how we'll start .
so that's the first step to make thi translat .
so , in the second step of the translat , we might choos the follow phrase .
so we might see that we have thi german sequenc of word , translat as we shall be .
sorri , thi should be we shall be .
and again , we see languag model score , we see phrase model score , and distort model score .
so the languag model score , again , we're us a trigram languag model .
we have the probabl of we given start todai , those ar the , the two previou word here .
we have the probabl of shall , given we in todai .
and final , we have the probabl of be given we and shall .
so on q paramet for each of the english word that we've chosen .
on the phrase model side side , we have log of t of thi german string , then thei're given thi english substr given we shall be .
and final , we have a distort cost .
and again , notic in do thi , we haven't skip over ani german word .
we've , we've , we've chosen the next phrase complet adjac to heut , which is the the , the current translat , that's right , the current translat ha translat heut .
and the next phrase is adjac to thi .
and so we've skip over <num> word and so thi cost is <num> .
so here's the next step of the translat .
we've decid to us a phrase entri , which sai that thi word in german , which is seen right here at the end of the sentenc correspond to the word debat in english .
and , so , we can make thi translat step again build up anoth word in the english translat .
we again have three score a languag model score , phrase base model score and a distort model score .
the languag model score is simpli a singl term , becaus we have a singl word be gener .
the probabl of debat , given that the previou two word ar shall and be .
the phrase model score sai what's the probabl of thi german word , condit on the fact that we have the english word debat .
and then , final , we have someth more interest go on in the distort model here .
so rememb , that is again go to be some neg valu , sai minu <num> , which is us to discourag long rang re order in these model .
and we have six here , and that's becaus i have skip over on , two , three , four , five , six word , and so , thi word incur a cost of minu <num> time <num> .
thi would be minu <num> .
so that's a fairli heavi penalti in thi case for skip over sever word and thi process will continu .
so mayb at the next step , we'll choos these three german word to be translat as the re open .
and again , thi is go to have a languag model score , a phrase base model score , and distort score .
and then , final we would translat , we might translat thi german sequenc as be the sequenc of the mont blanc tunnel .
so the final translat is , todai we shall be debat the reopen of the mont blanc tunnel .
we've done thi us a sequenc of phrase .
first thing we had todai , first thing we had todai , then we had , i think , we shall be .
and then debat , and then the , the reopen , and of the mont blanc tunnel , and we've basic deriv these phrase us our phrasal lexicon .
and we've order these phrase in some order left to right .
so that's just a sketch .
in the next lectur , we will go into consider depth of how we actual decod with these translat model , but thi is the basic process .
a translat is go to involv pick a sequenc of phrase like thi .
each choic is go to involv a languag model score , a phrase score , a distort score .
and we're actual go to try to search over the space of all possibl translat .
we're go to search for the translat with the highest score under the combin of these three paramet type .
okai , so in thi next segment of the class , we're go to talk about how to decod with phrase base translat model .
that is , how we actual appli phrase base translat model to new sentenc to produc translat .
so to recap , the critic ideal in a phase base model is the idea of a phrase base lexicon .
so , a phrase base lexicon contain phrase entri f , e .
so these ar pair , where each f is a sequenc of on or more foreign , foreign word , and each e is a sequenc of on or more english word .
so , if we look at thi particular german sentenc , we shall us as an exampl throughout these slide .
and sai we want to translat that into english , then the phrase base lexicon is go to provid variou entri which mai be relev to thi particular sentenc .
so here ar some exampl entri in the lexicon , we might sai , these two word in german correspond to we must or mayb these three word in german correspond to we must also .
so we have the first two word here and the first three word here .
thi singl word in german , which appear here , might be translat as serious , so these ar three entri from the phrase base lexicon .
in practic , a lexicon might have , you know , million of entri .
and we saw in the last lectur segment how we can extract these kind of lexicon from larg quantiti of translat exampl , by first run the ibm translat model to deriv align , and then base on those align , pull out these phrase base entri .
i'll us g of f , e to refer to the score for lexicon entri .
so each of these ha a score .
and i wouldn't mind have minu <num> here , minu <num> here , minu <num> here .
so for exampl , thi might be the log of the ratio of account .
thi is veri similar to maximum likelihood estim , thi could be thought of as t of f given e with the condit probabl of the foreign sequenc of word , f condit on the english sentenc sequenc of word e ratio of these two count .
and we're go to take the log of thi as we saw in the last lectur , it's conveni to take log , and then , we'll start sum these differ score for differ phrase us in the translat .
so throughout thi lectur , i'm go to consid phrase base model consist of three thing .
so firstli , a phrase base lexicon consist of f , e pair , exactli as i just show you , the second thing we're go to make us of is a trigram languag model for the languag which we're try to translat into .
so throughout thi lectur , i'll assum that we're try to translat from german into english .
and so , we have these phrase entri as on compon .
the second compon is a trigram languag model for english .
so we have a languag model , for exampl , a trigram languag model for the languag into which we're try to translat .
so thi will have paramet exactli as we saw in the first lectur of thi class , for exampl , paramet such as the probabl of also , given that the previou two word were we and must .
and final , we're go to have a distort paramet .
thi is a singl paramet , thi is typic neg .
and as we'll see , thi will penal thing from move too far in the translat process , when we start to us these phrase to translat particular exampl .
so that's essenti it .
these ar the three compon of a phrase base translat model .
the littl bit more about the trigram languag model .
so these paramet can be train on veri larg quantiti of english text .
and the trigram languag model is go to be invalu in provid a prior distribut over which sentenc ar versu aren't like in english .
so , here's some notat i'll us throughout thi lectur for the given sentenc that we're try , try to translat .
we can extract a set of possibl phrase that ar applic to thi particular sentenc , and it'll be us to us the follow notat for these phrase , s , t , e .
so s is go to be a start point of the phrase , and t is go to be the end point of the phrase , and e is a sequenc of on or more english word .
so on phrase that is applic to thi particular exampl is <num> , <num> , we must , so let's number these word .
and that sai that these two word , wir mussen , can be translat as we must .
okai , so word <num> to <num> inclus can be translat as we must .
okai ?
so i'm go to us capit p , script p , to denot the set of all possibl phrase for a particular sentenc , okai ?
so , these ar go to be deriv through entri .
thi come from the fact that we have some phrase entri in our lexicon , sai that these two word can be translat as we must .
and becaus , these two word , wir mussen , appear here we can appli thi entri .
it's just go to be a littl bit more conveni to think of these phrase onc we have a particular input sentenc as have a start point , an end point and an english string .
so p is go to be the set of all possibl phrase for a sentenc .
so thi is go to be a set .
so thi might , for exampl , includ <num> , <num> , we must .
it might includ <num> , <num> , we must also , sai that these three word can be translat as we must also .
it might includ an entri sai <num> , <num> , is serious , that would come from an entri sai that the word ernst , which is sixth here can be translat as serious , and so on , and so on .
thi set might again , be quit larg , and notic that mani of these phrase will overlap .
for exampl , here , i have an entri for word <num> through <num> , and here , i have an entri for word <num> through <num> .
so in summari , big p is just the set of all possibl phrase for an input sentenc , where a phrase is a start point and an end point and an english string .
so for ani phrase p , so let's sai p is equal to <num> , <num> , we must .
i'll sometim us the follow notat .
so , sp , tp , ep ar it three compon .
so s of p is <num> , t of p is <num> in thi case , and e of p is equal to we must .
okai ?
and so these three function ar just pull out the three differ compon of the phrase .
and then , final , g of p is go to be the score of the phrase , so thi might be minu <num> .
and that will come from the origin lexicon , so if we have thi entri , entri sai wir mussen , we must , and g of thi is equal to minu <num> .
rememb , thi is typic log count of thi whole phrase divid by count of the english .
thi is just sai that's the score from the phrase in the tabl , okai ?
so again , p is go to be the set of old phrase for the sentenc , each phrase is go to have some score , some probabl .
and we can easili calcul the set just by take our phrase base lexicon and appli it to the particular input sentenc that we're try to translat .
okai , so now again , when we have a particular input sentenc we're try to translat , we're go to defin the idea of a deriv which is basic just a sequenc of phrase drawn from the set p .
okai , so a deriv is just a sequenc of phrase , p <num> , p <num> up to p capit l .
where l can be ani constant integ .
okai so these , these sequenc can be ani length .
on the next slide , we'll talk about constraint on these deriv , which make them valid or invalid .
so here's on exampl deriv .
so thi is sai , to translat the exampl that i've just shown you we can choos firstli a phrase which sai that we translat word <num> through <num> as we must also .
next , a phrase translat word <num> through <num> inclus as take .
then we have <num> <num> as thi critic , and <num> <num> is serious .
okai , so a deriv is just go to be a sequenc of phrase .
and these phrase ar go to defin a translat in the obviou wai .
so i'm go to us e of y , just to pull out the english string .
the translat underli thi particular exampl , and that's taken just by concaten these english string , so we must also , follow by take , follow by thi critic , follow by serious .
okai ?
so , in short , a deriv is just a sequenc of phrase where each phrase is again , an s , t , e tripl .
s is a start point , t is an end point , e is an english sequenc of word .
so , let's now look at what make a valid deriv , for an input sentenc that we're try to translat .
so for ani input sentenc x , which consist of n word , x <num> through x n , i'm go to us y of x to refer to the set of valid deriv for that sentenc .
so thi slide is go to give the abstract definit .
on the next slide , i'll give them an exampl illustr these idea .
so y of x is go to be a set of all finit length sequenc of phrase .
so p <num> , p <num> , up to p l , where l can be ani length .
where we satisfi the follow constraint .
so firstli , we sai that each phrase , rememb each phrase is a tripl s t and then e , which is a sequenc of english word .
so each of these phrase ha to be a member of the set of phrase for x <num> through x n .
so each phrase ha to be valid under the phrase , base lexicon .
second constraint , is kind of obviou , it just sai that each word in the input sentenc ha to be translat exactli onc .
and the third case , is a littl more interest .
so again , thi is an abstract definit , we'll give an exampl on the next slide illustr it in some detail .
but phrase base system make us of someth call a distort limit , and thi is go to be a hard constraint on how far phrase can move .
so a typic valu might be littl d equal <num> .
thi is the distort limit .
and , what thi is basic sai , rememb t of p k is the end point , of the k phrase .
and s of p k plu <num> is the start point of the next phrase .
okai , so we might have a sequenc of two phrase , we must , and then we might have <num> , <num> , also , or someth .
and we're go to look at thi endpoint , that's t of p k , and thi start point , and make sure that those two valu aren't too far from each other .
so we're go to have a hard constraint , over how far phrase can move .
and so thi is take the absolut valu of t of p k plu <num> minu s of p k .
so in thi case it would be the absolut valu of <num> plu <num> minu <num> , and thi is equal to <num> in thi case , so thi distort would be <num> , which is less than or equal to <num> , and so we've satisfi the distort limit .
okai , so , intuit , these two phrase ar right adjac to each other .
i've translat first word <num> and <num> , and then word <num> and <num> , and <num> is adjac to <num> .
and so , the distort for thi particular pair of phrase is <num> , and that satisfi the distort limit .
in addit , we have a constraint at the start of the sentenc , sai the start of the , the first phrase , <num> minu the start of the first phrase ha to be less or equal to d .
okai , so we come to thi an exampl of thi in a moment .
the reason for thi distort limit is realli twofold .
firstli it's go to reduc the search space of possibl translat , and that will make translat more effici .
and second empir , it's been found that have a distort limit that's form , can veri often improv translat perform , prove that these phrase ar move too far the translat tend to be rather poor .
the flip side of that is that thi is realli an incred crude , wai to control reorder between differ languag .
differ languag have differ word order .
and we're simpli impos a limit , sai we're onli allow two languag to vari in word order by thi hard limit , d equal <num> .
at the veri end of thi lectur , i'll talk about , more recent method , or altern approach that try to build more sophist model of thi reorder effect .
okai , so let's illustr these definit with an exampl .
i'm go to show you some differ possibl deriv for thi exampl sentenc , and were go to check if thei're valid or invalid .
so here's a first possibl deriv .
so we first , you know , intuit , we first translat wir missen auch as we must also .
and secondli we so these , let's number these word <num> , <num> , <num> , <num> , <num> , <num> , <num> .
the second phrase sai let's translat word <num> to <num> inclus .
that's thi singl word , nehmen , and we'll translat that as the word take , and then we translat word <num> and <num> , as thi critic .
and final we'll translat word <num> , as serious .
okai .
so thi is our final translat which is actual a pretti good translat of thi german input .
we must also take thi critic serious .
and it's been produc through thi deriv , thi sequenc of phrase .
so thi is actual a valid deriv and we can check that .
so , on we need to check is , each word translat onc , or exactli onc .
that's the first thing we need to check , okai .
and if we look , we can see that , sure enough , everi on of these word is translat exactli onc , and so we satisfi that constraint .
the second constraint is the distort limit .
and so , that's a littl bit more complic let me go into that in some detail .
so , we're go to assum , that that distort limit d , is equal to <num> .
and thi is how it goe , so , i'm go to check the end point , of each phrase , and the stop point of the next phrase , so <num> and <num> , and i'm go to calcul the valu for <num> plu <num> minu <num> .
if you rememb , what we want wa t , the magnitud of t of p k plu <num> minu s of p k plu <num> , is less than or equal to d , where k p k is the kth phrase , p k plu <num> is the k plu <num> phrase , t is the end point of a phra phrase , s is the start point of a phrase .
so if i compar <num> and <num> , i calcul thi .
thi is equal to , thi is the absolut magnitud , thi is the absolut magnitud of <num> plu <num> is <num> minu <num> is minu <num> , which is equal to <num> , and that's less than or equal to <num> .
okai ?
so , these two phrase ar essenti close enough to each other .
that's what thi is sai .
we go through term by term .
so next i compar <num> to posit <num> .
again , take the end point of thi phrase , to the start point of the next phrase .
and i got <num> plu <num> minu <num> .
and that is actual equal to <num> , which is less than or equal to <num> , so good , that's also valid .
next on i check is <num> follow by <num> .
the endpoint of thi phrase , the stop point of thi phrase , so i look at <num> plu <num> minu <num> .
thi is actual the magnitud of <num> which is clearli less than or equal to <num> .
it's <num> becaus these two phrase actual ar consecut in the sentenc .
so <num> , <num> , and <num> ar consecut .
and then final , i look at thi veri first posit .
so rememb , i also had magnitud of <num> minu s of p <num> is less than or equal to d .
so i have <num> minu the start point of the first phrase is just <num> .
so thi is magnitud of <num> , is less than or equal to <num> .
okai , so in short thi deriv satisfi , firstli , thi constraint that each word is translat exactli onc , and second , that none of these phrase move too far that the distort limit is satisfi .
okai and the distort limit is calcul by look at the end point of each phrase and start point of next phrase , and make sure that thei ar not too far from each other .
okai , so here's a second deriv , which we can , we'll see we can rule out veri quickli as be invalid .
and so again , thi is a sequenc of phrase where each phrase consist of a start and end point in the sequenc of english word .
the problem with thi , of cours , is that these two word have been translat twice .
so let's just give a count of how mani time i've translat each word .
i think it were , <num> as onc , <num> and <num> as onc , <num> as onc , and <num> is zero time .
and so we've clearli , you know , thi is a fail deriv .
thi is not a good deriv , becaus it doesn't satisfi the constraint that each word is translat exactli onc .
here's a fine , final exampl .
and so we'll again check the two constraint with thi particular deriv .
so the first constraint is , you know , exactli onc .
each word translat exactli onc .
and in thi particular case you can see that that's the case , if you look at these phrase , everi word is translat exactli onc in thi exampl .
and then the second question is , what about the distort limit ?
and so , let's go through us the procedur i just show you to check thi .
so firstli let's compar <num> and <num> .
and again , i'm go to assum that d , the distort limit , is equal to <num> .
so <num> and <num> .
so i take the magnitud of <num> plu <num> minu <num> , which is the magnitud of <num> , so minu <num> is less than or equal to d and so that's fine .
so the , thi first pair of phrase , satisfi the distort limit .
let's look at the second pair though .
so here i compar <num> to <num> , and here i have <num> plu <num> minu <num> , thi magnitud , thi is <num> .
and is that less or equal to d ?
no .
so we've immedi seen that thi pair of phrase violat the distort limit .
there's too much of a dist distanc between posit <num> , and posit <num> , under thi definit .
so thi is actual an invalid deriv .
so in spite of satisfi the constraint that each word is translat exactli onc , we violat the distort limit , and thi is rule out .
so just a final word on thi idea of the set of possibl deriv .
so , rememb y of x is the set of valid deriv for a particular input sentenc x .
thi will in gener be exponenti in size .
or a littl more precis the number of element or the number of valid deriv is go to grow exponenti fast with respect to the sentenc length .
ok .
that's a usual observ .
so thi is go to be a veri , veri larg set of possibl deriv .
okai , so we're almost there with the definit of the decod problem of phrase base model .
the final definit is the follow .
so , for a given sentenc x , again x is go to be some sequenc of word x1 through xn , thi is go to be some german sentenc in our particular exampl .
we gener from thi a set of valid deriv , so each of these is go to be a sequenc of phrase .
okai ?
and the set , as i said befor , could be extrem larg .
and the translat problem is go to be to try to find the deriv in the set which ha the highest score on the subfunct f of y .
okai ?
and fy is roughli speak someth like log p of y .
p of x given y , where thi is a trigram languag model and thi is the translat model .
i sai roughli speak becaus it's actual question whether phrase base model as we'll defin them realli rigor defin a model of thi form , but that's roughli the intuit , okai ?
but a good translat , rememb y is go to be a deriv , is go to both have good probabl under a trigram languag model and also have good probabl under thi , thi translat model .
but let's be absolut explicit about what f of y is .
so , if we score a deriv , we're go to take into account three term .
and again i'm go to give the abstract definit here .
we'll go into thi in consider detail on the next slide with an exampl .
but the three term ar the follow .
so firstli rememb e of y is the , the sequenc of word in the translat , so thi could for exampl be we must also take these critic serious and h of e of y , is just go to be the score under a trigram languag model .
okai ?
so , thi score h is go to score our english sentenc .
sorri .
thi is mayb not so clear , but thi is an english sequenc of word .
thi is go to be , actual , the log probabl under a trigram languag model score .
okai second term .
so now , rememb our deriv y consist of a sequenc of phrase p1 , p2 , up to pl .
each of these phrase is go to have some score and so g of p1 , for exampl , might be the score of the first phrase , might be minu <num> or someth .
and we're go to sum up the score of the individu phrase , okai ?
so that's the second part of the of the score , and then final we're go to have what is call a distort score .
so , in add , in addit to the strict limit on distort distanc , as i show you on the previou slide , we ar actual go to have a cost associ with distort .
so , we're go to take eta time thi distanc that we calcul befor , basic the distanc between each phrase and the next phrase , and eta is some paramet that's typic neg , which penal re order .
so , it penal larg valu for thi differ between two phrase .
so , eta is typic neg .
it might take some valu like minu <num> or minu <num> or someth like thi .
and it's typic chosen to optim translat perform , okai ?
so , there ar variou wai to choos eta in a wai that optim translat qualiti .
okai .
let's look at how thi express is calcul us a particular exampl .
so , thi is the definit of f of y .
and the decod problem is go to be to try to find the deriv that maxim the sum of these three differ score .
okai .
so , let illustr thi with an exampl .
thi is our input sentenc .
thi is on possibl deriv y .
and i'm go to calcul f of y , which is go to be the score for thi particular deriv .
okai .
so and as i said , thi is go to consist of the sum of <num> term .
a languag model score , so at first we have the languag model .
secondli , we have what ar call the phrase score .
and final , we have the distort score .
okai .
so , let's go through these in turn , to firstli , the languag model .
so , i'm go to have a sum of term .
so firstli , i'm go to have log of q of we given star star .
and then i'm go to have log of a must given star we , becaus must is the second word in thi translat .
and then log of also , given we must .
and then log of take given must also , and so on , and so on .
so , thi is becaus , under thi deriv , the translat is , we must also take thi critic serious .
and so i have basic a sum of term .
in each case , i take the trigram paramet .
i take it log and i sum it all togeth , all these log .
okai .
so , that's the languag model score .
and again , thi is go to reflect how like thi final translat is as a sentenc of english .
so , we're make critic us of a languag model and score thi deriv as to how plausibl a sentenc of english it is .
so , that's the first compon .
now , let's look at the phrase score .
so , now i'm go to have a set of term , so g of <num> , <num> , we must also plu g of <num> , <num> , take plu g <num> , <num> , thi critic and so on and so on .
so rememb g wa a function that take a phrase and give it a score , typic a log probabl .
and again , we're go to have on of these g term for each of these phrase .
so , the <num> , <num> , <num> , <num> phrase , we're go to have four g term .
so , that's the phrase score .
final , let's look at how the distort score score for thi particular exampl .
okai .
so , i'm go to go through each phrase in turn look at how far it is from the previou ph , phrase .
the first thing we're go to do is look at the start point of thi first phrase .
and just look at on minu on .
that's absolut valu time eta .
so that correspond to , that's the first term , which is basic eta time <num> minu s of p1 .
so thi is penal how far the first phrase is from the phrase start of the german sentenc , thi is actual zero in thi case .
and then let's look at the next term .
so , now i'm go to compar <num> to <num> , the end of thi phrase to the start of thi on .
and i'm go to take <num> plu <num> minu <num> , take it absolut valu time eta .
okai .
and then i'm go to look at <num> versu <num> .
so , i'm go to look at <num> plu <num> minu <num> time eta and <num> versu <num> .
so <num> plu <num> minu <num> time eta .
and again , eta might take some neg valu , for exampl eta equal minu <num> , typic chosen to optim the perform of the translat model .
so , you can see that these term ar basic go to give penalti whenev we see a distort that's nonzero .
so thi is zero , thi is zero .
i think thi valu is <num> and thi valu is <num> .
and so , the final cost in thi case is <num> time <num> eta , as the final distort cost of thi particular exampl .
okai , but to jump to back , back to the high level , we have a sum of three differ type of score , languag model , phrase score and distort score .
the languag model score how like the result translat is as a sentenc of english .
the phrase store , score can be thought of a measur how well these english phrase match the underli german .
for an exampl , how well we must also match wir mussen auch .
and then final these distort score penal phrase move long distanc in these translat .
so if we go back to thi problem .
so thi is the translat problem , we have an input sentenc x , we're go to search through all possibl deriv for the sentenc .
each on ha thi score , so f of y is equal to the score i show you befor .
and we're go to try to find the highest score deriv under thi score , okai ?
so , again , thi set y of x is typic exponenti in size .
and so , certainli the , the brute forc method of explicitli enumer everi possibl deriv , then score it , is in no wai go to be feasibl .
so , the next thing we're go to do is develop an algorithm for thi problem .
now , it's worth note , thi problem is actual veri hard , it's almost certainli an np hard problem .
and , that is go to mean that we're go to have to develop a heurist method of , for solv thi problem , base on beam search .
so we'll see how we develop an approxim search algorithm for thi problem , but an algorithm which realli perform quit well in practic .
so , a first kei idea in thi algorithm is go to be the idea of a state .
so a state is a <num> tupl .
here's an exampl .
we might have a state , we must , and then , sai we have a sentenc of length x <num> , x <num> , up to x <num> .
the first two element of thi tupl ar two word , we must , essenti correspond to the last two word for translat , we'll see again with an exampl in the next slide how thi work out .
b is go to be a bit string of length of , the , the of the same length as the input sentenc .
so , we might have thi bit string here .
r is go to be some integ , and alpha is go to be some score .
okai , so what ar these state intuit correspond to ?
thi is sai , i form a translat end the word we must .
and , which translat word <num> and <num> in the sentenc , but none of the other word .
so thi bit string is go to record , keep track of which word in the input sentanc have actual been translat .
thi posit <num> , is go to be the endpoint of the last phase .
so , for exampl , we might have us the phrase we must , as the last phrase in build the state .
and thi record thi .
thi is go to allow us to enforc the distort limit and also to calcul distort score .
and then final , thi is go to be some score , indic the score of the partial translat that ha reach thi state .
the initi state is alwai go to be the follow .
it ha star , star .
so these ar the usual start symbol in our languag model , and thi just reflect that the start of the translat alwai start off with star , star .
we have a bit string specifi that none of the input word ar be translat , so it's all 0s , seven 0s in thi case assum the input is of length seven .
we have <num> as the endpoint of the previou phrase , becaus we're at the veri start of the sentenc .
and we have <num> for the score becaus thi partial translat hasn't us ani phrase yet , and so the score is essenti <num> .
so it's go to be veri us to think of the search space in thi translat problem , as essenti , a direct graph , with the state i've shown you , on the previou slide , as the mode in thi graph .
let me explain what i mean .
so let's assum that thi is again the sentenc we're try to translat , and what i've shown you here is again , the start state .
so we have star , star , we have seve 0s specifi these seven word , none of them have been translat at thi point .
we have <num> , thi is the endpoint , of the last phrase , and <num> is the score .
so given thi state , i can choos the first phrase in the translat .
so let's , for exampl , assum we choos thi phrase here .
so we translat the first two word as we must .
i'll label thi arc with thi choic of phrase .
and here's the state we end up with .
so it's go to be , we must .
now i'm go to have the follow bit string , specifi that the first two word have been translat .
the other on haven't .
i'll have <num> , becaus that's the end point of thi phrase here .
and i'll have some score like minu <num> , for exampl .
okai ?
so these arc ar go to be label with choic of phrase , and thei're go to leav from on state to anoth .
intuit , thi arc correspond to the choic of thi phrase <num> , <num> , we must , at the veri start of the sentenc .
let me talk a littl bit about how the score is calcul .
so in thi case it would be the follow .
it would be a combin of the languag model score , the phrase score , and also the distort score .
so we'd have languag model score , which were log q of we , given star , star , plu log q must , given star , we .
so , we have on of these log q term for each word we and must .
and then we have g of <num> , <num> , we , must .
recal that that's the score for thi choic of phrase , for cor , match we must with wir mussen .
and final , we have ader time the distort .
and in thi case , the distort is where we have the previou phrase end at point <num> , we have <num> here , so we have by the usual rule <num> plu <num> minu <num> , and thi is actual equal to <num> , so there's no distort cost here .
and , that sum of term is go to give us thi score here .
so thi is basic the score sorri , it's <num> plu that .
so we start with a score of <num> , and then i've ad in these term correspond to thi transit .
now , there might be sever other possibl , so here's anoth on .
so we could also sai <num> , <num> , we must also .
so now we have an auch correspond to the choic of translat the first three word , as we must also .
and thi would lead to a state with must , and also , as the last two word .
a bit string specifi that the first three word were translat .
an endpoint of <num> becaus thi phrase end at point <num> , and then it gain some score , which in thi would have three log q term , on for we , on for must on but also .
you'll have a g term of <num> , <num> , we must also , and in fact the distort cost will again be <num> here .
here's a third exampl .
anoth choic might be the follow , <num> , <num> , <num> , so we might sai <num> , <num> , also .
so that correspond to translat thi word auch as also .
and , thi would lead to the state star , also .
and we'd have <num> specifi that thi third word had been translat .
we would have <num> , becaus thi end in point <num> , and again , some score minu , mayb minu <num> or someth .
okai .
now , each of these state can also have outgo on .
i should also sai at thi point there can be mani , mani more .
essenti , we have on auch for everi possibl phrase that could come at the start of a sentenc .
so these phrase will also have outgo auch's .
so for exampl , we might have the follow .
so we might then sai let's , <num> , <num> , <num> , <num> , <num> , let's translat word <num> and <num> as thi critic .
okai , so that's anoth choic of phrase , and that would actual lead to a state which is thi critic , and for them we would have <num> .
we would have <num> as the end point , and we'd have some new score , mayb minu <num> .
that new score is calcul as thi score plu variou term , languag model term for each of these two word in the phrase .
again a score for the phrase itself and a distort score and so on .
and similarli , these differ state will also have outgo auch .
okai , so , if we think of thi entir graph , where for each state we list all possibl outgo auch , all possibl phrase that can follow the state , we can have a veri larg search space .
so everi path through thi direct graph is go to correspond to a translat , and the final state in these translat will look like the follow .
we might have someth like critic , serious , so some choic of two word at the end of a sentenc .
critic , the bit string , at these final state , is go to be complet , specifi that we've complet all the word in the sentenc .
we might have some end point , for exampl <num> , and some minu , some score , <num> .
okai .
and so the translat task can be visual as try to find a path to on of these end , end state , that ha the , the high score .
we're try to find the end state as the highest possibl score .
so that's a us wai to visual the search space .
it's import to realiz that thi graph is exponenti in size .
why is that ?
well if we take ani on of these bit string , there ar <num> to the n possibl bit string .
so , there ar go to be an exponenti number of note in thi graph .
and so that mean that we ar not go to be abl to explor thi graph , exhaust .
but , thi is a veri us pictur to have in mind , a graph , a direct graph where we have state at the , at the node , and these outgo auch ar label with choic and phrase .
and at each point , the phrase combin with the previou state specifi the next state in thi graph .
so , a littl bit of notat to reflect thi .
so for ani state q , for exampl star , star <num> , <num> , <num> , <num> , <num> , <num> , <num> , <num> , <num> .
and so if thi is q , we'll defin p h of q , to be a function that return the set of possibl phrase that can follow thi state .
so there might be thing like <num> , <num> , we must .
<num> , <num> we must also .
mai have <num> , <num> , also , and so on , and so on .
so , each of these phrase ha to satisfi two differ constraint , okai ?
so , for on thing , we've gotta make sure that we don't translat the same word twice .
and so , the phrase p , so if p is a member of the set p h of q , p must not overlap with the bit string b .
okai .
so in thi case the bit string is empti .
and so it , ani of these phrase will be fine , becaus , for exampl , word <num> to <num> ar certainli not translat .
but if we consid a differ state , so , sai we take q equal we must , and then <num> <num> .
so , we have the state reflect the fact that the first two word translat someth like thi .
then the state we , sorri the phrase .
start again , the phrase <num> , <num> , we .
thi is mayb on possibl phrase , is definit not in p h of q .
so it's not a phrase that can follow thi , becaus it would mean that we translat word on again .
so you can see now how these bit string come into plai .
thei constrain the set of possibl phrase you can choos at each point .
and particularli , you can onli choos phrase which translat new word .
okai .
the second condit is the distort limit must not be violat .
so rememb in each of these state , we store the end point of the previou phrase .
so for exampl thi <num> , mean that the last phrase i chose end at point <num> , and that allow us to immedi check the phrase distort .
so , we can , for exampl , follow thi phrase , choos someth like <num> , <num> , also , becaus <num> plu <num> minu <num> is equal to <num> , which is less than or equal to , d , <num> rather .
let's sai the distort limit is equal to <num> again .
on the other hand , if we take some phrase like i don't know <num> , <num> serious .
okai , then in thi case , if we look at the distort , we have <num> plu <num> minu <num> , that is equal the magnitud of minu <num> , which definit violat thi constraint , and so thi phrase would not be allow to follow thi state .
okai .
so , to summar , two constraint , ani phrase in a set p h q , rememb thi is the set of phrase that can follow the state q , must not overlap with the bit string , and also must satisfi the distort limit .
okai , so let's look at anoth exampl .
so thi is q , sai thi state here .
the two end of must and also , at the end of the translat we have word <num> , <num> , <num> translat , the other on aren't .
we have the posit of the last endpoint of the last phrase is <num> , and then <num> .
so ph q , is again go to be a set of possibl phrase , which could follow the state .
and so let's see .
so we've translat word <num> to <num> .
so natur thing here would be thing like <num> , <num> thi , critic .
okai , becaus , thi phrase certainli doesn't overlap with thi bit string .
and , in addit , <num> is close enough to <num> that thi is allow .
or we can have <num> , <num> serious .
again , there's no overlap in the bit string and <num> is far enough , sorri , close enough to three , but there's no violat .
we might even have <num> , <num> take .
again , there's a no overlap in the bit string and we can verifi <num> plu <num> minu <num> is equal to magnitud of minu <num> is less than equal to <num> , that's fine , assum that the distort limit is again <num> and so on and so on .
so it's go to be a whole set of possibl phrase , and notic that just by look at the state , we can calcul which phrase can actual follow it .
anoth veri us definit is go to be the follow .
so , for ani q , and for ani phrase p , i'm go to defin next of q and p to be the state form by combin state q of phrase p .
so what doe that mean ?
so , next let's sai we take q here .
and we take on of these phrase , for exampl <num> <num> take .
okai ?
so thi is go to return a new state , which is form by basic concaten thi phrase on the end of thi state i've shown you here .
so the state in thi case would be , also take .
becaus those would be the last two word in the translat .
we would have the follow on , two , three , and then take is in posit on , two , three , four , five , six , seven .
and so , now , we updat the seventh bit to reflect that .
so on .
we record the endpoint of thi phrase , seven .
and , we updat the score .
so thi would be minu <num> or someth , where thi is again calcul us a combin of the trigram languag paramet , the phrase score and the distort score .
okai ?
so thi is basic just implement the graph definit i show you earlier , so we have thi transit .
<num> , <num> , take with thi particular phrase , so thi is just exactli what i've shown you here .
so here ar the formal detail of how we actual defin thi next function .
i'll just go over thi quickli , becaus i think you've gotten the intuit from the last slide .
so the next function take in thi state q and a phrase p .
so the state q consist of e1 , e2 , bit stream b , a posit r and a score alpha .
and the phrase consist of a start point and an endpoint and a sequenc of capit m word will caus epsilon <num> through epsilon m .
then thi next state is a state q prime with new element e1 prime and e2 prime , b prime , r prime , alpha prime .
these ar calcul as is follow .
so let me just , briefli give a sketch here , e<num> prime is e , epsilon m minu <num> .
e2 prime epsilon m , so we just take the last two word in thi phrase , and we've got to be slightli care about the case where thi phrase is just on word long .
these definit captur that .
and then we updat the bit string , so we sai the new bit string ha valu <num> for ani i in the rang s to t .
so these , these the start and endpoint of the phrase , otherwis , we just copi across the bit string from the previou bit string .
we defin the endpoint , r prime to be the endpoint of thi new phrase , that's t .
so r prime is just t , and then , alpha prime , the score for the new state is go to be alpha , the score for the old state plu the score of the phrase , g of p .
and then we have some languag model score , these ar trigram languag model score , and then we have a distort paramet , and then we have a distort cost .
notic that we have r , the endpoint of the last phrase , we have s , the stop point of thi new phrase , and we can calcul thi distort cost in a veri straightforward manner from them .
okai , so that's a formal definit of the next function .
we're almost there in term of defin beam search but here's on more definit which will be veri us .
it's a veri simpl function , which will return true or fals .
depend on whether two state , q and q prime , ar equal under a particular definit of equal .
so , sai i have two state , q and q prime .
the equal function is go to return true , if e1 is equal to e1 prime , e2 is equal to e2 prime , b is equal to b prime , and r is equal to r prime .
so it's actual go to ignor the score , and it's just go to come take into account these four exampl .
and it's go to return true onli if these four thing were equal .
we'll see too , why , thi definit is import , and why in particular we don't check the alpha and alpha prime score .
'kai , so we mai , have , for exampl , two differ wai of reach a particular set .
so sai , we must also mayb someth like thi , and mayb we have anoth state which look like thi .
so it actual ha the same compon for these first few entri , but ha differ score .
so thei sai thi is q and thi is q prime , in thi case , eq of q and q prime is equal to true .
so now , we can put these idea togeth in the final decod algorithm .
the beam search algorithm for phrase base model .
so , the input to thi algorithm is a sentenc x1 through xn .
for exampl , thi might be a sentenc in german .
and in addit , we have a phrase base model .
so , the phrase base model consist of a lexicon , l , rememb that consist of a veri larg set of german phrase pair with english phrase , for exampl .
we have a languag model which i've call h .
we have a distort limit , littl d .
so , for exampl , d is equal to <num> .
and we have a distort paramet here .
so , given the input in the phrase base model , we can defin these function ph of q and next q of p .
rememb , ph is a function that take a state as it input and return the phrase that can follow that state .
and next q , p take a state togeth with the phrase and return the next state , the state form by concaten thi phrase to the state q .
so , let be concret and assum we have and put sentenc of length <num> .
so , we have x1 and x2 upto x7 as the input .
and a kei data structur is go to be thi set of q's .
i'll us thi capit q for each q , will beam .
so , i have q0 , q1 , q2 , and then , right up to q6 and q7 .
and we'll see , each of these qs is go to contain state , which ar creat dure the search process .
the initi step is to set thi first q to contain a singl element , littl q0 , rememb littl q0 is thi initi state .
it's go to be someth like star , star , on , two , three , four , five , six , seven , 0s .
and then posit <num> and score <num> , okai ?
so , we just have thi initi state here .
okai , so the next thing we're go to do is go to loop , for i equal <num> to n minu <num> .
so , we're initi go to start at q0 , and we're go to move through each of these qs in turn .
and what do we do for each valu of i ?
the first thing we do is we look at everi possibl state in q sub i .
so , in thi case , we would just look at the initi state .
and then , we explor everi possibl phrase that can follow thi initi state .
so , we're go to look at all phrase p and ph of q .
so , we might have thing like <num> , <num> , must .
basic , correspond to the outset of outgo arc here .
<num> , <num> , sorri , that should be <num> , <num> we must <num> , <num> we must also .
mayb <num> , <num> also .
okai , so we're go to have some set of phrase that could follow thi initi state and we're go to explor each on in term .
for each on of these possibl phrase , we calcul the next state .
we call thi q prime , and then we at that state to be appropri q .
so , for exampl if we look at <num> , <num> we must , we get the follow state , we must <num> , <num> , five 0s , <num> , and then , some score , minu <num> .
so , thi state is go to get ad to what , on of these q's .
which on doe it get ad to ?
it actual get ad to q2 and that is becaus the number of word translat in thi state , q prime is equal to <num> .
so , q2 is go to store a whole bunch of state with the on restrict that each bit string in these state ha exactli <num> word translat .
so , the idea in these beam search algorithm is , in some sens , to group togeth state which have the same number of word translat .
so , that's what q0 is .
we have <num> word translat , q1 ha on , q2 ha two .
irrespect of the posit , we're group togeth state which have the same number of word translat .
so , if we look at these three phrase here , thi phrase here , <num> , <num> , is go to lead toward a , a member of q3 .
so , we're go to add some state to that .
thi on's go to add to q1 .
and so , in thi first step , when we consid i equal <num> , we consid the singl state q0 , we consid all wai of extend it .
and that's go to lead to a bunch of new state , q prime , which ar ad to these later qs .
so , notic we have thi add function .
we'll describ thi in a moment , but intuit , thi is just go to add some phrase sorri , some state q prime to qi .
critic , where i is the length of q prime .
thi is the number of word that ar be translat in q prime .
add is also go to take q and p , the previou state , and the phrase that'll just be for some addit bookkeep .
okai .
so , we actual move through these qs on by on .
so , q0 is go to take thi on state and expand it in these variou wai .
now , i'm go to get to q1 , and i'm go to find on or more state in q1 .
for each on , i'm go to go through the same process , i'm go to find all of the phrase that can follow thi state .
i'm go to concaten them , and i'm go to add entri later .
so , as you go through thi , you're go to popul more and more state in these variou qs .
okai .
final , we'll hit q6 .
we'll take off all of the state from thi and and , and popul , in thi case , we can onli popul q7 from q6 .
so , we alwai pick up a state from the earlier q and extend it by popul a state in a later q .
the final thing we do is we return the highest score state in q sub n .
so , q7 , in thi case .
so , we would have some set of state in here .
each on of them will have a score , and you simpli return the highest score state in , in thi , in thi set , in thi q .
and that will correspond to the highest score translat found by thi beam search process .
in a veri similar wai to dynam program , backpoint can actual be us to trace back the step in construct that particular translat .
and we therebi get the final translat .
so , let's just look at two function which plai a critic role here .
on is thi add function , which as we'll see , is veri simpl .
and secondli , we critic had a beam function .
okai so , actual when i said we take q in out of each state q , i .
so , if we take q0 q1 is equal to some set of state , q2 is equal to some set of state .
if we're not care , these set will quickli grow exponenti in size .
you're go to end up with a huge number of state .
becaus as we've said befor , the number of possibl state in these model is exponenti in size .
and so , thi beam function is actual go to onli consid a small subset of the item in a particular q .
and expand onli that small subset .
and that's go to make thi whole thing effici .
okai .
so , let's look at these two function , beam and add .
so , here's a definit of add .
add take a q , capit q , and state q prime and add q prime q .
and rememb q prime is equal to next q and p .
so , thi is just some record of a histori that wa us to creat q prime .
the first thing we do is we check to see if there's someth alreadi in the set which is equal to our current state .
okai , so rememb the equal function .
so , i if , for exampl , have someth like the follow , must , also <num> , <num> , <num> minu <num> .
okai , so mayb thi is q prime .
thi is the thing we're ad and we have some exist state which look extrem similar .
in fact , it's equal under the definit of equal i gave earlier .
so , i should add a posit here , here as well , <num> .
okai , so these two state ar equal , under the definit i show you earlier , becaus the two word , the bit string , and the last posit of the final phrase were all equal .
the onli thing which differ is the score .
thi step is go to be veri similar to dynam program .
if the two thing ar equal , we simpli keep the highest score element becaus that record the highest score path to thi particular state .
and there's no reason to keep the lowest score path , okai ?
so , we do two thing .
we check if the score of the new thing be ad is creat from the old score .
and if so , we will move the old state and we add the new state .
and we record backpoint .
so , these backpoint ar go to sai , i reach state q prime by extend state q with phrase p .
okai .
otherwis , if we don't find anyth in thi q which is equal to the current state be ad , we do the follow .
we simpli add it .
we add q prime , and then we set the backpoint to set thi record .
okai .
so , that's the add function .
so , basic we're just ad q prime to q .
we're record a backpoint .
and we're be a littl care if we have multipl state which ar equal , but have differ score , we simpli take the highest score of those state .
the final function is thi idea of beam , so beam q is go to be , basic a subset of just the highest score item in q .
how do we defin that ?
well , we sai we have some b some set q and it ha a number of possibl state and each of these ha a score .
so , let's write down a few score here and so on and so on .
we first find the highest score state in the set , minu <num> , for exampl , so mayb that's the highest score .
and sorri , that should be max , not arg max .
okai , so alpha star is the highest score for ani state in thi set .
beta is some beam width paramet where it sai , well , how far ?
so , for exampl , we might take , beta , beta is equal to <num> .
and so , then we subtract the beam .
so , minu <num> , minu <num> is equal to minu <num> .
and we basic keep ani state which have score greater and equal to thi .
and throw ani other state awai .
and so , for exampl , a state with minu <num> will get thrown awai or a state with minu <num> will get thrown awai .
okai .
so , beam is just go to keep all of those item within a certain toler of the highest score element in that q .
the intuit here is that we can , sort of , throw awai some translat even befor thei're complet as be veri suboptim , if thei have low score at thi particular state .
so , let's just go back to thi algorithm and see how these two function ar us .
so rememb , we go for i equal <num> right the wai up to n minu <num> .
we onli remov state which satisfi the beam condit .
okai , so we onli remov high score state .
at each point , we extend that state , and then we call thi add function .
and thi add function will actual throw awai i term where we have two state that ar equal , but on ha a higher score than the other .
so in thi next segment of the class , we're go to look at log linear model .
log linear model offer a veri power set of techniqu as an altern to the smooth estim techniqu we've seen earlier in thi class which ar , in some sens or other , simpl wai of estim and , paramet of model .
and defin model .
so we'll the see that log linear model allow us to make us of much represent for mani natur languag process problem .
and we'll see that thei us some rather interest and veri power idea about how to estim the paramet of these model .
so we'll be describ the basic framework of log linear model and then we'll be look at their applic in a number of problem in natur languag process .
so the first exampl problem i'm go to us to motiv log linear model is the languag model problem , which we saw right at the start of thi class .
so just to recap quickli , the problem is as follow .
we defin w sub i to be the ith word in a document , and then , our task is to estim a distribut .
thi is the condit distribut over wi given the context or histori which is the previou i minu <num> word .
so here's on exampl , thi is a passag taken , taken from chomski from the 1950s .
so sai we have thi sequenc of i minu <num> word , our task is to estim a distribut over the word that appear at the ith posit at wi .
now , of cours , on model we studi extens in the school wa the idea of a trigram languag model .
so again , just a quick recap on thi , so a trigram estim is defin as follow .
we defin thi , thi estim of a probabl of , of a particular word , sai model and given the previou context .
so , thi is the word we're try to predict , thi is wi .
we defin that as a combin of three maximum likelihood estim , assum that we're us smooth .
so we have the maximum likelihood estim of model given that the previou two word ar ani and statist .
we have the maximum likelihood estim of model , given just the word statist , and then final , we have the so call unigram estim of just the probabl of see model condit on no context .
and of cours , these maximum likelihood estim ar defin as ratio of count , as we've seen here .
so if i want to estim the probabl of some y given some context x , i take the ratio of these two count and these lambda .
these three valu lambda <num> , lambda <num> , lambda <num> dictat the relev , rel weight of these three estim .
these lambda ar posit and thei sum to <num> .
so that's a trigram languag model , you should be veri familiar with those at thi point in the class but thei have some veri clear defici .
okai , so these model make us of onli bigram trigram , and unigram estim .
okai , so thei basic restrict themselv to a veri small window , condit onli on the previou two word in the context .
and if we think back to that passag , there could be all kind of quot featur of the context , which could be us in predict the distribut over the next word .
so , for exampl , we might want to come up with an estim that condit on the fact that the word two back the word two posit back is the word ani .
notic that we've , in some sens , skip over wi minu <num> in thi case and just look at the word to back .
we might condit on the fact that the previou word is an adject , so that give us a coarser estim which ignor the exact ident of the previou word .
but it is an estim that we might onli be abl to make roughli reliabl given the amount of data we have .
we might condit on the previou word end on a particular suffix or prefix , for exampl , ical .
we might condit on the author of the entir articl .
that's like to influenc thi distribut and we might condit on long rang featur .
so we might , for exampl , condit on the fact that the word model the fact that it doesn't occur somewher in the previou context .
or we might condit on the fact that some other word , for exampl grammat , occur somewher in the previou context .
notic critic , these featur , and actual also thi featur , in some sens , go beyond just the two , previou word of context to either consid the entir document or some meter inform about the document , for exampl the document's author .
the point be that all of these estim could provid us distribut us inform about the distribut of the next word and the document , all of these featur of the context could be us in estim thi distribut .
and of cours , the trigram model ignor all of the inform that i've shown you down here .
so let just imagin that were try to defin thi estim probabl of thi particular word model , given the previou i minu <num> word in the document .
and we want to incorpor all of these piec of inform that i just show you .
so , on natur first attempt at thi would be to defin a smooth model that's veri similar to the trigram model that we'd seen earlier in the class or rather similar to the smooth method we'd seen for those model .
so i could take these nine differ maximum likelihood estim that i've shown you here .
so here , i have the regular trigram , bigram , unigram estim .
here , i condit on the word to that be ani , the condit on the last word be an adject .
a condit on the author , the presenc or absenc in model the context .
the presenc or absenc of grammat in the context .
i take all of these estim and i simpli take a linear interpol of these estim , so i have now paramet , smooth paramet .
thi lambda <num> , lambda <num> , lambda <num> , up to lambda <num> , and these will all great link to <num> and thei sum to <num> .
okai ?
so , i want to go through thi as a thought experi , as a first wai that you might try to build a model that make us of all thi differ inform .
in practic , thi kind of approach quickli becom extrem unwieldi .
and , in fact , is beset by all kind of practic problem , realli make it a non starter .
so while thi method , a trigram languag model that mai us just these three estim and these ar three definit of context that work quit well .
onc you try to extend these model to includ differ type of featur , thei becom extrem unwieldi .
as we'll see , log linear model , a veri clean and i think veri eleg solut to thi problem of incorpor multipl sourc of inform in these estim .
so here is a second , veri import motiv exampl for log linear model and that's the problem of tag .
specif , in thi exampl , we're go to look again at part of speech tag .
so as a recap , the problem here is to take some sentenc , sentenc , some sequenc of word as input .
and to map thi to a represent where each word ha an associ tag , for exampl , n for a noun , v for a verb , p for a preposit , and so on and so on .
so here is a natur estim problem associ with thi thi particular problem .
okai ?
so if we take a particular word in a sentenc , our task is go to be to estim a distribut over the possibl tag at that posit .
so again , that might be <num> or <num> possibl part of speech tag .
so , ti is go to be the ith tag of the sequenc , and we'll us wi to refer to the ith word in the sequenc .
okai ?
so we're go to try to estim the , the distribut over potenti tag of the ith posit , that's t i .
now , under the definit of the problem i'm go to give you here , we're go to condit on two thing .
firstli , we can potenti condit on ani inform in the entir sentenc , so w1 through wn .
here's the input sentenc .
and secondli , we can condit on ani inform in the previou i minu <num> tag .
so in thi particular case , t1 through ti minu <num> is equal to the tag sequenc nnp up here or v , vb , dt , jj .
so that's actual a sequenc of length five .
we're try to estim probabl of t6 , given thi previou sequenc of tag , and the entir sentenc as input .
okai , so thi look slightli differ from the kind of paramet you saw in hidden markov model for tag .
but we'll see a littl later that if we can come up with accur estim of these condit probabl , then thei lead to a veri direct and veri power tag model , which is an altern to the , the problem it well , is an altern to the hidden markov model we've seen earlier in thi cours .
so again , the thing to realiz here is that in come up with thi estim , we could look at all kind of featur of the histori or context .
okai , so if we look at thi particular exampl here , where we're try to tag the word base .
so we're try to estim a distribut of the tag at thi posit .
we can look at variou thing to sai we're try to estim the probabl of see the word base tag as an nn , that's a common noun , singular noun in english .
we can condit on the fact that the current word be tag is the word base or we could condit on the fact that the previou tag ti minu <num> is the tag jj .
or , we could condit on prefix or suffix inform about the word be tag .
so i could condit on the fact that wi end in the singl letter e or the fact that wi end in the pair of letter s follow by e .
i could condit on surround word , in thi case .
i could condit on the fact the previou word is import or i could condit on the fact that the next word is the word from .
so again , if we continu with thi exampl , anyon of these featur of the previou context could be us in predict the distribut over the tag at the ith posit .
and we could , onc again , come up with a method base on linear interpol .
the old method for smooth we saw within the context of trigram languag model , but it would quickli becom realli unwieldi as you incorpor more and more sourc of inform .
so , in the remaind of thi lectur , we're go to describ how log linear model can solv mani of these problem that we've just seen .
so , i'll first give basic definit of log linear model .
we'll then , talk about how to estim paramet in log linear model .
and final , we'll talk about smooth or what is often call regular in the log linear model .
thi is a wai of deal with paramet estim problem in veri high dimension space in case where we have veri larg number of paramet .
we'll see how we can emploi a veri simpl , but highli effect method for smooth or so call regular in these model .
folk , so , first let's give a definit of the problem that we're try to solv .
so , we're go to assum that we have some set of possibl input , call thi script x .
so , for exampl , thi could be the set of all possibl document histori w1 , w2 , up to wi minu <num> .
so , i here , can vari , thi is go to be a sequenc of word , the previou word in the document , thi set of input in the set of all possibl document prefix , that's clearli an infinit set in thi case .
we have a finit , okai , so we'll assum that thi label set is finit .
so , thi is the set of possibl , what we call label in the particular context .
so , for exampl , thi could be the set of all possibl valu for wi which would typic be to v if v is the vocabulari that we're look at .
mayb v union stop , if in addit we have a stop symbol .
our aim is then , is to provid a condit probabl , p of y given x for ani x y pair , where x is in the set of possibl input and y is in the set of possibl label y .
and we're go to see how log linear model build a model of precis thi form .
so , let's see how these definit fit with our first exampl problem , the languag model problem .
so , in thi case , each x is a sequenc of i minu <num> word .
so , it's of the histori as the previou word in the document .
and each y is a word wi that we're try to predict .
and of cours , given the definit i show you on the previou slide , we're try to estim the probabl of y given x .
so , in thi case it's p of wi given w1 through wi minu <num> .
and again , script x is a set of all possibl document prefix .
and then , put an infinit set , and script y is the set of all possibl outcom at ani posit role .
all possibl label at ani posit .
thi is the set of word in the vocabulari .
so , a first kei idea in log linear model is go to be the idea of featur , and what we'll call featur vector represent .
so , here's how thi work .
i'll first give the abstract definit , and then we'll go through a particular exampl .
so , in gener , a featur is go to be some function .
so , we're go to have featur f1 , xy , f2 xy , up to fm xy .
we have littl m featur .
so , each featur fk is a function that take an x , y pair as it input .
so , x is again , in the set of all possibl input , y is in the set of all possibl label .
rememb , we're try to model the probabl of y given x .
so , each on of these featur fk for k equal <num> to m , is simpli go to take an x , y pair input and map it to some posit or neg valu , some , some real valu .
so , i'm go to us thi r symbol to mean the set of all possibl real valu .
everyth neg valu like minu <num> or minu <num> , posit valu , valu <num> , and so on .
so , we could potenti map ani x , y pair to ani real valu .
in practic , at least in natur languag process , the featur that we often see ar what ar call binari featur or indic function .
and these featur ar go to map each x , y pair to either <num> or <num> .
1is often in , interpret as true .
and <num> is in , interpret as fals .
where we consid a featur to be ask a particular question about an x , y pair .
we'll see mani exampl of thi in a moment .
so , we have these featur f1 through fm .
it's often conveni to concaten them into a vector .
okai , so i'm go to us angl bracket to enclos a vector .
so , thi is on possibl vector of dimens <num> .
the featur vector f of x , y is the vector form by take the output of featur <num> , and the output of featur <num> , and right awai up to the output of featur m .
so , we now have an m dimension represent of an x , y pair .
and thi is often refer to as a featur vector .
so , that's all rather abstract .
let's go through a particular exampl illustr thi kind of definit .
okai , so again , back to languag model .
each x is a histori , w1 through wy minu <num> .
for exampl , thi portion here , each y is an outcom , for exampl wy might be model .
okai , it could ani word in the vocabulari .
and here ar some exampl featur and these actual captur the kind of featur we saw in the when we consid thi exampl at the start of thi lectur .
okai , so f1 , xy might be defin to be an indic function that take the valu either , either <num> or <num> , take the valu <num> .
if y is equal to model , and <num> otherwis .
so , thi featur could potenti look at ani inform in x or ani inform in y .
and in thi case , it's a veri simpl featur that simpli look at the ident of the label y , in thi case .
thi might be refer to as a unigram featur .
becaus it look at the singl word and ignor all context , a bit like the unigram estim we saw in trigram languag model .
here's a second featur , f2 x , y .
thi is <num> , if y is equal to model and wi minu <num> is equal to the word statist .
okai .
so , thi is what you might think of as a bigram featur .
so , what i'm do here realli is replic the kind of inform we see in trigram languag model .
but we'll soon see how we can go further than trigram languag model with thi approach .
the third featur , f3 of x , y is <num> if y is equal to model .
if wi minu <num> is ani , wi minu <num> is statist and <num> otherwis .
and so , thi is basic a so call trigram featur .
okai , so again , each of these featur look at an x , y pair and return some real valu .
these ar indic function or binari featur which simpli return <num> or <num> depend on whether some question about the x , y pair is true or fals .
let's defin some more featur .
okai .
so , here ar some other .
we could defin f4 x , y to be <num> if y is equal to model , and wi minu <num> is equal to the word ani .
thi is often refer to as a skip bigram featur .
it's a bigram becaus it look at two word .
but it's a skip becaus it skip wi minu <num> .
so , it's look immedi to the word two word back , ignor the ident of the on word back .
these featur can be us in some context .
here's anoth featur .
thi is <num> if the word be predict is model and the previou word is adject , is an adject .
assum for the sake of argument here , we have some determinist function that look at wi minu <num> , and figur out whether it's an adject or not .
we have anoth featur which look at the fact that y is equal to model and the previou word end in the four letter ical , i , c , a , l .
that might be a strong indic of the type of the previou work .
we might have a featur look at the fact , we have y equal model , and the fact that the author of thi document wa chomski .
we have featur condit on the fact that y is equal to the word model .
and that the fact that the word model is not seen in the previou i minu <num> word .
we have a featur which look the fact that y equal model and the fact that grammat is seen in the previou i minu <num> word .
and you can see that you could defin mani , mani featur of these type .
which , in everi case , look at the label .
okai , so thei're , in fact the featur we , we see for reason , we'll see shortli .
alwai , in some sens , take the label into account , but thei , in addit , consid some context .
okai , some featur of the previou i minu <num> word or some method featur of the data , for exampl , the author .
it's import to think a littl more about how we actual defin these featur in practic .
so , in the exampl i just show you , we had thi featur f3 which wa <num> if thi particular trigram .
ani statist model wa be form when we look at the x , y pair , and wa <num> otherwis .
in practic , of cours , thi captur inform about just a singl trigram .
and you would realli want featur which consid , in some sens , all possibl trigram or all possibl bigram or all possibl unigram .
and that's exactli what we do with on import caveat .
what you would probabl do in practic is the follow .
you would introduc on trigram featur , for everi uniqu trigram scene in the train data .
that is for all tripl of word u , v , w seen in the train data , we creat a featur which is <num> if we have that trigram u , v , w and <num> otherwis .
so , rather than have just <num> trigram featur , you might easili have a few million trigram featur .
and actual , there's no real harm in these model of have a veri larg number of featur .
what i have done here , i've just , been slightli care the number of thi featur is go to be defin as n u , v , w , where n u , v , w is just a , a hash function , that map each trigram to a differ uniqu integ .
okai .
so , we've just basic index all of the distinct trigram we see on our train data and includ each on of those trigram as a featur in our model .
notic , kei here is we do not includ trigram not seen in train data .
on kei reason for that is there ar simpli too mani that would lead to a huge number of featur .
basic , a feat , a number of featur which is basic v cube , where v is the size of the vocabulari and that's wai too mani .
and the other thing is these unseen trigram , as we'll see , we don't realli have ani evid in which to estim that paramet .
and so , everi includ in the model .
okai .
so , when we see paramet of a log linear model , we'll see that for each of these differ trigram , we have a paramet .
we see how to estim those paramet from data .
we onli estim those paramet for the trigram that ar actual seen in the train data .
so let's now consid our second exampl problem , which wa part of speech tag .
so rememb in thi case , each x is a histori which consist of a sentenc .
for exampl we might have the dog saw the cat .
we have some posit i , which is the posit be tag , so let's sai for the sake of argument , it's posit <num> in thi case .
so we're try estim distribut of potenti tag of the word the .
and then we have a sequenc of previou tag .
t <num> through t i minu <num> , so we might for exampl have d n v as the context , the previou tag in the sentenc befor thi fourth posit .
so each x encapsul all of thi contextu inform , the entir sentenc , the previou sequenc of tag and , just for a littl bit of bookkeep , the posit that we're actual tag .
each label y , is a part of speech tag , so it's go to be on of , sai , <num> or <num> or <num> tag typic , you might see in a given languag .
and again , we're go to assum that we have m featur , f k x y .
each of these featur is go to take an entir x y pair and return some real valu .
and again , we will make heavi us of indic function , which return <num> or <num> , depend on whether some properti of x pair with y is true .
here ar some exampl featur , and these ar actual taken from a part of speech tagger , develop by adwat radner parki , in the mid 90s , which made direct us of log linear model in a veri eleg and effect wai for tag problem .
here ar some exampl featur .
so f <num> x y is <num> , if the current word w i is the word base , and the tag we're predict is v t .
so , thi look at a word tag pair , and for thi particular featur , we look at the word base in conjunct with the tag v t .
in practic , you would again introduc , a veri larg set of featur of thi form , basic for everi possibl combin , of word and tag , or at the veri least , everi possibl combin of word and tag that you've actual seen in your train data , assum we have tag sentenc to train the paramet of thi model .
so thi is on clearli veri import type of featur , the featur that basic captur the tendenc of a particular word to take a particular tag .
you can think of thi , if you think back to hidden markov model , as be anal , analog to e of base , given v t , or at least when we see the paramet associ with thi featur .
that paramet is go to plai a similar role to the submiss paramet that you saw in hidden markov model .
here's a more interest featur though , f <num> , as i've defin it here , is <num> , if the word you're predict end in ing , and the tag is v b g .
thi is a van trebach tag reserv for verb which ar gerund .
so thing like like , or talk , all of these verb in english which typic end in ing , and thi featur is go to captur the tendenc for word end in ing to take thi particular tag .
again , you might design a veri larg number of featur like thi for sai , all possibl prefix and suffix combin with all possibl tag , or again , at least those combin which you've seen in train data .
actual , we can , essenti , give the full set of featur type that ar us in ratnaparkhi's tagger .
it's a fairli simpl set of definit .
so let me go through these .
so he us featur which look at the word and tag in exactli the same , as i've shown you in the previou slide .
he us spell featur which consid all prefix and suffix , suffix up to length <num> , in conjunct with tag .
so here ar some exampl .
you know , thi featur we just saw , conjoin ing as a suffix , togeth with v b g .
here's anoth featur which is on if the current word start with the , the three letter prefix pre , and the tag is nn .
and you can imagin these ar just two featur of a veri larg number look at all prefix and suffix seen in train data , in conjunct with all possibl tag .
here ar some other featur that ratnaparkhi us .
he us contextu featur .
so , these ar featur which actual ar analog to the trigram tag paramet you saw in hmm .
so , rememb , we had paramet such as thi , which is the probabl of see a transit verb , given the previou two tag of determin and adject .
the analog featur to that , is thi featur , which is <num> , if t i minu <num> , t i minu <num> and y form thi particular triagram .
okai , so again , rememb we have some sentenc , w <num> through w n .
for exampl , the dog saw the cat .
we have some posit we're tag .
for exampl , i equal <num> .
and we have previou tag t <num> through t <num> , for exampl , equal to d n v .
and so i can basic look at these previou <num> tag in conjunct with the label y be predict , and creat a featur like thi , track a trigram of tag .
thi is the bigram featur , so it look at the fact that t i minu on is j j and y is equal to v t .
and final , unigram featur , which onli look that the label y , and basic thi will allow us , to basic captur the rel frequenc of differ tag , mayb v t is a frequent tag or not so , so frequent , frequent tag .
thi is the most naiv featur , becaus it condit in the no context .
but , of cours , it can be us , becaus it doe provid some evid that's veri robust .
we don't need to estim we don't need too much data to estim the paramet associ with thi featur .
here ar some other featur that ratnaparkhi us .
here is a featur which look at the label y , the fact that it equal to v t , and it look at the previou word .
again , thi go to be on featur of mani , which look at the previou word in conjunct with the current tag .
thi is veri differ from anyth we saw in hmm .
we didn't see hidden markov model make us of these featur , conjoin the previou word , with the current tag , and inde it would be quit , quit difficult to extend , hidden markov model to take into account thi inform .
we have anoth featur , which look at the ident of the next word .
and the current tag , and again , thi will be on of mani featur which look at the next word on the current tag .
so in summari , the final result of all thi is the follow .
we can come up with practic ani question , what we've call featur , which look at histori tag pair .
and basic , for a given histori x conjoin with a label y , we get a featur vector , and becaus our featur have been indic function , binari function that return <num> <num> , we could sort of pictur thi as follow .
f is go to take into account an entir histori , so thi is x .
take into account sentenc posit in previou tag , and it consid a particular tag for exampl v t of the sixth posit , and it just return a binari vector summar the result of all the question that we've ask about x and y togeth .
and we get a differ binari vector for everi possibl tag in the sixth posit .
now , i don't want to go into thi in too much detail , but , in practic , these featur vector ar often spars , in fact thei're almost alwai spars .
thei have rel few 1s versu 0s .
so while we might have a veri larg number of featur in these model , it's not uncommon to have a few hundr thousand , or a few million featur .
typic , a much smaller number of those featur ar equal to <num> , for ani particular x y pair .
you might have , i don't know , the order of a few ten , of featur for exampl , which ar equal to <num> , becaus most of the question we ask can be fals , and onli some of them ar go to be true .
see the note , which ar post along with thi lectur , for much more discuss of thi particular issu .
and it's import , becaus it lead to model which ar rather more effici than you might think , given the larg number of featur which ar in these particular model .
so that's the definit of featur in log linear model .
again , we're assum we have m featur , f sub k , for k equal <num> to m .
each feat , featur map an x y pair to some real valu .
that valu is veri often <num> , <num> in the context of natur languag process problem .
so in addit to featur , we ar go to defin paramet vector .
these ar the second kei compon in log linear model .
so if we have m featur , we're go to assum that we have a paramet vector which is also of dimens m , and so here i us r to the m , to refer to the space of all possibl m dimension , real valu vector , okai ?
so , for exampl , if we have m featur , sorri .
for exampl , if we have m equal <num> featur , then our paramet vector will be some vector consist of <num> real valu .
each of these valu might be ani valu in in the real , ani posit or neg valu or <num> .
so given a featur vector f , and a paramet vector v , we can map ani x , y pair , to what we'll call a score what we'll call a score , which again , is some valu which could be posit or neg .
and we do thi , simpli by comput the inner product between v and the featur of vector f x y .
so i us thi dot here to refer to the inner product or dot product between these two vector .
a littl more explicitli , what thi mean is we sum from k equal <num> to m , we multipli in v sub k time f sub k and we need to sum these term , okai ?
so if i hand you an x , y pair , you can comput the score by first comput the valu for the featur , for each of these m featur , and then comput thi inner product , simpli by look up the paramet and multipli these by the , the variou featur valu .
so , the kei question which follow from these definit , is how do we go from these score , to probabl ?
we realli want the probabl of y given x .
thi could be ani posit or neg valu , it's certainli not necessarili between <num> and <num> it doesn't necessarili form a well form distribut .
for exampl we want p , if we sum over y , as we want thi to sum to <num> , we want p y given x to be greater than or equal to <num> .
clearli we still have a bit of work to do in go from these score , which can be arbitrari posit or neg valu to these probabl .
but we'll see there is a veri appeal wai of do thi , which is us within log linear model .
so let's continu with the languag model exampl to illustr thi idea .
so , in practic , what is go to happen is the follow .
we have some histori x , which in thi case is a sequenc of previou word .
and we want to go from x to a distribut , p y given x , for or y in our label set .
now the first thing to realiz is that , under the definit i've just given you , for each possibl valu of y , we can comput it score , by take the inner product of v with f , appli to the context x , conjoin with the label y .
so , if we think about you know , again , we ar try to predict the , the distribut of a , the next word in thi particular context .
we can enumer all possibl here .
so all label in the set y .
in thi case that's go to be the vocabulari of thi particular languag model we're interest in .
and for each of these label we can comput , thi valu , thi score .
and we'll see in a moment how we go from these score , to a distribut of p , y , given x .
intuit , we'll see that higher score , mean that we will end up with higher probabl , and these item such as those here which have low score , will end up with lower probabl .
so how do we thi ?
the kei definit is go to be down here .
so , to re , to recap , we have some input domain x , a set of possibl input , a label set y , our aim is to provid a condit probabl , y given x , for ani x in the input set , and y in the label set .
a featur is a function , sorri , f sub k , that take a next wai parent return some real valu .
we often have binari featur which return <num> or <num> .
we have a featur vector , f x y , which is go to be some m dimension vector .
so it might , for exampl , be <num> , <num> , <num> , <num> , <num> , <num> , <num> , for ani x y pair .
and we have a paramet vector which is some vector real .
so we might have someth like thi .
and of cours , these two vector of the same dimens , we have as mani paramet as we have featur .
and then we defin the probabl under our model , as follow .
so thi can be read as , the probabl of y , given x , under paramet valu v .
so , for particular set for these paramet f , we'll have a distribut of y given x .
and of cours , a littl later , we'll see how we can actual estim , our paramet valu v , from actual train exampl .
so let's look at thi definit .
what do we do ?
well , for a particular y that we're interest in , i can again calcul v dot f x y , and thi is go to be some score .
it could be posit or neg .
okai , so , it's posit or neg , it's clearli not in ani sens of probabl .
but we'll see through thi transform , which i show you here , we can turn these valu into probabl .
so how do we do that ?
well firstli , if i exponenti thi , so we take the valu of e , and rais that to the power of e dot f .
thi thing , is now greater than <num> , so , we have a strictli posit valu here .
and so in some sens we've gotten a littl closer to probabl in that we've at least gotten rid of neg valu , but more importantli , if v dot f is a larg valu , a larg posit valu , thi will be larg , if it's a larg neg valu thi'll be close to <num> .
and in fact , we can get thi arbitrarili close to <num> , if we set thi to be more and more neg .
let's look at the denomin of thi definit .
so here i have a normal term , where i sum over all possibl label .
so i sum over all possibl valu for y prime in my label set .
and i calcul the score for each of these label , and that is the denomin .
so we have the ratio of these two term .
thi is a normal constant .
sorri , normal term .
which will ensur that we have a well form distribut .
okai , so , it's easi enough to see that we have p of y given x under v is greater than <num> , becaus both the numer and the denomin ar strictli posit term .
now let's look at what happen if we sum , over all possibl valu of y .
we want to ensur that thi express sum to <num> , okai ?
so let's look back at thi .
sai i sum over y , and the set of all possibl label of the ratio of these two term .
it's easi enough to see that thi is equal to <num> , becaus i can bring the sum over here and i just have the ratio of two term which ar the same .
so a littl bit of algebra can verifi that these two condit ar satisfi .
so we've gone from score v . f which can be ani valu posit or neg , to probabl which satisfi these criteria that thei're greater than <num> , and thei sum to <num> , so these probabl ar valid .
let's trace thi through for how thi work on the previou exampl , how thi definit work .
so sai in thi particular case , i want to calcul the probabl of the word model , given the current histori x , which is the sentenc under paramet valu v .
and the first thing i'll do is i'll calcul the score for everi possibl label y .
so i have these score , and i'm go to transform them us the definit i show you on on the previou slide .
so the first thing i do is i take thi score , and that's go to be the numer , or at least e to that score is go to be the numer .
so e to the <num> .
and then , on the denomin i have a sum of term , follow e to the <num> , plu e to the <num> , plu e to the <num> , plu e to the minu <num> , and so on , and so on .
to give anoth exampl , sai we want to calcul the probabl of the word , the , given the context x , under paramet v .
that's go to be e to the minu <num> here .
and actual , i'll have the same sum on the nomin .
so the same normal term .
okai , so the sum of sever term for everi possibl term of the vocabulari there .
you can see immedi that if the score for ani particular word is larg , thi probabl will be close to <num> , at least if it's much larger than the other score .
and similarli , if a score is strongli neg , it in gener it's go to mean that the , the probabl of the associ word is go to be close to <num> .
so that is basic the complet definit for the model form for log linear model .
again , we have some condit probabl of y given x under paramet valu v .
to calcul thi , we comput the inner product between d and f .
rememb thi is equal to sum k equal <num> to m , v sub k time f sub k of x y , and we comput the normal term by sum over all possibl label y prime and take e of e to the power of v dot f of x and y prime .
and thi transmiss basic take us from the score to a well form distribut over the possibl valu for y .
so the next thing i want to talk about is how we actual estim these paramet the from train exampl .
so where in gener we ar go to assum that the set of featur is fix .
so that's be defin come into the problem .
and the paramet estim problem is go to be to estim these paramet v from our train exampl .
befor i talk about paramet estim i want to talk about on other thing which is where log linear model get their name from , why these thing ar call log linear model .
so that the reason for that is the follow .
and why the name ?
so if you recal , we had p y given x under v is equal to e to the v . f x y over sum y prime e to the v . f x y prime .
so let's look at what happen if i take the log of thi .
okai .
so i take the log of p , i'm assum the log is natur log .
so the log of thi is go to be log of e to the f v dot f x y , minu the log of sum y prime e v dot f x y prime .
thi is just us the usual rule of take a log of a ratio of two term .
and of cours thi is equal to v dot f x y .
becaus the log , and assum thi is again , thi is natur log , and the expon cancel each other essenti .
and so , what we end up with is thi log probabl , in two term .
here we have the inner product between v and f , appli to x and y of the element we're interest in .
and here we have log of thi normal constant .
and so thi is a linear term , that's why we call these thing log linear model .
becaus we end with someth which ha a linear term as thi first term .
thi normal term is a function of x until it doesn't end on y .
that's the second term in thi express .
okai , so that's the reason for the name , and as we will see when we deriv paramet estim , it's go to be us to bear in mind that when we take the log of p , we end up with these two term .
it'll be easi to manipul thing in thi form .
okai , so now we're go to talk about paramet estim in log linear model .
thi is the problem of take a set of train exampl as input and produc as output .
a set for the v paramet that i show you in the previou part of thi lectur .
so a first kei idea in paramet estim is go to be the follow .
i'm go to assum i have some set of train exampl .
so each train sampl consist of an x , y pair , so i'll write xi , yi to be the ith exampl of my train set , go to assum i have littl n train exampl .
each xy pair is such that the xi is a member of the set of input , the yi is thi , a member of the set of possibl label .
so , as on exampl in the languag model problem , each xi would be a sequenc of word , such as the dog saw .
we have three word in thi case , and each yi would be a singl word , which wa seen follow the sequenc of word .
okai , and you can see , it's easi enough given a larg amount of text to gather train exampl of thi form , which basic consist of context togeth with the word appear as the next word in that particular context .
so , we're again go to emploi maximum likelihood estim in thi scenario , at least , as i first pass through thi problem .
a littl later , we will see how to smooth these estim , how to regular them , but for now , let's consid maximum likelihood estim .
and that mean the maximum likelihood paramet v sub ml ar go to be the paramet valu , out of the space of all possibl n dimension vector that maxim some function l of v .
so l of v is go to be a function that take a paramet vector as input and return some valu , which is basic go to be a measur of how well those paramet fit the data .
more precis , it's go to be the log likelihood of the data under the paramet .
so what doe that mean ?
l of v is defin as follow , so we have a sum over the n train exampl .
i have a sum from i equal <num> to n , and then , i have the log probabl of the ith train exampl , so that's log of p of yi given xi under paramet v .
okai , so the train sampl is fix , but as we vari the paramet v , these probabl will chang , thei'll becom higher or lower .
and so , here , we just have a sum over all of these log probabl .
on log probabl for each item in the train set and that's how we defin l of v .
now , intuit , we would like these probabl to be as high as possibl , reflect the fact that our paramet v fit the data well .
and so , if we maxim thi function l of v , we'll have put high probabl on the train sampl that we actual see .
more formal , there ar mani nice properti you can deriv of maximum likelihood estim which appli quit gener and certainli appli to thi particular case .
okai , so just to remind you , p take the full length form , it's ev dot f , dot dot dot , over some normal term .
take thi kind of form .
if we take log of thi whole thing , we end up , as i show you just previous on thi lectur , e v dot f minu log someth , okai ?
and so , thi express here , to be more explicit , ha thi form where i have sum i equal <num> to n , v dot f , xi yi .
so thi is actual the featur vector on the ith train sampl .
and here , i have these kind of log normal term , i have the sum i equal <num> to n , log .
here , i have a second sum over possibl label y prime , e or v dot f of xi , conjoin with i prime , y prime .
okai .
so , that is the basic definit of the maximum likelihood estim problem .
we're go to try to choos the paramet v to make thi function l of v as larg as possibl .
and so , that the big remain question is how do we actual optim l of v ?
how do we find these maximum likelihood estim ?
befor we get to that , i want to talk about on critic properti of l of v , which is the follow .
lv is concav , and thi mean essenti , it is a veri nice behav function .
so although , in the gener case , find a close form solut to thi arg max is not go to be possibl .
becaus , lv is concav it's fairli easi to optim it .
so what's it mean to be concav ?
imagin , we just have a singl paramet , v1 , in case we have the on dimension paramet , a vector in thi case , and i have a graph of v1 versu lv .
the concav function look like the follow .
but basic , if you take ani two point , so two point here , then thi line goe underneath the function , and in particular , thi mean that if we find a local optimum of thi particular function , lv , it's also go to be the global optimum .
intuit , thi mean we us ani kind of hill climb techniqu .
it is go to reach the global maximum of thi function .
so thi is a concav function , and in some sens , it's rel easi to optim .
there have been mani result in optim show that we can effici find the maximum of thi kind of function .
here's a function which is not concav .
so thi function ha multipl local optima which ar not actual global .
so thi is the global optimum , these ar local optima , which ar not global optim and thi is hard , thi is nonconcav .
veri roughli speak , thi is gener a much harder optim problem .
so , the beauti or on beauti thing about thi function , lv , is that it is actual a concav function .
of cours , we're go to gener thi to the multidimension case , but similar definit appli , and so , thi is a concav function , which mean that if we us some kind of hill climb techniqu , we will in gener , converg the global optimum of thi function that we're try to optim .
so again , to recap .
unfortun , it's difficult to find close form solut for thi maxim problem .
howev , the fact that thi function is concav mean we can us hill climb method , for exampl , as we'll see in a moment , gradient ascent as a wai of ultim thi function .
so we will in fact , us gradient base method to optim thi function .
what doe that mean ?
so now let's imagin , we have l of v where v actual ha two paramet .
okai .
so what i want to do here is sketch a , some set of contour line for a hypothet version of l of v , so these ar contour line exactli as you'd see , see on a map .
okai , so that the peak of thi function is right here and these ar , line of equal valu of l of v .
so a gradient base method look like the follow .
we start at some point , mayb we'd start at the origin , or someon might start with all paramet equal to <num> , and we calcul the gradient , which just mean we calcul a direct which is the steepest direct from thi point .
and we might move as far as possibl in that direct .
so we might , for exampl in thi case , move to here , which is the optim point , if we restrict it to move in thi direct .
so basic if you think of as climb a hill , you start at some point , you look at the steepest wai up the hill , and you walk .
the distanc in that direct , which take you to the maximum on thi particular line , and then you have a new point .
again , we calcul the gradient we move in thi direct , and that point we might be fairli close to the optim point here , okai ?
so that's the basic idea behind gradient base method , or gradient ascent at each point , we calcul the gradient then move in that direct , move in the steepest direct until we're we've , we're suffici close to the optimum of thi function .
so the gradient actual take a fairli conveni form .
so rememb again , l of v is the follow , so sum i equal <num> to n of v dot f , sum i equal <num> to n of thi log function .
and let's see what happen when we differenti thi , with respect to a particular paramet v sub k .
okai .
so if i differenti at thi term , i simpli get sum of i equal <num> to n of fk .
okai so thi is , thi is , thi is veri easi to calcul , thi first term .
i simpli sum over all the train sampl .
calcul the valu of the k featur .
in that case we have the first compon of thi deriv .
the deriv through respect to k .
thi log term take a littl bit more work to differenti , and i won't take you through these step in all the gori detail .
again , you can refer to the note that ar provid for thi class , for a detail explan .
but what we end up with is actual a fairli simpl express .
so we have a sum i equal <num> to n .
and now i have a sum of all label y prime .
i have the featur valu fk appli to xi , which we're sum over in conjunct with y prime which rule ar sum over .
and now , remark , all i have here is in fact the condit probabl of y prime under our current model .
so these term ar often refer to as empir count , becaus if these fk's or indic function zero on .
thi is basic the number of time thi particular , particular question for the kth featur ha been true on our train sampl , and these ar often refer to as expect count .
becaus thei kind of the expect number of time the featur ha fire under our current model .
most importantli , both of these term ar fairli easi to calcul .
calcul these empir count , we just sum over the train sampl , calcul the fk's for the expect count is a littl bit more work .
we have to sum over the train sampl , calcul the probabl distribut over the label under our current paramet on that particular train sampl , and multipli that in with the featur vector definit fk of xi and y prime .
but nevertheless , thi is fairli straightforward to calcul .
so given these definit , i can calcul the deriv through the respect v1 , v2 right up to vm .
so in thi particular exampl , i calcul the deriv of the respect to v1 , v2 , and that will give me the direct in which i move the gradient i'm try to move .
.
so here's a first sketch of a gradient ascent method .
and it work as follow .
so our goal is to maxim thi function l of v .
where the deriv take the follow form , and what i've actual done here is us .
usual vector calculu definit of deriv , so i have d l v by d v is now go to be a vector whose first compon is the deriv with respect to the first paramet and whose second compon is the deriv with respect to the second paramet .
and these ar deriv in the wai i show you on the previou slide .
so thi thing is a n dimension vector itself , basic give the gradient .
and thi is easili calcul us the definit i just show you .
and so the most naiv gradient base method would form as follow .
we would initi v to be the vector of all zero .
at each point we would calcul the gradient for thi capit delta .
and then we basic do the search i wa talk about for okai .
so we would , start at some point , we'd calcul the gradient , we'd do a line search to find the optim distanc to move .
and we would move to that point , so beta star is basic how far we have to move , and so we're calcul the optim valu for the function if we just move in thi on direct .
so beta is the distanc we move in , in the , in the , in the direct capit delta .
and then we reset v to be thi new valu here .
and we iter into converg , so again we calcul the steep and sort of thi steepest line of ascent .
either the gradient move the optim distanc in that direct and so on and so on until we basic have reach some optim valu , or some valu that we think is optim .
the good new is that you in gener will not have to implement these kind of gradient ascent method .
thei've been develop for year , actual decad and quit sophist method have been deriv which in practic work extrem well .
so , on thing to note is that the vanilla gradient ascent method i've just show you can be rather slow , it can tend to sort of get stuck in these long vallei and take quit a few iter .
so a more sophist method to us , if you're interest in look it up , conjug gradient method , on of the , on veri commonli us method for these log linear model is an algorithm call lbfg .
the good new is that mani implement of thi algorithm or other conjug gradient style algorithm ar avail .
and thei in gener assum a rather simpl interfac .
okai .
so thei're basic go to assum that given a paramet , vector v , you will calcul the valu for l v and you'll calcul the gradient .
okai , so thi is the object valu .
it's on thing you can calcul .
thi is the gradient .
and with these softwar packag , thei will then optim the function .
and what thei'll basic do is start at some initi point for exampl , all zero , and thei'll move in some direct which take into account both the gradient .
and also the previou direct that the algorithm ha move in , and these can be veri , veri effici at optim the function l v .
and that's about it .
so if you can implement function l of v , and d l by d v , thi gradient function .
given on of these exist packag .
you can optim l of v quit effici and find the maximum like distanc .
so we've now describ the basic form of log linear model .
and we've talk about he paramet estim in these model .
the final piec of the puzzl is go to be to talk about smooth and regular , which is go to be a slight modif , but a veri import modif to the paramet estim method that i've just describ .
okai , so let's look at a particular problem with the maximum likelihood estim that i've just describ .
and we'll do thi by look at a particular exampl .
so , let's return to thi problem of part speech tag .
rememb , in thi case , each histori x consist of a sentenc .
w1 through wn the sequenc of previou tag , t<num> through ti minu <num> .
and also posit i , and so we have these three thing that we're condit on .
and the label y is a tag for exampl vt .
okai , let's sai we have a featur definit , sai a 100th featur , is go to be <num> if the word we're tag is base and the tag we're choos is vt .
okai .
now , let's assum that for the sake of argument in our train data , we see the word base three time , and we see it with the tag vt everi singl time .
and that 's not a unrealist situat .
in mani case , we will see word with the same tag everi time .
so , if you look at the gradient at the maximum , we're go to have dlv by dl dv1<num><num> is equal to <num> .
so , if we hill climb to the maximum of thi function i wa talk about , at thi global maximum , all the gradient ar go to be zero .
and so , we can reason about to some extent about our final paramet by us thi properti that the gradient ar equal to zero .
and that mean that if you look back at the definit of gradient , thi equat will be satisfi .
so , here we have a sum of i of f <num> xi yi .
here we have a sum of i and y , py under the , the , the model time the featur valu .
so , i'm not go to go into tremend detail about thi deriv , but the import point is the follow .
you can ensur that in thi case , the onli wai these gradient ar go to be satisfi is if thi condit probabl vt given xi under the paramet v is equal to <num> whenev we see the word base in our histori in our train data .
and that is essenti go to mean that thi paramet , v100 , is go to tend toward infin as we optim our solut .
that's go to happen in almost all case .
so , for these probabl to be on , we're go to set our paramet to be tend toward infin .
and that mean for ani test data exampl , whenev we see a word base be tag , we're go to estim it probabl as well .
so , thi model is essenti smooth .
it look veri much like , you know , if you think about estim our trigram paramet of the languag model .
so we have dog given saw , the , and let's sai we have , thi is equal to count saw the dog over count saw the .
let's for the sake of argument assum that thi count is equal to sorri , thi denomin count is equal to sai <num> , and thi numer count is equal to <num> .
and thi estim is go to be <num> .
so , we're go to estim the probabl of see dog given the content saw there as be probabl on .
becaus everi time i've seen saw the , i see dog as the next word .
thi is in some sens an extrem estim .
it doesn't seem realli justifi on the basi of three exampl to estim on .
thi is veri similar .
thi particular exampl here , on the basi of basic three exampl , we're estim thi probabl to be on .
now in practic , becaus we us larg number of featur in thi model , thi is an actual a veri sever problem .
you will end up with mani of the paramet diverg to veri , veri larg valu .
and that will lead to realli rather bad gener in test data exampl in the same wai that thi maximum likelihood estim for trigram languag model , gener thi poli to new sentenc .
these maximum likelihood estim for log linear model will in gener , in gener , gener poli becaus of thi problem where probabl ha go to <num> and paramet diverg to infin .
so , what i am go to show you here is veri simpl and effect solut to thi problem call regular .
and the basic idea is go to be to modifi our loss function slightli .
so again , i'm go to take the optim paramet , v star , to be the paramet v which maxim function l of v .
and what i have here is just exactli the same as what i saw , show you befor , some i equal <num> to n of log p y i given x i under the paramet v .
but now i have a new term which is show you here .
so rememb , we're maxim thi function l of v .
i have minu lambda over <num> .
so lambda is some paramet , is greater than <num> .
and lambda is usual chosen by valid on a develop set .
so usual , you try sever differ valu of lambda and see which model perform better on some held out sort of beta .
it's veri similar to the lambda as we saw for the languag model in that sens .
and then , here i have a sum from k equal <num> to m over vk squar .
so , thi is essenti the length of vk squar .
thi is the euclidian norm , or length of vk .
we squar that .
so , what is thi go to do in practic ?
now we think of our function lv .
it ha two term .
thi first term can be thought of as measur of how well v fit the data .
it's a measur of the more likelihood of the beta , the higher thi valu , the higher the probabl we've , we've given to the actual train exampl .
so , thi term is go to drive us toward fit the data as well as possibl .
but the second term is go to penal larg valu for vk sorri , thi should be v squar .
v squar , the length of thi vector .
so , thi is go to encourag us to keep our paramet valu rel small , or at the veri least , to keep the sum of squar of our paramet paramet valu rel small .
and thi can be thought of as a penalti on the complex of the model .
if you think of simpler model have smaller paramet valu , thi will encourag model with smaller paramet valu .
and so , these two part of thi subject function will also plai off against each other .
thi is a fit to the data , thi is someth which tri to keep a paramet valu rel small .
thi second term can be justifi in variou wai .
but , to cut a long stori short , there ha been a great deal of evid both empir and also in theoret analys , that by includ these kind of penalti , on primit valu , you can learn effect in veri high dimension space .
so , we can now actual have m , our number of featur be extrem larg .
but as long as we penal larg paramet valu , we can still get model which gener veri well to new test exampl .
and in fact , under thi kind of strategi , these log linear model ar highli effect model in mani scenario .
we optim thi function l of v again us gradient base method .
and the gradient look veri similar to what i show you befor .
so now , if we differenti with respect to the paramet vk , these two term were exactli the same as what i show you befor .
thei resolv from differenti thi term here .
and final , when i differenti thi express here with respect to vk , i simpli get lambda time vk .
so , there's a veri simpl chang to the gradient .
so , you can run conjug gradient method or lbfg which is often the method of choic for these model .
under thi new object , and it requir a veri small modif to the method in the simpl chang to the gradient that we saw .
so , here's on exampl of an applic of log linear model .
we'll see sever other in the next few lectur of the class .
but , just to complet the stori of that languag model , let's talk about an earli paper by chen and rosenfeld from <num> .
and thei tri a veri simpl experi to sort of valid log linear model with the kind of regular i just show you .
and so , in thi case , thei just replic a trigram languag model , but where thei us these log linear method to estim the paramet of these model .
so , in thi case , our featur look at a histori .
so , our x that we're condit on is a pair of previou word , and our y is a singl word .
so , thi is just a languag model problem that where we go back to the scenario where we just condit on the previou two word , and thei introduc trigram , bigram , unigram featur .
for exampl , the trigram featur might , here might look at thi particular trigram .
thi bigram , thi unigram and as i describ befor , thei basic includ all trigram , bigram and unigram seen in trend data .
or rather , thei includ featur for everi trigram , bigram and unigram seen in the trend data .
you can then defin a local log linear model where our estim of wi given the previou two word is we gain exponenti thi in our product f . v , and we normal by sum over all possibl word in the vocabulari .
and we could estim these paramet v us gradient ascent on the regular log likelihood function that i show you on the previou slide .
so , there ar a coupl of interest thing to note with thi log linear method to languag model .
on is the follow .
if we us plain maximum likelihood estim , so that is no regular .
we just us maximum likelihood estim as i show you as , as our first attempt at a parameter method .
then , for these veri simpl method , the q paramet end up be the regular old maximum likelihood estim for trigram model simpli the ratio of these two count .
so , thi is what we would have refer to as qml , of wi given i minu <num> and i minu <num> .
so that sort of emphas how these model ar realli unsmooth in the pure maximum likelihood case , and thei will have all the defici we saw of maximum likelihood estim for trigram languag model .
howev , chen and rosenfeld show that with the regular method i show you , thei actual get veri good result .
if you look at the plexiti , it perform at least as well as the discount method or the new interpol method .
we saw wai back in the first lectur or two of thi class .
so that's veri reassur .
so , out of the box , log linear model get highli competit result on thi particular problem .
and that's appeal becaus in mani wai .
the method i've describ is rather cleaner and more eleg and more principl than the estim method we saw us discount and linear interpol for exampl .
the onli downsid in thi particular case is that , recal that to calcul the probabl of ani w given some context , the previou two word , we calcul e of v . f .
and on the denomin , we have a normal term which involv sum over all the word in the vocabulari .
comput thi sum is slow .
we're liter go to have to sum over all possibl word in the vocabulari .
and so , that is a downsid in thi particular context of languag model .
have said that , log linear model ar incred effect in mani domain .
and we mai see a resurg of their us in languag model as peopl start to come up with approxim or other method to deal with these kind of term more effici .
thei certainli have the benefit of be abl to incorpor featur in a much more flexibl and clean wai than the method we saw with linear interpol .
so in last week's lectur , we develop log linear model .
thi veri new wai of look at model for natur imag process , and also paramet estim and natur imag process .
in the current week's lectur , we go to look at variou applic of log linear model .
two problem in natur languag process .
and , the first problem we're go to cover in thi segment is go to be the applic of log linear model to tag problem .
we'll see thi is a veri import exampl of how log linear model can be us in lp .
and we'll see how to develop power altern to the hidden mark of model tagger that you saw earlier in thi class .
so histor , log linear model for tag ar often also refer to as maximum entropi markov model .
why is that ?
well , log linear model , or sometim refer to , particularli in the earlier literatur , as maximum entropi model .
so we , we won't realli go into the reason for that name .
but it's worth bear in mind becaus sometim you will see the term maximum entropi model us , and then you'll know what it is .
it's basic just a log linear model .
and the markov come in becaus these ar essentailli go to be model for tag .
that share mani of the characterist of hidden markov model .
although , as we'll see , thei provid a veri us altern to hidden markov model .
so first , let's give a quick recap of the tag problem .
so , rememb we consid a coupl of veri import exampl of tag problem much earlier in thi class .
and the first on we look at wa part of speech tag .
so the problem here is to take a sentenc as input , and provid an output where each word in the sentenc now ha an associ tag .
for exampl , profit is a noun sword is a verb at is a preposit and so on and so on .
so extract the problem is to take a sequenc of word as input and produc a sequenc of tag as the output of thi model .
so each word get a singl tag , the other import tag problem we look at wa the problem of name entiti recognit .
so in thi case the input is again a sentenc and the output from the model is a segment of that sentenc .
where we identifi import segment correspond , for exampl , to compani such as boe .
locat , such as wall street or peopl , such as alan mulal in thi particular exampl .
and we describ how the name entiti recognit problem can also be frame as tag problem .
let me just remind you of that .
where each word is tag with on of a number of possibl tag list down here .
for exampl , some word ar tag as na , mean that thei're not part of an entiti .
wherea , other word ar tag as the start of a compani or the continu of a compani or the start of a locat , continu of a locat , or the start of a person and the continu of the person .
so that's just a remind that we can frame thi name entiti problem , thi name entiti recognit problem , which is a segment problem , actual as a tag problem .
so , as befor we're go to treat thi problem as a machin learn problem and so , our goal is the follow .
we have some train set which consist of , perhap , a quit larg quantiti of exampl sentenc where each sentenc ha the underli tag mark , like thi .
and from thi train set , we need to induc a function or an algorithm that map new sentenc to their underli tag sequenc .
and in , i think it wa the second week of class we saw hidden markov model and the viterbi algorithm as on wai of achiev thi goal .
so recal in hidden markov model , we would read off paramet estim by take count from our train sampl .
and then for a new test exampl we would us the viterbi algorithm to find the most like text sequenc under the underli model .
okai , so now let's consid how we can develop a log linear model for thi tag problem .
so in gener we'll assum that we have some input sentenc , w1 through wn .
for exampl , thi might be someth like , the dog bark .
and i'll sometim us the follow notat .
i will us w sub <num> colon n to refer to thi entir sequenc .
okai , so wi is the i'th word in the the particular sentenc .
and in addit , we have some tag sequenc , t1 through tn .
so from here , we might , for exampl , have the sequenc d n v equal to t1 through tn , and similarli , i'll us thi notat t sub <num> colon n as shorthand for thi entir sequenc .
now the kei idea in log linear model , is , we'll see how to construct a model , of the condit probabl of ani tag sequenc .
given an underli word sequenc .
so there ar go to be mani possibl tag sequenc for our given input sentenc .
so let's sai , for exampl we have , you know , mani other possibl tag sequenc .
i'm write a few of them down here and so on and so on .
we ar go to assign each of these tag sequenc a probabl .
so we might have <num> , <num> , <num> . <num> , <num> . <num> , <num> . <num> , and so on .
and if we sum up all of these probabl , thei will sum to <num> .
okai so we have a well defin condit distribut over the set of possibl tag sequenc for thi particular input sentenc .
now on veri import thing to realiz is that thi is in quit stark contrast to the case we saw for hidden markov model , for hmm .
so rememb , in hidden markov model , we actual defin a joint probabl distribut , over tag sequenc , pair with word sequenc .
okai ?
so , first interest thing to see in these log linear model , is that we will directli model thi condit distribut , and we wont actual attempt to model thi joint distribut here .
so that's the first kei differ from hidden markov model .
onc we've deriv a model of thi particular form , we can then appli it to a new test sentenc .
so if i have some test sentenc w1 through wn then i can take the most like text sequenc t star to be the tag sequenc t1 through tn that maxim thi condit probabl and remeb with hmm we would had someth veri similar .
in fact we would have t star lin 2n , is equal to augmax .
t , <num> , 2n .
you have the joint probabl of t , <num> 2n , and w , <num> , 2n .
so , it's realli veri similar , except again , we're us a condit probabl here .
for the log linear case where it's in , be , befor the hidden markov model , we had the joint probabl .
so there ar go to be some kei question we'll answer in thi lectur .
you know , firstli , how do we defin thi condit distribut ?
how do we estim the paramet of the model ?
which underli thi definit .
and final , we'll have to answer the question how do we actual effici find thi most like tag sequenc for a given input sentenc ?
so the critic question is go to be how do we model thi condit distribut ?
the probabl of a given tag sequenc , t on through tn given a particular word sequenc w1 through wn .
and again we'll see thi method us where we first us the chain rule and secondli we us independ assumpt which , simplifi the model .
so here .
is a first applic of the chain rule .
so i'm go to write thi probabl , as a product of term .
so i have on term for each posit , j equal <num> to n .
and then i have the condit probabl of the jth tag valu .
condit on the full sequenc of word , w on through wn .
and in addit condit on the previou j minu on tag .
thi step is as usual with the chain rule , it's exact in that ani distribut , ani condit distribut of tag given word can be decompos into a product of term in thi wai .
so that's step <num> , the second step is to make an independ assumpt .
which actual look veri similar to the markov independ assumpt seen in trigram languag model and then in our trigram hmm .
so what have i done here ?
i have basic replac thi sequenc of tag t<num> through tj minu <num> with just the last two tag in that sequenc .
tj minu <num> , tj minu <num> .
and , i've just been a littl care here to make sure to defin t0 and t minu <num> to be a special star symbol , which is basic the symbol of the start of the sentenc .
so what i'm sai here , is that each tag inform onli depend on the previou two tag .
more formal it mean that the valu for thi random variabl , tj , is condition independ of all the tag preced the previou two tag onc i condit on the entir sentenc , and also the previou two tag .
so , again thi look veri much like the log of assumpt we'd seen earlier in the class .
the onli real differ is with condit on the entir sentenc throughout thi decomposit .
sort of carri along thi entir sentenc .
here again is the independ assumpt thu made in thi model .
so let me give you a particular exampl .
sai i want to write down the condit probabl of the tag sequenc dnv given the three word the dog bark .
then thi is go to be decompos into product of three term .
firstli the probabl of d given that i have the sentenc the dog bark and given to the previou two text with star which were all start symbol .
so our sequenc , essenti , look like thi , we have star , star , d n v .
we're go to have <num> probabl term for each of these <num> tag .
so the second term is go to be p of n given the dog bark , and star d .
and the third term is go to p of v given the dog bark , end of d n .
so again each tag ha a probabl term p of t j that's condit on the previou two tag , and the entir input sentenc .
the critic question now is go to be , how do i estim these probabl ?
how can i defin a model for these probabl , and estim the paramet of that model from train exampl ?
and we'll see that thi is go to be a direct applic of the idea from log linear model that you saw in the lectur in the last week of thi class .
so let's consid thi model problem that we're left with a littl bit more close .
and to do thi , i'm go to us the follow exampl .
so if we consid thi word base , there ar mani possibl tag .
at thi particular posit for thi word base .
and we'll us script y to refer to the set of possibl tag .
so we have , for exampl , singular noun , plural noun , transit verb , intransit verb , preposit , determin , and so on and so on .
and our task is to go to be to estim the probabl for ani on of these possibl given the context .
so , what is the context ?
the context is go to be the previou <num> tag .
and in thi case the determin adject , and potenti ani featur of the sentenc .
so rememb we have p of t j , given w <num> through w n , sentenc , then tj minu <num> tj minu on .
so we can condit on the previou two tag , and we can condit on ani inform in the entir sentenc .
and of cours , thi is a veri complex object that we're condit on .
there ar mani , mani possibl featur of thi context , which could be us and that's realli the motiv for us log linear model in thi context .
so , to appli log linear model , we're go to have to develop a littl bit of a notat .
and a critic idea is go to be the idea of a histori , which basic captur all of the inform in the context .
all of the contextu inform we can condit on .
and a histori is go to be a <num> tupl consid consist of the follow .
we have the previou two tag , t minu <num> , and t minu <num> .
we have the entir sequenc of word in the input sentenc .
w1 , through wn .
and then , we also need to be a littl bit care , to make sure we condit on the posit of the sentenc that we ar tag .
so i , is go to be an index , of the word be tag , the posit in the center .
so let's see how thi <num> tupl , is substanti for thi particular exampl .
in thi case we have t minu <num> and t minu <num> , equal to the <num> tag dt , follow by jj .
we have w1 through n , consist of the entir input sentenc , the entir sequenc of word .
and then we have the posit i is equal to <num> becaus if we count the word in the sentenc , we have on , two , three , four , five <num> , thi is at posit <num> in the sentenc .
so again , thi histori basic captur all of the inform about the context .
the previou two tag , the entir sentenc , and the particular posit in the sentenc .
we'll us script x to refer to the set of all possibl histori .
so that's go to be a veri larg set .
actual , it potenti , an an infinit set , a countabl infinit set , if we allow sentenc of all possibl length .
so given these definit we can appli a log linear model in a veri direct wai .
and so here the gain is a re cap of the idea of featur vector represent as us veri directli in log linear model .
so , in gener in a log linear model we're go to assum some set of possibl input or histori , in all tag case , thi again is go to be a <num> tupl , t minu two , t minu on , w on to w n and some posit i .
and we'll also assum that we have some finit label set which we us , which we denot by script y for exampl , thi might be the set of all possibl part of speech tag , mayb d , n and v .
our aim is to provid a condit probabl , py given x , for ani histori x combin with ani label y .
so the kei represent trick in log linear model is to make us of featur .
so a featur is a function that take a histori pair with a label , and map it to some valu .
and we saw last week , when we consid log linear model that mani of the featur us in nlp ar these rather simpli binari featur where the result of the function is either zero or on .
these , these featur typic ask question about the input x in conjunct with the label y and return zero if that question is fals on if the answer to that question is true .
and then , given a set of n featur , f sub k if equal on to m , we can defin a featur vector to be a vector consist of the m valu of these m differ featur in the model .
and so , basic we end up with map , a particular .
histori or input pair with a particular tag , to some featur vector which repres the output sorri the result of all of these featur or question about the histori in conjunct with the label .
so here's some exampl of how we might instanti featur in the tag problem .
so we have x instead of all possibl histori of thi form , y is a set of tag .
we have variou featur .
so the first exampl featur is the follow .
so f<num> is go to be <num> if the current word wi is the word base and the tag t is equal to vt .
so thi intuit is go to captur the affin .
for that particular word base we were consid , for have the transit verb tag .
now thi is just on featur , of cours .
you would in practic defin mani potenti featur .
quit like correspond to everi possibl word pair with everi possibl part of speech tag .
here's a second type of featur , f two , and thi sai the follow .
it's go to be on if the current were wi , end in the suffix ing , and that tag is vbg .
thi is a gerund verb .
thi is the tag for a gerund in the pen tree bank , for exampl .
and , word end in ing in english ar veri often tag as vbg .
now , you'll start to see someth interest happen here , which is that in a sens thi featur is captur by the paramet in an hmm which is e of base given vt .
that is the natur paramet in an hmm which captur the affin for the word base to be a trasit verb .
but thi realli is a new type of featur , which look at some spell featur of the word be tag .
in thi case , the suffix ing .
these kind of featur ar much more difficult for hmm .
and in fact , we saw in the program assign to thi cours and actual in the lectur slide for hmm .
the you know on common wai of deal with these kind of spell featur wa to introduc map from rare and infrequ word to mayb <num> or <num> differ word class .
but that wa realli a rather crude wai of do thing .
it's much more conveni to be abl to defin featur like thi which will look at the spell in conjunct with the tag .
and actual those ar two featur type from road g by adwait ratnaparkhi which is on of the earliest applic of log linear model to tag .
actual probabl the earliest .
and i can actual describ the full set of featur he us in hi model .
so there's a pretti simpl set of featur and yet , it's still close to state of the art in that , if you us these featur in a part of speech tagger for exampl , you will get a state of the art model .
so , he introduc word tag featur , so featur like the on i've shown you here , that pair a particular word , with the ident of wi as a particular word with a particular tag .
and he includ on featur like thi for everi word tag pair seen in your train sampl .
so , a quit larg number of featur which pair word with underli tag .
he also consid spell featur , which look at all prefix or suffix of length , less than equal to four .
okai , so here is thi exampl featur again , which look at the suffix be i of g in a particular tag .
and here is a featur , which look at the prefix , and fire if the particular word wi , start with the three letter pre .
and the tag in thi case is a common or a rather a singular noun .
so again , we're go to consid all prefix and suffix of length less than or equal to four potenti combin with all possibl tag which ar possibl , a quit larg number of featur .
but these featur ar invalu when you're deal with infrequ or unknown word .
these kind of spell futur can be veri us in di , disambigu those word .
ratnaparkhi also look at contextu featur .
so these first three featur look veri much like trigram .
bigram , and unigram featur , over tag .
so thi first featur is on , if the tag sequenc , t minu <num> , t minu 1t is equal to dt jj vt , and it's zero otherwis .
so thi captur a particular trigram of tag and you would most like have on featur like thi for everi possibl trigram of tag .
or at the veri least on featur like thi for everi trigram of tag seen in your train sampl .
here's a bigram featur , so thi face is if the previou tag is jj and the current tag be propos is vt , so that's a bigram of text .
and then final we have a featur which just look like a unigram , look like a tag itself , a lone t and just fli on the fact that it's a vt .
now , of cours , we're go to have featur , sorri , paramet in our model .
v103 , v104 , and v105 , which ar basic go to be in some sens correl with the probabl or likelihood of thi trigram or thi bigram or thi unigram , of tag .
ratnaparkhi also made us of featur , which , in addit to look at the current word wi be tag , so gener we might have , for exampl , thi might be the word base .
we can also look at word surround thi .
see if it left or the immedi right .
so here is on featur which is on if the previou word .
wi <num> equal there and the tag equal vt .
well we might have a featur look at the next word .
the fact that wi plu <num> equal the and the t equal vt .
so you can see that the flexibl of log linear model , allow us to defin all kind of featur , which look at the current tag be propos as well as featur of the context .
whether those featur ar the previou two tag in the context , or their surround word , in thi particular context .
so that wa a descript of a set of potenti featur in our log linear attack .
thi is just a recap of local linear model then take featur and produc condit probabl distribut of the form py given x under a paramet vector v .
so we've alreadi said that we have featur f sub k for k equal <num> to m , and thi give us a featur vector .
and recal that in log linear model , we also assum we have a paramet vector , v in r to the m .
so , if m equal <num> for exampl , we might have f of xy is equal to <num> , <num> , <num> and we might have v is equal to <num> , <num> , <num> .
we can take the inner product of these two thing , so v of f xy is equal to <num> time <num> plu <num> time <num> plu <num> time <num> , so that is equal to <num> .
and we can comput thi inner product for everi possibl label or in conjunct with x .
so thi condit probabl , as we describ last week in the lectur on log linear model , is to find out e to the power of v of f divid it by a normal term , which involv the sum of all possibl label and e of v of f with that particular label .
how do we estim the paramet of these model ?
where do these v valu come from ?
thei typic come through learn paramet on a set of train exampl in particular us the regular log likelihood method we describ last week in the class .
so , abstractli we assum some train set consist of xi , yi pair for i equal <num> to n .
then we choos our paramet valu v star to be the vs that maxim the sum of two term .
so firstli , i have the sum of log probabl , my train exampl .
and secondli , you have a neg term .
so lambda is some constant which is greater than zero dictat the rel weight of thi term .
and what i have here is basic the length squar of my paramet vector v .
or more directli , as written here , the sum of squar paramet valu .
so thi regular is go to keep paramet valu small .
wherea , thi term is go to encourag the paramet valu to fit the train sampl as well .
and as we said befor , thi regular term gener it lead to much better gener in case where we could potenti have veri , veri larg number of featur .
so concret , in log linear tagger , our train set , these xi , yi pair is go to be a set of exampl , wwhere each xi consist of a histori .
for exampl , dt , jj , the red dog <num> .
so rememb , a histori consist of a pair of tag , the sentenc , and the posit that is be tag .
and each yi is go to consist of a part of speech tag .
so we can take our train exampl , our tag sentenc .
and for everi posit in our train sampl , we can extract a histori .
we can extract a tag , and we can us those histori tag pair as the train exampl for train the paramet of a log linear model .
so we've now describ some kei step in construct a log linear tagger .
firstli , how to defin featur that take into account featur of the context or the histori , in conjuct with the current tag be predict .
secondli , we've describ how to estim the paramet of thi model , essenti by optim regular log likelihood .
in exactli the same wai we saw in last week's lectur on log linear model .
the final piec of the puzzl is the question of how we appli a log linear model to a new test sentenc .
so thi mean we're go to take some test sentenc input , for exampl , the dog bark .
and we're go to search over all possibl tag sequenc , for the tag sequenc that maxim thi condit probabl .
so , you might consid the tag sequenc of ddd , ddn , dnd .
and so on and so on .
and we're go to return the tag sequenc that ha the highest valu for thi condit probabl .
now , as usual and as argu earlier in thi cours , brute forc search just isn't go to scale to ani appreci sentenc length .
there's no wai i can explicitli enumer all possibl tag sequenc , calcul the probabl of each subsequ , and then return the highest on .
but we'll see that we can again us the viterbi algorithm .
and actual it's a veri minor variant of the viterbi algorithm as seen for hidden markov model .
and that will find the highest score or the highest probabl tag sequenc veri effici .
and the critic observ is that we're go to make the assumpt that thi condit probabl take the follow form .
so notic i have a product from i equal on to n and i have q of t i , given the previou two tag , the entir sentenc and the posit .
the kei properti of thi definit is that at each point i condit just on the previou two tag .
i have thi markov like assumpt .
and that will again allow us to appli dynam program or the viterbi algorithm to thi problem of find the highest score tag sequenc .
as it happen in our case .
thi q , thi estim is the output from our log linear model .
which make direct us of paramet and these featur that we saw earlier .
so let's build up the viterbi algorithm in thi case in a veri similar wai .
to the wai we solv for hidden markov model .
so , a kei idea will be to defin a dynam program tabl , so pi  kuv , is go to the maximum probabl of ani tag sequenc end in tag u and v , at posit k .
so a littl bit more formal , we'll defin a function r , which take a sequenc of k tag , t1 through tk .
and calc , it calcul their probabl onto the model .
so notic , thi is basic a truncat form of the probabl for an entir tag sequenc .
we're now multipli through articl <num> through k , q of ti , given the previou two tag .
and we defin pi  k , u , v .
to simpli the maximum probabl for ani tag sequenc end in u and v , at posit k .
so that's what thi definit is sai .
again , that's veri similar to what we sai for hidden markov model .
in the hidden markov model , we also saw dynam program tabl of thi form where thi pi  valu correspond to maximum probabl of prefix of tag sequenc .
here is the recurs definit .
so firstli , we sai that pi  of <num> star , star equal <num> .
that's the base case of the recurs .
thi is ident to the hmm algorithm so no chang there .
and then , a recurs definit look like the follow .
so again will defin s sub k to be the set of possibl tag at posit k in the sentenc .
that's exactli the same as the definit for the algorithm for the hidden markov model .
and then for ani k , for ani u and sk minu <num> , and v and sk , i defin pi  k , u , v to be the follow .
i take the max of all tag , the posit k minu <num> , i multipli n pi  of k minu <num> , tu .
so that's a pi  valu of posit k minu <num> and note that we have posit k over here .
and then here , i just have the condit probabl of v given the previou two tag t and u .
given the entir sentenc , w and given the posit , k .
and so , thi would be the output from a log linear model .
now , i'm not go to go through the justif for thi recurs in great detail .
but it is extrem similar to the justif we saw for the recurs , when appli to hidden markov model .
so , in fact , what's happen here essenti is that in hidden markov model , instead of thi term , we would have had someth like the follow .
we would have had q of v given t u and then secondli e of wk , which we have the k for word , given tag v .
so we'd have thi product of two term .
the trigram term for a tag .
and then the , the emiss term for the probabl of w k be emit by v .
in thi log linear model , we have thi new term which the condit probabl of v given t and u .
under the entir sentenc w <num> through n , and the posit k .
otherwis , thi recurs definit is almost exactli the same as what we saw befor .
so , here is the viterbi algorithm which put all of these idea togeth .
and again , i want to stress , it's veri familiar to the viterbi algorithm you saw for hidden markov model .
so the input to the algorithm is the test sentenc , w1 through wn , which we'd like to tag .
and in addit , we'll assum that we're provid with a log linear model that provid the condit probabl of ani v given tag t and u , given an input sentenc and a posit .
so those ar the two input to the model .
the model then proce as follow .
so in the initi step , we set pi  <num> star , star equal to <num> .
and then we do the follow .
so for everi posit k , in the rang <num> to n , we consid all possibl tag , u in posit k minu <num> , and v at posit k .
and we're go to fill in the valu for pi  k of u and v .
we do that by search for the tag t , at posit k minu <num> .
that maxim the product of these two term .
so i have pi  of k minu <num> , t and u .
and then i have thi q term refect , reflect the condit probabl of the tag v in thi particular context , under thi particular histori .
as befor , i keep track of back pointer for everi k , u , v tripl .
which just record the arg max .
that is the tag , t .
that actual achiev thi maximum .
so that's the main bodi of the algorithm .
onc i've complet these step , i can then find the highest score tag sequenc by follow the back pointer backward through the sequenc .
so at first we set tn minu <num> , tn .
to be the uv pair that maxim pi  , n , u , and v .
and then i work backward through the sequenc from posit n minu <num> down to <num> .
at each point sai the case tag is equal to the back pointer from posit k plu <num> with tk plu <num> and tk plu <num> .
so i'm essenti work backward through the sequenc .
first recov the last two tag , sai d of n , d and then n , and then fill in the previou tag on by on , backward through the sequenc .
onc i've done that , i final return the tag sequenc t <num> through tn , and that is the highest probabl sequenc under the log linear modal .
so i want to finish thi segment on log linear tagger , with an exampl of a particular , applic , that illustr their real strength .
and thi wa in a paper by andrew mccallum and other .
which wa anoth absolut semin refer in the us of log linear mole .
so mccallum compar a hidden markov model and and a log linear tagger on a fact segment test .
we'll see in a second just what that is but the main point is that in thi particular domain model condit probabl of a word given a tag is veri difficult .
so rememb , the hmm had two type of paramet .
for exampl trigram tag paramet , the probabl of v to the t of u .
and then admiss paramet , word given tag .
thi can be a difficult distribut to model , and it realli hamper the abil of hmm to take into account compec , complex featur .
when you build tag model .
so thi is the , the faq segment task that mccallum and hi collabor look at .
and so in fact thei're go to be tag entir line of a faq .
so a faq is go to be a sequenc of line .
as we've seen here , we have on , two , three , four , five , six , close to <num> line .
some of them will be blank , notic we have a few blank line here .
and the goal wa to assign on of three possibl tag to each line .
so a line can either be part of a header , so we have these head tag for the header of the faq .
thei can be part of a question , so we have a tag here , question , sai thi is a question .
and final , thei can be the tag answer , if we have an answer to a question .
so you can see how thi is a us task in that if you can perform thi kind of tag accur .
then you could automat pull the question answer pair out of thi rather unstructur data that you would see on these , at least these type of thing .
so , on thing that's worth note is that each word in thi tagger , is an entir sentenc .
okai , so we're tag sentenc by sentenc .
so the item that i'm actual tag ar quit complex object .
thei're entir sentenc .
so it'll be critic in thi model .
to make us of variou featur or entir sentenc or entir line , rather .
so , here ar some featur that ar us in the sort of by mccallum and other .
and these ar gener binari featur , which will be true or fals for a particular line .
so we could ask whether a line begin with a number .
whether is begin with a autom .
whether it begin with punctuat .
whether it begin with a question word , where these will be in a sort of close set of word like woo , who , where , what , and so on .
we can look at whether the line is blank .
whether it contain an alphanumer , whether it contain a number , and so on and so on .
so you can imagin make up a whole bunch of featur of individu line which might give some strong evid about the content of that line .
and most importantli , would give strong evid about whether it fall into on of those three categori , a header , a question , or an answer .
so , we can now describ the featur that we us within the log linear tagger in thi work .
and thei're realli rather intuit .
thei ar go to look at properti of each line .
for exampl , the fact that a line begin with a number .
in conjunct with the current tag , sai question , and also the previou tag .
so in thi case we're go to look at a bi gram of tag .
rememb we alwai have the abil to look at the previou on or two tag in the context .
and in addit , we look at some featur of the line .
for exampl , the fact that it begin with a number or it contain an alphanumer , contain a nonspac , contain a number and so on , and so on .
and so , thi is the basic set of featur which were us in the log linear tagger .
a more challeng task is actual come up with a hmm tag model for thi particular problem .
again , and thi is becaus the quot , word , in thi particular .
applic ar entir line .
so how do i estim the probabl of thi entir line , given the fact that the , the tag is question ?
so the first method that mccallum and the other us wa to break thi down into a sequenc of term , on for each word .
and so we'd have mission paramet where we condit each word and term on the ident of the tag question in thi case .
and so i have e of <num> given question , is that first word , time e of what given question is the second word .
and so , here , i just have on e term for each word in the sentenc .
thi look veri much like a languag model for each tag , for exampl , question .
in fact , thi is essenti a unigram languag model , which is try to defin the condit probabl of a particular entir sentenc or entir line condit on a particular tag .
so , that's the first method that these gui look at .
and onc we have these distribut , we can appli the hidden maskov model tagger in the usual wai .
here's the second method thei us .
and in thi method , thei took each line in train or test data , and immedi transform it into a sequenc of featur .
so , in thi case , we'd have a sequenc sai that it begin with number , contain alphanumer , contain nonspac , contain number , previou is blank .
which mean that the previou line is blank , for exampl .
and then we again us what is basic a , a kind of a unigram languag model .
where we have on term for each of these featur .
so we have , a probabl that begin with number given question , time the probabl of contain alphanumer given a question , and so on and so on .
so , it look veri much like the model on the previou slide , but we just go through thi transform step first .
which extract the import featur from each individu line rather than simpli stai with the word .
so let's look at the result for the differ method .
and thi is go to be in term of precis and recal at recov entir segment .
where a segment might correspond to a header or to a question or an answer .
so thi first result or thei call a me stateless .
thi stand for maximum entropi stateless .
rememb , a maximum entropi model is basic a log linear model .
thi wa a log linear model but where there wa no condit on the previou two tag in the context .
we onli condit in each case on featur of the individu line be tag .
and that model perform quit poorli .
that's realli just evid that the surround context in the form of the previou two tag is actual veri us .
tokenhmm is the first approach to build a hidden markov model that i show you in the previou coupl slide .
where we simpli calcul the probabl of an entir line , by basic creat a unigram languag model .
which had a singl term for each word in the line .
featurehmm wa the second hmm solut i describ to you , where in a first step , we took a line .
and transform it to the sequenc of featur , repres that line , rather repres featur of that line .
and we can see , we get a slight improv here , these number ar get better and better .
and of cours , memm is a log linear trigram tagger .
and again , these thing ar sometim refer to as maximum entropi markov model .
so an memm perform consider better than ani of these other , the other method .
there's a veri clear advantag of a hidden markov model .
okai .
to summar thi segment of the class on log linear tagger .
these were the kei idea .
the first kei idea wa , to directli model the condit probabl of ani tag sequenc condit on a word sequenc .
us a decomposit , where we have a product of term .
we have a product of posit , i equal <num> to n .
and at each point we condit the valu for the ith tag on the previou two tag and on the entir sentenc be tag .
thi left , left us with a model problem .
the question of how do we actual model these condit probabl , we're condit on a lot of potenti context here .
there ar mani potenti featur that might be us , and so the second kei idea in log linear model wa to estim these term us log linear model .
that mai quit rich us of featur look at the context preview through text and potenti realli arbitrari context to these surround word .
the final insight in log linear model , is that for a given test sentenc w1 through wn , we can us the viterbi algorithm , to find the highest score or the highest probabl tag sequenc .
and we can us the viterbi algorithm , becaus we do have thi markov style assumpt of these model .
that the ith tag , depend onli on the previou two tag in the context .
final , the kei advantag of hidden markov model , is realli the flexibl in the featur definit that log linear tagger can us .
we're now empow to make up all kind of featur , which if you look at the current tag and the previou histori where the histori consist of all of thi inform .
so , for exampl it wa veri easi to incorpor prefix and suffix featur about word , or inform about the previou word , or the next word , when we're , we're tag the current word .
all kind of other featur of the input sentenc , and the surround context .
and thi realli is a , kei reason for prefer log linear tagger over markov model .
the downsid , of cours , is that thei're slightli more complic , in term of estim .
the paramet estim method for log linear tagger can be a littl bit more complex and can be a littl bit more computation expens .
but in gener it's worth the extra effort .
so in the last segment of the class , we saw log linear tagger , where we show that log linear model could be appli in a veri effect wai to the tag problem .
in thi next segment , i want to describ how we can gener thi gener approach to a much wider class of problem .
we're go to consid the pars problem again , us a method call histori base pars .
again , make extens us of log linear model .
but hopefulli you'll that the gener method that i'll describ in thi segment of the class could be potenti appli at a larg class or problem in nlp or for that matter other field .
so , here's a quick recap of how log linear model were appli to the tag problem .
so , rememb we us thi notat w1 colon n , to refer to an input sentenc w1 through wn .
and similarli t1 colon n is a tag sequenc .
the cri , first critic idea wa to defin the condit probabl of ani tag sequenc t1 to n condit on a word sequenc w1 through wn by first us the chain rule , and secondli , us independ assumpt .
and as a result , we end up as thi condit probabl , have a product of term we have product here from j equal <num> to n .
at each point , we have the j'th tag , in a sequenc t sub j .
and that's condit on the entir input sentenc and the previou two tag in the sequenc .
so , again , we have a mock off style assumpt where the j tag depend just on the previou two tag .
but in addit , we condit on the entir sentenc at everi point .
we're left with the problem of estim these p term and that's a complex problem becaus we're condit on a larg amount of inform .
there ar a larg number of possibl featur you might want to incorpor in a model which predict thi condit distribut .
so , we us log linear model for thi task , which as we argu , ar quit flexibl in term of the type of featur that can be includ .
final , we us the viterbi algorithm , onc again , to comput the highest probabl tag sequenc for ani input sentenc .
so , given an test sentenc , an input sentenc , w1 through to wn , we can search for the tag sequenc that ha the highest valu for thi condit probabl .
and that can be achiev us the veterbi algorithm .
so now , we're go to consid how to gener thi approach to a much wider class of problem .
we'll focu on the paus problem .
but as you'll see , we'll see some gener principl emerg in how we can appli log linear model to more complex problem .
and , the class of model we'll describ ar often call histori base model .
thei're actual condit histori base model becaus we'll be model condit distribut , such as p of t and w .
so , the critic question is the follow .
here i'm us the notat capit s equal to the input sentenc .
i'll us thi throughout these slide .
how do we defin the condit probabl of big t given big s if t is not a tag sequenc , but rather it some other structur for exampl a pars structur ?
so how can we gener the idea that i show you for log linear tagger to the case where thi structur t might be a pars tree , or some other object , mayb even a translat , for exampl , of the sentenc .
so , here's a veri high level outlin of the approach we'll us .
and throughout the rest of thi segment of the class , we'll instanti the variou choic shown in thi outlin .
so , there ar essenti three step , rather four step , in these condit histori base model .
the first on is go to be to take our structur , for exampl , a pars tree and repres it as a sequenc of decis , d1 through dm .
so , we'll see veri soon that thi sequenc of decis , basic correspond to us build the tree in some kind of bottom up left to right order , over the input .
notic that thi valu m , the number of decis , is not necessarili the same as the length of the sentenc .
in the tag model we saw , the sequenc of decis were essenti the n tag decis , correspond to the n word in the sentenc .
but as we go to more complex structur , we're go to see that the number of decis can actual be differ from the length of the sentenc .
the second step is to us the chain rule to decompos the probabl as follow .
it's the condit probabl to tree t , given a sentenc s .
it's go to be a product term , where at each point i have the condit probabl of the ith decis in my sequenc .
and i condit on the i minu on previou decis .
and in addit , the sentenc which is input to the model .
now , notic here we haven't actual made ani markup assumpt , and the tag case , we we're to replac thi with the di minu 2di minu <num> , reflect the fact that we make a kind of mark off assumpt that we just condit on the previou two decis .
in fact , in the histori base pars model that we see , we're not go to make thi assumpt .
okai ?
so , we could potenti condit on ani inform in the context , ani inform in the sentenc be paus and ani inform in the previou sequenc of decis .
the third step is go to be to us a log linear model to estim thi cost of condit probabl of the decis given the previou i minu <num> decis and the input sentenc .
and we'll make us of the flexibl of log linear model in term of the featur that thei can includ .
and thei'll be , thi will be anoth set where we realli see the power of log linear model in term of the represent thei can us .
the final question is search .
so , in gener we're go to take some sentenc s as input .
and we're go to try to find the pars tree t that maxim thi condit probabl t given s .
where thi , of cours , is defin through thi express here .
now , crucial , we're not go to make a markup assumpt in thi case .
and that mean , in the gener case , that dynam program algorithm won't be avail .
we won't be abl to make us of the trick that we saw in dynam program to give us the exact but effici algorithm search in these model .
but we'll see that we can us someth veri similar to the kind of beam search algorithm we saw for phrase base decod .
we can us a veri similar kind of algorithm in thi particular context .
so , the first import question in thi kind of approach is , how do we implement step on ?
how do we repres a tree as a sequenc of decis ?
and so , the next sever slide in thi segment will go over thi in some detail .
thi is an exampl tree that i'm go to us throughout thi part of a lectur .
so , we have a sentenc .
the lawyer question the wit about the revolv .
we have some standard pen tree bank style tree , and pars tree for the sentenc .
and you'll notic that we have head word in thi tree in the same wai as we saw with lexic context free grammar .
so , i'm go to focu on an approach develop by adwait ratnaparkhi in hi phd thesi .
thi is hi approach to pars .
thi is on wai of repres a tree as three sorri , of repres a tree as a sequenc of decis .
and he actual us three layer of structur in these decis , which we'll go over bit by bit .
the first set of decis correspond to part of speech tag decis .
so actual , the first level of the pars model look veri much like a regular log linear tag model .
and then , we build up what ar call chunk .
and then final , we'll build up the remain structur in these pars tree .
so , here's layer <num> .
so , just to recap , we're try to repres a tree as a sequenc of decis , d1 through dn .
so t is go to be repres as a sequenc of decis .
we have some input sentenc , w1 , w2 , up to wn where littl n is the number of word in the input .
so , in ratnaparkhi's parser g , the first n decis in the sequenc of tag decis .
so , d1 through dn is just the sequenc determin a noun , verb , determin a noun and so on , seen left to write in thi particular input sentenc .
okai , so the first n decis ar simpli the tag decis as seen in the log linear tag model .
so that's the first layer , and simpli part of speech tag .
the second layer correspond to what ar call chunk .
so in the second layer , we're go to have a sequenc of decis that recov the chunk within the pars tree .
a chunk , under ratnaparkhi's definit , is defin as ani phrase where all the children ar part of speech tag .
so , for thi particular pars tree that i show you in thi exampl we're work on , you can verifi there ar actual three chunk .
the lawyer ha a np chunk , the wit ha an np chunk , and the revolv ha an np chunk .
and you can see each of these constitu , each of these three np , satisfi thi condit that all the children ar part of speech tag .
and actual , in all case , thei have two children .
firstli , a determin , and secondli , a noun both of these part of speech .
so now , phrase ar on veri common type of chunk .
other common chunk ar adjp titl phrase , and qp which ar actual essenti numer phrase .
so , thing like <num> might be a qp .
ratnaparkhi introduc thi level , i think , becaus he found it wa benefici to first recov these low level chunk befor you recov high level of the pars tree .
so , these chunk , as it turn out , can be recov with quit high accuraci .
and onc thei've been built , we can build the higher level of represent on top of these .
so , recal that the whole game here is to defin a map from pars tree to decis sequenc d1 through dn .
and so the second layer , thi layer of chunk , is also go to be encod as a sequenc of decis .
and our first n decis in the pars process ar go to be tag decis .
then next n decis ar go to be what ar call chunk tag decis .
and that mean i'm go to actual tag each word at thi next level in the tree .
with some tag indic whether or not it part of a chunk .
specif , i'll have tag such as start np at the start of each noun phrase chunk , or join np .
thi is basic sai we have the continu of a noun phrase chunk , and other for word which ar not part of chunk .
so notic , as we saw on the previou slide , i have three np chunk in thi particular exampl .
thei're mark here , on , two , and then three .
and you can see how these start and join , and other annot encod those chunk decis .
so , we have m decis total , and the first two n have now been cover .
the first n ar these part of speech tag decis , and the next n ar these chunk decis , start , join , other , start , join other , start , join .
so recal that the whole game here is to defin a map , from pars tree to decis sequenc , d1 through dm .
and so , the second layer , thi layer of chunk , is also go to be encod as a sequenc of decis .
and our first n decis in the pars process can be tag decis .
the next n decis ar go to be what ar call chunk tag decis .
and that mean i'm go to actual tag each word at thi next level in the tree .
with some tag indic whether or not it's part of a chunk .
specif , i'll have tag such as start np at the start of each noun phrase chunk , or join np .
thi is basic sai we have the continu of a noun phrase chunk , and other for word which ar not part of chunk .
so , notic as we saw on the previou slide , i have three np chunk in thi particular exampl .
thei're mark here , on , two , and then three .
and you can see how these start and join and other annot encod those chunki decis .
so , we have m decis total , and the first two n have now been cover .
the first n ar these part of speech tag decis , and the next n ar these chunk decis , start , join , other , start , join , other , start , join .
so , we've now seen these first two layer of structur .
firstli , the part of speech tag for the sentenc and secondli , the sequenc of chunk decis .
we're now go to see the third layer of structur , where the remain part of the pars tree ar built .
and again the game here is to encod those remain part of the pars tree , as a sequenc of decis .
so the decis we'll see ar join , or start or check .
and , so we're go to altern between two class of action .
first we'll choos either join x or start x , where x is some label .
for exampl , n , p , s , v , p and so on .
and after each point where we've chosen join or start , we then choos check equal ye or check equal no .
i'm go to illustr thi with our exampl tree over the next few slide , so thi will at first appear rather abstract , but hopefulli as i go through the exampl , thing will becom more clear .
at a high level , here's the mean of these action .
so , start x is go to be an action that start a new constitu with label x .
for exampl , we can start a new np , we can start a new s , we can start a new vp , and so on .
and it alwai act on the leftmost constitu , which ha no start or join label abov it .
join x is similar , except it continu a consitiu with label x .
and again , it alwai act on the leftmost contitu with no start or join label abov it .
so , inuit the first thing we'll do , is choos to either start a new constiuent , or to continu an exist constitu .
and then we'lll appli thi check oper .
so ccheck equal no , that mean we do noth essenti .
but if we sai check equal ye , then as we'll see thi will take the previou draw or start action , and convert that into a complet constitu .
okai , so that's the abstract descript , now let's see how thi pan out for thi particular exampl .
so , here's the state we've reach after the first two level of decis .
name , the part of speech tag decis , thi layer here , and secondli the chunk decis , which we'll choos thi chunk , thi chunk , and thi chunk .
so thi is the structur we've built in the first two n decis , where n is the length of the sentenc .
the next thing i'm go to do is pick the left most constitu , which ha no join or start annot abov it .
that's that , thi np in thi case , and i'm go to choos either to start or join a constitu here .
in fact start is realli the onli option becaus i can't continu constitu .
no constitu have been built at thi point .
and in fact , for the pars tree that we saw for thi sentenc , thi decis will be to start an s .
becaus if you rememb we actual have someth like thi in the structur of the tree .
so thi is the lawyer .
the nontermin abov thi np wa an s and thi is basic make the choic to stop that s .
the next thing we do is a check oper .
and so there ar two possibl , check equal ye and check equal no .
so if i choos and check equal s , ye , that mean i would have actual said , okai , thi s constitu ha complet .
i just want to start it and then i would have build thi s , and said , okai , that's a complet constitu .
for the past three , i've show you for the sentenc , thi s ha more than on child .
and so we definit don't want to sai check equal ye and complet the constuent term at thi point .
so check equal no and basic that mean we leav thi structur unchang .
the next oper is to look again , we look at the left most non termin that doesn't have ani structur abov it .
or rather , it doesn't have start or join abov it , that rule out thi structur , here , so we look at thi first part of speech , here .
and we could either choos to start a new constitu or we could choos to join s , which would mean we'd continu thi s constitu .
in thi case we choos start vp becaus if you think about the structur in the tree , the correct pars tree in thi case ha a vp start at thi point .
and that's what thi decis here reflect .
the next thing is again a check oper .
we sai check equal no , if it wa ye , we would've complet thi vp .
and we would have thi structur here , but that is actual incorrect for thi sentenc , for thi pars tree , i've shown you .
and so , check equal no , we leav thi vp uncomplet .
next step , again .
we go through thi process of find the leftmost nontermin , which doesn't have ani structur abov it .
that's thi np in thi case becaus thi ha structur , thi ha structur .
and we can choos to start a new constitu , or we could choos to join an exist constitu .
in thi particular case , we're go to choos join vp , which mean that thi noun phrase is actual go to be the second constitu on thi particular vp .
we go through a check oper .
we could sai check equal no or check equal ye .
and for thi particular pars tree , i actual have three children .
i had a vt .
i had an np .
i had a preposit phrase come up , which is go to be built over here .
so in thi case , we're go to sai check equal no .
otherwis , i would have immedi complet thi constitu vp with these two children underneath it .
and that would not be correct for thi particular pars tree .
next step , you're hopefulli get the idea now , we pick the less most sub tree that doesn't have ani structur abov it .
that's thi preposit here .
we could choos to join the previou constitu or start a new on .
we sai start pp , in thi case the preposit phrase isn't complet .
so we sai check equal no .
again we go back to the left most constitu that isn't complet , or doesn't have structur abov it , that's the np .
we could start a new constitu , we could choos to join the previou on .
in thi case , we choos to join the preposit phrase .
and now , final , someth interest happen with respect to the check oper .
so , let me go back to the previou state .
so , thi is the , the posit i've reach and now i'm go to do a check , which is basic go to sai , is thi constitu .
thi preposit phrase , thi most recent constitu that i've built .
is thi now complet ?
now , in thi case it is complet , there's noth more to the right .
there's no wai we can build ani addit structur , and so in thi case we sai check equal ye .
and notic that we've now built thi preposit phrase .
okai ?
so we've now convert that start joint structur to thi preposit phrase structur .
and then we continu , where again we're go to pick the leftmost structur which ha no join or start label abov it , that's thi preposit phrase .
we can choos to start a new contradict or join a previou on .
we choos to join vp in thi case .
that mean we're go to extend thi , thi verb phrase that we've been build .
and again , we're go to follow thi with a check .
and in fact , if we think about thi , the onli thing we can realli do here , becaus there ar no more structur to our right , is sai , check equal ye , and complet thi verb phrase .
and that , inde , is what happen .
so check equal ye , and also i now have vp with these three children underneath it .
i pick the leftmost structur , i choos in thi case join s as my decis .
i now have these two structur in sequenc , so i'm go to do a check oper .
the check in thi case equal ye .
and at that point , i've actual complet the tree .
so , that's a sketch of the sequenc of decis for thi particular pars tree .
so let's just go back to what we're try to acheiv here .
rememb the game is , to try to come up with a sequenc of m decis , which describ how the pars tree is construct .
the first end descis ar level on .
those ar the part of speech tag decis for each individu word .
the second end decis ar level two .
thei correspond to the chunk decis in the particular pars tree .
and the four remain decis ar level three ar the start and check oper that build the remain structur .
and notic again that we altern .
we alwai have start check or join check until the final structur the complet pars tree is built .
but anywai , to summar , thi is the wai that ratna parkei chose to repres pars tree as sequenc of decis .
firstli in part of speech tag , secondli in chunk and final in the start , join and check oper that build the high level of structur .
okai , so we're now go to go back and fill in the rest of the step of thi histori base approach to paus .
rememb the first step is to somehow repres tree as sequenc of decis and that's what i've just shown you on the previou slide .
the second step is to , sai that the probabl of a tree is by the chain rule .
a product of term , where at each point , a condit the , the ic decis on the entir sentenc's input and the previou i minu <num> decis .
so the remain step of thi approach ar go to be to deriv a log linear model that estim these condit distribut .
and final to somehow implement search for the highest probabl pars tree .
so let's talk about thi next .
how do we us a log linear model to defin thi condit probabl ?
so the basic idea is rather simpl actual .
follow the usual definit of log linear model , thi condit of probabl is go to be e rais to the power f . v .
where f is some featur vector and v is some paramet vector .
divid by a normal constant where sum l all of my decis .
so , notic here .
i have f , which is a featur vector that look at a histori , in combin with the outcom di .
so the histori in thi case , consid the entir sentenc s , and the previou i minu <num> decis .
and the outcom of cours is the di th , decis .
similarli , if i look at the denomin , i have a sum of all possibl decis at thi point in the pars tree .
so , we us script a to refer to the set of possibl decis at thi point that would consid .
so f is go to be a function that take a histori out compar , and map it to featur vector .
and v is go to be paramet vector and of cours we're go to take f of all thi context and a product of v .
exponenti it and then normal in the usual wai we've seen with log linear model model .
so let's think a littl bit more about thi featur vector .
it could potenti look at ani inform in the histori in combin with the decis outcom at thi point .
rememb the histori consist of a sentenc and a sequenc of i minu <num> decis .
thi in fact is go to correspond to some partial built structur .
okai , so we mai have some structur like thi , mayb with some start and variou other piec of structur .
at each point in those slide i show you previous with the decis , we have some decis that we need to be made .
so let's sai for the sake of argument we're try to make a decis over thi constitu .
and we could potenti condit on ani inform in the surround sentenc or in the previous built structur .
the part of speech tank , the chunk and so on that i built .
so again , we ar go to leverag the flexibl of log linear model in defin quit rich featur .
that can look at the decis be made in conjunct with the contextu inform .
so the big question is , how do we defin thi featur f .
and so i'm just go to give brief sketch of the kind of featur that ratnaparkhi us in hi particular pars tree .
so the featur vector definit can vari depend on whether the next decis fall into the four differ class .
i've basic shown you on the previou slide .
so thei can vari depend on whether i have a tag decis next , a chunk decis .
or on of these stop join decis at the stage after chunk , or a check equal no versu check equal ye decis .
and so he actual develop differ featur , vector definit for each of these four case .
veri briefli , for the tag decis , he us exactli the same featur as we've seen befor for part of speech tag .
so , he basic us exactli the same approach as for the part of speech tag case .
for the chunk decis , he us veri , veri similar decis .
sorri veri , veri similar featur which again consid the word be chunk .
spell featur of the word be chunk , the previou the word , the next word , the previou two chunk decis and so on .
realli these featur mirror the featur us for part of speech tag .
the onli subtleti with these featur .
is that thei can themselv look back at the tag decis .
so i could ask question about what is the part of speech tag for the current word that i'm try to chunk .
that obvious is a veri us featur .
let me now give a sketch of the featur us with these join or start decis .
so , we can look at head word , we can look at constitu label , we can look at start , join annot .
all consid the n'th tree rel to the decis be made .
where n constitut the previou two constitu , that's the choic that ratnaparkhi made .
so these head word were cover us the head find rule that we saw right at the start of the class from lexic pcfg .
and so we'll assum that we do have these lexic item avail , thei can be us .
but in addit , we can look constitu label , part of speech type of label and so on .
similarli we can look ahead , so we can look at the head word , constitu label , rel to the decis , at posit <num> .
that is actual the current tree be built upon .
and also on posit ahead and two posit ahead , posit ahead .
so to give a sketch , i have some set of sub tree .
some of them have join or start annot and then i'm try to make the decis here .
i can look at the not termin label here , the part of speech tag here , not tabl , termin label here .
the head word of each of these constitu .
and similarli i can do thi for the previou two constitu .
he also look at bigram featur .
which might conjoin inform here between these two constitu , or here between these two constitu .
or trigram featur which would look at three sequenc of label three .
where we might look at the three non termin label along here or three non termin here , or the three non termin label here .
final , we might look at variou punctuat featur .
becaus punctuat is often veri us in paus .
thing like comma , colon , semicolon , the presenc of those in the neighborhood of a decis can be quit us in disambigu these pars tree and in decid which posit to take .
so thi is realli just a sketch , the main point i'm try to get accross here .
is that when we make thi decis , join or start at a particular constitu , we can emploi quit rich represent of the context surround that decis .
for the check decis , there ar two choic , either check equal no , or check equal ye .
and so we might , for exampl , if we had check equal ye right at the start of the pars tree i show you earlier .
we might have construct thi pars tree here , which is actual a rather bad pars tree fragment .
wherea , so thi is check equal ye .
if i'd chosen check equal no , i would've left thi as a start s , and would've , we would've had thi kind of structur .
so for these check decis ratnaparkhi us a set of featur , which ask a varieti of question about the constitu which would be built , if , check equal ye .
so , we might for exampl , have a featur sai , well , in thi case , i've construct a rule , s goe to n s goe to np .
and presum , by the wai , thi is bad .
thi is an indic that someth's gone wrong , and so thi featur should get a neg weight .
we might ask a varieti of other question about the constitu that i've construct through thi check , check oper .
so that's it for basic the featur effect definit in ratnaparkhi' model .
that's a fairli high level sketch of the type of featur he us .
onc those featur vector have been defin , we can train the paramet of the log linear model in the usual wai , us typic some kind of regular maximum likelihood approach .
basic us an ident algorithm with the on we saw for train the paramet in gener log linear model or specif in the log linear model for tag .
again i'll stress that on nice thing about these model is that we can make us of quit rich represent , quit rich featur vector definit .
now rememb in part of speech tag we made thi markov assumpt , that the j th tag depend onli on the previou two tag .
in addit to the entir sentenc .
but in the pars problem , i've , in gener , made us of featur that might be sensit to all kind of decis , that ar made in the past .
that's becaus , when i wa make thi decis here i want to condit on all kind of context .
in the past in the futur part of speech tag and chunk decis .
and so thi kind of markov assumpt doesn't realli make sens , or at least it would be veri limit in the case of pars .
now becaus thi decis di could depend on arbitrari decis in the past , becaus i do not make a , an independ assumpt like thi on .
thi realli mean that dynam program is go to be out of the question .
becaus it doesn't , the problem , the decod problem , doe not have the requir structur for dynam program .
i . e , it doe not have some kind of markov structur .
so instead , ratnaparkhi us a beam search method .
veri similar to the beam search method that you saw for phrase base translat to code .
i won't go into the detail , but intuit , thi is go to build sever possibl pars tree , left to right , us the kind of decis we just describ .
each of these pars tree is go to have some probabl .
and at each point we'll just keep around sai the top <num> or <num> most probabl pars tree under the model .
so that partial set of decis that i've made in construct each on of those pars tree .
so that's basic it .
what we've seen here , is a wai of us log linear model , to construct a veri differ type of pars model from the problist context free grammar , style model we saw earlier in thi class .
either of these approach ha it strength and weak .
ratnaparkhi's method , is in some sens rather simpl , all you need to do is defin these featur vector definit , and you have an approach , you have a parser .
and it's also quit power , in that you can incorpor quit rich featur within these featur vector definit .
ratnaparkhi's parser wa recent success with accuraci quit close to the accuraci for lexia' piec of chees , as i show you , earlier in the class .
more recent , these kind of model have been appli to a problem call depend pars , where , thei're extrem effect .
and thei produc some of the most effici and most accur depend parser .
and hopefulli we'll see some of these depend pars model next week , so you'll find out a littl bit more about that in the next week of class .
so , in thi next segment of the class , we're go to describ an algorithm call the brown et al , word cluster algorithm .
it's a rather remark algorithm and that it take in as input , a larg quantiti of unlabel raw text .
and as it's output produc veri us represent of individu word .
it's realli the first exampl we'll see in thi cours of a so call unsupervis learn algorithm .
in that , it doesn't requir annot data rather it just requir raw unlabel text as it , as it's input .
so the input to the brown cluster algorithm is a corpu of word , or rather , sentenc .
thi could potenti be quit larg .
we might us a few million or few ten of million or even a few hundr , hundr of million of sentenc .
and as i said , on advantag of the brown method is that it doesn't requir annot data .
so you can just us raw un annot text that you might find on the web or in new wire data or variou other sourc .
it can produc two type of output .
the first is a partit of word into word cluster .
and the second is actual gener of the first , which is a hierarch cluster of word .
so let me actual give you some exampl of these two type of output .
so firstli , here ar some exampl cluster .
thi is actual from the origin paper by brown and colleagu from <num> .
and so again , rememb , the input to thi algorithm is unlabel text .
what thei've shown here is some differ cluster that their algorithm recov .
so thi is basic a partit of the word and the vocabulari into cluster , in such a wai that similar word appear in similar cluster .
and these ar realli rather strike .
so the first on , if we look here , ha word such as fridai , mondai , thursdai , wednesdai , so all of the dai of the week and in addit weekend in thi case .
and again , i stress , thi is deriv complet automat from unlabel text .
we'll soon how thei did thi .
the second cluster seem to consist of month name .
and then we have anoth cluster with word like peopl , gui , folk , fellow , ceo , chap , doubter , commi , unfortun , bloke .
here we seem to have a miner cluster , water , glass , coal and so on .
we have man , woman , boi .
down here we have first name .
and so on and so on .
now , you can imagin that these kind of word cluster can be veri us in a pretti wide rang of natur languag applic , becaus now you have addit knowledg , not just about the ident of a particular word , but also about what class of word it fall into .
and actual , in the final part of thi segment , i'll show how these kind of cluster represent can be us veri directli in the problem of name entiti recognit .
and thei can give some big improv in that particular task .
so that wa an exampl of a set of word cluster .
now let's look at the second kind of represent that brown cluster can produc , which is a hierarch represent of word .
to illustr thi , let me give a veri simpl hierarchi .
so sai i have just six word in my vocabulari , sai appl pear , boi girl , mayb said and report .
so we might , in that case , have a hierarch cluster , which look like thi .
so if i take ani node in thi tree that i have drawn , drawn here , i end up with a cluster of word .
so thi node here , for exampl , correspond to the two word , boi and girl .
thi node here correspond to the two word , appl and pear .
thi node here correspond to the two word , said and report .
i can go higher in the tree , so thi node here would correspond to a cluster with four word , appl pair , boi and girl .
and final , the top node in the entir tree is just the entir vocabulari .
so thi is the least interest cluster you can think of .
but we can see here that we have some kind of hierarch cluster , which reflect the fact that boi and girl ar veri similar , as ar appl and pear .
but in some sens , these four word ar more similar , becaus thei're ar all noun , than these two word , which ar verb .
and so , a hierarch represent allow cluster with differ level of granular .
now , it'll be us to think about these hierarchi as assign bit string to each word in the vocabulari .
so whenev i have a branch in thi tree , i can have a <num> for the left branch and a <num> for the right branch .
so similarli i can have <num> versu <num> here , <num> versu <num> here , and so on and so on .
so now , if i think about these word , appl correspond to the bit string <num> .
pear correspond to the bit string <num> .
boi is <num> .
girl is <num> , and so on and so on .
said , i think , is <num> , and report is <num> .
notic that these bit string can be of differ length .
for exampl , two versu three ar the two length here , becaus differ word mai be at differ depth in thi tree .
so now we can think of a prefix , or ani prefix that these bit string defin a cluster .
so <num> is the cluster appl pear or <num> is the cluster boi girl .
<num> is actual all four word , appl , pear , to girl , and <num> is just said and report .
so in ani case , it's go to be us to think of a represent of these hierarch cluster as bit string , as i've shown you here .
thi is an actual exampl of a hierarch represent of thi type deriv us brown cluster .
and thi is from a paper by scott miller and other from a confer call naacl in <num> .
and we'll look more close at thi paper later in thi lectur , where thei us these kind of cluster within the context of name entiti recognit .
but let's take a look at these .
again , you can see that the brown algorithm ha been realli remark effect in deriv us hierarch represent .
these represent were deriv from a few ten of million of word , i think of new wire text .
so if we see here , all of these word here have some bit string , which i think is common right the wai up here .
so the first sever bit ar the same .
so thei're deepli nest in some hierarch tree with the same path down to that node .
and again , we have word like lawyer , newspaperman , stewardess , toxicologist .
these ar clearli peopl in gener .
of cours occasion there ar error .
here's on , the word slang for some reason is in there .
but in gener , these represent look pretti good .
similarli , these ar compani name .
so again , these share some pretti long bit string , all the wai down here .
so thei're pretti deep in thi hierarchi .
and final , these ar first name .
okai , and again these share some pretti deep bit stream .
now , you can imagin if you think of the name entiti recognit problem , for exampl , you can imagin how us these represent can be , becaus for infrequ or rare word , or word which were never seen in train data , know that a word like consuelo appear in thi class of first name can be extrem us inform if we're try to build a name entiti detector .
so , how do we recov these represent ?
how do we deriv these represent from larg quantiti of unlabel dater ?
in a moment , i'm go to describ the mechan of the brown cluster algorithm .
but firstli , i want to talk about the intuit which underli thi algorithm .
the intuit is that if we have a larg amount of unlabel dater , we can leverag the fact that similar word tend to appear in similar context .
or a littl more precis , similar word have similar distribut of word to their immedi left and right .
so , for exampl , if i take the word the and the word a , these ar both similar word .
thei're both determin .
i can look at the frequenc of differ word , the immedi left and right .
so , we might find the preposit like , in and of frequent seen to the left of the word there .
the noun like dog or park or share , ar frequent seen to the right .
and so , you can , given a larg conceiv dater , you can basic look at the distribut of word , to the immedi left of the word and the distribut of word to the immedi right .
and i can do a similar thing for the word a .
and becaus these two word ar similar , i'm go to see a veri similar distribut or at least a somewhat similar distribut , where again , i see the most frequent word be noun like dog , park , or share to the right and preposit like in and of to the left .
if i take anoth exampl , if i take mondai versu tuesdai , if i look at the frequent word to the left , thei're go to be thing like last or on mondai , last mondai , variou thing here .
and again , we're go to see these word like last or on , frequent in thi context .
and similarli , there'll be a set of word which tend to appear after dai of the week .
and we'll see a similar right context for mondai and tuesdai becaus thei ar both dai of the week .
so , that's the basic intuit .
and the brown cluster method is on cluster algorithm .
thei're actual sever possibl algorithm .
we're go to focu on thi on .
but all of these differ wai of deriv these word cluster basic leverag thi intuit that similar word tend to have similar distribut of word to the left and right .
so , now let's look at the formul underli the brown algorithm .
so , we'll us script v to refer to the set of all word seen in the corpu .
i , thi is go to be our vocabulari .
and we'll assum that the corpu consist of a sequenc of word , w1 through w capit t .
for conveni , i'll assum that we don't have individu sentenc , we just have on veri long sequenc of word .
think of thi as , as the entir corpu concaten .
it's a minor detail , whether we split thi into sentenc or not .
and the output of the brown cluster model , for now , is go to be a cluster .
a cluster is a function c that map the vocabulari to the set on through k .
so , everi word in the vocabulari is go to be sign , assign an integ .
so , sai k equal <num> , we have <num> cluster .
we might assign the word the to <num> , a to <num> .
and then , mayb , mondai to <num> , said to <num> , and so on .
so , thi function is go to map each word to an integ , between <num> and <num> , sai .
basic , you're defin a map from word into the underli class .
.
so , in the brown model , there ar basic two type of paramet .
and thei look veri similar to the type of paramet we would see in a bigram hmm .
so , firstli we have a mission paramet .
so , we might , for exampl , have e of the given <num> , which intuit is the probabl of cluster on omit the word the .
okai .
so , each cluster from <num> to k is go to have a distribut over the differ word in the vocabulari .
the second type of paramet look veri much like the kind of transit paramet we saw in hmm's .
so , for exampl , q of <num> given <num> , would be the probabl of class <num> follow class <num> .
so , a brown cluster model is go to consist of a function c and that map vocabulari v to the set <num> through k , togeth with these paramet , these e and these q paramet .
it's realli veri similar to an hmm .
the main differ is that thi function c is determinist , that each word get map to a singl state .
we're not go to allow ani ambigu where differ word rather the same word , can belong to differ state in the hmm .
now , given set for these paramet e and q , and given a definit of thi function c , we can write down the probabl of our corpu as follow .
it's go to be a product of term , i equal <num> to n , and we're basic go to have a bigram model , wi given wi minu <num> .
that's what thi term here is .
but to calcul thi , we calcul thi in a rather differ wai than what we've seen befor .
so , there ar two term .
firstli , i have q of c of wi , given c of wi minu <num> .
that intuit is the probabl given that the previou cluster wa c of wi minu <num> , of choos the next cluster of cwi .
and then i have e of wi given c of wi minu c of wi , sorri .
that's what we have here .
so , you can think about thi as follow .
sai , the previou word is the , sai thi is wi minu <num> , and i want to calcul the probabl of the next word be dog .
and let sai for the sake of argument that c of the is equal on and c of dog is equal to <num> .
so , the fall into the first cluster and dog fall , fall into the 50th cluster .
then , in some sens , what i'm do here is i'm first map the word the to cluster on and then , condit on that , i'm map to class <num> as the next on .
and then , i'm gener the word dog from thi cluster <num> .
and the result is we have p of dog , given the , is equal to e of dog given <num> , sorri , <num> time q of <num> given <num> .
so that's the basic model .
the thing to realli , to bear in mind though , is that all thi machineri is realli there , so we can deriv thi cluster .
so , we'll see that in learn thi model , we're actual go to have a method that take a corpu' input and choos it paramet e and q and a cluster c .
but the cluster is realli what we ar look for as the output of thi model .
we want a divis of word in the vocabulari into a set of cluster , such that similar word appear in the same cluster .
and a littl later , we'll talk about how actual to optim thi , how to choos valu for each e , q , and c that attempt to maxim thi function .
.
let me give you on more exampl of how thi model work on a slightli more complic exampl .
so , here i have the full distribut , the definit i show you on the previou slide .
and now , let's assum that of our vocabulari v just consist of the word the , dog , cat , saw , or at least it ha those four word .
it might have mani other .
and so c of the is go to be <num> , c of dog is equal to c of cat is equal to <num> , and c of saw is equal to <num> .
that's reflect the fact that thi word appear in cluster on , these two ar in cluster <num> , and thi word , word is in cluster <num> .
let's assum we have these mission probabl .
so , e of the given <num> is <num> , e of cat given <num> is equal to e of dog given <num> is <num> , e of saw given <num> is <num> .
and let's assum we have some transit prob probabl .
let's not see how we can calcul the probabl of the dog or the cat .
well , the first thing we do is we notic that each of these word fall into a cluster , and so , the fall into cluster on dog into two , saw into three , the into on , cat into two .
and we can now write down the precis form of thi equat , for thi particular exampl .
so , firstli , i'm go to have some q term correspond to basic mark of sequenc under these , these five word .
so i have q of <num> given <num> .
i'm assum that the class , class for the , sort of the star word , the start of the sentenc is equal to <num> , and q of <num> given <num> time q of <num> given <num> time q of <num> given <num> time q of <num> given <num> .
look veri much like a standard hmm .
and then , i have e of the given <num> time e of dog given <num> time e of saw given <num> , and so on and so on .
on emiss term for each of these word .
so , again , veri similar to the calcul .
we see for hmm's .
the onli wrinkl here realli is that we have thi function c that map a word to a determinist choic of underli state .
and so , to calcul thi probabl , we'll first look up for each word , which cluster it fall into and then calcul the probabl of the sentenc as a product of q and e term .
so to summar , a brown cluster model consist of the follow .
we have some vocabulari .
to typic be the set of all word seen in our train cours .
again , that train cours might cost a few , cost us of a few million or a few ten of million .
or even a few hundr of million of sentenc or word .
we have a function c that defin a partit of the vocabulari into the k differ cluster .
typic mid k might be around a thousand for exampl .
we have a paramet , e of v given c for everi v pair with everi c , and then we have a paramet q of c prime given c for everi pair of cluster , c prime to the c .
thi intuit is the probabl of the next cluster be c prime .
given that the previou cluster is c .
and thi is a paramet sai what's the probabl of admit the word v .
given that i have the cost of c as the current cluster .
so , the critic question is go to be .
how do we take a train corpu as input , and as output produc these three thing , the function c and these paramet e and q ?
that's what we're go to come to next .
so we're now go to describ how we deriv a partit c .
from a train corpu .
a train corpu consist of a sequenc of word w1 , w2 up to wn .
just think of thi as the concaten of all the sentenc in our train data .
again thi might easili be a few 10s or 100s or million of word .
and what i'm go to describ here is a measur .
of how well a particular , partit c fit thi particular train corpu .
so thi is a function call qualiti , which take , as input , a train corpu and a partit c and return a valu reflect how well that partit c fit thi particular set of train sampl .
thi is noth more than the log likelihood of the data , as we'll see in a moment .
so thi function qualiti is go to drive the algorithm that actual pick a partit c .
we'll see soon how we can attempt to maxim thi qualiti .
but for now i want to describ .
the actual definit of thi term .
so what i have here is a sum from i equal <num> to n of the log probabl of the ith word and the corpu , wherea befor , thi log probabl consist of two term , the product of two term .
firstli , the probabl of omit wi from cluster wi and secondli , the probabl given that the previou word of the same cluster , w i minu on , that the next word in the same cluster w i .
so , we're actual go to see in a second how we can simplifi thi consider .
.
there's a number of step here which i'm not go to go through in detail .
i'll post a note describ thi .
so i'm just go to give a sketch on the slide of , of what we end up with here .
but we end up with a realli rather beauti express here , which wa deriv in the brownatail paper .
let's just talk a littl bit more about thi criterion , though .
so thi is actual a function of three thing .
it's a function of the partit c .
it's a function of the cube paramet .
and it's a function of the e paramet .
so in fact we're go to maxim thi function with respect to these three thing .
and thi function can be thought of as a measur of how well these three thing fit the train data .
so there's a bit of a cheat here in that thi qualiti function depend just on c , but actual e an q have to be modifi to .
a critic observ is that onc i have a function c , the valu for the e and q paramet can be deriv in the usual maximum likelihood wai .
so , for exampl , e of v given on .
is cgo to be count of the state on emit there , divid by the number of the number of time we've seen the state on .
or q of <num> given <num> sai , is go to be the count of our on be follow by two , divid by the count of on .
so if i fix the valu for c , thi partit , i can read off count like thi .
i can read off the number of time i see the cluster on .
the number of time i see the , the word there in that cluster .
or i can count off the number of time i see cluster <num> follow by cluster <num> .
and these relationship hold becaus if i fix c i still want to maxim thi function with respect to emq and thi will be the valu for emq that maxim function .
thi insight is critic , and it lead through a few line of algebra to follow form down here .
so i have two term here .
g is a constant , so we can ignor thi becaus we're go to be maxim thi function and thi constant is insensit to the partit c , but what we end up with here is a term where we sum over all pair of cluster , c and c prime .
i look at the probabl of see c follow c prime in the corpu .
and then i look at the log of p of c follow by c prime divid by p of c and p of c prime .
how these p s ar actual deriv ?
well , i can read off these count from the corpu .
so if i see , for exampl , the dog .
so ar the cat .
let's sai i have some function of capit c , which i'm try to evalu and sai that map the word the to on .
dog to two .
saw to <num> .
the to <num> .
cat to <num> .
have perform thi map , i can calcul the number of time i see c follow by c prime .
so , for exampl , n <num> , <num> is equal to <num> in thi case .
or i can comput the number of time i've seen a particular class , for exampl n <num> is equal to <num> and so on .
so i can read off these count directli , from my onc i have that h function capit c and so i can estim these probabl .
so what i'm describ here is a mechan wai to go from a partit c , to some valu which is actual equal to thi likelyhood .
and it involv just comput these probabl , the joint probabl of see c follow by c prime , and the margin probabl of see a cluster c and then comput thi express .
thi ident just follow through a few line of algebra .
so that's the most import point , realli , that we have simpl wai of evalu a cluster and it ha thi rel simpl form .
and actual , just veri briefli , for those of you who might have seen some , some inform theori , thi is actual the mutual inform , but again i should stress the main result in thi slide is that we now have a wai of take a partit c so some assign of word to cluster , and evalu the qualiti of that cluster .
so thi is a function that will take a cluster and measur how well it fit our train corpu , and it's deriv through likelihood .
it's basic go to be a measur of the likelihood of the corpu under thi particular classroom .
and the game the deriv from thi line to thi take a few time of algebra i'm not go to go into other detail with that we'll post the origin paper or note explain that in more detail .
so , let's think about how we can maxim thi function qualiti of c , and i'm go to describ a coupl of method to do thi .
the first on is a us thought experi , that's on thi slide .
it's too ineffici , but we'll see with some modif , we can actual make it effici .
and it's basic a kind of greedi , bottom up approach .
so , i start off with everi word in it's own cluster .
so i might have the and a , dog and cat , red and blue , and our aim is go to find sai k final cluster .
mayb , for exampl , we might find k is equal to <num> .
and in that case we do the follow .
so , we're go to run a seri of what ar call merg step .
and it realli a veri simpl idea .
each time , so we start of it partit here , i can consid all possibl pair of merg .
so i could consid merg the and a into singl cluster , or the and dog with the cat , the and red , the and blue .
or a and dog , a and cat , a and red , a and blue .
so , i can consid all possibl pairwis merger .
and so , for exampl , sai i choos to merg the and a .
then i end up with new partit , which is that c of the equal c of a is equal to <num> , sai .
c of dog is equal to <num> , c of cat equal to <num> and so on , and so on .
so , everi word is still in some cluster , except for these two word which ar now on a singl cluster .
now each time i consid a poten , a potenti mode step i form a new cluster capit c , and i can calcul the qualiti for that cluster , i mean the criterion i show to you on the previou slide .
so i'm basic go to consid all possibl merg step like thi , and find the merg step which maxim the qualiti of the result cluster .
so sai , in thi case , mayb i would find that the and there is actual go to be the merg step that maxim the valu of qualiti of c .
so that's the first merg .
i now go to a second merg step , have merg these two word into a singl cluster .
and now i do exactli the same thing as befor .
i consid all pairwis merger , but now i treat thi cluster as a singl unit .
so , i could choos to merg these two word with dog .
or thi two word with cat .
or these two word with red .
or these two word with blue .
or i could choos to merg dog and cat .
or i could choos to merg dog and red , and so on , and so on .
so again we're go to do all pairwis comparison .
well , we just have on fewer element now , becaus these two were too consid as a unit .
sai , for the sake of argument , we choos to merg these two .
and then , in the third step , i might do someth similar .
mayb i find these two word ar merg , and i keep do thi until i end up with my target number of cluster .
sai , we have k equal <num> cluster in thi case .
so that's , kind of heurist , greedi method , were at each step i pick the merg step that maxim thi measur of qualiti .
naiv thi is go to be veri expens , it's actual , you can show that it would take order to the cabbag size to the power <num> .
but the ibm folk , the brown et al paper give a slightli more effici algorithm which is cubic in the vocabulari , that's an improv for sure .
but actual , still too slow for realist valu of v .
so again , thi first algorithm is a thought experi , and we'll see that the second algorithm i give you is much more practic .
but thi thought experi wa us .
and then we'll us thi basic , kind of greedi bottom up method as the basi for our , our algorithm .
okai , so thi is actual the algorithm that peopl run in practic .
and it ha an addit paramet of the approach call m , typic valu will be m equal a <num> .
anoth properti of thi algorithm is we'll see that rather than produc just a partit or a cluster into k class , it'll actual produc a hierarchi .
so we might have someth like red , blue , dog , cat , you know hierarch cluster exactli the same wai i show you earlier in thi lectur .
so here's how it work .
we initi take the top m , most frequent word , for exampl , the top thousand most frequent word , and we put each of those word into their own cluster .
so , we might initi start off with sai the , a , cat , dog and so on list the top <num> most frequent word and a corpu , and thi is sort of a seed .
thi is the start point of the algorithm .
and then i iter over the remain word , so i go from the 1001th word right the wai through to the veri last word in my corpu .
and i'm go to consid each of those word in turn .
so mayb the 1001th word is the word , sheep for exampl , okai ?
so , i've ad thi , you know , i have <num> , <num> word here .
and i've ad the thousand and first most frequent word here , and now i'm go to again consid all possibl merg step .
so i'm go to consid merg the with a , or the with cat , or the with dog , right up to the with sheep .
i'm go to consid merg a with cat , a with dog , right the wai up to a with sheep .
and so on and so on .
so again i'm go to do thi all pairwis comparison , where i look at all pair of word , and choos to do a merg .
so that would result in two of these word be merg .
let's just sai , for the sake of argument , that we choos to merg cat and dog in thi case .
and so , even though the sheep wa choos as a thousand and first word .
it's import to realiz that it won't necessarili be part of the merg step , i mai choos to merg some other pair of word within thi initi <num> .
okai , so that's the first merg step .
now i add word <num> .
so mayb it's the word there , i don't know , someth like thi .
and notic again , i have a thousand , i have a thousand exampl here , so after thi merg step , i have <num> , so basic i have a <num> differ cluster here .
where a cluster might be a singl word or it might be a pair of word , like cat and dog .
and again i consid all pairwis merger .
so mayb for exampl i would take sheep and merg it with cat and dog , in thi case .
and again , that's go to reduc the number of cluster i have by on .
i then go again .
i go to the next most frequent word .
mayb thi is red .
thi is the <num> thousandth third most frequent word .
and again , i'm go to consid all possibl pair of merger , all possibl wai of merg .
either a singl word or an entir cluster with anoth singl word or anoth entir cluster .
all the wai through thi process , i'm us the qualiti criterion that i show you earlier as a kind of oracl , which given a cluster , will evalu the qualiti of that cluster .
and i'm pick these merg to maxim that measur of , of qualiti .
onc i've ad all of the word in the vocabulari , you basic have a cluster into , you can show you have a thousand differ separ cluster .
and the last thing we do is carri out m minu <num> final merg , for exampl <num> final merg step , and the result is go to be a full hierarchi over the entir set of word in our vocabulari .
and thi is exactli the process that the brown et al research us , and exactli the process that result in those bit string represent .
i show you earlier in the slide .
the run time is the follow .
you can show us the vocabulari size time m squar , where m is thi valu , sai <num> , <num> plu a linear term in the size of the corpu .
so while still expens , thi is nevertheless feasibl for quit larg corpora of train exampl .
and inde thi algorithm ha been appli to veri larg train corpora .
so just to recap , the output of thi whole algorithm , the output of thi process , is ahierarch cluster .
as i show you earlier , we have thi kind of tree like structur , with word at the leav of thi tree .
and each node correspond to a differ cluster , for exampl , thi node would correspond to the two word red and blue .
and so on and so on .
as i describ earlier , these hierarch constraint can be describ as bit string or thei can be map to a map from each word to a bit string .
so in thi case , for exampl , red would get the bit string <num> .
blue would get the bit string <num> .
so , these kind of represent can be extrem us .
it can be veri us to know , for exampl , that each of these word is , in some sens , similar .
thei're in the same cluster .
at least if we look at the first sever bit .
and what i'm go to talk about in the remaind of thi lectur is a veri effect applic of these represent , within the context of name ident recognit .
thi work by miller and other , publish by naacl in <num> .
so let's now describ how thi work .
so thi is the paper .
it's call name tag with word cluster and discrimin train .
it's a realli wonder paper , i think .
bring togeth the idea from these brown cluster represent .
in combin with , actual the log linear tag model we saw in the last week of the class .
so miller et al motiv their work in the follow wai and thi is actual an excerpt directli taken from their paper .
so thei sai thei , thei relat thi experi where thei had actual built a name ident tagger .
and thei were demonstr to a potenti user and actual thi work wa done at bbn compani , which develop some of the best name entiti or ha develop over the year some of the best name entiti recognit method and so thei had consider experi in thi domain .
and thei were sai that , you know ?
thei knew thi .
the technolog had perform well in former evalu .
had been appli successfulli in sever research group .
all it need , wa some annot train exampl for it to be train in a new domain .
but nevertheless , it wasn't quit what the user want .
and the critic problem wa thi need for annot train exampl .
so , as you've probabl realiz .
if we want to appli , or rather , we want to develop a name entiti recogn for a new set of entiti .
sai , go beyond peopl , locat and organ .
then , we're go to have to , at least if we us the method that i've describ earlier in thi class , we'll have to collect label exampl .
exampl where we have sentenc with underli segment , underli mark of the entiti .
and of cours gather thi kind of annot train data can be an expens proposit .
the same is true for the paus problem we've look at , also for the part of speech tag problem we've look at .
and actual mani other problem in natur languag process .
and so miller et al go on to emphasi thi point .
and , so , the actual the hmm base technolog thei had , which is veri similar to what we saw in the second week of thi class , requir roughli <num> , <num> word of annot exampl .
and to get to peak accuraci , mayb about a million word .
okai , so a fairli consider amount of train data .
and that's becaus there ar a larg number of paramet in these model .
and there's alwai spars data .
there ar alwai word which you have too littl inform .
so get more train data is alwai go to help .
and in realiti we need quit larg amount of train data for these model to work well .
so , the annot that thei actual emploi might annot thi data at , pretti fast rate , i have to sai , around <num> , <num> word per hour , that's impress .
but nevertheless , annot thi kind of quantiti of data might take sever dai of annot .
and , that might be feasibl in some case , but in other case that mai be just simpli be too slow , thi mai be be too expens .
so , the kei idea in thi work , wa to us the brown cluster inform to significantli reduc the number of label requir to build a name entiti classifi .
and so miller et al us someth veri similar to the lock linear tagger we've seen in the last week lectur .
so rememb in a long linear tagger the histori is go to be a tupl t minu <num> or t minu <num> rather t minu <num> , these ar the previou two tag .
a sentenc w1 through wn and posit i for exampl , we could have the histori , h is equal to , sai determin , nn , the dog laugh , <num> .
signifi that we're attack word , three in the sentenc and the previou two tag would determin our noun .
the label , which is y is go to be some tag for exampl we have y equal verb mayb .
and we have a featur of definit we have a set of feautr fk hy , the k equal <num> to d where d ar the number of featur .
and these ar gener question about the histori in conjunct with the label y .
so , thei kei insight of thi miller et al paper wa to includ featur in a log linear tagger , which not onli look at the ident of particular word in the context .
but also look at the cluster ident of the , the word at the current posit , and the previou posit and the next posit .
let me go over thi .
actual we have a list of the featur which we us in thi work here .
so thei us featur like the tag and the previou tag .
so thi would be a fairli standard featur , someth like the follow fkhy equal <num> if t minu <num> equal nn and y equal vb .
and so you have mani featur thi form be be <num> otherwis look at all possibl pair of tag .
we have feauter which look at the tag in the current word so thi would be a featur such as fkhy equal <num> , if wi equal less .
and y equal vb <num> otherwis .
and you would have most like a featur like thi for a veri larg number of combin of word togeth with possbil tag .
thei had a featur which look at the current tag for exampl vb and some spell featur of the word be tag mayb whether it's capit or whether it ha number .
thi is essenti go to look veri much like the map from word to these rare word featur , which captur the spell featur of the particular word be tag .
we have featur which look at the tag in the previou word , and tag in the next word .
these ar all pretti standard featur .
exactli the kind of thing we saw in the previou lectur on lock linear tagger so noth new there .
what new is these new featur , which look at the brown cluster inform that i wa just describ .
so what doe thi mean thi sai it featur look in the tag and the pref eight of the current word .
thi essenti is go to look at , the first eight bit of those bit screen represent that i show you for brown cluster .
so thi might for exampl sai fkhy is <num> if the label y is equal to vb and the first <num> bit of wy that's the word be tag is equal to <num> , you know some <num> bit string .
and you would have featur like thi for all <num> bit string that ar actual seen in your train data in conjunct with all tag .
now , rememb that given the wai brown cluster work , if we look at the set of word with thi particular prefix .
we're go to get a cluster of word at a certain level of granular , at an <num> bit level of granular .
so mayb you'd find , for exampl , red , blue , green , or some basic relat set of word in thi cluster .
and thi featur will fire for all of those word .
so we now have featur which gener across multipl word and moreov captur the membership of word in these differ cluster .
now in thi work thei look at eh <num> bit prefix , the <num> bit , the <num> bit , and the <num> bit .
and so , there look at everyth from a fairli coars level of granular .
at thi level you have at most <num> to the <num> , which is equal to <num> possibl cluster right the wai down to a bit string of length <num> where you have <num> to the <num> possibl cluster which is an enorm number at thi point you've probabl almost gotten down to individu word .
in addit to look at the current word , you can look at the bit string of the previou word , mayb of length <num> or <num> , or <num> , or <num> .
or you can look at the bit string for the next word with <num> length <num> or <num> , or <num> , or <num> .
but , again at a high level the critic idea is we now have featur which can look at the bit string correspond to word in the context .
and can therebi gener across word which occur in the same cluster .
so here's some result from thi paper .
thi is the first set of result .
so what we have on thi axi is the number of train sampl .
i believ in term of number of word .
so we go from <num> , <num> , to <num> , <num> , to <num> , <num> , to a million word .
thi is actual on a log scale , you'll notic .
and on thi axi we have the f measur accuraci of the name entiti recogn .
so thi measur is actual a combin of precis and recal .
it's actual the f measur is equal to <num> time the precis time the recoil over the precis .
plu the recal .
you can basic just think of thi as , kind of averag with the precis and recal , where <num> is perfect perform .
<num> , for exampl mean you have roughli <num> accuraci .
so it's realli just a wai of take these two number , precis and recal and form a singl number measur the , measur the accuraci .
so let's see what we have on thi curv .
so firstli we have an hmm , which wa basic the best model thei had up to thi point .
and thi wa the seriou model , thi wa a hmm model which thei had develop over sever year and it wa a veri , veri good name entiti recogn .
here in pink you can see we have the model i just show you which is the log linear tagger in combin with the brown cluster .
it's worth note that without the cluster , the log linear tagger ha veri similar perform to the hmm .
i haven't show that on thi slide , but for thi task the log linear model and the hmm had roughli compar perform .
so what you will see here is there's a pretti substanti differ between these two curv .
the cluster inform realli add quit a bit in term of accuraci .
over <num> here .
over <num> here , that's , <num> , <num> word of train i think .
and so on , and so on .
or , if you think about it anoth wai , if you want to achiev thi level of perform , you would need thi much data under the hmm .
and you would need roughli thi much data under the discrimin model rather the cluster base model .
and notic that thi is on the log scale , so thi is a pretti , pretti substanti differ .
so that in itself is , is quit an impress result .
the final result ar sure actual throw anoth veri us techniqu of thi problem and thi is a problem call activ learn .
so in most of the method that we've seen in thi class , you ar given a fix set of train exampl .
you train a model from those exampl and then you test on your test data , in activ learn you attempt to reduc the number of train exampl you need by dynam choos which exampl should be label .
and you , in particular , choos to label the most , us exampl at each stage .
what that would typic mean is you would train your log linear tagger on a small number of exampl , mayb <num> .
you would have a larg pool of addit exampl , which you could send to an annot for label .
and you would choos to la , label the exampl which ar most difficult for the log linear tagger .
mayb where the log linear tagger ha most confus by some measur or anoth .
and so in thi wai you end up rather than repeatedli label exampl for which you alreadi have a veri good idea of what's go on .
you , you choos exampl which ar more challeng , an which essenti contain more inform for the current learn algorithm .
so that's activ learn .
it's the idea that you can dynam select exampl , base on how inform thei ar .
thi is well known to give , some pretti signific reduct , in the amount of train data that's requir .
so here ar some differ , curv .
thi is the baselin model .
thi is just a log linear tagger with no activ learn .
and with , no us of those word cluster that i've show you .
if we us activ learn we get thi pink curv and we can see again that there's some fairli particular improv .
the yellow curv is the result with brown cluster so again you can see you good improv over the method that doesn't us cluster .
and final and most impress , we have a result which make us of both the brown cluster and also activ learn .
if we look at thi , the perform realli is rather remark .
i can get over monei percent perform with , sorri that's not a veri straight line , but with , on the order of roughli ten thousand word of train data .
and so , if we look at thi level of perform , we would have need a few <num> , <num> word of train data under the old method .
we would have been over here .
so thi combin of the brown cluster inform and activ learn ha vastli reduc the amount of label data that we need .
and as we saw in the motiv for thi work , that is certainli a veri us thing in mani context .
so what i've spoken about here is result for name entiti recognit but thi kind of techniqu can appli to mani other problem .
anoth problem where brown cost us , been show to be veri us .
it the paus problem where , again , we have sever problem with and these brown cost can realli improv thing .
the final thing i'll sai here is that , note that the us of model your model wa critic in that it enabl us to us these represent , these brown cluster represent , in a realli seamless and straightforward wai .
it would have been much more challeng to have us those represent within a hidden markov model .
so again as i said befor , a sell point of log linear model is the flexibl in term of the represent thei can us .
and here we've us that flexibl to directli leverag inform from brown cluster .
which themselv ar deriv from veri larg quantiti of unlabel data .
mayb 10s , or 100s , of million of word of unlabel data .
so we're now go to start the final topic in thi class , which is a framework call global linear model .
as you'll see , global linear model take mani of the idea from log linear model that we saw a week or two ago in the class , but extend them in some fairli radic wai .
thei ar a gener framework under which we can solv .
mani of the natur languag process problem we've seen so far ar tag , pars , potenti also other problem like translat .
so let's first give a brief review , of what we've seen so far in thi class .
so mani of the problem we've consid , involv supervis learn .
so in supervis learn problem , the gener task is to learn some function , capit f , that map member of sum set capit x .
thi is a set of possibl input , to member of sum set y .
thi is a , a set of possibl output .
so we're go to have sum function capit f , which map sum x to sum y .
so , for exampl in the pars problem , each input x is a sentenc , and each output is a pars tree .
in machin translat , each input might be a sentenc in on languag , sai french , and the output is a sentenc in anoth languag , sai english .
the languag we're translat into .
in part of speech tag , the input is again a sentenc , and the output is the sequenc of tag , in particular the part of speech tag , for that particular sentenc , that sequenc of word .
so in supervis learn problem , we've assum that we have some train set , which consist of xi , yi pair for i equal <num> to n .
so , for exampl , we would have n exampl consist of a sentenc and a pars tree .
or we might have n translat exampl where each exampl consist of a french sentenc pair with an english sentenc .
or we might , in a tag case , have n exampl where each exampl consist of a sentenc pair with a sequenc of tag .
and so , we're go to somehow learn thi map f , from thi train set .
so we're go to have some algorithm , that take thi train set as input , and somehow produc some function capit f as the output .
so almost all of the model we've seen so bar , far what i'll refer to as histori base model .
so at a veri high level , these model take the follow approach .
we break some structur , for exampl , the pars tree or tag sequenc , into what you might call a deriv , or basic a sequenc of decis .
so for exampl , we might have the sequenc of tag decis , or the sequenc of rule applic , in a context free grammar .
i'll give some exampl of thi in a moment .
each decis ha an associ condit probabl .
so we might , for exampl , in the case of an hmm , we had condit probabl of the form ti , given ti minu <num> , ti minu <num> , or e of xi given ti .
the decis here , be to choos the i tag , given the previou two tag , or to gener the ith word given the ith tag .
we multipli these decis to get the probabl of an entir structur .
so the probabl of a structur , is the product of these decis probabl .
we typic estim the paramet , us variant of maximum likelihood estim , so typic some kind of maximum likelihood estim with some form of smooth or regular .
and then final , thi function capit f , which map input x to label y , is defin as the label y , that maxim either the joint probabl of x and y .
think of hmn's or pcfg's as be on exampl of thi case , or the condit probabl of y given x .
thi is what we saw with log linear tagger .
so basic we ar go to us these histori base model , to defin either a joint distribut over input and output , or a condit distribut over output given input .
but in all of these model , we follow thi methodolog were we've broken structur down into sequenc decis , associ each decis with a probabl , and then multipli these probabl togeth , to get the probabl for , for an entir structur .
just briefli , here's a first exampl of thi pcfg .
so what is the sequenc of decis in a pfcg ?
we talk a lot about left most deriv , top down deriv .
so at each point in a deriv , we have some non termin , for exampl s , and our decis is to expand that , with some rule , for exampl , s goe to np vp .
each of these decis ha some associ condit probabl .
so q of s goe to np vp .
there's a paramet that basic specifi the condit probabl .
we're see the rule s goe to npvp , given that we're expand s .
the probabl of a structur as a product of these term .
so , if i have a rule with n rule , sorri a tree with n rule .
alpha i goe to beta i for i goe <num> to n .
then we just multipli togeth , these q paramet for each of the , the individu rule .
paramet valu ar usual estim us variant of maximum likelihood estim .
in thi case , thi just mean that our paramet estim is a ratio of two count .
the number of time we've seen the rule , divid by the number of time we've seen it on termin alpha .
and then final , as i said in the last slide , the final function from a sentenc to a pars tree , is to find as follow ; for ani sentenc x , we search over all pars tree y , and return the pars tree with the highest probabl the probabl here be a joint probabl of y and x .
here's a second exampl log linear tagger .
so in thi case , for a sentenc of length n , we have n tag decis , in left to right order .
so , for exampl , we might have the decis of determin , noun , verb .
if we had a sentenc of length three , for exampl .
each decis ha an associ condit probabl .
so , thi is go to be the probabl of i'th tag .
given the previou two tag , and given the entir sentenc .
the probabl of a tag sequenc condit on a word sequenc , is a product of term , where we have the probabl of ti , given the previou two tag and the entir word sequenc .
and in thi case we estim or we defin thi model , thi condit model of the i's tag , given the previou two tag in the sentenc , us a log linear model .
and we saw how to defin those model , and how to defin the paramet estim in those model .
final , for an input x , we defin the function f of x to be the y that maxim the condit probabl of y given x .
so , here's an overview for the remain of to , remaind of todai's lectur .
we're go to de , describ the basic framework of global linear model .
and we'll see that thei offer a veri differ wai of think about these supervis learn problem from the histori base model that i just discuss .
we'll talk about pars problem in thi framework , in particular , rerank approach .
and final , we'll talk about a variant perceptron algorithm , which can be us for paramet estim in these global linear model .
so , on kei idea is the follow .
we'll move awai from thi idea of deriv .
or in particular , we'll move awai from thi idea of attach probabl to individu decis , which go into build an entir structur .
instead , we're go to talk about featur vector over entir structur .
we'll call these , global featur .
and thi is realli where the name global linear model come from .
so , now we're go to defin function , which for exampl , look at an entir pars tree and match these to some featur vector .
so , in on sens , these featur vector map will be highli veri close relat to the featur vector map we saw for log linear model .
but in anoth sens there's a rather more radic move here to a situat where entir structur ar get map to these featur vector .
there ar sever reason for think about these model in these term or , there ar sever reason for introduc global featur .
first piec of motiv is , is that , it can offer consider freedom in defin featur .
so , thi will allow us to incorpor all kind of featur , and all kind of inform within our supervis learn problem , which were realli challeng to includ in the histori base model that we've seen up to thi point in the cours .
so , let me give a coupl of exampl of featur in the pars problem , which , as we'll see , ar quit easili incorpor within a global linear model .
but which ar much more difficult to incorpor within , for exampl , a probabilist context free grammar .
so , thi is on observ by mark johnson and some other from <num> , which is that there's a definit tendenc in natur languag for someth call parallel in coordin .
and thi is essenti the follow .
so , if i look at these two phrase , i have bar in new york and pub in london .
second phrase i have bar in new york and pub .
thi on ha an instanc of parallel .
and that's becaus bar in new york ha a veri similar structur , syntact speak , to pub in london .
so , i have two thing be coordin here , which have basic similar structur .
if we look at the second exampl , thi doe not have parallel .
so , we're coordin bar in new york with pub .
and these two constitu do not have the same intern syntact structur .
so , statist speak , we seem to see a prefer for these kind of structur , as oppos to these .
and thi kind of prefer can come in us when we're try to disambigu structur .
so , if i sai for exampl , i visit bar in new york and pub in london .
there ar go to be a few paus here .
and some of them ar go to , go , go to includ thi sort of parallel structur , where these two thing ar noun phrase and their coordin .
and some of these structur will not includ these two parallel structur be coordin .
know that there's a prefer for parallel in coordin can help me give a prefer for pars tree with these kind of parallel structur .
now i , i would challeng you to go back to the lectur on probabilist context free grammar and try to figur out how to incorpor thi prefer within a probabilist context free grammar becaus it is realli not entir straight forward .
so , that will be on of the motiv for global linear model .
we'll see that it's actual veri easi to incorpor thi kind of featur within that model .
here's a second exampl of the kind of featur which might be us but , which ar again rather difficult to incorpor within a histori base model , such as a probabilist context free grammar .
these ar semant featur .
so , imagin we have an ontolog or some kind of lexicon , which give properti of differ noun and verb .
so , on exampl of such an ontolog might be a resourc call wordnet .
thi is a veri famou resourc that ha an inform about a veri larg categori of noun and verb in english .
and thi ontolog might state , for exampl , that the word cappuccino ha the plu liquid featur , wherea , the word book doe not have that featur .
okai ?
so , cappuccino is a liquid .
and that's import if we're think about word like pour , where in gener , noun which have a plu liquid featur ar like to appear as object , wherea , noun which ar not liquid ar much less like to be object .
so , these kind of on thi kind of ontolog inform , where differ noun have differ properti , can be quit us again in model the probabl of differ path structur or in disambigu differ path structur .
so , sai we'd like to build a parser that ha a prefer for the verb poor , take noun with the plu liquid featur .
again , i would challeng you to think about how we would incorpor these kind of featur within a probabilist context free grammar .
it is somewhat challeng , wherea , again we'll see with global linear model .
it's rel easi to incorpor these kind of featur .
so , next , we're go to describ global linear model .
and we'll focu on the three compon of these model .
so firstli , we'll see that we have featur vector .
so f is go to be some function that actual map and entir xy pair to some featur vector in some d dimension space .
so x might , for exampl , be a sentenc .
and y it might be a pars tree , for exampl .
and so f will be a function that map an input output pair to a featur vector , again these featur vector ar go to be somewhat similar to the featur vector you've seen in log linear model .
but the differ is , thei now take in entir structur as their input .
the second compon is a function call gen .
which take an input x , and map the input to a set of candid .
so , for exampl , we might take a sentenc .
we have some function that map that sentenc to a set of candid possibl polici for that sentenc .
and we'll see there ar sever wai of essenti do thi .
final , we have a paramet vector v , that's the third compon .
thi is also go to be in d dimens .
we have the , the same number of paramet as we do featur .
and in gener the function f and gen will be fix , so thei'll be defin ahead of time .
and our train data will be us to set the paramet valu v .
and the paramet v ar veri similar to the kind of paramet we saw in the log linear model .
so let me talk about these three compon in turn , in a littl more detail .
so here's f , so again stick with the paus problem f take an entir xy pair input .
so in paus k , we would have a sentenc , which you can see at the fring of thi tree , pair with some past tree .
and we're go to defin some function that take thi pars tree's input and return a featur vector as a power put .
so in thi case the dimension , d , the number of featur .
is equal to <num> , <num> , <num> , <num> , <num> , <num> , <num> .
so we have a , a seven dimension represent .
in practic in mani of these model , d can be veri larg .
you can often have ten of thousand or hundr of thousand of featur in these model .
particularli in the pars case , if these featur start to take into account the lexic inform .
the identifi of these individu word .
f is go to plai a critic role .
and it defin how a candid structur is repres , thi is go to be the inform which goe into the learn algorithm , thi is go to be the inform which the learn algorithm leverag's in choos between good and bad structur for particular input in gener , we'll construct a featur vector .
but by defin a number of individu featur .
so each featur is go to be some function that take an entir structur as input , and return some real valu .
as it output .
a veri common type of featur will be a featur which count some kind of a substructur within the structur we're look at .
so for the pars case for exampl , it'd be veri natur to defin a featur which count the number of time a particular rule , sai a goe to bc or s goe to np vp or vp goe to vt np .
okai so we could have a whole number o featur which track count of differ rule in our grammar .
so in thi particular exampl if i defin hxry to be the encumb of time the rule a goe to bc as seen in the structur .
for thi particular tree we'll return the valu on , and for thi tree we'll return the valu two becaus we have two instanc of thi rule here . . .
and on instanc of thi rule here .
so in gener , we will defin a featur vector by defin a whole set of function .
we'll actual have littl d these function h1 through hd and will concaten these to get a featur vector represent of a pars string .
so each on might count the number of time the rule a goe to bc is seen within the tree .
h2 might count the number of time some other rule is seen within the tree and so on and so on .
in fact a veri simpl represent of pars tree would be to simpli have on featur for everi possibl context free rule in our underli grammar .
and so now , you can see that on pars tree might get map to on featur vector .
anoth pars tree might get map to anoth featur vector like thi .
and again , i'll stress that these featur vector map ar the input to the learn algorithm .
so it's realli import to come up with a good definit of these featur .
so the second compon of global linear model is thi function gen which essinti take an x as input and enumer a set of candid structur .
and so the learn model is actual go to choos between on of these structur but gen is critic in that's it's go to enumer the full set of candid that is go to be consid for a particular input .
so in the paus case gen would take a sentenc's input . . .
and produc a set of candid paus tree for that sentenc .
and we'll then see how we can us the featur vector in combin with paramet vector to choos between these differ paus tree .
so there ar a number of differ wai of defin gen .
here ar a few .
so , for pars , we could defin gen x to be the set of all possibl pars for x under a grammar for exampl , a context free grammar .
and notic in thi case , gen is larg .
becaus , in gener , the number of pars tree for a sentenc can easili grow exponenti with respect to the size of the sentenc .
actual , later in thi segment on global linear model , and probabl in next week's lectur .
we will see wai of deal with case where gen x is a veri larg set , like thi on .
anoth scenario which is quit common within these model is to defin gen x to be the top n most probabl pars under a histori base model .
so that mean we can think of the histori base model as be a baselin approach which for a given sentenc will produc mai be <num> or <num> or a <num> differ pars .
and so in thi case the size of gen is fairli manag , it is rel rather small the motiv in thi kind of set is that , the histori base model will do a pretti good job of gener <num> reason good pars .
and in particular , on of those pars mai be the correct on .
and then the global linear model will be us to select within these differ pars .
us more power featur than were seen in the underli histori base model .
in tack we could defin gen x to be the syllabl possibl tax sequenc for an in protect so if we have sai the dog bark and the sort of text we have is dnv .
then we have , basic everi possibl tax sequenc as gen , again thi set is quickli go to get veri , veri larg , it's go to grow exponenti quickli respect to the length of the sentenc .
and final in a translat set , gen x might be the set of all possibl english translat .
for some friend sentenc , mayb all possibl translat , for exampl , under a phrase base lexicon , with some particular distort limit .
but abstractli , the import thing to rememb is that gen , in gener , in some wai , we have , of the new rate , a set of candid , for an input sentenc x .
the final compon of global linear model is the paramit vector , and thi is what is go to be actual learn from the train exampl .
so i'll us littl v to refer to a paramet vector which is also in d dimens .
rememb that the featur vector ha d featur .
and we'll us f and v togeth .
to map a candid structur to real valu score .
so if we take some xy pair , in thi case , it's a sentenc pair with a paus tree .
we can first map that structur to a featur vector through thi function f .
and then secondli we can take the inner product between f and v , the dot product , in exactli the same wai as we saw for log linear model , to get an overal score for the pars tree .
so here we have the featur vector f .
and here i have the perimet that to which it sai .
for exampl the first featur you get to weigh <num> .
the second ha weight minu <num> , third ha <num> , and so on and so on .
i take the inner product of these two vector .
i get some score .
for exampl <num> in thi particular exampl .
now the score is go to have an interpret .
as be some measur of the plausibl of thi particular structur y when pair with x .
more like structur ar go to have higher score .
and in particular we'll see soon that the output from the global linear model will be the structur y that maxim thi inner product .
so again i want to stress that we've made a rather radic move here .
we've basic taken the technolog we saw with log linear model , but now appli it to entir structur .
so now an entir paus structur , get map through some featur vector , to some some featur vector represent .
an then the entir paul structur get a score which is the inner product of f and v and thi score is essenti go to replac what wed seen in other model in tehi class which wa either a joint probabl of y and x or a condit probabl of y given x .
well see that thi school product plai a rather similar role to these probabl .
now that i've given you these three part to the model , we can put thing togeth and see how the global linear model defin a function from the set of input to the set of output y so what's thi function capitol f so thi go to take some input x and it go to return some structur y and that calcul is actual veri simpl .
we simpli , search for the y in the set of candid gen x , that maxim the score i show , i show you on the previou slide .
and so , it's realli a veri simpl idea .
we simpli choos the highest score candid as the most plausibl structur .
so thi critic is how a function from input to output is defin , and you can see it depend on all three compon , on gen and f and v .
now , as i said befor , gen and f will in gener be fix , thei'll be chosen befor we see the train exampl a critic question we'll address is that given train exampl xiyi for i goe <num> to n .
so given exampl of the xy map .
how do we set these paramet v .
and thi is someth we'll come too shortli .
so thi line mai help us to visual the entir process a littl bit better .
we have some input sentenc and we want to map that to a pars tree .
thi function gen first enumer some set of candid pars tree for thi input sentenc , in thi case there ar on , two , three , four , five , six possibl pars tree .
each of these paus tree get map through a differ featur vector through the function f .
so we have on , two , three , four , five , six featur vector , on for each of these paus tree .
and then given a paramet of vector v , we can take the inner product of f and v to give a score for each of these differ postri .
so we have on two three four five six score .
so thi tree get score <num> .
thi tree get score <num> <num> <num> <num> <num> .
and then final , we find the highest score postri .
so <num> is the highest of these score .
so thi pars tree is actual select as the output of the model .
thi arg max oper pick the highest score tree .
you can see that , as you vari these paramet valu , v .
you're go to affect the score given to these differ pars tree .
and you're go to , go to affect the rank given to these differ pars tree and most importantli you can effect the output of the model , thi arg max , so the train procedur is essenti go to somehow manipul these paramet in a wai that we do well in recov the structur on these , on our train or test exampl .
so let me now talk about a first applic of these model in some detail .
now that is to the pars problem , but in particular to an approach call rerank .
we'll talk about how to rerank the pars or rather the end best pars from an exist histori base or probalist pars .
so , in rerank approach , we're basic go to do the follow .
we'll us some baselin parser .
in particular , thi might be a lexic pcfg , for exampl , which we know is quit a good model for pars .
we're go to us a baselin parser to produc it top n pars for each sentenc in train or test data .
so n for exampl might be <num> or <num> or <num> and i'm assum that i have some wai of find the top <num> or top <num> or top <num> most probabl pars under the lexic pcfg model .
in fact thei're a varianc of the kind of dynam program algorithm we saw earlier in the class , which will do precis thi , thei will return the top n most like pars .
now if you think about that , we can visual thi as follow , we have a sentenc and we gener a bunch of pars , sai the top <num> most probabl pars onto the lexic pcfg .
even if the first or the most like pars under the pcfg isn't the correct on , there's a good chanc that on of these hundr pars will be correct and there's an even better chanc that on of those <num> pars will actual be much , much better than thi singl best pars .
so in the particular experi i'll describ a littl later , we actual gener about <num> pars on averag for our <num> , <num> train sentenc .
thi gave us about a million , train pars if you consid all possibl member of gen for all possibl sentenc seen in the train data .
so our supervis data is go to look like sentenc tree pair , and we can do a coupl of thing there .
we can either take yi to be the true tree bank pars for that sentenc , or we could take yi to be the member of thi function gen , which is closest to the tree bank pars under , for exampl , precis and recal .
that's a minor detail .
just assum that we have some wai from our tree bank , of choos on of these member of genx as be the label , the correct pars for thi particular input .
now , let's talk about the represent f , the featur vector map we us .
so , the critic idea is , in these rerank approach we have a function f that take an entir tree as input and return some featur vector , so mayb some sequenc of featur .
and each of these featur is go to be a function map a tree to a real valu .
and critic we're realli in a posit where we can defin ani featur we could think of .
so , if we think back to those exampl of parallel and coordin .
thing like , bar in new york and pub in london .
then we could , for exampl , have a featur that identifi the fact that thi particular structur is a parallel coordin structur .
we could for exampl have a featur which count the number of parallel coordin structur we have within a pars tree or a featur which count the number of non parallel featur structur we have within a pars tree .
by introduc featur which track thi kind of all the frequenc of these parallel vers non parallel structur a model can learn to prefer a parallel versu non parallel structur .
here ar some other featur , actual some featur us .
anoth veri us featur is actual the log probabl of xy under our baselin model , rememb thi is lexic pcfg .
and so we can take the probabl , it's often slightli better to take the log of that probabl , and that might be our first featur .
thi basic give us a default rank , so at the veri least our rerank model should do as well as the lexic pcfg becaus it ha thi inform under the probabl from the pcfg .
here's anoth potenti featur , thi is a zero on featur , an indic function .
veri similar to the indic function we've seen on log linear model earlier .
thi is on if the x y pair contain a particular context free rule , for exampl thi entir context free rule here .
let me describ some featur us in a paper i wrote with terri koo back in <num> .
and throughout thi i'll us thi quit complic rule , vp goe to preposit phrase , vbd , np , np , sbar , as the exampl rule .
and the featur as we'll see ar gener orient in some sens around context free rule like thi on .
so the first set of featur we includ in the rerank model , were featur which consid individu entir rule .
so , for exampl , we might have on featur which count the number of time i've seen thi particular rule within the tree .
now , interestingli , the lexic pcfg we were us , broke these , larger rule down into smaller on .
roughli speak through binar , through thi process where we take thi rule and we might somehow binar it by introduc some intermedi non termin .
ok .
so , the underlin less close pcfg wa in someth close to chomski novel form .
and so it didn't realli have paramet correspond to entir larg rule , like thi on .
in the rerank model , we actual ad in featur correspond to the entir rule .
the motiv for that is that some of these rule ar veri import .
and while thi chomski normal form grammar is veri us in reduc the number of paramet in the model , it doe make some assumpt which miss some import properti consid the frequenc of entir entir context free rule .
so , we ad featur like thi and thei're quit us .
here's anoth featur center around thi particular rule exampl which captur someth with which the lexic pcfg wa miss .
and these ar sort of bigram featur within rule .
and these ar adjac pair of non termin to the left and right of the head .
so in thi particular rule , we're go to see the follow , bigram .
we're go to see on , two , three to the right .
so in each case we have the non termin , thi is a vp up here .
and we have a pair of consecut non termin , np np .
so that's where thi particular featur come from .
anoth featur would look at the fact we again have someth to the right of the head of thi rule .
so the vbd by the wai is the head of thi rule and thi would look at the fact i have a vp , an np and a sbar to the right .
final i have again a right word featur with vp sbar , stop acknowledg that stop is the final compon to thi rule .
and i have left word bigram here , which is left word , vp , preposit phrase , stop .
so these featur ar go to be captur bigram within rule and again the lexic pcfg we were look at realli miss these featur , and so introduc new featur which again can be us in disambigu good from bad pars tree .
here's anoth exampl of the type of featur you might includ .
so we call these grandpar rule .
and so these featur ar not go to be sensit to just the rule , for exampl what i've shown here , but also to the non termin directli abov .
an s in thi case .
so we've essenti introduc a featur correspond to a larger fragment of thi tree .
and again these featur ar not conclud within the last class pcfg .
and thei can be us in some case .
a minor variant of idea is to actual includ two level of context free rule so the fact we have an s goe to np vp here follow by vp be rewritten as thi entir rule here and we could introduc similar featur to thi for mani other two level rule seen in our train set .
.
so that give you the basic idea .
that's some exampl of a few of the featur we us in thi model .
and you can see that the basic idea is to start to introduc featur .
which look at larger fragment of these syntact structur and we should go beyond the kind of featur we saw in a simpl lexcal pcfg .
so , veri soon i'll talk about result us the rerank approach us the kind of featur i just describ .
but first i want to talk about a first method for paramet estim , name a method for find these v paramet and thi is a variant of the perceptron algorithm .
we'll see that it a veri simpl algorithm , but empir it turn out to work realli quit well on a , a , a wide rang of natur languag process problem .
so , here is the algorithm .
so , we're go to assum that as input we have some train set , some set of exampl xi , yi for i equal on to n .
and we have these function gen and f , which i just describ , so gen enumer candid for an input , f defin a featur vector map , and we'll assum these <num> compon ar fix .
the task is go to be learn , to learn our paramet v us these train exampl as inform .
so , we initi all of our paramet v to be <num> , so we set v equal the vector of all zero .
that's the start state , and then , i'll defin capit f for ani input x in exactli the same wai as befor .
so , we enumer all possibl structur y in the set gen of x .
for each structur , we can calcul a featur vector , f of x and y , and we take the inner product , with v to calcul the score for that structur , and we , we take the highest score member of gen as the final output of thi model .
initi , when all of these paramet ar <num> , all of these structur ar go to have score <num> , and so thi is go to be a tie .
let's just assum in that case that we have some arbitrari wai of choos between the tie member and the argmax .
that's a minor detail .
the other input to the algorithm is t , capit t , which is the number of iter the algorithm ha run for .
and on nice thing about the perceptron is that t can often be quit small , mayb <num> or <num> or <num> .
okai , so the algorithm proce as follow .
so , i make big t pass over the data .
that's what thi loop here doe .
and at each loop over the data , i pass over the exampl from i equal on to n .
so i go through a singl train exampl at a time .
for each train exampl i do <num> thing .
firstli , i calcul the current output from the model .
so z sub i is go to be the output of f on xi .
it's go to be the highest score structur under my current paramet .
now , if zi is equal to yi , that mean that i've correctli recov the correct structur on thi particular exampl .
and in that case , i do noth to the paramet .
i leav the paramet phi unchang .
the idea here is that , if it's not broken , don't try to fix it .
we have not made a mistak on thi exampl .
so let's leav the paramet unchang .
if howev we have zi not equal to yi , then the structur we've produc is somehow differ from the target structur yii .
and in that case i make an updat to the paramet v .
the updat look extrem simpl .
so we simpli take v , the new valu for v , is the old valu for v .
we add in the featur vector for xi , yi and we subtract the featur vector for xi and zi .
rememb , all of these vector ar in some d dimension space .
so the , the paramet of x is a d dimension , and the phi of x is also d dimension .
and so there's no clash here .
these , these comput make sens .
i'm ad and subtract d dimension vector .
intuit , what is go on here is that ani featur which ar seen in the true structur have their paramet valu increas , wherea ani featur seen in the incorrectli propos structur have their featur valu , sorri their paramet valu decreas .
so thi is kind of a reinforc step , which push up the paramet valu for thing seen in the truth and push down the paramet valu for featur seen in zi , which wa the incorrectli produc structur .
so that is the perceptron algorithm .
the thing to stress ar that the input to the algorithm ar set of train exampl .
and definit for gen and f and also specif of t , the number of pass over the train set and the output is , as i said , a paramet v .
as i hope you'll see , it a veri simpl algorithm which just reli on decod at each step , find the highest con structur under the current model then if we make a mistak , make a veri simpl updat to thi paramet v .
there's actual a quit rich theori underli the perceptron .
and justifi it , both in an algorithm sens , defin when it converg , and also in a statist sens , describ why it can gener well to new test exampl or at least specifi condit under which it is guarante to gener well to new test exampl .
we'll try to post some pointer to paper on those topic on the cours .
i'm not go into detail about the theori underli the peceptron .
so let me just conclud thi lectur with some result on the pars problem us the rerank model that i just describ to you , in conjunct with the percept .
so in on set of experi by myself and terri koon , we start with a baselin model which wa electrolyz .
pcfg .
which , at least , at the time , wa veri close to the state of the art in pars .
and that score around <num> precis and recal .
rememb , f measur is a kind of averag of precis and recal , in recov sub constitu within a pars true .
the rerank model that i just describ score <num> f measur , which is about <num> rel error reduct .
so about <num> of error have been correct by the rerank model .
so that's a fairli signific improv , given that these model ar start to reach quit high level of accuraci .
thi is actual a pretti signific improv , and inde i had develop these lexic p , pcfg's dure my ph . d thesi , and it wa veri , veri hard to push these ani further other than thi <num> measur which we see here .
here ar some other result more recent from eugen charniak and mark johnson in <num> .
thei emploi a similar approach .
but thei had better and best list , better featur , and also importantli a better baselin model than thi model i've shown you here .
and thei push accuraci from about <num> to <num> accuraci .
thi is actual veri , veri close to the state of the art in pars perform .
so , the rerank model , again , give a pretti signific gain and actual produc on of the veri best result we've seen on pars .
what i've shown you , though , in thi lectur , is a quit new wai of think about these supervis learn problem that we see in natur languag process .
thi idea of global linear model defin through gen f and v , and final the percept algorithm as on wai of train these paramet v .
thei give signific improv on these rerank problem , but perhap most importantli thei're go to open up a whole new wai of think about algorithm for problem such as translat or tag and pars .
and we'll see how we can appli these model in sever other context in the final week of lectur , which is the next week of thi class .
okai , so welcom to natur languag process .
my name is michael collin , i'm a professor in comput scienc at columbia univers .
i've taught thi cours for sever year now , most recent at columbia , and befor that at mit .
natur languag process is i think , a tremend excit field .
it build on insight from comput scienc , from linguist , and as we'll see , increasingli from probabl and statist .
it's also have a huge impact in our daili live .
mani , mani applic and technolog ar now make us of basic idea from natur languag process .
so , in thi introductori lectur , we're go to cover a few basic point .
the first question we're go to ask is , what is natur languag process ?
so , we'll discuss a few kei applic in nlp and also a few kei problem that ar solv in natur languag process .
the second question we'll consid is , why is nlp hard ?
so , we'll consid some kei challeng that we'll find in natur languag process .
final , i'll talk a littl bit about what thi cours will be about , what kind of materi we'll cover in thi cours , and what in gener you should expect take thi cours ?
so , at a high level , natur languag process concern the us of comput in process human or natur languag .
so , on on side of thi problem , we have what is often refer to as natur languag understand where we take text as input to the comput and it then process that text and , and doe someth us with it .
at the other hand , we have what is often refer to as natur languag gener .
where a comput , in some sens produc languag in commun with a human or user .
so , we should first consid some kei applic in nlp .
on of the oldest applic and a problem of great import is machin translat .
thi is the problem of map sentenc in on languag to sentenc in anoth languag .
and thi is a veri , veri challeng task .
but remark progress is be made in the last <num> or <num> year in thi area .
so here , i have an exampl translat from googl translat which mani of you will be familiar with , thi is a translat from arab into english .
and , while these translat perfect , you can still understand a great deal of what wa said in the origin languag .
so , later in thi cours , we'll actual go through all of the kei step in build a model in the machin translat system .
so , a second exampl applic is what is often refer to as inform extract .
so , the problem in thi case is to take some text as input and to produc some structur , basic a databas represent of some kei content in thi text .
so , in thi particular exampl , we have input which is a job post .
and the output captur variou import aspect of thi post .
for exampl , the industri involv , the posit involv , the locat , the compani , the salari , and so on .
and you'll see that thi inform is pull out from thi document .
so , the salari in thi case come from thi , thi portion here .
so thi is a , a critic exampl of a natur lang , languag on the stand problem where the promis to , in some sens understand thi input were unstructur text and to turn it into a structur data base kind of represent .
so there's some clear motiv for thi particular problem , inform extract .
onc we've perform thi step , we can , for exampl , perform complex search .
so sai i want to find all job in the advertis sector pai at least a certain salari in a particular locat .
thi would be a search that is veri difficult to formul us a regular search engin , but if i first run my inform extract system over websit all of the job post that i find in the web .
i can then perform a databas queri and , and perform much more complex search such as thi on .
in addit , we might be abl to perform st , statist queri .
so we might be abl to ask you know how is the number of job in account chang over the year , or what is the number of job in softwar engin in the boston area post dure the last year .
anoth kei applic in natur languag process is text summar .
and the problem in thi case is to take a singl document or , potenti a group of sever document and to try to condens them down to a summari .
which , in some sens , preserv the main inform in those document .
so here , i actual have an exampl screenshot from a system develop at columbia , which is call new blaster .
and thi is actual a multi document system .
it will take multipl document on the same new stori , and produc a condens summari of the main content of those document .
so in thi particular exampl , we have a larg group of document all about vaccin program .
and here is a summari which attempt to captur the main inform in all of these document .
so summar again ha clear motiv in make sens of the vast amount of data or text avail on the web and the new sourc , and so on .
it's veri us to be abl to summar that data .
anoth kei applic is what ar call dialogu system .
and these ar system where a human can actual interact with a comput to achiev some task .
so , the exampl i've shown here is from a flight domain , where a user is attempt to book a flight .
and so the user might come with some queri to the system .
and the system then goe and process thi queri , in some sens it understand that queri .
and in thi particular case it realiz that there's a piec of miss inform , name the dai of the flight and so the system them respond with a queri , what dai ar you fly on ?
the user provid thi inform and the system return a list of flight .
so in dialog system the basic problem is to build a system where the user can interact with a comput us natur languag .
and notic that thi type of system involv both natur languag understand compon , we have to understand what the user is sai .
and there's also importantli a natur languag gener compon .
in that we're go to have to gener text in some case .
for exampl clarif question as we've shown here .
so in addit to the applic i've just describ , we'll also consid some veri basic natur languag process problem which on depend mani of these applic .
and the first , we'll talk about is someth call the tag problem .
so abstractli tag problem take the follow form .
as input we have some sequenc , in thi case a sequenc of letter and as output we ar go to have a tag sequenc where each letter in the input now ha an associ tag .
thi is probabl best illustr through a coupl of exampl .
the first on is part of speech tag .
so the problem in thi case is to take a sentenc input , for exampl profit , soar , at boe co and so on , and to tag each word in the input with it part of speech .
so n stand for noun , v stand for verb , p stand for preposit , adv stand for adverb , and so on and so on .
so , thi is on of the sort of , most basic problem in natur languag process .
if you can perform thi map with high accuraci , it's actual us across a veri wide rang of applic .
the second exampl of a tag problem is what's call name entiti recognit .
so the basic problem here is , again , to take some sentenc's input .
and now , to identifi basic entiti in that sentenc .
so , entiti ar thing like compani .
so we have boe co here .
locat .
wall street here .
we might also , for exampl , identifi peopl .
again , name entiti recognit is a veri basic problem .
but it's clearli veri us in mani applic to be abl to identifi compani , locat , peopl , and other entiti type .
thi problem could be again thi problem could again be frame as a tag problem where each word in the input is tag either as not belong to ani name entiti , so na mean we're not part of an entiti , or we might be part of a compani .
sc mean we have the first word in a compani , it's a start compani .
cc mean we have the continu of a compani .
and similarli , sl mean we have the start of a locat .
cl mean we have the continu of a locat .
so again abstractli , a tag problem is a problem of map an input sequenc of item , usual word to some tag sequenc , where each word in the sequenc ha an associ tag .
these ar two kei problem , and there ar mani , mani other .
anoth basic problem in natur languag process is the problem of natur languag pars .
and thi goe back to work in linguist , go back to sort of the foundat of modern linguist with noam chomski work in the 1950s .
the problem here is again to take some sentenc as input , and to map thi to some output , which is usual refer to as a pars tree .
so we'll talk a lot about thi later in thi cours , but at a veri high level .
the pars tree essenti give some hierarch decomposit of a sentenc .
correspond to it grammat structur .
and onc we've recov these kind of represent .
we can , for exampl , recov basic grammat relat .
the fact that boe is the subject of thi verb is , or the fact that thi preposit phrase in seattl is a modifi to locat .
so , again thi is a veri basic problem in natur languag process .
well see that , if you can recov these represent with high accuraci .
it's us , again across a veri wide rang of applic to gain a first basic step in natur languag understand .
and try to make sens of what an underli sentenc mean .
and it's also quit challeng problem .
so next i want to talk about some kei challeng in nlp answer the question why is nlp hard .
and we're actual go to focu on on particular problem , name the problem of ambigu which is seen time and time again in natur languag problem in applic .
so to illustr thi we'll take an exampl sentenc .
thi is actual from livian lee , and the sentenc here is , at last , a comput that understand you like your mother .
and i think thi wa taken from some market blog for a natur languag understand system sometim back in the 1980s i believ .
the problem , of cours , with thi sentenc is that it is actual ambigu , ambigu .
there ar at least three re , read or interpret of thi sentenc .
so let's go over that .
so the intend mean wa probabl thi .
it's sai that the comput understand you as well as your mother understand you .
but if you look a littl harder , you'll see that there ar a coupl of other possibl interpret .
so on is , that the comput understand the fact that you like your mother .
so at last a comput that understand you like your mother .
anoth interpret , the comput understand you as well as it understand your mother .
so at last a comput that understand you like your mother .
and then , even if we look at these <num> interpret , <num> and <num> alon .
there's a question of , of whether thi mean , you know ?
doe the comput understand you well is the context .
doe your mother understand you well ?
without know that , you can't answer that question .
and then we have a similar ambigu here .
so ambigu also occur at the acoust level .
and thi is a , a kei problem in speech recognit .
so again , if we come back to thi exampl sentenc , then from a pure acoust point of view thi sequenc like your , can be confus with other thing .
so , for exampl like cure is a valid sequenc of two word in english .
and acoust , these two thing might be quit confus .
of cours , thi is much more plausibl as a sentenc in english than thi .
but nevertheless , if a speech recogn wa to reli on acoust inform alon , it's go to have to deal with all kind of ambigu like thi where two word or two sequenc of word ar confus .
it's interest to note that mani of these ambigu manifiest themselv at the syntat level .
what i mean by that is thei manifest themselv in the pars problem that i show your earlier .
so differ structur at the pars level , again these , these pars tree i show you , lead to differ interpret .
so in fact these two interpret where the comput understand you , as well as it understand your mother .
thi is it understand the fact that you like your mother .
actual , correspond to two quit differ syntact structur .
so on of the kei problem in natur languag pars , is essenti disambigu .
choos between differ syntact structur correspond to differ interpret .
here's yet anoth level of ambigu in languag .
thi is roughli speak , what we might call the semant level .
and thi is an instanc of what is often refer to as word sens ambigu .
so , if you look at it in a dictionari , you , if you look at mani word in english or other languag , thei have multipl differ mean , depend on the context .
so again mother wa a word in that sentenc i show you previous if look in a dictionari you'll find the convent or rather the , the , the frequent usag of thi word .
but you also find a much less much less frequent usag which is given here .
and a natur languag process system will have to disambigu between these two mean .
here's a bit more on word sens ambigu , so if i take the sentenc , thei put monei in the bank , it's pretti clear here what we mean by bank .
but there is an interpret where the monei is buri in mud , so we're go to have disambigu thi word to work out which of these interpret is intend .
here's anoth exampl , i saw her duck with a telescop .
so thi sentenc is actual ambigu in multipl wai .
i could be look at her duck us a telescop .
i could be look at her duck who ha a telescop .
you can go through mani other exampl .
but there's a kei word sens ambigu here which is the word duck .
duck can refer either to the anim or it can refer to the process of duck , the verb to duck and that lead to two quit differ interpret here .
so yet anoth level of ambigu is at the discours level and what i am go to show you here is an instanc of an anaphora .
thi is a problem of a pronoun for exampl , she and revolv it resolv it to the entiti that it refer to in a word .
so , let's again assum that we have our sentenc , except here i have alic sai , as the , the start of the sentenc .
and let's sai the continu is , but she doesn't know ani detail .
actual , i have two possibl continu here .
but she doesn't know ani detail , or but she doesn't understand me at all .
so she could refer to two differ thing .
it could refer to your mother .
or it could refer , at refer to alic .
and so there's an ambigu here that we need to resolv .
we need to figur out which on of these two entiti in the past she is refer to .
and actual it's go to differ for these two case .
if i sai , but she doesn't know ani detail , by far the most plausibl interpret is that she is refer to alic .
but if i sai , but she doesn't understand me at all , in thi case by far the most like interpret is that she refer to your mother .
so thi is again a veri challeng proposit , take pronoun and figur out what entiti thei refer to in the past discours .
so the final thing i want to talk about in thi lectur is the content of the cours .
what will thi cours actual be about ?
and so we're go to cover sever topic in thi cours .
on thing we'll look at is the basic , and sub problem that i mention earlier .
problem such as part of speech tag , pars , model for word sens disambigu , and so on .
a second major focu of thi talk will be on machin learn or statist method for natur languag process .
so machin learn techniqu have becom extrem preval for all of the natur languag applic and problem that i describ earlier .
as on exampl , modern machin translat system ar train automat from vast number of exampl translat .
statist method ar us to induc dictionari and other kind of model for that translat problem .
so we'll talk about mani basic mathemat or comput model base on machin learn techniqu appli to languag .
these includ probabilist context free grammar hidden markov model .
we'll talk about estivm and smooth techniqu right at the start of the calss .
we'll talk about a famou algorithm call the em algorithm which is wide appli in speech recognit and also machin translat .
we'll talk about a veri import class of model , call log linear model later in the class , and so on .
and final we will talk about variou applic includ inform extract , machin translat , and possibl natur languag interfac .
so here is a syllabu for the class , we're go to start off with a problem call the languag model problem , which will be in some sens a warm up .
introduc mani import idea , particularli an idea call smooth destin .
we'll then talk about tag problem , such as part of speech tag and ident recognit .
and we'll talk about hidden markov model , which ar a kei model for these tag problem .
we'll talk about the pars problem that i describ earlier , and we'll talk about model for that problem .
we'll then have a few lectur on machin translat where you'll see more or less from the ground up how we can build a modern machin translat system .
next we'll talk about log linear model and discrimin method .
these ar a kei model in statist natur languag nowadai .
and thei're appli to sever problem which we'll go over in these lectur .
and then final , to finish up , we'll talk about semi supervis and unsupervis learn for nlp , which is a , a veri import topic and a huge area of con research .
so in term of prerequisit for the class , come into the class you should know some basic linear algebra .
some basic probabl will be kei , so you should know what discreet distribut is , what a random variabl is .
and final , some basic knowledg of algorithm in comput scienc .
and secondli , there ar go to be some program assign on the cours , so you should have some basic program skill .
for exampl , if you can program in python or java , that should be plenti for thi cours .
on of these two program languag will be plenti for thi cours .
final , in term of background read for the cours , there ar a coupl of resourc .
so over the year i have develop some fairli comprehens note for mani of the topic .
actual for all of the topic's you'll see in the cours , so if you go to my webpag you'll find a link to those note , and thou , thei should be veri us in , as background for read lectur .
and in addit , as addit context i'd recommend you jurafski and martin speech and languag process text book , thi is by no mean essenti , but if you want to , want to read more on the subject , thi could be veri us .
so in thi last week of the class , we ar go to go further with global linear model , and show how to extend them to be appli to a coupl of veri import problem .
the first on , which i'm go to describ in thi segment , is tag problem .
so we're actual go to describ how the perceptron algorithm i show you last time in conjunct with global linear model can be appli to the tag problem .
the second problem we'll look at thi week is depend pars , which is a new type of pars problem and again a problem where global linear model and the percept algorithm can be appli .
the interest thing , as we'll see about both of these case , is that we'll make us of global linear model in conjunct with the dynam program algorithm that you've seen earlier in thi cours .
and by do thi we'll introduc a power new wai of appli global linear model to variou problem .
so to start , let me give a recap of how global linear model work .
and rememb there were three compon to a global linear model .
a featur vector f , thi function gen , and a paramet vector v .
so let me remind you what these ar .
so , f is a function that map ani input output pair to a featur vector .
so let's take paus as an exampl .
so in that case we might have x equal some sentenc , that's go to be the input , y is go to be some pars tree for that sentenc .
and rememb , f is go to map an entir x , y pair to a featur vector .
so you might , for exampl , have thi featur vector for thi particular pars tree in conjunct with thi particular sentenc .
gen is a function that take some input and map it to some set of candid structur .
so again , x , for exampl , could be a sentenc like , the dog bark .
gen x is now go to be some set of pars tree .
again , assum that we're look at the pars problem .
and as we saw last time , there ar variou wai you might defin gen of x .
you might , for exampl , defin gen of x to be the set of all possibl pars tree under a context free grammar .
that is under a particular context free grammar .
final v is a paramet vector .
so v is go to be some sort of paramet of the same dimension as f .
so if we have five featur here , we would have five paramet valu in the , so v , for exampl , could be thi .
and if we take the inner product between v and f , we get some score , which , as we said in the last lectur can be interpret as a measur of how plausibl thi particular structur y is , when conjoin with thi particular input x .
so as we saw last time the wai we put these three compon togeth is as follow .
we have some set of possibl input for exampl a set of possibl sentenc in a languag .
thi , thi is the set x .
i us y to refer to the set of possibl output , for exampl , the set of all possibl pars tree .
our goal is to learn a function , capit f , that take an input x and map it to some output y .
so , thi function is defin through the three compon gen of f and v as follow .
so for a given x , i enumer everi on of the candid structur .
so i do an arg max over all member y of gen x .
for each such structur y , i calcul it score through thi inner product .
and then i return the highest score structur , that's what thi arg max doe .
so it's veri in , intuit in a sens .
you enumer each of the candid , in turn , you calcul their score under thi inner product and return the high score candid .
a critic question , of cours is given a set of train exampl , exampl of input output pair , how do we actual set these paramet v ?
and we solv a perceptron algorithm for that .
i'll give a recap of that algorithm in a second .
but first , let's just look at thi schemat again .
thi is a figur i show in last week's lectur .
you can view thi whole process as follow .
you have some input sentenc , the function gen enumer a set of possibl structur .
in thi case we have on , two , three , four , five , six possibl tree .
each of those structur is map through thi featur vector map f to some featur vector .
so we have six featur vector , on for each of these tree in thi case .
and then we can calcul a score for each structur through the inner product between f and v .
and so for exampl , thi first tree might get score <num> , the second tree score <num> , the third on <num> and so on .
and then final , we select the highest score tree , which is thi first on , in thi case becaus that , that ha a score <num> .
and that is the final output for the model .
so again , you can see , as these paramet v ar vari , we will put differ weight , either posit or neg , on the differ featur in the model .
and these score will chang , and so in train the paramet of thi model .
intuit , we're go to try to look for valu of v that correctli recov the correct path on most of our train exampl .
so as we saw last time , a veri simpl , yet effect , algorithm for paramet estim in global linear model is thi variant , the perceptron algorithm .
so let's look at thi again .
so the train set is a set of exampl , xi , yi .
for i equal <num> to n .
so , for exampl , i might have sentenc pair with tree .
you might have a few hundr or few thousand or mayb even a few ten of thousand of exampl like thi .
initi , we set all of our paramet equal to <num> and throughout thi algorithm , i'm go to , i'm go to defin capit f of x in exactli the same wai as befor thi arg max function .
so , capit t specifi the number of iter of the perceptron .
it might be typic <num> , <num> , <num> or <num> , mayb iter .
so that is go to specifi how mani pass we actual make over the train data .
so , we make big t pass over the train data .
for each pass , we cycl over the exampl from <num> to n .
we calcul the output of the model on the i for exampl .
so z sub i is go to be some pars tree which is the , the current highest score tree under the model .
and if zi is equal to the , the truth , the yi , we leav the paramet unchang .
rememb , if it's , if it's not broken , don't try to fix it .
so if we don't make a mistak , we leav the paramet unchang .
on the other hand , if zi is not equal to yi , we perform these veri similar simpl paramet updat .
we sai , the new valu of v is the old valu of v plu , and here i have the featur vector , the xi togeth with the yi .
and then i subtract the featur of vector for xi in conjunct with zi .
rememb all of these vector ar in d dimension space .
and so , while , initi it might look odd to add paramet and featur vector , it's a perfectli legitim oper becaus these ar vector in the same dimension and space .
intuit , thi is go to increas the weight for ani featur seen in the truth structur yi , and decreas the weight for ani featur seen in the incorrectli propos structur zi .
so that's the perceptron .
as i said , it's a veri simpl algorithm and yet , it's a veri effect algorithm .
and in thi lectur , we're go to see how to extend thi basic approach to the problem of tag .
so , here's a quick remind of what is entail in tag problem .
so abstractli , the tag problem is to take some sequenc of word or letter in thi case as input , and as output produc a tag sequenc , where each element in the sequenc , how it now ha an associ tag for exampl , cdc an so on .
on of the veri first exampl we saw wa part of speech tag .
so in thi case the input is a sentenc , an the output from the tagger is a tag sentenc , where each word now get a tag .
for exampl , profit is tag as an n for noun , soar is tag as a v , for verb , at is tag as a preposit and so on , and so on .
the second crucial exampl we look at , wa the problem of name entiti recognit .
so , in thi problem again , the input to the model is a sentenc , a sequenc of word and the output is a tag sequenc .
the tag now encod the name entiti found within the sentenc .
so , for ani word which is not an entiti , we have thi tag na , wherea for word which the start of a compani , we have the tag sc .
for word which ar the continu of the compani , we have thi tag , cc .
and similarli , we have stop locat , continu locat , stop person , and continu person , and so on .
and we've seen two basic model for tag problem , hit mark up model hmm , and more recent we saw log linear tagger .
so those ar the first two method we've seen .
in thi lectur , we ar go to see a third method , which is how global linear model glm and the percetron , can be appli to tag problem .
so how ar we go to set up the glm for tag ?
okai , so as input to a model we have some sentenc .
so let's sai n equal <num> for exampl , we might have a sentenc w1 through <num> which is equal to w1 , w2 , w3 .
you might have some sentenc like the dog bark , for exampl .
now , defin t to be the set of possibl tag .
so , let's assum we have three , three part of speech tag d , n , v .
the first thing we're go to defin is the function gen .
so , rememb gen is go to take some sentenc's input and map it to a set of candid structur .
and in thi particular case , we're go to us thi definit , where gen for ani sentenc , simpli return all possibl tag sequenc of length n .
so , in thi particular exampl , gen of thi sentenc w1 , w2 , w3 would be the set which consist of all tag sequenc of length three us these three tag and so on , and so on .
okai , so thei're actual <num> to the <num> , that's <num> member of gen in thi case .
okai , so gen is realli the most simpl function you could think of , which simpli just list all possibl tag sequenc for the input sentenc .
now , critic , the size of gen is go to be exponenti in the length of the sentenc .
so in fact , the size of gen for some sentenc x , is go to be the number of possibl tag , so thi is the size of the , of the set t , that's the number of possibl tag rais to the power n , where n is the length of the sentenc .
becaus i have t possibl tag at the first posit , t possibl tag at the second posit , t possibl tag at the third posit , and so on , and so on .
so , if you think about global linear model , as we've present them so far the algorithm we've look at have had to explicitli enumer all of the differ member of gen , and score them under thi function v . f of x , y .
and thi clearli isn't go to be tractabl for thi definit of gen .
onc n get to ani appreci length , the size of the set will simpli be too larg for us to brute forc numer all of the differ element of thi set gen .
what we'll see , though , is that under particular definit of our function f , thing work out .
we can actual appli global linear model in a , in an effici wai .
and thi is what i'm go to talk about next .
so our goal now is to defin a function f .
that take a sentenc .
for exampl , the dog bark , in conjunct with a tag sequenc .
for exampl , dnv and return some futur ventur .
and so to do thi we're actual go to us a lot of the machineri that we develop for log linear tagger a littl earlier in thi cours .
when we us directli the concept we saw there so the first critic concept wa the idea of a histori .
which basic captur all of the context .
which is in plai when we're , when we're make a tag decis .
so as we saw a few lectur ago , in lock linear model a histori is a <num> tupl .
consist of pair of tag , t minu <num> and t minu <num> .
for exampl , if we're tag the word base , in thi case we would have t minu <num> t minu <num> is equal to determin follow by a , an adject , these two tag .
we have the entir sentenc .
so , in thi case , we have thi entir sentenc here .
it's the second element to thi tupl .
and then we have to be slightli care to specifi the posit in the sentenc that's be tag .
in thi case , we have the <num> , <num> , <num> , <num> , <num> , 6th word be tag .
thi is word number <num> .
and so that's the fourth element of thi histori .
so when we consid what possibl tag we should choos in thi particular locat , we're basic go to us the histori to encapsul all of the inform which goe into that decis .
now , recal that we've made the assumpt that the histori onli look at the previou two tag .
in the case of lock linear model , that wa motiv by concern about effici .
that allow us to us the viterbi algorithm .
in the decod problem for multimedia tagger .
and we're go to us thi assumpt again and we'll see how again , global linear model it also allow us to appli dynam program algorithm .
so now to go further with thi , again thi is just actual reus complet the technolog we saw from log linear tagger .
we will defin what i'll call local featur .
so rememb a global featur vector , look at an entir x , y pair , where x is for exampl a sentenc sai the dog bark .
and y is some tag sequenc .
so these featur of x is a global and thei look at an entir tax sequenc .
now i'm go to defin local featur , which just look at a histori tag pair .
and so thei'r go to look at a veri small window , in fact thei're go to look at a sequenc of three tag , three consecut tag becaus histori contain two previou tag .
and then we look at thi third tag .
so , i'll us the notat littl g for these local featur .
so , g sub s , is a function that take a histori tag pair as input and return some valu as it output .
veri often in natur languag , these local featur ar binari featur or indic function .
which ask particular question about the histori in conjunct with the tag be propos .
so here ar some exampl we saw actual in the log linear tag lectur .
g <num> for exampl , might be a featur which is on if the current word w i be tag is the word base .
and the tag be propos is v b .
g <num> might be a function which is <num> if the current word end in ing and the tag be propos in vbg .
we can also have contextu featur , which look at the tag be propos and these previou two tag in the histori .
so here's a featur which is on if the trigram of tag , the previou <num> and the tag be propos , ar the sequenc determin , adject , verb and so on .
and as we saw in the lectur on local linear model on great thing about .
these , kind of featur vector definit .
is that you can essenti ask ani question about the histori in conjunct with the tag be propos , that might be us in , in disambigu the tag at the current posit .
so now , if we think about an entir xy pair .
okai ?
so i'm just again , emphas that our goal .
is to defin a featur vector represent of an entir input x with an entir output y .
so i sort of have x togeth with y here .
i have a sentenc with a particular tag sequenc .
if there ar , there ar n word in that sentenc , then there ar n histori tag pair between that x , y pair .
so thi particular sentenc ha on , two , three , four , five , six word .
for conveni , i'll just number them .
and we have six histori tag pair for thi particular exampl .
so the first histori tag pair ha the tag decis nnp , becaus that's the tag for the veri first word .
and the histori ha the previou two tag as the start symbol , the star symbol .
we have the entir sentenc , and we have posit on .
the second histori tag pair ha rb , becaus that's the tag of the second word , the entir sentenc , posit two .
and the previou two tag , a star and then an np .
the third tag decis is vb , becaus that's the , the ident of thi third tag here .
and the previou two tag ar nnp and rb .
and so on and so on .
you see , we determin at the fourth posit , adject at the fifth posit , base at the sixth posit .
and each of these case we have a histori where the sentenc , of cours , is stai constant .
but we have the posit be indic of the particular tag and we also have the previou <num> tag at each of these posit , which obvious ha chang as we modifi thi index , thi posit of the sentenc .
now each of these decis , each of these posit .
is go to have a local featur vector g .
so i'm go to have a featur vector thi first decis , which consist of the histori in conjunct with the tag nnp .
where the histori is is thi particular histori here .
thi featur vector ha g of the second histori in conjunct with rb and so on .
and so there ar go to be six local featur vector correspond to the six differ histori tag pair in thi particular syntax sentenc .
so , let me now show how we can leverag these local featur vector in defin a global featur vector that map an entir tag sequenc in conjunct with a sentenc to a featur vector .
so we're now go to see how to construct a global featur vector f , which map an entir sentenc pair with an entir tag sequenc to a vector through these local featur vector g .
and the wai we do it is simpli to sum up these local featur vector at the differ posit in the sentenc .
so , here i have a sum of i equal <num> to n .
n is the length of the input sentenc , the length of the tag sequenc .
i assum that h sub i is the ith histori .
so , the histori of the ith posit , where i equal <num> , <num> , <num> , <num> , <num> , and <num> .
and ti is the ith tag .
and i simpli sum these featur vector .
so , the global featur vector here is equal to a sum of local featur vector .
so for exampl , we take each of these histori tag pair , so g of h1 on t1 .
let's sai thi ha a featur vector equal to thi .
and then thi on might have <num> , <num> , <num> , <num> , <num> .
and then we have <num> , <num> , <num> , <num> .
<num> , <num> , <num> , <num> .
<num> , <num> , <num> , <num> .
<num> , <num> , <num> , <num> .
then f , for thi particular word sequenc pair with thi particular tag sequenc , would be the sum of these featur vector .
so in the first posit , i'd sum <num> , <num> , <num> , <num> .
i have <num> here .
second posit , i have <num> .
the third posit i have <num> , <num> .
the final posit , i have <num> , <num> , <num> , <num> .
so the global featur vector is , is form through a sum of these local featur vector .
typic , as we've seen in mani exampl , in log linear tagger in thi class and also in the featur i gave you earlier in thi lectur .
typic , the local featur ar indi , indic function .
thei ar <num> , depend on whether some question about the histori in conjunct with the tag is true or fals .
so , for exampl , g101 might be an indic function , which is <num> , onli if the current word , wi , end in ing and the tag be propos if vbg .
what then happen is that these global featur becom count becaus thei ar sum of these local featur .
so the 101th compon of our global featur vector , rememb we have f is equal to sum , i equal on to n , g , h , i , t , i .
so it's go to be a sum of these g featur appli to everi possibl posit in the tag sequenc .
in thi particular case , f101 is go to be the number of time a word end in ing is tag as vbg in thi entir sequenc .
so while the local featur vector ar indic function , the global featur vector ar actual count of differ occurr within the tag sequenc .
so now when we put thing togeth , we'll see that thi wai of defin the global featur in a global linear model lead to a veri conveni properti , which is that in spite of the size of gen , rememb thi is go to be exponenti in n , where n is the length of the sentenc .
in spite of the size of thi , we ar go to be abl to effici calcul thi function f that take a word sequenc , a sentenc as input and prove a tag sequenc at it output .
so how doe that work ?
so rememb , through a global linear model , thi function f is go to be the highest score tag sequenc .
so here i have an arg max of all possibl tag sequenc in gen .
an exponenti of text sequenc .
the score within the arg max is v . f , where f is thi global featur vector definit that take an entir sentenc pair with an entir tag sequenc and return some featur vector , for exampl , <num> , <num> , <num> , <num> .
someth like thi .
now , if we substitut in the definit i show you befor , thi f is go to be a sum from i equal <num> to n of g , of hi , comma ti , where hi is the ith histori in thi tag sequenc and ti is the ith tag in that tag sequenc .
so thi is , again , just substitut in thi definit for f , where we take the sum of these local featur vector .
now , of cours , i can rewrit thi slightli , take the v insid the sum .
so the inner product be between v and the sum of these local featur vector is equal to the sum of inner product between v and g , just through some fairli straight forward result from linear algebra .
and so , thi term here can be interpret as the score for hi in conjunct with ti .
so thi is basic a score indic how plausibl thi part , particular histori tag pair is .
so thi is the final problem we end up with .
we end up with thi problem of find the highest score tag sequenc , where tag sequenc is score by a sum of these local score for each histori tag pair in the input .
onc we've gone to that point , it shouldn't surpris you that dynam program can be us to find the arg max .
in fact , it's a veri simpl task to modifi the viterbi algorithm , which we origin saw for h and m's .
and then we saw for log linear model .
it's veri easi to modifi thi to find the highest score tag sequenc under these score .
so , now we've seen that if we defin , our global featur f , as a sum of local featur g , we can effici calcul thi function , from sentenc to tag sequenc , us the viterbi algorithm .
that is critic for two reason .
on is , if we want to appli our global li , linear model to a new test sentenc , we clearli need to calcul thi function f .
and we need to be abl to calcul it effici .
secondli , if we think about the perceptron algorithm , the main comput step in the perceptron algorithm is , at each train exampl to find the highest score tax sequenc , under the current paramet set .
and so it is go to repeatedli , calcul thi argmax .
and again , it's therefor critic that we can do thi effici .
so , let me just talk briefli how the perceptron work , for these type of model .
so again here is a recap of the perceptron algorithm .
we have input xi , yi if i equal <num> to n .
in the tag case these will be sentenc pair with entir tag sequenc .
we set v equal to <num> .
we defin fx to be thi argmax , where we , iter over all member of gen , score each member , find the highest score structur .
and then in the actual paramet estim step , we take big t pass over the data .
we go over the exampl on by on , so we have i equal <num> to n .
and thi is the critic step .
so at each point , we find the highest score tax sequenc under the current model .
if it's not correct we make some updat to the paramet .
so in the , in the tag case , thi algorithm look like the follow .
we have a train set , set of train exampl , consist of word sequenc pair with tag sequenc , us n sub i to be the length of the ith exampl , here , or the ith sentenc and the ith tag sequenc .
we initi set all the paramet v be equal to <num> .
and again i'm go to take big t iter of the train set .
i visit the exampl on by on so i take i equal <num> to n .
i'm iter over at the train sampl .
the first thing i do is , i calcul the highest score tag sequenc , under the score v . f .
and we're go to assum , that f , is equal to , a sum over these local , featur vector .
and that mean that critic , we can calcul thi high score text sequenc under the current paramet us dynam program , us the viterbi algorithm .
if the output from our model z is not equal to our target output t , so if thei ar yeah , i want a more tag error and the sequenc we propos , then we do thi veri simpl updat again .
we sai , v equal v , plu f , minu f .
where thi f , look at the correct tag sequenc , and thi f look at the incorrect tag sequenc .
so again at a high level , the critic thing is we've leverag , thi kind of definit of f , that it's a sum of local featur vector .
in a sens , that we can now us dynam program to find the highest score tag sequenc for each train exampl , under the current paramet .
and then we us the standard perceptron updat .
so here's some result from the perceptron , on a coupl of data set compar it to a log linear tagger .
thi is first part of speech tag data .
and you can see the error rate of the percept is veri competit actual slightli lower than the result for log linear tagger .
and secondli we have , the problem of noun phrase chunk .
so , thi is actual the problem of take a sentenc as input and recov noun phrase boundari , where i onli look at noun phrase which ar non recurs .
so , these veri low level noun phrase , like the dog or cat .
so thi is a segment problem , the same wai the ident recognit is a segment problem .
and thei be treat as a tag problem , the same wai that mayb an ident recognit could be treat as a tag problem .
so in noun phrase chunk again we see similar level of accuraci .
but perhap slightli higher accuraci for the perceptron .
so the perceptron is certainli a competit algorithm , in thi particular domain in tag problem .
it offer a veri interest altern to log linear tagger .
it's , it's , it's a simpl algorithm , and it's realli rather simpl to implement .
and perhap most importantli , thi wai of think about global linear model , where gen is exponenti in scienc , but f is defin through some sum of local featur vector .
we'll see that thi is a veri power idea .
tag wa on of the first problem which wa address us thi kind of techniqu .
but in the next segment of thi class , we'll look at depend pars , which is anoth applic of thi techniqu , and where thi idea of decompos f , as the sum of g , thi ha realli been emploi veri effect .
so in thi next segment of the class .
we're go to look at global linear model for third problem , mainli depend pars .
we'll see that we can make us of mani of the idea we saw in the last segment , when we consid the perceptron for tag problem .
but we'll see in thi segment that we develop a veri differ approach to pars , from the pcfg and the lexic pcfg that we saw earlier in the cours .
depend pars model ar now actual veri wide us both in research and also in real world applic .
so , in thi segment , we'll first defin the depend pars problem .
we'll then talk about how global linear model can be appli to depend pars .
and then final , we'll talk about some result from work by ryan mcdonald and other , who were amongst the first to appli , actual thei were the first to appli , global linear model to the depend pars problem .
so , a depend pars look like the follow .
depend structur ar essenti an altern form of syntact represent , from the treelik structur we saw earlier in the cours .
so rememb , earlier in the cours we saw these kind of tree structur , which were deriv from , for exampl , a context free grammar .
depend structur ar anoth wai of repres syntact inform .
what we have here , is a set of depend arc .
so each depend arc is a direct arc between two symbol , which signifi that there's a grammat relat between those two symbol , or rather those two wor , word .
so root is a special symbol in thi particular structur .
i have a direct arc from root to the verb saw , becaus that in a sens is the root of the sentenc .
it's the main word in the sentenc .
i have a direct arc from saw to john , becaus john is the subject of saw , and that's what thi direct arc signifi .
i have a direct arc from saw to movi , becaus movi is the object of saw .
and final i have a direct arc from movi to a , becaus a is a modifi , actual a determin , modifi to the word movi .
now , in the simplest case , these depend structur ar unlabel , which mean thei look just like the structur i've shown you here .
it easi enough to go beyond thi , to have lebel on these arc and natur that's a veri natur thing to do .
so in label structur , you might label thi as be a subject , thi arc as be an object art , arc .
mayb thi would be the main verb arc .
and thi would be , sai a , determin arc , dt .
so , throughout thi lectur , i'll actual stick with the unlabel case where these arc don't have label .
but it's veri easi to extend the techniqu i'll describ to the label case .
and in a wai , the label case is a littl more intuit .
you can see exactli how recov these kind of depend style annot , certainli carri a lot of us syntact inform about the sentenc be analyz .
and is , in some sens , quit equival to the pars tree that we saw earlier in thi class .
in fact , a littl bit later i'll talk about how to directli deriv these depend structur from pars tree of the form we've seen earlier .
so , it'll be us to develop a littl bit of notat , for depend structur .
so each depend is go to be a pair h , m , where h is the index of a head word and m is the index of a modifi word .
so when i have , ever i have on of these direct arrow , i have a head , for exampl , saw , and a modifi , for exampl , john .
so let's number these word .
we alwai number the root as zero .
so there ar four direct arc in thi depend structur , and that mean there ar actual four depend .
so i have <num> , <num> , that's thi depend here .
i have <num> , <num> , thi depend here .
i have <num> , <num> , thi depend here .
and i have <num> , <num> , thi depend here .
so throughout thi lectur , we'll repres these depend structur as set of depend , where depend is an h , m pair .
h is the head .
m is the modifi .
so the depend pars problem which we ar consid in thi segment , is to take a sentenc as input , and produc on of these depend structur as the output .
and again , i'm go to focu on the case where these arc ar unlabel .
that's the simpler case , but it's easi enough to extend the approach i'll describ to the case where to the case where these arc actual have label .
so let's now talk about what make a depend structur well form .
that is how do we defin the set of possibl depend structur for a given sentenc .
and we're go to focu on two constraint which will be import .
so the first constraint is that these direct ark form a direct tree with thi root symbol at the root of the tree .
so thi basic mean that if i pick ani word in the sentenc , for exampl movi , there's go to be a direct path from the root to that word .
so in thi case the path is first thi arc root saw .
and then thi arc sort of move .
so everi on of these word is connect to the root through a direct path .
and thi structur in addit form a tree which mean for exampl there ar no cycl in these path .
we have no cycl like thi .
the second constrain which we'll come back to in some detail in a second is that there ar no cross depend in these structur .
so cross depend would look like the follow .
i have a structur like thi , sai thi is the root , and these ar the four word in the sentenc .
you can see that these two depend actual cross .
and so in thi definit i'm go to us in thi lectur , we're go to rule out these kind of structur .
i'll talk more about thi in a second .
so if we come back to thi simpl exampl , john saw mari .
there ar actual on , two , three , four , five possibl depend structur in thi case .
five structur that satisfi those two constraint that i just show you .
the correct on is here but of cours there ar these four incorrect structur for thi particular sentenc .
so the pars problem is essenti go to correspond to search though these differ possibl structur .
in find the on which is most like or more , most plausibl under some model .
let me just talk a littl bit more about these cross depend .
so what i've shown you here is a depend paus structur which doe have on of these cross depend .
so notic here we have a cross depend .
structur which allow these cross depend ar often refer to as non project structur .
and thei an , can actual be us in some scenario and depend on the languag and the construct involv you can actual see these kind of non project structur .
here's an exampl from english , where arguabl the correct depend paus certainli doe have a cross depend .
for the purpos of thi lectur , we're go to assum that these structur ar not allow so thei're not on the space of possibl depend structur for a particular sentenc .
and that's a pretti good approxim for a languag like english .
right a the veri end of the lectur , i'll talk a littl bit more about extens .
to these model which do allow structur of thi case .
but for now assum these structur ar so there's been consider interest over the last few year , year in depend pars , you'll you'll see later in thi lectur it , it ha certain advantag in term of effici and simplic .
but let me talk a littl bit about resourc .
so , in <num> , a confer call conll had , what wa call a share task .
basic , a friendli competit between differ group , where thei releas depend pars dataset for actual , <num> languag .
so arab , chines , czech , danish , dutch , german , japanes , portugues , sloven , spanish , swedish , turkish .
and <num> differ group develop depend pars system and compar the result on these data set .
a veri similar setup wa seen in conll <num> .
the protocol describ in todai's lectur is larg base on a phd thesi by ryan mcdonald .
which introduc a global linear model for depend polici and show that these model were veri success .
we'll be treat thi problem as a supervis learn task .
so we'll assum we have train exampl consist of sentenc .
pair with depend structur .
so thi is what each train sampl would look like .
and so there's a question of where do we get these resourc from ?
where do we get these depend bank from ?
so there's a coupl of wai that these dataset aris .
for some languag , a famou exampl be czech , peopl have actual hand construct depend bank .
so thei have annot larg quantiti of data , again mayb thousand or ten of thousand of sentenc .
where each sentenc is annot with it's outlin depend structur .
so you can think of thi as an ana , analog to a tree bank .
we had seen for exampl , the penn wall street journal tree bank , where peopl had annot constitu tree .
in some case , for variou languag , rather than annot these kind of constitu tree , thei directli annot these depend structur .
now for other languag where these depend bank ar directli avail , we mai have these constitu , or tree bank , like the penn wall street journal tree bank .
and you hopefulli won't be supris that it is actual fairli straightforward to extract depend structur from tree bank .
the pcfg's that i show you earlier in thi cours open up a direct link between these constitu structur and the depend structur we're consid in thi lectur .
so let me talk a littl bit more about that .
so here is a constitu base tree , someth like the kind of tree you would find in the pen wall street journal treebank .
and i think thi is an exampl that we saw earlier in the core , cours .
and we've lexic thi tree in the wai i describ in the lectur on lexic context free grammar .
so each non termin ha an associ head word .
for exampl , thi np ha presid or thi vp ha wa .
or , sorri thi is miss , thi sh , thi s should also have wa .
thi m p ha she , thi s bar ha that .
so , in the same wai as i show you .
earlier in the cours , when we look at lexic pcfg's , we had these rule that propag head word up through these context free tree .
so , that wa a step , thi lexic step wa us in deriv lexic pcfg's .
but it also open up the chanc of take these tree structur .
and convert them , to depend structur .
so rememb each depend can be repres as a , pair , h , m .
where h is the index of a head word , and m is the index of a modifi word .
and so in thi particular case , thi exampl tree is go to be convert to the follow set of depend .
let me give you an exampl .
so with thi rule as told goe to m p hillari , v p told , we can extract the depend we're told is the is the head , and hilliari is the modifi .
so that give me thi depend here two on .
we have a direct arch from told to hillari .
similarli if we look at thi rule here , we have told as the head and clinton as the modifi .
and so we have an arch two , three .
so of these word on , two , three , four , five , six , seven , i have an arch two , three .
from told to clinton and similarli i have an arch from told to that two four , becaus i have that also as a modifi told .
and we can similarli go through the entir tree pull out all depend .
right at the top of thi tree , we have told as the root of the entir sentenc .
and becaus of that , we have an arch zero two .
root goe to told .
so to summar .
the main point of thi slide , wa that , if we have a tree bank , that is a set of sentenc with annot , context free tree , we can convert thi into a so call depend bank .
and thi step is through lexic .
exactli the , the , the the step of lexic we saw , within the context of lexic pcfg's .
and onc we've done thi , we have train and test data for the depend paus problem .
so on veri interest properti of depend pars , and a compel reason for take depend pars veri serious , is the effici with which we can find these depend pars structur .
so , when we look at probabilist context free grammar .
we saw , thi dynam program algorithm , the cky algorithm .
and it had a runtim which wa cubic in the length of the sentenc and also cubic in the number of non termin in the grammar .
so n might , for exampl , be <num> or <num> or <num> , the number of non termin might be depend on how you defin it , <num> or <num> , or <num> .
again , some reason larg number .
and thi is quit challeng .
thi is actual a pretti larg number .
and you can see that thi g cube term can certainli lead to problem in term of effici .
lexic pcfg pars is at least under the method we saw .
and to the 5th where n is the length of the sentenc .
and again cubic in the number of non termin in the grammer .
so thi is realli get veri expens .
algorithm were unlabel depend pars , the gain ar gener base on dynam program .
we're not go to see these in detail becaus that thei're fairli complex and it will take some time to go over them .
i post the refer .
but there ar some rather beauti algorithm for dynam program .
over depend , a , pose structur .
in particular due to jason eisner , i'll post the paper .
remark these algorithm ar cubic .
in , in the length of the sentenc .
but have no depend on the size of the grammar .
in fact , in some sens depend grammar don't realli have non termin , so thei don't realli have these g term .
so there might be a small constant in front of thi n cube , to mayb a factor of eight , or someth like thi .
but a constant that is wai , wai smaller than g cube .
and that actual mean that these depend pars algorithm , ar , in practic extrem effici .
and that is a major .
reason for like i said , take the depend pars veri serious .
so it ha two properti which i think ar veri benefici .
so thei're veri effici pars .
secondli thei ar veri us represent .
so , if we can recov these kind of depend structur , thei're us in mani applic in natur imag process .
so , now let's describ how global linear model , glm can be appli to depend pars .
so , throughout thi section i'll take x to be a sentenc , so x is go to be a sequenc of word x1 , x2 , up to x sub n .
and realli , to defin a global linear model , we need to give two definit .
so firstli , we have to defin gen , that take a sentenc as input and return a set of candid depend structur as it output .
and secondli , we have to defin f .
which is a function that take a sentenc togeth with a depend structur and return a featur vector represent .
so gen is go to be veri simpl .
it is simpli defin as the set of all depend structur for the sentenc x .
so for exampl , for john saw mari .
gen , with the sentenc , will return all possibl depend structur over these three word .
critic the size of gen is exponenti in n where n is the length of the sentenc be paus .
so we're again in a situat where gen is go to be a veri larg set of possibl structur , at least for appreci n sentenc of sai <num> , or <num> , or <num> word .
thi is go to be an extrem larg set .
so f of cours is go to be a function that take , as input , a sentenc in conjunct with a particular depend structur and return a featur vector .
so it might return some vector like thi which is the represent us by the learn algorithm us by , for exampl , the percept algorithm when set the , the paramet of the model .
and we'll see that there is a wai of defin f that get us around thi problem of the exponenti size of gen .
becaus we can again , us dynam program for search for the , most like tree under thi model .
so let me now talk about how we can defin f .
so , the kei idea is go to be , again , to defin f as a sum of local featur vector .
so rememb in the last segment on the percept algorithm and the global linear model for tag , we saw a veri similar thing , where f wa defin as a sum of local featur x is g .
in the depend paus case , we're go to have on local featur vector for each depend in the structur y .
so y is a structur and thi is go to be map to a set of depend .
where again , each depend ha a head and a modifi .
the head is an index of a word , where zero is the root word .
and the modifi is also an index of a word in a sentenc .
the local featur vector can look at ani inform in the sentenc and can look the ident of thi head in the modifi word .
so let me illustr thi with an exampl .
so here we have a sentenc x with a depend structur .
so the sentenc x is john saw a movi .
so we have x1 equal john , x2 is equal to saw , x3 is equal to a , x4 is equal to movi .
and the depend structur y is basic equal to a set of depend where we have <num> , <num> , <num> , <num> , <num> , <num> , and <num> , <num> .
so , it would be us to , to number these word <num> , <num> , <num> , <num> , <num> .
so , for exampl , the depend <num> , <num> correspond to thi direct arc between saw and movi .
now we're go to have some featur vector map that take x in conjunct with i and return some featur vector .
and through the definit on the previou slide , thi wa defin as sum of all depend in y of g of x , h , m .
so for thi particular exampl , thi is go to be g of x , <num> , <num> plu g of x , <num> , <num> plu g of x , <num> , <num> plu g of x , <num> , <num> .
so again where plai thi trick where we defin a global featur vector as a sum of local featur vector .
where now we have a local featur vector for each of the direct arc and it depend so we might for exampl have featur vector like the follow .
so mayb of , of our four featur vector look like the follow .
and in thi case , the sum would be , on , two , three in the first posit .
so <num>' on and <num> zero .
just <num> in the second posit , becaus i have on <num> and 0s elsewher .
in the third posit , i would have <num> , becaus again , i have a singl <num> in the third posit .
and in the final posit i also have <num> .
so that's go to be the final global featur vector .
it's the sum of these local featur vector .
again what we're see here is someth which is veri common in these kind of model .
which is that these local featur vector have on or zero .
these ar go to be featur .
we'll talk about thi more in a second .
these can be featur which ask variou question about these , differ depend .
and then the global featur vector is go to be a count .
for exampl , there ar three depend within thi structur , such that thi first question wa true , i . e the first question had a valu of <num> .
so let me give you an exampl of how we might defin these , featur vector .
so , mayb the first featur , so thi is the first compon .
so we're go to have g , x , h , m is go to be a vector compos of g1 , x , h , m , g2 , x , h , m , right the wai up to g sub d of x , h , m .
where d is the dimension of the featur , featur vector .
as the number of featur .
so each of these differ function g1 , g2 , up to gd , will look at an x , h , m tripl and return some valu .
so , on such function might be the follow .
so thi is <num> , if x sub h is equal to saw and x sub m is equal to movi .
okai , so in particular we would have g1 of john saw a movi .
and if we have <num> , <num> as the depend .
so that's thi particular depend here .
thi is go to be on in thi case .
okai ?
so thi particular function , thi particular featur look at the two word , that stand in some depend relationship between x , h and x , m .
and return <num> if thei're a particular word , <num> otherwis .
we would typic defin a featur like thi , for everi possibl pair of word in the vocabulari .
okai , so again , as we've seen often with these log linear model .
we might have a veri veri larg number of possibl featur .
on for each pair of word .
these featur ar veri us .
becaus certain pair of word ar veri like to stand in a depend relationship .
other pair of word ar much less like to stand in a , in a depend relat , a depend relationship .
let me give you some other exampl of featur .
it's veri common , actual , to not just includ the word in the input x .
but also to includ part of speech tag .
becaus those can provid us inform .
and so now if we have g2 of h x m , here's anoth featur .
now think of x as includ both the word and also the part of speech tag in the input .
so you could sai someth like on if the part of speech tag for h is equal to vbd and the pars speech tag for m is equal to nn , is <num> , otherwis .
so , these kind of featur would be introduc for everi possibl pair of part of speech tag in the underli part of speech tag set .
and these kind of featur ar veri us becaus certain part of speech tag ar much more like to be seen in depend relationship than other on .
anoth type of featur which we might us could look at the distanc between the two word .
okai , so we could sai thi is on if the distanc between h and m .
so , the differ , thi is the absolut valu of the differ is equal to <num> and <num> otherwis .
so thi featur return the valu <num> if the two word involv and the depend ar veri far from each other .
and these kind of featur can be us becaus there certainli is a statist tendenc in languag for these depend to be between close by word .
that's actual an empir observ we're told across mani languag .
you do see occasion depend which ar veri long , but mani of them ar veri short depend where we just have an adjac word or we have .
depend involv word within a two or three word window .
now what you'll hopefulli see from these exampl , is we have consider freedom in defin these featur .
which , look at the sentenc , the index of the head word and the index of the modifi word , we can incorpor all kind of inform .
about the word x , h , and x , m .
mayb the part of speech tag for those word .
mayb the distanc between those two word .
mayb surround inform so we can look at the part of speech tag or word of the left or right of the two word that stand in teh depend relationship .
so now let's describ how these featur factor definit can be us with , within the context of global linear model for depend pars .
so , recal that to run the perceptron algorithm , we need to be abl to calcul thi function , capit f of x , x is a sentenc .
for exampl , john saw mari and , thi function is go to take a sentenc and map it to some depend structur , for exampl thi structur here , john saw mari .
and it's defin in the usual wai in local linear , in global linear model .
gen of x is defin as the set of all possibl depend structur for thi input sentenc , and i'm go to take a max of all possibl structur .
and we're go to score each of those structur us w . f , where w is a paramet vector , in d dimension space , and f is a featur vector .
so , again , as we saw in tag , it's critic that we can comput thi function f of x , effici , for a coupl of reason .
on is , when we get a new test exampl , we want to be abl to pars it , we want to be abl to appli thi function , okai .
but the second reason is that to run the perceptron algorithm that learn these wai to w from exampl .
we need to repeatedli calcul thi function on the train exampl .
rememb , the perceptron typic calcul the output on a particular trend exampl , under the current paramet .
and then it doe a veri simpl updat to those paramet if we make some error .
so , f of x is crucial within , within the perceptron algorithm .
and again , let's just emphas thi set gen , is typic veri larg .
so , it's exponenti in n , where n is the length of the sentenc .
as i've just shown you , we have defin thi function capit f , sorri lower case f .
the featur vector function through a sum of local featur vector .
each of these featur vector take a sentenc and look at just a singl depend within the structur y and return a featur vector .
so , thi problem here is reduc to the problem of find the highest score structur in gen where the score of ani structur is through thi sum of term , on for each of the depend .
now i'm not go to explain the algorithm for thi .
i'll try to post a paper that specifi the algorithm .
but , critic we can make us of dynam program for thi problem , and it actual run in cubic time in the length of the sentenc .
and intuit , that is becaus we've decompos the score from an entir structur into a sum of local school across just depend within the structur .
and that decomposit allow us to us dynam program algorithm that ar alreadi quit similar to the algorithm that we saw for pcfg , so thei're similar to the cky algorithm .
but which leverag depend structur in realli rather beauti wai .
and again jason eisner ha some wonder paper on these algorithm .
so to summar , we've defin global featur vector through local featur vector , that allow us to find the highest score depend structur under the model veri effici us dynam program .
we can us that dynam program method both within the perceptron algorithm when we train these paramet w , and also for decod new test sentenc .
to finish up thi segment let me just talk about a littl bit about the kind of featur that mcdonald actual look like .
and thei're , thei're veri similar to the the featur i show you .
so rememb , the featur vector's go to take a sentenc , a head word , and a modifi word .
so we defin the w to be the i'th word in the sentenc , and t of i to be i'th tag .
so the part of speech tag for the i'th word .
so again , we're go to assum that as input , we don't just have the sentenc .
we also have the part of speech tag for that sentenc becaus the part of speech tag can give us valuabl inform .
there ar some veri simpl unigram featur , which just look at the ident of the head word , or the identifi , ident of the modifi word .
while the ident of the head tag or the ident of the modifi , ident of the modifi tag .
more interestingli , there ar bigram featur which ar rather similar to the featur i show you on the previou slide .
so , we can look at the ident of the <num> tupl , wh , wm , t sub h , t sub m .
so , we could have featur which ar true for a particular combin , so mayb saw , john , vbd and nnp .
so we have a featur which is on if and onli if we saw a depend between a word saw tag as a vbd and a word john tag as an nnp .
and we can also defin featur which look at variou subset of thi <num> tupl .
for exampl , we might just look at the two word involv .
we might just look at saw and john .
we might just look at the two part of speech involv .
or we might look at just the fact that we have john tag as an nnp modifi some vbd , and so on .
mcdonald also us contextu featur for exampl , you might look at not just the tag , if we have a particular depend .
so thi is word h and word m .
you might be interest not at just the part speech tag for thi nth word and the part speech tag for thi hth word .
you might also look at part speech tag to the immedi left and right of thi head and modifi word .
so , for exampl we might look at the fortupl , th , th plu on .
so that's thi part of speech , thi part of speech and then tm minu on tm , so thi part of speech and thi part of speech .
and there ar sever kind of featur you can defin here which look at the part of speech of the , the modifi word , and the head word and also the part of speech of the previou word or the next word for each of these case .
so that clearli give us a richer context in which to consid a particular depend .
final an import classic featur ar what , ar what ar call in between featur .
so empir we ar , ar statist speak some part of speech , for exampl a verb caus a depend to be rel unlik .
so if a verb is seen between these two word , that diminish the probabl of see a depend .
so , it seem empir if you look at least english , the chanc of it see a depend cross certain part of speech such as a verb is , is much less than a chanc of cross of other part of speech .
and so mcdonald introduc featur which would track the tag of the head , tag of the modifi , and also would check whether a particular tag , for exampl vbd , wa seen between these two word .
so thi turn out to be anoth us featur type .
so , let me finish thi segment with some result for global linear model , taken from mcdonald's implement of these type of model .
so , what i'm show here , is a tabl with depend accuraci .
so , thi is the proport , of depend that we propos that ar actual correct .
thi is actual an evalu measur we saw wai back in the cours , when we look at lexic context free grammar .
we talk about evalu the a , a , accuraci of a parser by look at the number of depend it , it got correct .
thi model is a lexic pcfg , that i develop for my phd thesi .
and of cours , onc we have a pars tree from a lexic pcfg , we can extract the depend from it , us the method i describ earlier in thi segment .
and also the method i describ in the section on lexic pcfg .
and thi model score just over <num> in term of depend , accuraci .
here ar a coupl of result from mcdonald from <num> .
the simpl model that i just describ , get close to <num> accuraci .
and for a slightli richer model , which i'll talk about in a second , call a second order depend model , we can get about <num> and a half percent accuraci .
so , thi is pretti impress .
the depend pars model actual ar extrem competit with lexic pcfg , in term of recov these depend relat .
and there ar veri , some , there ar some veri signific advantag in depend pars approach .
thei ar quit simpl .
it realli is just a matter of defin these featur that i just show you , implement dynam program .
and then final implement the percept algorithm , for exampl , for paramet estim .
thei're not onli simpl , but thei're veri effici , as i'd said befor .
thei ar cubic time in the length of a sentenc .
and rememb that for pcfg , for exampl , the run time wa cubic in the length of the sentenc , and also cubic in the number of symbol .
so , g is the number of non termin .
and thi term add a lot , thi could easili be <num> cube or <num> cube , <num> cube , easili into the thousand .
so , depend parser can be significantli more effici than a full p , pcfg parser .
let me just talk a littl bit about extens to what i've just describ to you .
there's been a lot of interest also in non project depend structur .
so , actual defin gen to includ these structur with the cross depend .
and for some languag , for exampl czech , these kind of cross depend , depend ar seen frequent , and so it's import to allow them .
and that lead to a whole new research topic , which is how do we find the highest score non project structur .
it turn out that dynam program doesn't realli well , actual , dynam program can be emploi in some case .
but there ar also other interest algorithm base on span tree algorithm , for exampl .
anoth veri import extens is to look at second order and third order depend pars .
what i mean by that is the follow .
so , the featur vector represent i have shown you , have look at a singl depend at a time .
so , it look at the singl depend in isol from all of the other depend in the sentenc .
so , we just look at thi on depend , and that's reflect becaus we have a local featur vector that look at the sentenc .
and just the ident of the head and the modifi in that particular depend .
in higher order depend pars , for exampl second order depend pars , we start to look at depend togeth .
so , we might actual have a featur , which is sensit to the fact that i have these two depend togeth in the pars tree , or mayb even these three depend togeth in the pars tree .
so , we can now have local featur vector .
then mayb don't just look at the head and the modifi , but might also look at the grandpar .
so , mayb thi is h , m , and thi is h prime .
and might also look at some other modifi , mayb thi modifi here .
so , it's easi enough , actual , to extend the approach i've describ , to allow local featur vector that look at larger piec of depend structur .
and thi involv some complic to the depend , the dynam , dynam program algorithm that ar us , but these model still remain tractabl .
with these kind of modif , the state of the art is around <num> to <num> , at least on english wall street journal data set in term of recov depend annot .
and thi realli is a pretti impress level of accuraci .
so final to summar what we've seen in thi week's lectur , global linear model requir definit of thi function gen that enumer a set of candid for ani input , and thi function f that map ani candid structur to a featur vector .
the kei idea is in tag and depend pars model that we have describ thi week ar as follow .
firstli gen ha been defin to be the set of all possibl structur .
for exampl all possibl tag sequenc or all possibl depend pars for a particular input .
and that mean the gen grow exponenti quickli with respect to the size of the input .
so , gen is exponenti and in size .
it grow exponenti fast , with respect to the length of sentenc .
to get around thi , we've defin thi global featur back to f , through a sum of local featur vector .
so f take an input x and some structur y as it's input .
it return a v2 vector .
thi ha been defin as a sum of a local featur vector , which look at sub part of the structur , y .
in the tag case the , the local fea , featur of axi g look at trigram of text .
in the depend pars case , thei look at singl depend within the pars .
given thi definit of f , we can us dynam program to find the highest score structur under the model .
and that's critic becaus we need to appli the model to new test exampl and moreov the percept algorithm , which we us for paramet estim requir us to repeatedli decod our train sampl .
to repeatedli find our highest score structur under the model for each train sampl in turn .
final , it's worth note , we've seen the perceptron algorithm for paramet estim , but there ar certainli other option .
in term of estim the paramet of these global linear model .
on veri famou exampl is condit random field , and these ar essenti giant log linear model where we can us gradient ascent for paramet estim .
anoth option that ha been wide consid is so call larg margin method relat to support vector machin .
and so i cover the perceptron , as it's rel simpl and it introduc mani of the main idea in global linear model .
but there ar certainli comput paramet estim method , which have their own advantag .
okai .
so , the first topic we're go to cover in thi cours is the problem of languag model .
languag model is on of the oldest problem studi in statist natur languag process .
it's a veri basic problem , and it's a veri us problem .
it languag model ar us in a veri wide rang of natur languag applic .
so , we're go to cover a number of thing .
i'm firstli go to defin the basic problem .
we'll then talk about a veri import class of languag model .
these ar call the trigram languag model .
these ar extrem wide us .
we'll talk about how to evalu differ languag model , how to measur the effect of differ languag model .
and then final , we'll talk about a coupl of estim techniqu for languag model .
firstli , someth call linear interpol , and secondli , someth call discount method .
both of these method ar wide us within languag model and as we will see later in the class , thei're also us in mani other problem in natur languag process .
so , these basic estim techniqu ar wide us in other area .
so , to get us start , here ar a coupl of definit .
we're go to assum that we have some set v .
and thi is a finit set , and thi is go to includ all of the word in our languag of interest .
so , imagin we're construct a languag model for english , for exampl .
we might have a set of the contain word such as the , a , man , telescop , and so on and so on .
and it's not uncommon for thi set to be realli quit larg , it might easili contain thousand or ten of thousand of possibl word in , in the languag .
so , given thi underli set v , i'm go to us v dagger , thi symbol here , to refer to the set of all possibl sentenc or string in thi languag .
and a well form sentenc take the follow form .
it ha zero or more word , where each word is drawn from the set v .
follow by a special symbol , the stop symbol , okai ?
so , the us of thi stop symbol at the end of each sentenc is initi go to look a littl peculiar .
but , we'll see soon why it's veri conveni to includ thi symbol when we start to develop a probabilist model for the languag model problem .
so , just to recap , a sentenc could have ani sequenc of word .
it could be a sentenc that make sens .
for exampl , thi sentenc here , or it might be some sentenc that , that realli , realli doesn't make sens .
we get to have sequenc like the , the , the , stop .
so , ani sequenc of word drawn from thi vocabulari follow by stop .
and we'll also includ a sentenc where we have the stop symbol alon .
thi is the case where the sentenc is basic of zero length , there ar no word befor stop , just to be complet precis .
so , given these definit , we can now defin the languag model problem .
so , i'm go to assum that we have a train sampl of exampl sentenc in the languag we're interest in .
let's just assum that's english for now .
so , for exampl , you might collect all sentenc that you've seen in the new york time over the last <num> year .
or you might collect a veri larg set of exampl sentenc from the world wide web and you can think of mani other exampl .
and thi train sampl can again be quit larg so to be concret , in the mid 90s , for exampl , it wa pretti common to make us of , you know , roughli <num> million word of data in these train sampl .
and by the end of the 90s , it wasn't uncommon to us mayb a billion word .
often , again , chosen from newspap data , for exampl .
and more recent , over the last sever year , peopl have start us web data to construct languag model .
we might even get into a scenario where we have hundr of billion of word , potenti train data .
and the main point here is just that these train sampl can get quit larg .
so , given a train sampl , our task is the follow .
we want to learn a distribut p over sentenc in a languag .
okai .
so , p is , is go to be a function and it satisfi <num> condit .
so firstli , for ani sentenc x , rememb , the dagger is the set of possibl sentenc in the languag for ani sentenc x , we have p of x is greater and equal to <num> .
and secondli , if we sum over all sentenc in the languag , we have someth that sum to the valu <num> , okai ?
so , p is a well form distribut over sentenc in the languag .
so , our task is go to be to take a train sampl .
the exampl sentenc as input and output and function p as the output of thi process .
so , here ar some exampl .
we might , for exampl , assign the probabl <num> to the minu <num> to the sentenc compos just the word , the , follow by stop .
we might assign <num> time <num> to the minu <num> to thi particular sentenc , and so on and so on .
we just assign a probabl to everi sentenc in the languag .
now roughli speak , we would like a good languag model to assign high probabl to sentenc which ar like in english and low probabl to sentenc , which ar unlik in english .
so , for exampl , thi sentenc here is pretti ill form .
you're unlik to see thi as a sentenc and that ha rel low probabl .
so , veri soon , we will start to talk about techniqu that solv precis thi problem , thi problem of take a train sampl input and return a function p as it output .
but the first question to ask realli is , you know , why on earth would we want to do thi ?
at first sight , thi seem like a rather strang problem to be consid .
but actual , there's veri strong motiv for consid it .
so , there ar two reason i'll give for consid the languag model problem .
the first is that languag model ar actual us in a veri wide rang of applic .
so , speech recognit wa realli the first applic of languag model .
and languag model ar critic to modern speech recogn .
other exampl ar optic charact recognit , handwrit recognit , anoth exampl we'll see later in the cours is machin translat .
so , in short , languag model ar actual us in mani applic .
i'll come back to thi point in more detail in a second but the other reason for studi languag model is that the estim techniqu that we develop , later in thi lectur , will be veri us for other problem in nlp .
so , for exampl , we'll see problem such as part of speech tag , and natur languag paus , and machin translat where the estim techniqu describ ar appli veri directli .
so , let me go back to thi first issu and describ in a littl more detail how languag model ar relev to the problem of speech recognit .
and thi will be a fairli high level sketch , but hopefulli you'll get the basic idea .
so , the basic problem in speech recognit is as follow .
as input , we have some acoust record .
so , thi is actual somebodi speak .
on on axi we have time .
on the other axi , we have the amplitud or energi .
and in a speech recogn , we typic go through some pre process step , someth like the follow .
we would typic split the sequenc into rel short time period .
these ar often call frame .
each frame might be , for exampl , around <num> millisecond long .
and then for each frame , we might perform some kind of fourier analysi , where we get energi of differ frequenc within that frame .
so , the detail aren't too import , but thi is the kind of pre process we might carri out .
have perform thi pre process , the task is then to map thi acoust input to the word which were actual spoken .
so , let's sai , for the sake of exampl recogn speech wa what wa actual spoken in thi case .
the speech recogn take an acoust , sequenc input , and output a sentenc or sequenc of word as it output .
now in practic , there ar often mani possibl altern sentenc which could have been spoken which ar quit confus .
so , anoth exampl sentenc might be , wreck a nice beach .
it's a famou exampl from the speech process commun .
and the issu here is that these two sentenc ar quit similar from an acoust point of view .
and if you simpli look at a measur of how compat thi sentenc is with thi acoust versu thi sentenc , it's quit possibl you might confus these two sentenc , and thi is just on exampl sentenc .
in practic , there ar mani , mani , mani other possibl which might have a reason degre of , of fit with the acoust input and might be quit confus with the true sentenc recogn speech in thi case .
now , if we have a languag model , we can actual evalu a probabl , p , of each of these sentenc .
and a languag model add some veri us inform to thi whole process , which is the fact that thi sentenc , recogn speech , is probabl more probabl than the sentenc wreck a nice beach .
so the , the , the languag model is go to provid us addit inform in term of the likelihood or probabl of differ sentenc in a languag , and again , there ar go to be mani other down here with which some of them which might look acoust like a , like a veri good match to the input , but which ar complet unlik as sentenc in english .
so , in practic , modern speech recogn us two sourc of inform .
firstli , thei have some wai of evalu how well each of these sentenc match the input from an acoust point of view .
but secondli , thei also have a languag model which give you a , essenti a prior probabl over the differ sentenc in the languag .
it can be veri us in get rid of these kind of confus .
okai , final , let's talk about a kind of veri naiv method for languag model , just to get us off the ground as a though experi .
so , sai , we have n train sentenc .
mayb a few million sentenc from the new york , new york time , for exampl .
and for ani sentenc , x1 up to xn , i'll just defin c of x1 through xn to be the number of time that , that sentenc is seen in our train exampl , okai ?
a veri simpl estim is then the follow , where we defin p to be simpli c over n , okai ?
so , we simpli count the number of time the sentenc be seen and divid by the total number of sentenc seen in our train corpu .
and thi is a languag model , you can verifi that p is alwai greater and equal to zero .
and also if you sum over all sentenc , p will sum to on .
it's a perfectli well form languag model .
but it ha some veri , veri clear defici .
most importantli , it will assign probabl <num> to ani sentenc not seen in our train sampl .
and we know that we're continuiusli see new sentenc in a languag .
so , thi model realli ha no abil to gener to new sentenc .
so , the most import question in thi lectur is essenti , how can we build model to improv upon thi naiv estim and in particular , model which gener well to new test sentenc .
okai so in the previou segment of thi lectur we gave the basic definit of the languag model problem . .
and in thi segment i want to talk about trigram languag model .
which as i've said befor ar an extrem import and wide us type of languag model .
so trigram languag model build heavili on the idea of markov process which ar a veri import concept in probabl and statist .
so let's first talk about markov process and then we'll describ how we can us them to construct a trigram .
languag model .
so in a markov process we have the follow scenario .
we have a sequenc of random variabl x1 , x2 up to xn .
and each random variabl can take ani valu in a finit set v .
for exampl , v might be the set of word in the languag which we're interest in .
and for now , we'll assum that the length n is fix .
so everi sequenc ha the same length .
for exampl , we might have everi sequenc have length <num> .
we'll first cover thi case where the length is fix .
and then we'll go on to gener thi , to allow n to vari .
allow the valu for n .
to itself be a random variabl .
but for now n is fix .
our goal is to build a model , of the joint probabl , of x <num> take some valu of littl x <num> , x <num> take some valu of littl x <num> , of <num> x n .
take some valu littl xn and each of these valu is in the set v , okai ?
so we have a joint probabl distribut over the valu of these n variabl .
it's worth note that there ar a huge number of possibl , valu xy make to up to littl xn .
if fact if we have the vocabulari .
v , whose size is size v , as i've written here .
thei're a size v to the power n differ possibl sequenc .
and so we have a distribut over thi veri , veri larg set of possibl .
that's becaus i have .
i think that the first posit i had v possibl here .
if i think about the second posit i had v possibl here .
and up to the nth posit i had v possibl there .
and so i end up with v to the power n .
okai .
so , how do we go about model thi , thi joint probabl ?
and it's basic go to be <num> step in deriv what's call a first order markov process .
and the first step is to us the chain rule of probabl to decompos thi entir express as a product of express .
let me explain a littl bit about what's go on here .
when i sai we're us the chain rule .
so rememb if we have event a and b and we want to sai what's the joint probabl of event a and b happen ?
for exampl we might have the joint probabl that x <num> is equal to littl x1 .
and x2 is equal to littl x2 .
just consid the first random variabl in the sequenc .
then the chain rule sai the follow ; that i can decompos thi into a product of <num> term .
the first on is p of a .
and then i multipli that by the condit probabl of p of b given a .
and thi follow directli by the definit of condit probabl and if , if we appli thi to thi veri simpl case where we have a sequenc of length <num> , thi mean i can decompos thi as the probabl that x <num> taken , take the valu x <num> , multipli by the probabl that x <num> take the valu x <num> given that x <num> is equal to x <num> .
okai , so that's the chain rule appli to just two event and we can appli thi to longer sequenc .
so if i have the joint probabl over <num> event , a , b , and c .
chain rule sai i can decompos thi as p of a time p of e given a .
time p of c given a and b .
and so notic we have first the probabl of a then the probabl of b then the probabl of c and each point we condit on the previou event in thi particular sequesnti order , and again we can appli thi to a sequenc of length three .
so if i have thi express here thi can be written as , actual exactli what i've written up here , thi is the same .
time p of x3 equal x3 given x1 equal x1 and x2 is equal to x2 .
okai .
it's import to realiz that thi kind of decomposit us the patr us the chain rule is exact .
i can alwai take the joint probabl <num> event and decompos it in the follow wai .
so , what i've written here .
is simpli , thi chain rule appli to the full sequenc , of random variabl x <num> through x n is realli a , a gener of these two case i've shown you here , for arbitrari n .
so the first term , is p of x <num> equal x <num> .
and then i have a product of term , on for each posit from i equal <num> to n .
each term ha the probabl that xi equal xi , and i condit on all of the previou valu for the random variabl in the sequenc .
and again , it's critic to realiz that thi equal is exact in that i can alwai take a joint probabl of thi form and rewrit it in , in , us the chain rule in thi form .
so that's the first step in drive a first order markov process .
now let's go over the second step .
and here is where we make critic the markov assumpt .
ok so again , thi equal is exact .
and thi , the qualiti follow by the markov assumpt .
so what is that assumpt ?
it is a first order assumpt .
we'll see why we call it first order in a second but thi is just sai that for ani posit i in the rang <num> .
to n .
for ani sequenc , x1 through xi .
the probabl that xi is xi , given these previou valu in the sequenc .
is equal to the probabl that xi equal xi .
condit on just the previou valu at i minu <num> .
and so we've essenti made the assumpt that thi random variabl , the posit i , basic depend onli on the valu of the random variabl at i <num> .
to be a littl more precis , thi independ assumpt is sai that thi random variabl is condition independ of all the previou random variabl , random variabl onc i condit on i <num> .
so everyth up to i minu <num> is irrelev .
and that leav us with thi express here .
where we sai that the joint probabl of the sequenc is equal to product of term , x of x <num> equal x <num> .
and then i have a product from i equal <num> to n .
and at each point i have the probabl that xi is equal to littl xi , given that xi minu <num> is equal to xi minu <num> .
okai .
so thi is clearli a huge assumpt .
i'm just assum that each random variabl depend on the previou valu .
but it's go to be veri us , in that it consider simplifi the model .
and as we'll see it consider simplifi the number of paramet in our underli model .
okai .
so , we've defin first order markov process .
we'll now see how to gener these to what ar call second order markov process .
and , in fact , thi will be a veri straight forward gener of what i've just shown you .
so , the task again is to model the joint distribut over a sequenc of n random variabl .
and in a second order markov process , we make the follow assumpt , that thi joint probabl is again a product of term .
we have the probabl that x1 is equal to littl x1 .
we have probabl that x2 is equal to x2 , condit on x1 equal x1 .
but the , for element further along in the sequenc , i equal <num> to n , the valu for the ith random variabl depend on the previou two random variabl .
so rememb , in a first order markov process , we would have had thi term alon be condit on .
we now also add the term two posit back .
so , thi in a sens is a slightli more power model , and that it can captur a broader class of distribut .
we're condit on a littl bit more inform .
we're essenti just condit on the previou two element in thi sequenc rather than the previou on .
so , if we condit on what the , the previou element just i minu <num> is call a first order process .
you condit on the previou two element , is a second order process .
and you can go further , you can , in fact , defin third order and fourth order of markov process in the natur wai .
so , that's essenti it .
that's a second order markov process .
to make thing a littl bit simpler , note these term ar a littl awkward .
to make thing a littl bit simpler , we're actual go to write down a second order markov process , as follow .
i now have a product from i equal <num> to n .
and at each point , i have the ith condit on the valu at i minu <num> and also i minu <num> .
and i'm now care to essenti defin a coupl of random variabl x minu <num> and x0 , essenti at the start of the sequenc , and these random variabl alwai take the valu star .
so , you can think of as alwai start thi markov process with xi sorri , x minu <num> equal to star x0 given star , and then we have x1 .
thi next element , mayb that's some word there .
x2 is then next element and so on and so on .
okai .
so , thi just make thing slightli simpler basic from a notat point of view .
so , up to now , i've assum that the length of the sequenc , n , is fix .
but realli we'd like to gener thi kind of model .
so , the length of the sequenc , n , is also a random variabl .
what thi mean is , we'd like to defin a distribut over all possibl sequenc where the sequenc length can also vari .
and you can see now how we're get , get closer and closer to the languag model problem becaus with languag model , it's essenti to model a distribut over sentenc where the length of the sentenc can vari .
and there's actual a veri simpl solut to thi , a direct extens of second order markov process , or first order markov process for that matter .
and we'll essenti just sai that the nth random variabl is alwai equal to stop , where stop is a special symbol .
it's special and that it is not in the regular vocabulari in the markov process , it's not a member of the set v .
sort of an addit symbol , so it's not in the set v .
and it's onli ever seen at the end of a sequenc .
and then , we can us a markov process exactli as befor .
we can sai that thi joint probabl is equal to product of term , i equal <num> to n , at each point , we condit the valu of the ith random variabl on the previou two random variabl and we ar just assum that xn is equal to stop .
so , we now have a wai of write down the probabl of the sequenc , for ani length , we get a distribut of all length sequenc .
intuit , what's go on here is that i have a process where , at each point , i'm gener the valu of the ith random variabl condit on the previou two random variabl .
so , you can think of gener a sequenc of random variabl in left to right order .
first x1 , then x2 , then x3 , up to xn .
and at each point , there's a possibl that thi i thread and random variabl will be stop .
and if i see the stop symbol , i then just stop and output that sequenc as the output of thi , thi process , thi sampl process .
a littl bit more formal , you can show that under quit mild condit we do , under thi addit , under the stipul , that thi is equal to stop .
we will have a well form distribut over all possibl sequenc of vari length .
and i don't want to go into the detail of that .
but that's a fairli straightforward applic of the theori behind markov process to show that we have a well form distribut .
but again , the main thing is just to think about the intuit , where at each point , we're gener some symbol , xi , and if we ever gener stop , we immedi termin the process .
so now , we'll see how we can appli these idea veri directli to the languag model problem .
so and we'll see how trigram languag model can be deriv as a direct applic of second order markov process .
so , a trigram languag model will consist of <num> thing .
firstli , we'll have some finit set v , which is just go to be the vocabulari in the languag model in exactli the same wai as befor .
so , it might have word like the , a , beckham , and so on and so on .
again , thi might be a fairli larg set .
it might be a few thousand or a few , even a few ten of thousand of word .
and then , the second part of a languag model is a set of paramet .
so , for everi sequenc of three word , u , v , w , so thi will be refer to as a trigram , the sequenc of three word , we have a paramet qw given uv .
we'll see veri soon the role these paramet take .
w could be v sorri , ani element of v , or it can be the stop symbol .
and u and v could be element , ani element of v in addit to these special star symbol we saw in the second order markov process .
and given these definit , the model defin a distribut as follow .
so , for ani sentenc , x1 through xn , where each xi is in v , for i equal <num> to n minu <num> , and where xn equal stop , the probabl under the trigram languag model is defin as a product of term .
so , i have i equal <num> to n at each point .
i have q of xi , given x minu <num> , x minu <num> .
i'll illustr thi in a second with a , an exampl but basic these paramet correspond directli to the probabl that xi is equal to xi given xi minu <num> is equal to xi minu <num> , and xi minu <num> is equal to xi minu <num> .
so , thi is basic just a rewrit of the second order markov process i've show you in the previou section .
so , that's the formal definit , but thing should becom a lot clearer if we go through an exampl , becaus what we end up with is realli a veri simpl model .
so , let's take a simpler sentenc , for exampl , the sentenc , the dog bark .
as befor , we have the stop symbol at the end of the sentenc , we'll alwai have that .
so , in thi case , the probabl assign to the sentenc under the languag model is a product of term , so we've notic that we have each of the word there , dog , bark , and stop .
and at each point , we condit on the previou <num> word in the sequenc .
so , for exampl , bark is condit on the , and dog , becaus those ar the , the previou <num> word in the sequenc .
so , what we're do here is essenti , treat sentenc as be gener by a second order markov process , where each word in the sentenc is chosen condit onli on the two previou word in the sentenc .
and that is a trigram languag model .
so , let's just talk briefli about thi assumpt , that each word depend onli on the two previou word .
so , that clearli is a veri naiv assumpt .
and , in fact , it's possibl con , to construct all kind of exampl , where that independ assumpt is veri clearli violat .
and , in fact , later in the cours , we will see model , for exampl , probabilist varianc of context free grammar , which ar arguabl much more realist model of languag and that thei captur much longer rang depend than just the previou two word in the sentenc .
have said that , trigram languag model ar tremend us .
it turn out that thei ar quit difficult to improv upon and thei have the benefit of consider simplic .
and that all you have to do is estim these trigram paramet in the model .
thi estim problem we'll come to a littl later in thi lectur .
but to summar , for ani sentenc , it probabl is a product of term , we have on of these q paramet for each trigram in a sentenc where we condit each word on the previou two word .
so , thi leav us with a remain estim problem , which is that , we want to estim these q paramet in the model .
so , for exampl , we might want to estim the probabl of laugh , given the , the previou <num> word with and dog .
and we'll spend quit a lot of time on thi estim problem .
it turn out to be quit a challeng program .
let's talk first though about a veri natur estim for thi quantiti , and thi is often refer to as the maximum likelihood estim .
and it's realli a veri intuit estim .
so , recal again , we've assum that we have a train set .
so we have some exampl sentenc in our languag .
we might typic have mayb a few million , a few ten of million , or mayb even a few billion word , or sentenc .
from these train sampl , we can deriv count .
so , for exampl , count of the and dog , would be the number of time i've seen the word , the , follow by dog .
and my train sampl , count of the , dog , laugh , would be the number of time i've seen the trigram sequenc , the , follow by dog , follow by laugh .
and the maximum likelihood estim , which i've written here for thi exampl , is the ratio of these two term .
so , i have the ratio of the trigram divid by the ratio of what we often call the bigram , the sequenc of two word that we're condit upon .
and that's a veri intuit and a veri natur estim .
mani peopl would have come up with thi as a first guess for how to estim the paramet in thi model .
so , maximum like estim will be veri us .
thei will form a start point for the estim method we will develop .
but thei do have a veri clear problem , and that is the follow .
we have a huge number of paramet in our model .
even though we've made thi assumpt , that we onli condit on the previou two word at each point , we still have a veri larg number of paramet .
so , if we defin n to be our vocabulari size , well , there ar n possibl for thi word , n possibl for thi word , n possibl for thi word , plu a , plu <num> or <num> , if we take the stop , stop symbol and start symbol into account .
but as a good estim , we have n time n time n .
we have n cube paramet in the model .
and so , for exampl , if we have a vocabulari size of <num> , <num> , we have <num> , <num> cube .
that's <num> time <num> to the <num> differ paramet .
so that is a veri larg number of paramet , irrespect of how much train data we have .
thi is go to be a veri larg number , in comparison to the number of train examnpl that we have .
and , thi manifest itself in the follow wai .
in mani case , thi count on the numer mai be equal to <num> .
becaus we simpli haven't seen thi particular triagram in train data .
and in that case , thi estim will be equal to <num> .
that's problemat becaus there ar so mani trigram possibl , mayb <num> time <num> to the <num> that just becaus we see the trigram zero time in train doesn't mean that we should sai thi estim is equal to <num> .
where still , in some case , thi denomin mai be zero , mai be i've never seen the word , the , follow by dog .
and in that case , thi ratio is complet undefin and the estim realli fall apart .
so , the main point here is that we have a veri larg number of paramet .
we have a larg amount of train data .
that would mean that mani of these count ar equal to zero .
and that will lead to estim be unrealist low or actual ill defin .
but we'll soon see that we can modifi these style of estim in wai that make them quit robust to these problem .
okai , so , in the previou section , we have defin the languag model problem and we have introduc thi veri import class of model , trigram model for languag model .
veri soon , we will describ estim method that allevi the problem with spars data that i describ in the previou session .
but befor i get to estim , i want to describ how we can evalu the effect of differ languag model .
and to do thi we will describ a veri import measur call perplex which is wide us as a measur of the perform of a languag model .
so now let's defin perplex .
so as i've said befor , we've assum in the languag model problem , that we have some train data consist of mani sentenc in the languag in which we ar interest .
in addit we're go to assum that we have some test data sentenc .
us the low m to refer to the number of sentenc .
so we have s1 s2 s3 up to sm .
where each of these is a , is a sentenc in the languag .
so s1 for exampl might be the dog laugh stop .
it could be ani sentenc .
critic , while we will assum that the paramet of the languag model ar estim on the train exampl , we'll assum that the test data is a separ held out set of exampl , which were not us in estim of the paramet .
so , the test data ar on new , unseen exampl .
given the test data , it's natur to look at the probabl that our languag model assign to those test data sentenc .
so , here i have a product from i equal <num> to m .
p of si where thi p is itself some product of term for exampl the given star , star time q of dog .
given star there time and so on and so on .
thi , thi function p is a function of our paramet .
now a good languag model intuit should assign as high probabl as possibl to these test data sentenc .
you can think of thi valu as be a measur of how well the languag model predict these test data sentenc .
it's actual slightli more conveni , to instead of look at thi product , we look at the log of thi product .
so if i take the log by the usual rule of log of product , i end up with the follow which is a sum , from i equal <num> to m .
that the log probabl under the languag model with sentenc si .
and again , the higher thi quantiti thi log probabl the better our languag model is as a model of these test data sentenc .
so thi would actual be a , perfectli good measur of the qualiti of a languag model .
but we're go to transform thi quantiti in a , a coupl more step which will be conveni .
which will turn out to be conveni .
and so first i'm go to defin thi valu l , and thi is just divid thi log probabl by capit m where m is the total number of word in the test data .
so thi can now be interpret as the averag log probabl , word by word in my test data .
and it's basic a quantiti which is now normal with respect to the length of the test data sentenc .
so in some sens is now stabl with respect to the length of the test exampl .
thi log by the wai is go to be defin as log base <num> and the complex is now defin as <num> to the minu l .
so , i take thi log probabl i divid by the number of word and then i rais thi <num> to the minu l .
on import point about perplex is , the higher thi quantiti is , the log probabl , the better the fit of the model to the test exampl .
and that mean , becaus i have <num> to the minu l here .
lower quantiti of perplex ar better than the higher quantiti .
the lower the valu for the perplex , the better the languag model is as a fit to our test data exampl .
so thi is the standard measur of qualiti of a languag model .
let's talk a littl bit more about thi .
give some complex valu , and also talk a littl bit more about some import intuit behind complex .
so , on thought experi is the follow .
sai we have some vocabulari v is befor i had to find capsul n to be the size of v plu <num> .
and let's sai i have the dumbest possibl languag model , where each paramet is just <num> over capit n .
so rememb w in thi case can be ani valu in v or it can be the stop symbol .
and thi languag model simpli assign the uniform distribut over all possibl word at each posit .
so it complet ignor the previou two word .
and in addit , it ha no model of the rel frequenc of differ word .
the fact that thi word , in english , might be more frequent than that word in english .
it's , as i said , the dumbest possibl languag model you could think of .
it's easi enough to go through the equat on the slide i show you previous .
and recov the fact that l , in thi case , is log <num> over n .
and the perplex is <num> to the minu l again .
and the perplex in thi case come out as simpli n .
basic the vocabulari size plu <num> .
so , you can think of the perplex as a measur in a sens of the effect vocabulari size .
under the dumbest possibl model we simpli get the vocabulari size .
and on the statist model we can improv upon thi becaus some word ar more frequent than other , and we condit on the context , and so on and so on .
here , to just give a littl bit more intuit ar , some valu for perplex taken from a paper by , joshua goodman call a bit of progress in languag model .
here he consid assess where the vocabulari size wa <num> , <num> .
and i believ it wa newspap text .
and here ar some number .
so for a trigram model , which is just as we defin befor .
we condit each word on the previou two word .
and with appropri estim techniqu which we'll describ a littl later in thi lectur .
the perplex came out at <num> .
notic that is vastli smaller than the vocabulari size <num> , <num> .
so we've done much , much better than simpli predict a uniform distribut over each word and term in the sequenc .
you can see that thi statist inform ha given great benefit in term of perpliex .
goodman also look at other model so on is a bigram model .
a bigram model is veri similar to a trigram model but at each point we condit onli on the previou word xi minu <num> .
so thi is essenti a first order mark off process .
rember thi , a trigram model , is a second order mark off process .
so we're condit on less inform just the previou word and of cours we lose some statist power there and the perplex doe go up a bit to about <num> .
which is a pretti signific chang from <num> .
he also look at unigram model .
in thi case , the probabl for ani sentenc is a product of term .
i have q x i which actual complet ignor the context .
and so we just have a paramet at q x i for each possibl word .
and so we ar predict to a , ignor the previou two word .
thi model will have the abil to model the differ in frequenc of differ word , but will have no abil to model context .
the perplex in thi case wa close to <num> , <num> .
so you'll notic that thi is realli , dramat larger than the valu for trigram model .
we certainli gotten a lot of benefit of condit two previou word .
it's still rather better than <num> , <num> word of the vocabulari size .
so , in short , trigram languag model's give consider improv over just a uniform distribut of the vocabulari or unigram , or biagram model .
you can of cours , go further to what i call <num> gram model , which condit on the previou <num> word , or <num> gram model .
and you will , given enough train data see some imrov if you do that .
the trigram ar get pretti close to the state of the art for thi problem .
i just want to finish thi segment with a littl bit about some histori behind the idea of languag model .
and a critic refer is due to shannon and thi is realli on of the earliest piec of work on thi problem . thi is from <num> .
cole shannon wa realli on of , a huge figur in inform theori .
huge earli figur in inf , inform theori .
and he actual did experi where he look at basic bigram , trigram languag model and so on , build a , build as we did from mark up process as hi start point .
and it's a veri interest paper , it's well worth read .
and amongst other thing , it actual measur how well human perform as languag model .
so , it actual look at how well human will predict the next word , or mayb the next letter in a text .
turn out that human realli perform quit , quit well on thi probelm .
probabl better than the trigram model i've just describ to you .
actual probabl better than ani languag model that we've come up with at thi point .
so that's a veri , on veri earli piec of research .
anoth veri interest refer is due to chomski .
so , syntact structur is a huge influenti book , in modern linguist .
and it's a wonder book .
i highli recommend it , if your interest in languag .
and here he ha a , a detail argument about the role of probabl in statist in linguist , specif it's role in grammat , predict whether a sentenc is grammat or not .
so thi is a veri involv argument it provok a lot , it ha provok a lot of discuss .
i won't get into it , but he ha argument for why probabl realli doesn't have much to do with grammat , and it's a veri interest argument .
i , as i said , i won't get , get into the detail .
and as i said , and he i think is it's not entir clear but i think he's veri much ha sharon's experi on mark of process on languag of mind .
there ar clear argument that these process fail to captur the kind of longer distanc depend which ar a critic part of languag .
and as i said , later in the cours we will see more power model , such as context free grammar , which begin to get at more interest long rang depend .
have said that , as i said befor .
trigram languag model ar tremend us in practic .
and in applic such as speech recognit , and machin translat and so on .
thei plai a critic role .
and thei've been veri hard to improv upon .
okai .
so , so far in thi lectur , we have describ the basic languag model problem .
we've introduc a veri import class of languag model , name trigram languag model .
and final , we've spoken about how to evalu differ languag model us thi measur which is call perplex .
so , in the final part of the lectur , i'm go to talk about estim techniqu for , for trigram model m , name , linear interpol and discount method .
and we'll first focu on thi method call linear interpol .
so just to recap , on of the primari challeng involv in estim of trigram languag mode is the spars data problem that i describ earlier in the lectur .
so , let's just go over thi , thi argument again .
so , a veri natur estim for a trigram languag model is what's call the maximum likelihood estim .
so , the gener form is as follow .
if i'm estim the paramet q for some word wi , condit on previou word wi minu <num> and wi minu <num> , then we simpli defin thi paramet estim as the ratio of two term .
on the numer , i have what is often call the trigram count and thi is simpli the number of time i've seen the sequenc of word , wi minu <num> , wi minu <num> .
wi , in my train corpu .
on the dot denomin , i have the bigram count .
and that is simpli the number of time i've seen the word wi minu <num> , wi minu <num> , in my train data .
so , let's take a specif paramet as an exampl .
sai , i want to estim the paramet correspond to the probabl of laugh , given that the previou two word ar the and dog , i just have the ratio of these two count , the trigram count and the bigram count .
now , as i said earlier , in gener , in these model , we ar go to have a veri larg number of paramet .
so , if our vocabulari size is n , then there ar roughli n cube paramet in our model .
as on exampl , if n is <num> , <num> , then we have <num> , <num> cube .
that's around <num> time <num> to the <num> paramet .
so , even with the veri larg train set that we us nowadai to estim the paramet of a languag model , thi is a veri , veri larg number .
now , becaus of thi inevit , mani of the count us in these estim will be equal to zero and that will lead to all kind of problem .
and in mani case , these q paramet will be equal to zero that happen if thi count on the numer , the trigram count is equal to zero .
where still if the bigram count is equal to zero , thi count on the denomin , thi estim is complet undefin .
so , thi lead us to the estim method we're now go to consid .
and as i've said , we're first go to consid thi method call linear interpol .
so first , let's get some definit .
q sub ml is go to be our maximum likelihood of estim .
and thi is the trigram paramet estim base on the ratio of count that i show you on the previou slide .
but in addit to thi trigram estim , i can also describ what ar call bigram and unigram estim as follow .
so , the bigram estim .
a game we us q sub ml look at a word wi and just the previou word wi minu <num> .
so , thi is an estim which condit onli on the previou on word as oppos to the previou two word in the context .
and again , thi is simpli defin as a ratio of count .
on the numer , i have now a bigram count .
and on the denomin , i have a unigram count .
if we go a step further , to the unigram estim , thi is actual an estim of the probabl of a word that complet ignor the context .
so now , we're not even go to condit on the previou word in the context .
and thi is , again , to find as a ratio of count on the numer i have the unigram count .
that's simpli the number of time i've seen the word wi in the corpu .
and on the denomin , thi express here is go to be the number , total number of word in the , in the corpu .
so , the total number of word in our train data .
so , thi is simpli a ratio of the frequenc of the word wi to the total number of word i've seen in my corpu .
so , if we look at these three differ estim correspond to these three differ level , thei have differ strength and weak .
and , the trade off here is often refer to as the bia varianc trade off in statist .
the trigram estim ha the benefit that it condit on a lot of context , name , the previou two word , and so it ha rel low bia .
given enough train sampl , these count will be rel high and thi will converg to a reason estim of the probabl of wi , given the context .
in contrast , if we look at the unigram estim , it complet ignor the context .
and so , it'll fail to captur contextu effect .
and so , it'll converg to a less good estim as the number of trade sampl increas .
convers , howev , the trigram maximum likelihood estim ha thi problem that mani of these count will be equal to zero .
so , we need a veri larg number of train sampl to get an accur estim of the trigram , a maximum length of estim .
convers , for the unigram estim , these count will converg rather quickli to their expect valu .
and thi estim will quickli converg to the true unigram distribut underli the data .
the bigram estim is somewher between these two extrem where condit on a reason amount of context .
and it converg reason quickli to it true underli valu .
so , what we'd realli like to do is to come up with an estim which trade off these differ strength and weak with these three estim .
and thi is where linear interpol come into plai .
so in linear interpol we ar go to come up with an estim that take into account the maximum length of estim at the trigram , bigram and unigram level .
and thi new estim is actual go to be a weight averag of these three maximum length estim we have <num> addit paramet , lambda <num> , lambda <num> , and lambda <num> .
and these dictat the rel weight of the <num> maxim likelihood estim .
and we have some constraint on these lambda valu .
thei have to sum to <num> .
.
and thei have to be greater or equal to <num> .
so , for exampl , we might have lambda <num> equal lambda <num> equal lambda <num> equal a third , which basic mean we give on third weight .
to each of these maximum likelihood estim .
so let's go through a specif exampl , a specif paramet .
sai we want to estim the paramet correspond to the probabl of the word , laugh .
given that the previou <num> word ar the and dog .
under thi definit , and assum that our lambda valu ar all equal to third .
we would have <num> 3rd time the maximum like estim of laugh given their dog and then <num> 3rd time the maximum like estim of laugh .
given just the word dog , then final a 3rd time the maximum length of estim of just laugh without ani contextu sensit .
so the motiv for thi step is that in practic we end up with a new estim which incorpor inform .
from all three maxim like g estim , and thu in some sens , incorpor the strength of all these three estim .
so we now have a new estim which is sensit to the previou two word in the context .
but the estim is also robust , in that it doe incorpor inform from these more robust estim of the bigram and the unigram level .
so , shortli we'll see how we can actual estim these lambda paramet , again us some data .
but first , i want to show you an import point , which is that thi paramet estim q is , in fact , a valid estim .
so it's import to verifi that our estim correctli defin a distribut .
and by thi i mean the follow .
defin v prime to be the vocabulari with the stop symbol .
and now for ani uv bigram we're condit on , we want to make sure that if we sum over all word w and v prime , we have someth that sum to <num> .
and that's fairli simpl to show , i'm just go to go over that in thi slide .
so here ar the step in prove thi properti .
so what i've done here is substitut .
for a q the express i just show you on the previou slide .
so thi is the interpol estim , so i have lambda on , lambda two , lambda three .
these three paramat dictat the rel weight of the three estim and i have my three maximum length of estim the trigram , bigram and unigram estim .
so the first thing to notic is that these lambda paramat do not vari as w vari .
so , thei can be brought outsid the respect sum .
and so , thi express simplifi , slightli .
i have lambda <num> time sum of w of here , plu lambda <num> , i have the sum of w here .
plu lambda <num> , time w of the unigram estim .
next we can notic , it's simpl enough to show that these sum ar all equal to <num> .
that's becaus the maximum i could g estim themselv correctli defin a distribut .
or equival you can go back to the definit of term of count and , and convinc yourself of thi properti .
and so we end up with simpli lambda <num> , lambda <num> , lambda <num> .
some of these <num> term and by definit rememb thi wa a constraint of the lambda .
these <num> term sum to <num> .
it can also be shown , thi is the other properti we need for thi to directli defin distribut , that for ani uv bigram for ani w in the set , v prime , thi valu is greater than or equal to <num> , and that's , that's trivial to show in thi case .
so , the final question we need to answer is how do we actual estim these lambda valu .
so thi is gener done as follow .
so firstli we'll take part of our train set , and we will take some of our sentenc , and we will take some of the sentenc in our , in our train set , and hold these out .
as what is call valid data .
so you can visual thi as , as follow .
imagin we have our train data .
it consist of mani million of sentenc .
we might take some portion , some rel small portion and now roughli speak sai <num> percent to <num> percent of our data and us thi as what we call valid data .
so the count us in the maximum likelihood estim will be taken from the main portion of the train data .
but in addit , for ani trigram c , w1 , w2 , w3 , i'll defin c prime to be the number of time that trigram is seen in thi valid portion of the train data .
so , we then proce as follow we're go to defin a function l which is a function of <num> paramet which basic measur how well our model fit the valid data .
so , l is defin as follow , we have a sum of all trigram w1 , w2 , w3 .
and then here i have c prime , so thi is go to be the number of time thi particular trigram is seen in the valid data .
mani of these count of cours ar go to be zero .
and then over here i have log .
of q , of w <num> , given w <num> , w <num> .
so thi is my paramet estim for thi particular q .
and q is defin through the in fact hidden in here is an express which depend on the and is defin , as befor , as the , the wai to averag of these <num> maximum like hood estim .
so , as these <num> lander paramet vari , these q valu will vari , and becaus of that , the function , the valu for l will vari .
and so we'll set up an optim problem .
where we attempt to maxim l .
under our constraint that these lambda ar posit , and that thei also sum to <num> .
so again , you can interpret l as a measur of how well the particular valu for lambda <num> , lambda <num> , and lambda <num> fit the valid data .
in fact , it's a fairli easi exercis to show that if you maxim thi function , with respect to lambda , you also minim the complex of the model , on the valid data .
so we're actual try to pick the lambda that minim the complex of our languag model .
and henc fit the valid data as well as possibl .
now i'm not go to go into ani detail of how thi maxim is perform in practic .
but it is in fact a fairli simpl oper to solv thi problem .
i want to talk about on last issu which is import in practic .
and thi is intend as a sketch but you hopefulli get the idea .
and thi is that , in practic it's actual veri import to allow these lambda to vari a littl bit .
in particular to allow them to vari depend on the differ count us in our estim .
and here , here's how thi is done in practic .
so sai we're condit on a particular bigram wi minu <num> wi minu <num> .
we're actual go to partit differ bigram depend on the account .
so in thi definit i have partit into four differ subset .
so thi function pi  take a bigramm's imput and return <num> if the bi gram count is <num> , return <num> if the count is between <num> and <num> .
return <num> if the count's between <num> and <num> .
and return <num> otherwis .
so thi partit is gener ar chosen by hand .
but thi is probabl a fairli typic of the kind of definit you might see .
now , onc we've defin thi partit , we give a slightli refin version of the linear interpol .
where these lander vari , depend on the valu for pi  .
so , i now have paramet lander <num> .
sub on lambda on sub two lambda on sub three .
and these three paramet ar us if the count of the bigram is equal to zero .
where i have paramet lambda two on , lambda two , two , lambda two three .
i us these three paramet if the count is between on and two .
and so on and so on .
so notic that these lambda now vari depend on which partit the bigram fall into .
now , what's the motiv for thi step ?
it basic mean that these lambda can vari depend on the partit which the bigram fall into .
and , henc , thei can vari depend on the count of the underli bigram and thi is actual import in practic .
again these lander can be optim us valid data in a veri similar wai to the wai i show you on the previou slide .
and on crucial aspect of thi is that in particular if thi bigram counter is equal to zero .
thi paramet , lambda <num> , will actual be , be equal to <num> , becaus thi maximum like estim will actual not be defin in that particular case .
okai , so in the final part of thi lectur we ar go to describ a second estim techniqu .
thi is what's often call a discount method .
and thi will build on mani of the same intuit as linear interpol .
but as we'll see it's a slightli differ wai of do thing .
and it's also veri often us in practic .
okai , so , to understand discount method let's first start with an intuit illustr by the exampl i've shown here .
so , what i've shown is the count for biagram where the first word in the biagram is the word the .
so , for exampl the follow by dog is seen <num> time .
the follow by woman is seen <num> time and so on and so on .
and i'll assum in thi exampl that in thi list i've shown all bigram whose count is greater than zero .
so i have the full list of word which ar seen on or more time follow the word the , there ar about ten of , ten of these word in total .
and so , the number of time i've seen the word the is go to be the sum of these count .
it's actual <num> , in thi case .
so now , if we consid the maximum likelihood estim in thi case , there ar go to be for exampl , <num> divid by <num> , for the probabl of dog finder .
<num> out of <num> probabl of woman follow , fall there and so on and so on .
just the ratio of thi count to the number of time i've seen the word , the .
now , on thing to observ here is that , in gener these estim ar go to be systemat high .
particularli in case where we have a larg vocabulari .
rememb , we might have a few thousand or , or ten of thousand of possibl word form the word there but we've onli seen the word the , <num> time .
and so , it's just a lucki few word which were actual seen after the word , the .
and that's a rather inform descript , but you can actual show rather more formal that these estim ar go to be systemat high .
that's particularli true for these low count estim .
for exampl the probabl of street fall the be <num> over <num> is go to be a rather high estim in thi case .
so discount method build on that intuit in the follow wai .
we're go to defin these new discount count .
i'll us count star to refer to a discount count .
as simpli count of x minu <num> .
for ani bigram x who's count is <num> or more .
so look back at thi exampl .
if the count here is <num> , for exampl .
the discount count is go to be <num> .
similarli , an origin count of <num> translat to a discount count , <num> .
and for all of these case where the count is on .
the discount count is <num> .
and we can again , defin estim now base on the ratio of the discount count to the number of time we've seen the word there .
so for exampl we now have <num> out of <num> as the estim for probabl of dog given there or <num> out of <num> , to the probabl of countri given there .
so notic we have essenti lower each of these estim , through these discount method .
now , onc we've done thi we'll see that there is actual , some miss or left over probabl mass .
what do i mean by that ?
well if we sum each of these term we end up with an express someth like <num> out of <num> .
plu <num> out of <num> .
and so on and so on .
final we have plu <num> time <num> out of <num> .
that's actual a valu which is <num> out of <num> if you do the calcul .
and thi is actual less than <num> .
so we have a seri of probabl which sum to less than <num> .
and that leav some so call miss probabl mass whose valu is <num> minu <num> over <num> , thi is equal to <num> over <num> .
thi , is in a sens , the probabl mass that we have left over after thi discount method .
now , the basic idea in discount method , is go to be to take thi miss probabl mass and divid it between other word in the vocabulari which aren't in thi list .
ie word where the count follow the word the is equal to <num> .
those unlucki word which were never seen in a bigram with the as the first word .
so a littl bit more formal we'll defin for ani word wi minu <num> alpha of wi minu <num> to be thi miss probabl mass , and thi is defin as <num> minu here i have a sum of a w .
count star of w i minu <num> w , divid by count of wi minu <num> .
so in the particular exampl i just show you , you can verifi that thi miss probabl mass , is in fact <num> out of <num> .
so let's see how we can deriv the final estim base on thi discount method .
follow thi idea of divid the miss mass between the word whose count is <num> for a particular contextu word .
and thi is a method go back to katz so it's often call a katz back off model .
and here we're deriv a bigram estim , becaus q of bo , thi is go to be the back off estim of wi , thi is condit on wi minu <num> is go to condit just on the previou word in a moment we'll see how to defin a trigram estim in a similar wai , but first of all we'll consid the bigram case .
okai , so for a particular word wi minu <num> , i'll defin <num> set .
so big a of wi minu <num> is the set of word whose bigram count is greater than <num> .
so for exampl , for the word there it would be the <num> word that i've shown you on the previou slide .
in contrast , b of w i minu <num> is the set of word in which the count is equal to <num> .
so thi the set of word which ar never seen follow the particular word we're interest in .
as befor i'm go to defin the miss probabl mass through thi express .
thi is go to be <num> minu .
here i have a sum of all the word whose count is greater than zero .
count star of w i minu <num> w , divid by count w minu <num> .
so thi is the probabl mass which is left over .
okai so given these definit the back off estim take two form depend on whether a word is in the set a or if a word is in the set b .
if we have a word in the set a , we simpli take the discount count .
so thi might for exampl be <num> divid by the number of time we've seen wi minu <num> .
so thi might be <num> for exampl .
convers if the word is in the second set b that mean thi count is equal to <num> .
we do the follow .
so rememb that alpha of wi minu on is thi miss probabl mass .
i've said befor we were somehow go to divid thi between the differ word and the set b .
and we divid thi in proport to the maximum likelihood estim at the unigram level .
so qmlfwy is the unigram maximum like estim i show you earlier so the numer , that's what i have here .
in the denomin i have a sum over all word and b and qml of that w .
and so thi is just a normal turn which ensur that i'm split the alpha in proport to these unigram maximum like estim .
so you can see onc we've defin these discount count , through for exampl subtract a valu like <num> from each count in your train data .
it's realli quit simpl to deriv thi , thi estim method .
and it work realli quit well in practic .
so thi method can be extend in a fairli natur wai to katz back off model for the trigram case .
so we're now go to defin an estim of wi given two previou word .
and thi is deriv in a veri similar wai to the bigram case .
so again i'm go to defin set a and b .
which now depend on the two word be condit on .
so thi first set a is the set of word , w .
such that the trigram count is greater than zero .
and the second set b is the set of word , w .
such that the trigram count is equal to zero .
so if we look at thi back off estim , the first case is as follow , if we come across some word wi , such that the trigram can't count as greater than <num> , we take the ratio of the discount count , minu the count .
so thi again is go to be someth like count of the trigram minu some constant sai <num> .
and thi is simpli the , the count of the bigram be condit on .
now again , thi definit give us some miss probabl mass .
and we have an express for that miss mess down here .
so alpha of wi minu <num> , wi minu <num> is go to be <num> minu .
now again i have a sum of all word whose count is greater than zero .
and i have thi , thi estim i show you count star divid by count .
so thi is the miss probabl mass and again for ani word which is seen in the set b , i for which thi count is equal to zero , we defin the back off estim in the follow wai .
we're again go to split thi miss mass between these word in the set b and now we're go to split thi in proport to the back off estim at the bigram level .
so thi is just the estim i show you on the previou slide .
we essenti have a recurs estim here where we're go to look at the back off estim at the level below the bigram level .
so here on the numer , i have that back off estim .
and down here , i have a normal term , where i sum over all word in b and i have that back off term here .
so thi is simpli take the miss probabl mass alpha .
and split it in proport to these back off estim at the bigram .
so , that's essenti it , that is a trigram back off method .
the on paramet in thi approach is thi valu sai <num> that i'm us and discount .
thi is the valu that i subtract from account to form thi discount count .
and so thi will be typic some valu between <num> and <num> .
and on wai to choos thi valu is again by optim on valid data .
so you test a seri of valu between <num> and <num> .
for each on , see how well the languag model form on thi valid set of data .
and pick the , the discount valu which give you the best perform on that valid data .
a valu of <num> wouldn't be untyp it's usual somewher around there .
okai .
so , that complet the lectur .
so , just to summar some main lesson we've learn here .
there were three step in deriv the languag model i've shown you .
the first step wa to expand the joint probabl over a sequenc of word , w1 , w2 , up to wn , us the chain rule of probabl .
thi is what i show you in the portion of the lectur on markov process .
and the second step wa to make markov independ assumpt .
in particular , assum that the probabl of some word , wi , condit on the entir previou sequenc of i minu <num> previou word , actual depend onli on the previou two word in thi sequenc , we call thi a second order markov assumpt .
and the final step in deriv thi , these estim wa to smooth these diagram estim essenti us low order account .
and that wa done either through the method of linear interpol or through thi discount method that i just show you .
so , just briefli , languag model is a huge industri and there's been a lot of research in improv method for languag model .
some area of particular interest ar method that model the underli topic of document or other long rang featur of a document .
so , condit on just the previou two word is certainli limit and in , in some case , you might want to condit on the fact that a , a document is about sport , or is about polit , or the gener topic that can influenc the word that ar seen in the document and might be import to condit on that .
or we might condit on word which ar outsid thi two word window , there's been consider interest in that problem .
anoth type of model we'll see later in the class , is languag model built base on syntact model .
languag model that explicitli try to incorpor grammat inform , inform about what sentenc a grammat versu non grammat in a languag .
and again , these model can often captur the long rang featur , which fall outsid just a , a two wai window .
a rule though , it can be quit , quit difficult to improv upon languag model .
thei're simpl , thei're veri effici and thei can get us a long wai in mani problem .
okai .
so in thi next segment of the cours we ar go to describ tag problem .
tag problem ar a fundament import in natur languag process .
and we will look at hidden markov model , which ar a wide appli type of model for thi class of problem .
so i'll first describ the tag problem , and i'll give a coupl of kei exampl from natur languag process of tag problem .
we'll then describ a veri import paradigm for think about supervis learn problem .
thi is the paradigm of gener model , or the noisi channel model .
it's a wide appli methodolog for develop method for supervis learn .
and , we'll see it appli again and again in thi cours .
final , i'll describ hidden markov model tagger which ar on instanc of gener model .
we'll give basic definit .
we'll describ paramet estim method .
and , we'll describ the viterbi algorithm .
which is an algorithm of great import in a hidden markov model . .
so thi slide show a tag problem call part of speech tag .
which is a veri import problem in natur languag process .
and is actual on of the veri earliest problem consid in statist or machin learn approach to nlp .
statist model for thi problem go back to the late 1980s .
so the problem is as follow .
as input , to the model we have some sentenc .
some sequenc of word .
and as output , we have a tag sequenc .
what i mean by that is that in thi output from the model each word in now given an associ tag so profit , for exampl ha a tag n which if we look at the kei stand for noun .
soar ha the tag v which from the kei mean verb .
at is a preposit .
that's a p and so on and so on .
so the task is to take the sentenc's input and to word by word assign a part of speech to each word in that input .
so why is thi , in ani sens , a challeng problem ?
well , it turn out that ambigu is go to plai a crucial role in thi problem , as it doe in mani other problem in natur languag process .
so , let's look through the word in the sentenc .
if we take prophet , for exampl .
it's certainli a noun in thi context , but profit can , of cours , also be a verb in english .
so if i sai , the compani profit from it endeavor profit is a verb in that context .
let's look for some other top is a verb in thi particular sentenc but it can also be a noun .
if i sai , for exampl , the top on the cake .
forecast is a noun in thi sentenc but it can also be a verb .
quarter is a noun but it ha a much less frequent usag where it can also be a verb .
result is a noun , it can also be a verb .
and so on and so on .
so as a rough estim you might find the word in english on averag can take <num> or <num> possibl part of speech .
and thi isn't onli true of english .
it , it's true in mani other languag , probabl most languag .
in addit to english .
so here is a second exampl of a tag problem .
and thi is the problem of name entiti .
recognit .
again , thi is a veri import problem in nlp .
and actual , in the first program assign for thi cours , you will build a complet name densiti recogn .
so you'll build a model for thi task .
so what is the name densiti recognit problem ?
the problem , in thi case , is to take , again , a sentenc .
a sequenc of word as input .
and now in the output , we're go to identifi name entiti in the input sentenc .
so a name entiti might be a compani , or a locat , or a person .
those ar three veri common entiti type .
and in thi output we've identifi boe co .
as a compani .
wall street as a locat .
and alan mulal as a person .
so basic problem .
take sentenc input and mark up all the entiti in that sentenc in the output .
so again , thi is a veri basic problem in and it's us in , in a wide rang of applic .
you can imagin all kind of case where identifi entiti would be a us .
task .
now at first glanc , thi doesn't look like a tag problem , becaus a tag problem need to assign a tag to each word in the input , and here we alreadi have a segment , an identif of subseg of the sentenc .
but on the next slide , i'll show you how we can map thi problem veri directli to , to a tag problem .
so how doe thi work ?
what i've basic shown you here is the same exampl again .
but where i've repres the segment i show you on the previou slide , ha a word by word tag .
so we have variou tag , na stand for someth that is not part of an entiti .
so , if you notic , we have again , we've tag everi word in the sentenc in turn .
and sever of these word ar na , thei're not part of an entiti .
and then for the compani entiti type .
i have tag correspond to the start of a compani and the continu of the compani .
so if we look at boe co .
here .
we've tag boe as the start as the start of a compnai and cc as the continu of the compani .
and similarli we have wall street , which is a locat .
wall is the start of a locat ; street is the continu .
alan mulal ; the same thing happen again .
so what i've basic shown you here is that we can take the name entiti recognit problem and map it directli to a tag problem , where we ar go to tag each word in turn in the input sequenc .
so a goal is go to be the follow .
we're go to treat thi as a machin learn problem .
and so we'll assum that we have some train set .
some set of train exampl .
and so , thi is actual a set of sentenc taken from on veri commonli us resourc , call the wall street journal tree bank .
and here we actual have close to <num> , <num> train sentenc .
where each train sentenc consist of a full input sentenc , togeth with the unlerli part of speech tag .
and these train sampl have actual been annot by hand .
so human annot have gone through and sentenc by sentenc mark these kind of annot .
so that's a quit labori job , but it doe have the benefit that we have a readili avail sort of train exampl for thi problem .
problem .
so , the wall street journal tree bank wa a veri earli exampl of a corpu of thi form and now there ar mani , mani other copera across mani languag and mani differ genr .
so , given thi train set our problem is go to be the follow , we learn a function or algorithm that will take a new sentenc input and map that sentenc to a tax sequenc .
so we're go to treat thi whole problem as a supervis learn problem .
to develop a littl bit more intuit for how we might develop a model for thi problem , or what kind of inform might be us in develop a model .
i want to talk about two differ type of constraint that plai a role in the part of speech tag problem or actual ani tag problem .
so and i'll call these differ type of constraint local versu contextu .
so , local constraint take the follow form .
if we look at a particular word of english for exampl can , and again we're consid the part of speech tag problem here , can ha a prefer to be a modal verb .
okai so it's , it's much more often seen as a verb in english but it can also see , be seen as a noun .
so a prioriti can ha a bia to have on part of speech over anoth part of speech and we'll refer to that as a kind of local prefer .
the local prefer of word to take on part of speech over anoth .
but we have to balanc these local constraint against what ar call contextu constraint .
which ar that some part of speech sequenc ar much more like than other .
so as on exampl a noun is much more like than a verb to follow a determin .
okai .
so if i have a determin .
the tag is dt .
thi is a word like the or a .
after a determin , i'm veri like to see a noun .
and on thi noun , i'm much like to see a verb .
okai so there is some contextu inform sound text equal to is a much more like than other .
and we'll have to balanc these two type of constraint when we both tag model and it worth rememb that these prefer is sometim in conflict .
so if i take thi sentenc here , the trash can is in the garag .
and if i consid thi word can , then can ha a local prefer to be a modal verb , but is clearli a noun in thi particular context .
and that's becaus the surround word , the surround syntact structur , dictat that thi realli ha to be a noun in thi context .
so , two cours two sourc of constraint , local versu contextu .
and we'll see that we can build a model which actual balanc these two type of constraint .
so , in the last section i describ the tag problem .
and describ how we essenti have a supervis learn problem where we have exampl tag sequenc .
and we want to learn a model , or function , from those exampl tag sequenc .
in thi next section of the lectur i want to describ a method for deriv supervis learn algorithm call gener model .
and as i said befor , thi is a veri import kind of supervis learn model and we'll see it come up at time and time again in the cours .
so in supervis learn problem we have the follow set .
we're go to assum that we have a set of train exampl which i'll write as xi yi for i equal <num> to m .
and each xi is refer to as an input wherea each yi is refer to as a label .
so let's be specif in the part of speech train exampl .
we might have the follow , x1 might be the follow sentenc .
the dog laugh .
and y <num> would be the underli tag sequenc for that sentenc .
for exampl , determin , noun , verb .
we might have x2 is equal to the cat bark , and y <num> is equal to determin , noun , verb again .
and so on , and so on .
so , in the part of speech tag case , each train exampl consist of a sentenc , as the what's call the input , and an entir tag sequenc as the associ label .
and we might have a few hundr , or a few thousand , or mayb even a few ten of thousand of exampl like thi .
so , given these train exampl , and supervis learn problem , the task is to learn a function f , that map input x , to label f of x .
so we want to , for exampl , in part of speech tag , again , learn a function that take a sentenc as input .
and map it to a part of speech tag sequenc as the output .
so , the first kind of model you might consid for supervis learn , is what's call a con , condit model .
and that goe as follow , in a first step in the , the learn step we will actual learn a , a distribut , py given x from the train sampl .
so , we'll have a method that take these train sampl as input and return a distribut py given x as the output .
and in gener thi distribut will have variou paramet which ar estim from these train exampl .
a bit like the , the trigram paramet we saw on the first lectur on languag model .
okai so that's step on .
we learn a condit distribut py given x from the train exampl .
in a second step , for ani test input x , we simpli defin f of x , to be the y that maxim thi condit probabl .
so , in appli thi model that we've learn , we take an input x as the input the model , search through all the differ label y , and return the most like y , under thi condit model .
okai .
so that's a condit model .
it's a veri natur approach .
it's probabl , <num> of the first thing you would think of for these task .
but as i'll show in a second , there is an altern .
so call gener model .
so , in gener model , we , again , assum the scenario where we have train data .
noth chang here .
and the task is again to learn a function that map input x , input x to label f of x and in gener model , we actual do someth slightli differ from what i show you befor , in that we're to learn a joint distribut .
p x y over input x pair with label y .
so rememb on the previou slide we had p y given x as a condit model .
we're not go to do that .
instead we're go to learn a joint model p x given y .
now veri often , thi model take the follow form .
we us bay' rule , or , rather , the rule of condit probabl , to factor thi into two differ term .
p of y , thi is often refer to as a prior .
thi can be thought of as a measur of the prior likelihood of the label y .
how like y is a priori , and thi is a sort of condit gener model .
so thi is the condit probabl of x given y .
the probabl of gener x given that we have y , we'll see soon and we'll see mani time in thi cours that it veri natur often to come up with model of thi kind of form .
so , on interest thing about these joint distribut is thei're quit flexibl .
and thei ar in , in some sens more gener than the discrimin model i show you earlier .
let me just illustr thi through thi import relationship .
so , given i've , i've learn the joint model , i can alwai calcul the condit probabl of ani y given x us bay rule as follow .
so we have p y time p of x given y .
that's the form i've shown you here i'm assum that we're us thi form .
and on the denomin i just have p of x .
like i said , thi is just bay rule , where p of x is deriv as sum over y , p of y , time p of x , given y .
okai .
so the import point here , is that , given a joint distribut , i can easili deriv a condit distribut .
and in that sens a joint distribut is a littl bit more gener than a , than a condit distribut .
so there's actual a lot of back and forth between these two model type .
thi is often refer to often as a discrimin model or rather i should be more precis estim py given x directli is often refer to as a discrimin model or rather i should be more precis .
and we will actual see a lot of discrimin model later in the cours .
p of x given y , sorri p of x and y , is often refer to as a gener model .
and that is what we're go to see in thi lectur .
and there ar pro and con of these two model type .
there's a lot of veri interest research in both of these area and a lot of back and forth between these two model type .
okai , so that's a gener model , we learn thi joint distribut .
the import question though is , how do we appli that to a new test exampl ?
and here's where we see someth interest .
so as befor , we're go to take some input x .
and we're go to defin f of x as simpli the y that maxim the probabl of y given x .
and i can substitut in here us bay' rule , the form i show you on the previou slide , assum here that i have a gener model .
and an import characterist of thi equat is the p of x doe not , vari , with y .
so thi denomin is actual constant with respect to y .
so notic we're take arg max over y we're search for the y that maxim thi entir term .
becaus thi denomin is constant , if we're look for the arg max we can simpli discard it and look for the y that maxim the product of two term , p of y , and p of x , x given y .
and that can be veri conveni becaus from a comput point of view , calcul px can sometim be rather pain .
and so we don't actual need to do that when appli thi type of model .
okai .
so next , we will describ hidden markov model .
which , as we will see , ar an instanc of the gener model approach that i , i just describ .
so , in hidden markov model , the basic idea is the follow .
i have some input sentenc x , which is consist of n word .
so each of these is a word , you know thi could be the dog , and so on and so on .
and i have some tag sequenc , y1 through yn , so thi might be determin a noun and so on .
and an hmm is go to defin a joint distribut of tag sorri , over word sequenc pair with tag sequenc .
so , thi is precis an instanc of the gener model i just show you in that , if we think of a word sequenc as an input and a tag sequenc as a , quot , label .
we now have a joint distribut over these thing .
and thi is a distribut over all possibl word sequenc x1 through xn and all possibl tag sequenc , y1 through yn .
where these two sequenc have the same length .
onc we have defin a model of thi form .
and learn the paramet of thi model from a set of train exampl .
the output from the model is go to be the .
so here i have an input x .
which , again , is a sequenc of word .
the output for thi sequenc , is go to be , the sequenc of tag y <num> through y n , which maxim thi joint probabl .
so here , i'm basic go to have a search over all possibl tag sequenc for the input sentenc .
i'm go to find the tag sequenc that maxim thi probabl .
now , thi in itself is an interest problem becaus the number of possibl tag sequenc grow veri quickli with n , it's exponenti with , with n .
and so brute forc search over all possibl tag sequenc is , in gener , not go to be possibl .
but we'll see it as a veri nice wai around that problem for the class of hidden markup model .
okai .
so , on thi slide i have given a formal definit of trigram hmm .
so here i have the formal definit and on the next slide we'll see an exampl which will probabl help to clarifi thing .
but let's go over thi definit .
i'm go to assum two set .
so v is go to be the set of possibl word in the languag .
so it might contain for exampl there , dog , cat , a , box , and so on and so on .
and i'll us s to refer to the set of possibl tag .
so for exampl we might have determin , noun , verb preposit , adverb and so on .
typic by the wai , we , we might have , i don't know on the order of a few ten of tag .
the , the wall street journal part of speech tag corpu g i refer earlier had approxim <num> tag .
in theori quit bit by corpu g in languag .
but that's not a bad estim .
okai .
so we have these two set .
and then a sentenc is a sequenc of word , x1 through xn , where each xi is in v .
and a tag sequenc is go to be a sequenc of tag .
y<num> through yn plu <num> .
sort add to yn plu <num> where yi is in s for i equal <num> to n and yn plu <num> is stop .
okai so i'm again go to make us of the stop symbol which we saw in the previou lectur on languag model .
it's go to plai veri similar role to what we saw in , in languag model .
so i've just extend .
our definit here so our joint distribut is now the sequenc s1 through sn .
togeth with text sequenc y1 plu yn plu <num> .
so thi for exampl might be the dog bark .
and thi might be determin nn , vb , stop .
so notic i have thi extra symbol at the end of the , the tag sequenc .
so , how do we defin thi joint probabl ?
so i have a , a product of two term here .
here i have a product from i equal <num> to n plu <num> of qyi given yi2 minu yi<num> .
notic that thi is veri , veri similar to what we saw in trigram languag model .
in fact thi , as we'll see veri soon , is noth more then a trigram model appli to tag sequenc .
so that's the first term and the second term is as follow .
i have a product from i equal <num> to n .
of e xi , given yi .
so the e paramet , for exampl , ar the follow .
we might have e of the , given dt .
and that , basic , correspond to the probabl .
given that i have the tag dt , that i see the word the .
so in some sens of the probabl of the tag dt emit , or gener the word the .
okai and i have on term for each word in the sequenc .
so the paramet in the model ar as follow .
i have a trigram paramet .
for everi trigram of tag , u , v , s .
where u and v can be in the set s .
and i'm , again , go to us the start symbol star .
thi is , again , is veri similar to what we saw in languag model .
and s can be ani member of s .
or it could be the stop symbol .
so that's the first set of paramet .
these ar basic .
the trigram paramet .
and secondli , for ani word in the vocabulari and for ani tag s , i have the condit probabl , or rather , paramet .
correspond to the condit probabl of x given s .
these ar often refer to as emiss paramet .
so , two paramet type in the model .
given these paramet i can then calcul the joint probabl of a word sequenc x1 through xn and a tag sequenc y<num> through yn plu <num> as thi product of two term .
so that's a littl , a littl abstract .
let's go through a concret exampl to illustr these definit .
and here i've assum that the sentenc is the three word the dog laugh .
and the tag sequenc is dnv stop .
so d for determin , n for noun , v for verb .
how do i calcul the probabl of thi word sequenc pairedd with thi tag sequenc .
so by the definit on the previou slide is work as follow .
so first we g have a product of q term .
and notic for each tag here dnv stop , i have an associ q term qd time qn time qv time qstop .
and at these point , i'm condit on the previou two text .
so these star symbol ar a veri similar wai to what we saw with languag model , thei ar sort of initi sequenc , sorri , initi symbol the star of the tag sequenc , so if d given star , star and if n given star d i have v given dn and then final i have stop given nv .
so , those ar the q paramet and then in the second part of thi express i have a product of e paramet on for each word .
in the input sentenc .
so if e of the given d , for thi word here , e of dog given n , so thi word here .
then final e of laugh given v for thi word here .
and intuit , these paramet correspond to the probabl of first gener the word there from there , d .
then gener the word , dog .
given the underli tag as n .
and final , gener the word , laugh .
give the underli tag as v .
.
so , rememb , the name of these .
these model is , hidden markup model .
let's just think a littl bit about why thei actual get thi name , and it'll also help us develop a littl bit more intuit for these model .
so thi product , of q term , is essenti a prior probabl , over tag sequenc .
so thi is the probabl of y1 , through y n plu <num> .
rememb in the noisi channel model , we usual had p xy .
is equal to p of y , time p of x given y .
so these cue term correspond precis to p of y , where y is now an entir text sequenc .
and thi is a second order markov chain .
so we've , basic , just appli the idea of markov model to thi problem of defin p of y .
thi is exactli the same as the form we saw for triagram languag model in the previou part of the cours .
okai .
so if we look at thi express , thi is actual the probabl of x1 through xn condit on y<num> through y n plu <num> .
where we're basic assum that each word xj is chosen depend onli on the valu for yj .
so we've made some fairli strong independ assumpt here , that each word onli depend on it underli public speech tag .
so thi lead to the name hidden markov model .
in some sens thi is a markov chain that is hidden in the sens that you can think of a gener process where we first choos a sequenc of tag under thi model .
and then for each tag we gener an associ word x1 up to xn .
these xj ar observ .
in these , tag sequenc , the ys ar unobserv .
and the problem on a test exampl is the follow , i receiv a sequenc of word , x1 through xn and i have to find the most like sequenc of tag underli those word , y<num> through y and plu <num> .
so , i have to uncov the most like set for that hidden sequenc .
okai so we've now introduc hidden markov model , and in the previou section i gave basic definit of those model .
in thi next section i want to talk about how these model ar learn from a set of train exampl .
in particular how those q and e paramet we saw , the trigram paramet and the emmiss paramet can be estim from a set of train exampl .
and thi is actual quit straightforward .
so let's take the q paramet as a start and let's sai for exampl we want to estim the q paramet correspond to the probabl of see thi tag vt given that the previou two tag ar these <num> tag here .
and , we can us our train corpu , of cours , to induc count of differ tag sequenc .
so , what i've shown you here is essenti a linear interpret method for estim .
veri similar to the method we saw for trigram languag model in the last , lectur in thi cours .
so , here i have an ml estim , maximum likelihood estim .
actual , thi is the trigram maximum likelihood estim .
and notic on the denomin , i have the number of time i've seen dt follow by , follow by jj , that's what we were condit on here .
and on the numer , i have the number of time i've seen thi sequenc of <num> tag .
so , thi is the trigram count , thi is the bigram count , and i've taken the ratio of these <num> term .
these count can be taken directli from those train sampl which consist of entir sentenc , togeth with their part of speech sequenc .
similarli here i have a bigram maximum like estim .
so again it's a ratio of count , number of time i've seen jj .
it's the second tag in thi sequenc we're condit on .
and then on the numer , the number of time i've seen jj follow by vt .
and final , i have the unigram ml estim .
our numer is number of time i've seen the tag vt .
and our denomin is the total number of tag i've seen in my train corpu .
so we have a linear ineterpol mthod .
we have these three smoothign paramet .
exactli as we saw for languag model , these lambda satisfi the constraint that thei sum to <num> , and thei're all greater than equal to <num> , and thei can be estim from data in a veri similar wai .
actual , an ident wai to , to what i show you for the languag model problem .
so , thi is basic a direct applic of the method we saw for languag model .
okai so secondli we've got to consid these omiss paramet for exampl e of base given vt is the probabl that the tag vt emit the word base .
and again , we can deriv maximum likelihood estim that take a veri intuit form .
so , on the denomin , i have the number of time i've seen the tag , vt .
and on the numer , i simpli have the number of time that i've seen the tag vt pair with the word , base .
and we simpli take the ratio of these <num> term .
so it's blindingli simpl estim .
so that's basic it , although there's on problem we need to worri about and that's the fall a .
so e of x given y is go to be zero for all y if x is never seen in the train data .
so what am i sai here ?
i'm basic sai , for ani word x that is never seen in he train data .
we're go to estim all of the submiss paramet to be equal to zero .
and , in some sens that mean we have no us inform about that word .
and , thi is actual go to be a real problem in these model .
but i'll describ a simpl solut to it .
why is thi import ?
let's come back to an exampl from befor .
and let's sai for the sake of argument that thi is a test sentenc .
and so , i would appli my tag model to thi test sentenc and try to find the most like sequenc of public speech tag for that sentenc .
then , we're veri like in a given test sentenc to encount word which we've never seen befor in train .
so mulal for exampl , quit possibl ha never been seen in our train data .
or mayb even top is a rel infrequ word that mai not have been seen in our train data .
an d actual if you look at the statist of english for exampl even with sai a million word of train data for tag , you will frequent encount word in testat , which you've never seen in train data .
what that mean is , let's take thi particular exampl , so e of mulal given y is equal to <num> for all tag y .
and that mean if thi is defin at our sequenc x1 through xn is equal to is equal to thi .
to easili verifi it the p of x1 , xn , y1 , through yn plu <num> is equal to <num> for all tag sequenc .
y<num> to yn plu <num> and that's becaus ani text sequenc is go to involv an emiss paramet like thi which is equal to <num> and so all of these probabl can be taken to be <num> .
and of cours at thi point , the model is complet broken down becaus all of my tag sequenc have probabl zero .
and in fact if i think about try to find the most like tag sequenc , rather the aug max of all tag sequenc of thi express we just have a tie where all tag sequenc get the same valu of zero .
okai .
so , so thi model is broken but there is a veri simpl fix .
so the common method which is us is as follow .
so in the first step we split the vocabulari into two differ set .
we'll do thi as follow .
i'll defin so call frequent word , to be ani word occur greater than some threshold .
so , for exampl , a typic valu might be <num> .
we defin a frequent word to be ani word .
occur five time or more in train and we defin low frequenc word to be all other word .
so thi is go to includ word seen less than five time in the train data and also word which seen zero time in the train data .
so all those new word that we see on test data exampl which have never been seen in train .
so , that's the first step .
in the second step , we're go to defin a map where we map low frequenc word .
so , all the word in thi set into a small finit set , typic depend on spell featur of those word .
i'll give an exampl of thi in the next slide .
but the basic idea is to take thi veri larg set of low frequenc word .
there might be mani , mani word that fall into thi class , and for each word just map it into on of , sai , <num> or <num> new word , thi , thi small finit set .
let , let me be concret by , by give an exampl .
and thi is from the paper by bikel and other from <num> , specif on the problem name entiti recognit .
so thei defin a map from these low frequenc word .
to roughli , think it's roughli <num> or <num> differ possibl word class .
and these were chosen by hand and us some intuit and insight about the problem that thei were attack which wa name entiti recognit .
so thi partit of the low frequenc word into these differ class ha been chosen to try to preserv some us inform in the name entiti recognit task .
so for exampl , if thei see an low frequenc word consist of two digit .
thei map thi to a class call twodigitnum .
four digit , fourdigitnum .
let's look at some other .
a word which is all capit get map to a word call allcap .
we have ani , anyth which is the first word of a sentenc , no period is map to a symbol call first word .
but that's essenti becaus even though that word is capit , the capit inform is , is not that us in that case becaus all word at the start of a sentenc ar capit .
ani word whose initi letter is capit there's other word not capit thi to initcap and so on and so on .
so let's continu thi exampl .
and let's come back to .
thi is the origin form , of our train data .
so again , i'm look at the main densiti problem here , and assum thi is a train exampl , where somebodi ha , actual annot the differ entiti , in thi particular exampl .
so befor the transform , the data look like thi .
but here's what the data look like after the translat transform .
and there ar a number of low frequenc word here which have been map to these kind of pseudo word .
these word in a small set of categori that preserv spell featur about the word .
so , profit is map first word .
boe is map to initcap , you have lowercas initcap , and so on and so on .
so you , as a human , can look at thi data , and you can see that even though we've discard the precis ident of these word , we've still preserv quit a bit of inform .
and most importantli , inform about the underli spell of those word , which will be us in the name entiti problem .
okai , so we perform thi transform to both our train exampl , and also our test exampl .
and then we simpli build a hidden markov model tagger , us data in thi format .
so , we now have paramet such as the probabl of see first word , given that the tag is na , or an emiss paramet specifi the probabl of see init cap .
given that i have the tag sc , and so on and so on .
so , we've essenti finess thi problem of low frequenc word , or word in test state and never seen in train , by close the vocabulari .
by map those low frequenc word to a much smaller set of <num> word which preserv inform about .
about the spell .
the nice thing about thi is that it's a veri simpl method .
onc we've done thi we can simpli read off straight forward maximum likelihood estim of these differ paramet .
the downsid is that it's clearli heurist in that some human expertis ha to go into the design of thi map to a small number of word class .
okai .
so , so far , we've given basic definit of hmm's , i've just describ how to estim the paramet of an hmm .
so , you basic know everyth there is to learn of a hidden markov model from a set of train exampl .
the final critic problem is how do we appli thi model to new test data sentenc ?
and thi is where we see the viterbi algorithm , come into plai , which is a veri famou algorithm .
it's an exampl of dynam program , a critic idea from algorithm and comput sscienc .
and it's realli a rather beauti wai of appli these model to test data sentenc .
so , what's the problem that we're left with ?
it's the follow .
so , we have some input sentenc , x thorough xn .
for exampl , it could be the dog laugh .
and thi is a new sentenc , it's , it's a new test data sentenc .
and , of cours , i would like to map thi to a sequenc of tag , for exampl , d , n , v .
and , to do that , i have my model .
so , thi probabl is defin by an hmm through the paramet estim from a train corpu .
and the task is to find the highest score tag sequenc under thi model for thi particular sentenc , x1 through xn .
okai .
so again , s is go to be defin to be the set of possibl tag .
let's sai , for sake of argument , we have these three tag here , d , n , and v .
and so , we're go to search over all valid tag sequenc where yi is in s for i equal <num> to n , and yn plu <num> is equal to stop .
we're go to assum that p take the form i show you for a trigram hmm .
so , thi is thi structur again .
so , just to recap , i have a product of q term , from i equal <num> to n plu <num> .
rememb , yn plu <num> is alwai equal to the stop symbol and then , i have a product of e term .
so , thi is go to be term such as e of the given that i have determin as the tag .
so .
first critic thing to realiz and a critic motiv for the viterbi algorithm is the observ that brute forc search is go to be hopelessli ineffici in thi scenario .
and by brute forc search , i mean , search through all possibl tag sequenc where we simpli enumer each tag sequenc in turn .
let me illustr thi with an exampl .
sai , again , we have the dog laugh as the input , so that is x1 through xn in thi case .
and let's sai that s , the set of possibl part of speech tag , is the size just <num> .
so , we have d , n , and v , for exampl .
then , i can list all possibl tag sequenc .
so , here it goe .
we have d , d , d stop , d , d n stop , d , d v stop , and d n d stop , and so on and so on .
okai , so each valid tag sequenc simpli ha on of these three tag , three possibl tag , at each of these posit follow by the stop sign .
and so , we could conceiv just go through each of these tag sequenc in turn .
for each tag sequenc , evalu the valu for p .
so , for exampl , we might find that thi is <num> , thi is <num> . <num> , thi is <num> . <num> and so on .
i'm just make these number up .
so , we could just simpli list all these possibl tag sequenc , appli the model or the form of the model to calcul these probabl and then , return the highest on , just , sai , thi on for the sake of exampl .
the problem with thi approach is that the number of possibl sequenc grow veri quickli with the length of the sentenc .
so , in thi particular case , i have <num> to the power of <num> possibl sequenc becaus there ar <num> choic here .
choic of the first tag , second tag , and the third tag and each of those choic have <num> possibl .
so , in the gener case , we have the size , number of tag , size , the set s rais to the power n as the number of possibl sequenc .
so , the number of possibl sequenc obvious grow exponenti , quickli , with respect to the sentenc length n .
and for ani appreci valu for n , sai , n is equal to <num> or <num> or <num> , thi number quickli get veri , veri out of control , and it becom hopelessli ineffici to do brute forc search .
so , if we go back to thi slide , the critic thing we're go to leverag in get around thi problem is that our probabl distribut take thi form and critic , these trigram paramet onli depend on subsequ of length <num> .
so , thi model ha a particular structur , and that will actual allow us to do search for the most like tag sequenc , solv thi problem much , much more effici than brute forc search .
okai .
so now , let's give some definit underli the viterbi algorithm , which will be the effici algorithm that solv these problem with brute forc search .
so , i'll defin n to be the length of the input sentenc .
so , i have some input sentenc x1 , x2 , x3 , up to xn .
and then , a central definit is go to be thi function r .
so , r is go to take a sequenc of tag as input .
so , we alwai have star , star as y minu <num> and y0 .
these ar the start symbol .
and then , we could have some sequenc of tag for y1 , y2 , y3 and so k is the length of the sequenc .
in thi case , k equal <num> .
so , r take in a sequenc such as star , star , d , n , v and basic calcul it probabl under the hmm us what is basic a , a truncat express .
we just have the first k term , a truncat version of the express we saw for full hmm .
so , i have product i equal <num> to k , we have these q paramet , and a product from i , i equal to k of these admiss paramet .
and so , thi is essenti a probabl of just the sequenc , which is of length k , k equal <num> in thi particular exampl .
now , we're go to defin a dynam program tabl .
so pi  k , u , v , is go to be the maximum probabl of a tag sequenc end in tag u , v at posit k .
to be complet precis , we need a coupl of other definit .
so , i'm go to , to defin s sub k , for k equal minu <num> to n , to be the set of possibl tag at each posit k .
so , again , if we think about thi input sentenc , x1 , x2 , x3 , up to xn and let's sai , s is all set of possibl tag , sai , d , n , v , p .
then , at ani of these posit <num> , <num> , and so on , up to n , we have <num> possibl tag , d , n , v , p .
and at posit <num> and minu <num> , we have a singl possibl symbol , star and these definit reflect that .
so , s minu <num> is equal to s0 , star , so the set of possibl symbol or tag at posit minu <num> and <num> , which is the star start symbol .
and for ani k and <num> to n , s sub k is equal to s , so it's the full set of tag .
so , thi notat will just make thing cleaner when we get to full definit .
okai .
so , pi  k , u , v , k can take ani valu in <num> , <num> , up to n , u take ani valu in sk minu <num> and v take ani valu in sk .
so , thi is go to be the maximum probabl of ani tag sequenc end in tag u , v at posit k .
more precis , pi  k , u , v is here , i have a max of all sequenc with k tag y1 through k proceed by y minu <num> , y0 .
these ar both star , alwai assum to be star .
such that yk minu <num> equal u , yk equal v , of thi r function i've shown you here .
so , that's the formal definit .
let me give you a particular exampl , on the next slide .
so , let's number these word .
so , <num> , <num> , <num> , <num> , <num> , <num> , <num> , <num> .
and let's sai , we consid the entri pi  <num> of p and d , what doe that correspond to intuit ?
so , i'm go to fix these <num> tag here to be p and d , and let's just assum that our set s is equal to d , n , v , p .
so , if we look at ani of the preced posit , we have <num> possibl tag at each posit .
so , we have all of these possibl , let me just write these out .
and we alwai have star , star as these two start symbol .
and so , there ar mani possibl differ sequenc of tag which end in p , d at posit <num> and <num> .
for exampl , i could have d , n , v , p , p , p , d , that's on possibl sequenc .
and there ar mani other , each of them will have a probabl , which is calcul by multipli to , multipli togeth the trigram probabl , and the emiss probabl , these q term and these e term .
and pi  <num> , p , d is go to be the maximum probabl for ani of these tag sequenc which end in tag p and d at posit <num> and <num> .
so now , let's give a recurs definit .
the critic idea here is go to be that these pi  valu can actual be calcul effici us a recurs definit , which is given on thi slide .
so firstli , the base case is go to be , let's sai , that pi  of <num> star , star is equal to <num> .
thi basic just reflect the fact that everi text sequenc start with star , star at it veri begin .
and then , we have a second definit which is more interest .
thi is the recurs definit .
so , it's sai for ani valu of k , for ani u and sk minu <num> , and v and sk .
so rememb , k can take ani valu , valu in the rang <num> to n , u is alwai go to be in the set sk minu <num> so thi is go to be on of the tag which ar possibl at posit sk minu <num> .
and , similarli , v can take ani valu in sk .
rememb , sk is the set of possibl tag allow at posit k .
and so , what do we sai ?
we sai that thi is equal to the max of all tag at posit k minu <num> .
so again , thi is the set of possibl tag at posit k minu <num> .
we take a max over that .
we comput pi  of k minu <num> , w , u .
and then , we have q of v given w , u and we have e of xk given v .
on the next slide , i'll illustr an exampl which justifi thi recurs definit .
it is recurs though becaus thi pi  valu depend on a set of previou pi  valu , the pi  valu at posit k minu <num> .
and so , we have a recurs definit where we're defin the pi's in term of other pi's , more specif , pi's at posit k minu <num> .
so , thi is rather abstract .
let me describ exactli how it is justifi .
okai .
so , to justifi thi recurs definit , i am go to us an exampl .
so , we'll us thi sentenc we seen befor and let's see how we can calcul the valu for pi  , <num> , p , d .
so , recal thi is go to be the highest probabl of ani tag sequenc end in tag p and d at posit <num> and <num> .
so first , let's see how thi express work out when we instanti it for thi particular pi  valu .
so , i have a max of a w in , in our case , <num> , so we're look at s5 .
thi is simpli the set of possibl tag at posit <num> and that is equal to the set of tag s , which is equal to d , n , v , p .
so , we have a max over those four possibl tag , and then i have pi  of <num> , w , p and then i have q of d given w , p .
then final , i have e of xk , k7 , in thi case , thi is the seventh word in the sentenc , the given d .
so , thi is the express which we can us to calcul pi  , <num> , p , d .
so , how could we justifi thi ?
their kei insight is that , if we think of ani tag sequenc end in p and d at posit <num> and <num> , it ha to have includ some tag at posit <num> , and thi max is explicitli search over these differ possibl valu for thi tag at posit <num> .
for each possibl valu of the tag , we multipli in the pi  valu and then a q valu of d given in the previou two tag and then final an admiss valu of the given d .
the reason we do that is , if we fix a particular tag , sai n at posit <num> , then the follow properti hold .
the highest probabl path go through tag n , p , d , at posit <num> , <num> , and <num> ha to have probabl pi  <num> , n , p time q of d given n , p time e of the given d .
so , that's the highest score for ani tag sequenc end in n , p , d at posit <num> , <num> , <num> .
why is that ?
well , the highest score tag sequenc end in n , p , d at posit <num> , <num> , <num> ha to includ the highest score tag sequenc end in posit n and p ar posit <num> and <num> , that's what thi probabl is , and then we multipli in the trigram paramet q and the emiss paramet e to take into account the probabl for thi tag and thi emiss probabl .
so , that's the highest probabl for ani tag sequenc go through n , p , d .
and we simpli do a search of all possibl tag at thi previou posit , and take the max , and that give us the highest probabl tag sequenc end in p and d at posit <num> and <num> .
so here is the viterbi alogrithm , which put these idea togeth .
so the input into the algorithm is a sequenc of word x1 , x2 , up to xn .
the output is go to be the maximum valu for ani sequenc of tag y1 through yn plu <num> of p x<num> through xn y<num> through yn plu <num> .
now , notic for now , thi is simpli go to return the maximum probabl for ani sequenc .
and of cours , in realiti , we want to find the tag sequenc that actual achiev thi max .
we want to calcul the augmax , in fact but we'll see veri soon how to do thi , for now , we'll just worri about the max and we'll see on the next slide how a veri simpl chang to the algorithm will allow us to actual comput the augmax .
okai , so we have an input sentenc , x1 through xn .
we have these paramet q and d , which have typic been estim from theta and then the algorithm proce as follow .
initi step is to set pi  <num> , star , star equal on .
rememb that wa the base case of the recurs that i show you .
and then , we have these definit where s sub k is the set of possibl tag at posit k .
so , s minu <num> is s0 is equal to the star symbol .
reflect the fact that i can onli have the star symbol , posit minu <num> or <num> .
and everywher els , sk equal s .
where thi is the set of tag .
for exampl , we might have d , n , v , p as the set of possibl tag .
so , the algorithm is go to proce to fill in these pi  valu , essenti left to right .
so we go in order for k equal <num> to <num> , right the wai up to , to n .
and at each point we rep , we appli thi recurs definit .
okai , so for ani posit k .
we consid all possibl tag pair , u and v , possibl posit k minu <num> and k .
and we simpli comput pi  k uv us thi max over ws at sk minu <num> , recal thi is a set of tag .
possibl tag the posit k minu <num> .
i am go to have thi pi  valu time the q valu time the c valu .
and then the onli final step rememb we're return the maximum probabl for ani tax sequenc .
the final step is to multipli in the stop probabl .
okai , so we have a max of all possibl u v pair at the veri end of the sequenc at posit n .
and then we make sure to multipli in the stop probabl , q is stop given u v .
thi is just sort of a peski littl bit of bookkeep we need to rememb about .
we need to worri about the stop symbol , that's what thi final step doe , and so that is it .
that is the viterbi algorithm .
okai , so , as i said , our real goal is the follow to take as input a sequenc of word , x1 through xn , and have thi output augmax , y<num> through yn plu <num> , p of x<num> through xn , y<num> through yn plu <num> .
so here i've actual shown the full viterbi algorithm , which make us of a crucial idea from dynam program call backpoint .
to actual allow us to recov the augmax , to recov the highest score tag sequenc .
and it's actual a , a veri small modif to the algorithm i just show you .
so the algorithm ha the same input .
again , it ha the same initi point .
and it's go to make us of these pi  valu in the same wai as befor and the same definit here .
and we again , proce from left to right k equal <num> to n , v , u , and v .
we calcul the pi  valu at each point .
and the onli chang to the algorithm is that in addit to recov to record pi  at each valu of k , u , v .
we also store a back pointer which i'll call bp of k , u , v and that is the augmax .
so , that's actual go to record which tag achiev thi max .
which tag wa most like at posit k minu <num> , given the fact that i had u and v at posit of k , okai ?
so thi is just an addit piec of bookkeep , record the augmax of each point .
so onc we've fill in these pi  valu and the backpoint valu we can actual find the highest score tag sequenc by go backward to the sequenc at the end of thi outdoor about .
so here's then how thi work , so after thi loop is complet i sai yn minu on yn is the uv pair that maxim pie and uv time q of stop given u , v , okai so if i found the last two tag in the sequenc and then i go backward through the sequenc at each point sai that yk is the backpoint of k plu <num> yk plu <num> yk plu <num> .
okai , so thi is just unravel the augmax's to find the highest score tank sequenc and that's what we find in return .
so that is it .
thi is the full viterbi algorithm , which now make us of backpoint to actual recov the highest probabl and sequenc for an input x1 through xn .
let me just briefli talk about the runtim complex to thi algorithm .
it is x , actual order n , time number of tag cube .
so it's linear in the length of the sequenc and it's cubic in the number of possibl tag in the sequenc .
how do we get that complex ?
well , so if we look at thi , we have , basic n time number of tag squar possibl valu enter thi loop .
okai , so we enter thi , thi loop thi mani time , and at each point we have to do a search over s possibl tag .
and so , we get an extra factor of s there and that's how we get the n time s cube runtim for thi algorithm .
so it is critic linear in the length of sentenc .
and that is an actual enorm gain over brute forc .
which had complex d number of tag rais to the power n .
we , we've gone from an exponenti time algorithm to actual a linear time algorithm in n .
and someth which is cubic in the number of tag it ha , so that's a dramat improv .
so , to summar , we have develop a full approach to the tag problem base on hidden markov model .
let's just go over some pro and con of the approach i've shown you .
on big advantag is the hidden markov model tagger ar veri , veri simpl to train .
it's simpli a matter of compil count from the train corpu in the wai i describ earlier and then deriv these simpl maximum likelihood of estim will be simpl lineal interpol estim .
thei perform rel well , so , for exampl , for the name entiti recognit task in the work that i cite by beckel and other from the late 90s .
thei perform over <num> accuraci in term of recov these name entiti .
so , that's a pretti good level of perform .
the main problem with these model is that as i said , we have thi problem of estim these paramet e of word given tag .
and we saw a rather heurist method for deal with low frequenc , low count word by group these word into differ class , depend on their spell .
and that's realli a black heart .
it take a lot of human intuit , human intervent in the problem .
it's a , it's a rather clumsi approach to thi particular problem and it becom increasingli difficult if the word that ar input to the model ar more complex .
later in the cours , we'll see other tag task where thi approach realli get out of hand .
and so , later in the class , we will see altern tag method , which ar slightli more complex than hidden markov model , but i think have a much more satisfactori solut to thi particular problem .
okai , so in thi next portion of the class , we're go to look at the pars problem in natur languag process .
and in thi lectur , we're go to discuss context free grammar , which ar a veri import formula for natur languag pars .
and the pars problem is of great import .
it ha relev to mani applic in nlp .
and it goe back to fundament idea in theoret linguist which ar then appli in comput model .
so in thi final segment i want to talk more about the ambigu problem , and give you some exampl of how ambigu come up in the kind of grammar that i've just shown you .
okai , so the first sourc of ambigu we're go to consid is part of speech ambigu .
and thi is actual someth we saw with the lectur on part of speech tag earlier in thi cours .
it correspond to the observ that mani word in english can take multipl possibl path of speech .
so here's on exampl , the word duck can be a singular noun but it can also be an intransit verb .
so it's ambigu for it path of speech .
these type of part of speech ambigu frequent lead to multipl possibl pars structur for a particular sentenc .
and i've shown you , <num> exampl of a third phrase structur for the string , saw her duck with the telescop , down here .
so in thi first structur , duck is a singular noun .
and thi structur basic correspond to the interpret where somebodi is see the duck us the telescop to see the duck .
let's look at thi second structur , in thi case , we have duck as an entransit verb , and actual we have , her duck , as a sentenc embed within thi verb phrase .
it's in fact , the sentenc is an argument to the verb soar , form a vp here .
thi interpret basic correspond to the interpret where i am see somebodi duck us the telescop to see that person duck .
so , here i have a real duck in the world , and here i have somebodi duck .
can't see , see these two differ interpret .
so that's our first sourc of ambigu .
part of speech ambigu .
the second sourc is preposit phrase attach .
i'll just recap , we had thi exampl from earli on in thi lectur , where i have the sentenc , i drove down the road in the car . and i notic that we had two possibl structur for thi sentenc , on where in the car is modifi drove , and thi correspond to the natur interpret where i'm drive in the car .
but as we said there is a second , somewhat crazi , interpret where the road is actual locat in the car , and in that case , the preposit phrase is actual underneath thi noun phrase .
so i want to give a second exampl of a preposit phrase attach ambigu .
which i think illustr an interest properti .
so here we have a sentenc .
john wa believ to have been shot by bill .
and .
the most plausibl interpret ha bill do the shoot , so thi preposit , preposit phrase by bill modifi shot essenti .
okai , so somebodi believ that bill shot john .
john .
there is actual a second interpret here , which is much less intuit , at least for human , which is that by bill , thi preposit phrase , modifi their , their belief .
so let's just actual paraphras these two interpret .
the first is that bill shot john .
somebodi believ that bill shot john .
and the second interpret ha bill believ that john .
ha been shot .
so thi preposit phrase , by bill , could modifi shot , correspond to the read that bill ha been do the shoot ; or it could modifi , believ , where bill is do the , the believ .
what is interest about thi exampl , is that it show that there is a veri strong prefer by human for preposit phrase to modifi the most recent verb .
so , both of these interpret bill do the shoot or believ , ar quit plausibl as state of the world .
so us a priori belief about what is like in the world , either of them look quit plausibl .
and yet , human have veri strong prefer to the shoot .
interpret .
and the reason behind that is that there's a strong prefer for preposit to modifi the most recent verb in a sentenc .
so let's move on to anoth exampl of ambigu us the grammar i just show you .
and thi concern noun premodifi .
so i think earlier in these slide , i show you thi structur .
or someth veri similar to thi structur .
so , here we have the sequenc of word the fast car mechan .
and actual , here we have an n bar over fast car , and thi structur correspond to the interpret , where we have a mechan who work on fast car , okai ?
so , it's a mechan who whose specialti is fast car , for exampl .
okai , becaus we have fast car as an entir sub phrase premodifi mechan .
but under the grammar , i show you , we have a second structur which i've shown you here and in thi case we have a car mechan who is fast so thi adject modifi thi entir sub screen car mechan , okai so as a car mechan who in addit is fast so <num> ambigu an ambigu here is an mechan who work on fast car or it is a car mechan who is fast and we end up with <num> differ structur .
thi kind of ambigu within noun premodifi is absolut preval and you see it everywher in sentenc in english and other languag .
so in thi lectur i'm first go to give an introduct to the pars problem .
we'll then describ context free grammar .
i'll then give a veri brief sketch of how we can appli context free grammar to develop a model of the grammat structur seen in english .
and final , i'll focu on ambigu , and describ some exampl of ambigu structur .
ambigu is an extrem preval problem , as we've seen .
in gener , in languag process .
and it's certainli a veri preval problem in natur languag pars .
so here , in a nutshel , is a definit of the paus problem .
so as input we take a sentenc , for exampl , boe is locat in seattl , and as output we ar go to produc an object call a pars tree , and so the pars tree is a tree structur with the word in the sentenc at the leav of the tree .
so boe is locat in seattl , you see the leav here .
and we see the tree ha label on these intern node such as np , pp , vp , s and so on and veri soon we'll describ exactli the role that these differ label plai .
but at a veri high level , we now have some kind of hierarch decompositoin , of the sentenc into thi assoc tree stuctur .
so the pars problem ha it's root in theorit linguist .
and it realli goe back to the start of modern linguist , larg due to chomski in the 1950s .
and on book i highli recommend if you want to learn more about sort of the root of modern linguist is a book by chomski , i believ from <num> call syntact structur which is a huge influenti book , it's realli a beauti book .
and sinc then , there ha been an enorm amount of research and linguist on syntact structur of languag look at a wide rang of formal , i've list a few here , for exampl , lexic function grammar , lfg , head driven phrase structur grammar , tree adjoin grammar , categori grammar and the formal we're go to concentr on todai is formal of context free grammar which ar realli fundament and , in some sens , form the basi for all these modern , modern formal .
so as it turn out , we're again go to treat the pars problem as a supervis machin learn problem .
and so we're go to assum that we have train data consist of sentenc , pair with their underli depositori .
so here , i actual have a depositori .
so , thi is on exampl actual taken from a resourc , call the penn wall street journal treebank and i don't expect you to read everi word of thi .
we have a sentenc around <num> word in length .
and we actual have a full syntact structur for that sentenc .
so where did these syntact structur come from in thi train data ?
thei've actual been annot by hand .
so , the penn wall street journal tree bank wa on , a veri earli exampl of a resourc .
we call it a tree bank .
treebank is a collect of train data consist of sentenc pair with pars tree .
and the penn treebank had around <num> , <num> sentenc .
thi is , think roughli , on million word of data .
and thi ha been annot by hand .
so human have actual gone through , sentenc by sentenc .
thei have an understand of the underli linguist theori .
and thei have annot these full pars g structur for each of these sentenc .
now that clearli is a veri labori task , but the net result is that we do have a larg amount of train data from which we can train a supervis learn model .
so , a usual setup , for exampl , is to take some portion of the entir data set as a train sampl , mayb <num> , <num> sentenc might be typic , and to take some portion as a test sampl .
and to somehow train a model that aim to take sentenc as input .
output and produc pars structur as output , we train that on the train exampl , we can test the perform of model on thi test exampl so we can get an accur measur of how well the model is actual perform so let's now give a sketch of the inform that is repres by these pars tree structur .
again focus on a veri simpl exampl , the sentenc the burglar rob the apart .
and again we will go through a number of level of represent in these structur .
okai .
so the veri first level if we look at the level abov each word in the tree , we see that we basic have the part of speech for each word .
and so we have dt here for determin , n here abov burglar , the noun , v abov rob , for v , and so on and so on .
so thi veri first level of the tree .
simpli encod the part of speech text sequenc for the input sentenc in exactli the same wai as we saw with the part of the speech text sequenc in the previou section of thi cours on part of speech check .
so that's the first level , if we look a littl higher , we're go to start to see some hierarch group of word into phrase or what ar often call constitu .
so if i take ani node in thi tree , take , for exampl , np , then n , np node , here .
it indirectli domin a sequenc of word .
so , thi domin the two word there and burglar , in thi case .
and that mean that thi particular substr , the burglar ha been identifi as a constuant or phrase of type np .
np actual stand for noun phrase .
later in thi class we'll talk extens about what these differ categori ar .
but as you can see the main point here is that we get thi , thi heirach group .
thi np here , we have a second np , again thi interact domin a substr in the sentenc .
in thi case , therefor follow by apart .
and so we identifi the apart as anoth np .
if we go a littl higher in the tree , thi vp domin the substr , rob the apart .
and so we know that thi substr is of type vp , that stand for someth call a verb phrase .
and then final at the top level of the tree we have s , which stand for sentenc .
and that s node pin directli domin the entir sub sequenc so that domin the burglar at the apart so we know that thi substr is an s .
so final , and in some sens , most importantli , these pars tree encod veri import grammat relationship within a sentenc .
so let me again come back to thi exampl to illustr thi .
so let's look at thi littl tree fragment i have up here .
so thi is in some sens a templat .
whenev i see a structur like thi so i have some word here with a v abov it , with a vp abov that , and an s here , and an np off to the left .
i can simpli read off the fact that i have a subject under thi node here .
and i have the verb here and these two thing stand in the subject verb relationship .
so let's appli thi to that tree we can appli thi templat and see that rob is a verb and moreov the burglar is the subject of that verb .
so these tree make thi particular relationship subject verb complet transpar .
if i can recov thi pars tree i can simpli read off or i can identifi fragment of the tree where i see thi templat and read off relationship such as subject verb .
let me give you a second import relationship .
so , whenev i see a substructur of the follow form .
so i have some word here , and i see an np to the right under here .
so , thi is actual go to be a direct object .
and thi , sorri , is go to be the verb .
and so whenev i see thi configur , i can read off a relationship between a verb and it direct object .
so if i appli thi to thi particular exampl , i can see that the apart is the direct object of thi particular verb .
and there're mani other templat correspond to mani other grammat relationship seen in english or the languag that we're look at .
so in short , these syntact structur allow us to read off differ grammat relationship .
and that's crucial becaus it allow us to identifi who did what to whom .
if i want to identifi who did the rob , i need to find the subject .
if i want to identifi what wa rob , i need to find the direct object .
object .
now in thi exampl and mani other exampl in thi lectur , i'm go to us realli rather simpl , short sentenc and so thi might almost look like a trivial problem to you .
but what i want to emphas is that when you get longer sentenc , sai <num> , <num> , <num> , <num> word in length , it is a veri difficult job to recov these grammat relationship .
so these pars structur ar invalu in reveal grammat relationship , such as subject , verb or verb , direct object .
the final thing i want to talk about is on , veri direct applic of these syntact structur just to give you a littl bit more intuit about how thei might be us .
and that is the problem of model differ in word order between differ languag .
specif , for the problem of machin translat .
so let's sai for , for the sake of argument , we're try to build a machin translat system , which take japanes as input , and produc english as it output .
now , there's someth interest about these two languag , in that , english is pretti rigidli it , english's word order is pretti rigidli subject fi , the , verb , follow by object .
so for exampl , i can sai ibm bought lotu .
thi of cours is the subject , i have a verb , and then i have an object , so i have thi order by far the most frequent word order in english .
on the other hand if we look at japanes , it is actual predominantli subject object verb .
and so thi particular sentenc would be ibm lotu bought .
here i have a paraphras of the japanes .
thi is the word order that we would see .
if you're go to build a machin translat system you need to worri about the differ in word order and obvious take the japanes's input , and not onli translat each word but also move those word so thei're , so that thei're in the correct order in english .
again , thi is a veri simpl exampl , and so thi might seem like a trivial problem , but thing quickli get out of hand when we have longer sentenc .
so , here's an exampl .
so , the english here is , sourc said that ibm bought lotu yesterdai .
and you can sort of see that everyth's scrambl and sort of fall in thi kind of word order .
so , said is now at the end of the sentenc .
so , i have sourc , thi is the end of the subject .
and i have the verb now at the end of the sentenc , wherea it wa in the second posit here in english .
and if i look at thi subclaus here , that's basic translat as thi paraphras down here , and notic that the word that is at the start of the phrase in english .
it goe to the end in japanes .
notic again , that bought come in the second posit in english , or at least it come between subject and an object , but it's after these two thing here .
so i have subject , object , verb , and so on , and so on .
so basic , thi kind of reorder ha been appli recurs in a sentenc .
so if you're try to model differ in word order between these two languag , english and japanes , it can be quit difficult becaus of the complex i show you on the previou slide .
but , much of that complex can be reduc if we have syntact structur .
so what i've shown you here is essenti a pars tree , with the japanes word order , so sourc , yesterdai , ibm , lotu , bought , that , said , so superfici that look veri , veri differ from the word order in english , but with these pars structur you can actual recov the english word order by just rotat a few of these phrase within the tree .
so if identifi these three spot , what doe that mean ?
so if i pick up thi verb and move it in front of thi sbar , so that will be the first thing i would do .
okai so that's a kind of an instanc of rotat where we're just swap the order of these two thing .
and i can do a similar oper at thi node .
so , i swap the order here and i pick up thi and move thi over here .
and final , i do similar oper here , where i swap the order of these two thing and move thi over here .
if you go through those step , you actual recov the english word order .
so , there's a veri simpl descript of differ and word order .
for thi particular exampl which just correspond to rotat at differ point in the tree reflect the differ in word order between english and japanes .
for exampl , the fact that we have subject verb object in english and we have subject object verb in japanes .
okai , so in thi next segment , i'm go to describ context free grammar , which ar a veri import formosum us within the pass problem .
so first give a formal definit .
and on thing i should note , is that context free grammar ar not onli us in linguist .
thei're also of central import in comput scienc .
both in theoret comput scienc , and also in program languag , where thei plai a veri , veri import role .
okai , so a context free grammar ha the follow compon .
so context free grammar , g , is a , context of n , sigma , r , and s , where we have the follow element of thi so n is go to be a finit set of what we call non termin symbol .
sigma is go to be a finit set of what ar call termin symbol .
r is a set of rule .
come back to thi in a second .
and final , s is some member of n , which is refer to as a distinguish or star symbol in the grammar .
so each of these rule take the follow form .
we have someth on the left hand side of the rule x , x ha to be non termin .
and then on the right hand side of the rule , we have a sequenc of item , y on through y n .
where each of these y's can either be a non termin n , or it can be an element of sigma .
notic that n could be <num> .
so actual thi right hand side of the rule could just be what we often write as epsilon , which is the empti sequenc , sequenc at length <num> .
so that's a veri abstract definit .
let me give you a concret exampl .
so here is an extrem simpl grammar for english , but it is us for illustr the definit that i just show you .
so i have the four element of the context free grammar , n , s , sigma , and r .
so n in thi case , as i said , is a finit set of what ar call nontermin . . .
so we have symbol like s , np , vp , that you've seen appear in those pal tree that i show you earlier , and notic we also have part of speech .
thing like dt , vi , vt , nn , in .
these ar actual go to be part of speech in the grammar .
the , the distinguish start , start symbol is simpli go to be s .
okai ?
and then final we have a set of word in the languag .
so sigma might consist of the follow set of word .
it's to be a veri small vocabulari here .
morist grammar would be have a much , much larger vocabulari .
and sorri , final we have a set of rule .
okai ?
so here ar the rule in thi particular exampl grammar .
and as i said , on the left hand side of the rule , i alwai see a nontermin .
for exampl , for exampl s vp .
or np or pp and so on .
and on the right hand side of each rule , i see a sequenc of zero or more symbol , where each symbol could be ani element of n or sigma .
so in particular , i have s goe to np vp , here .
vp goe to vi , vp goe to vt np , and so on and so on .
over here i have rule where actual the left hand side is a singl symbol , for exampl vi , and the right hand side is a word , for exampl , sleep .
so , thi is on simpl exampl grammar .
later in the class we'll talk a lot more about what these symbol mean but just briefli .
s basic stand for sentenc , vp is an abbrevi for verb phrase , np is a noun phrase , pp is a preposit phrase , and so on and so on .
so that's a forma definit of context free grammar and also an exampl of a particular context free grammar .
the next crucial concept is go to be the idea of a deriv , or more specif , more specif what's call a leftmost deriv .
so a deriv is a sequenc of string , s1 through sn , where thi sequenc of string ha the follow properti .
so s1 ha to be equal to s , thi distinguish start symbol in the grammar .
as n ha to be made up of element of sigma alon .
so sigma star is just the set of all possibl string which can be deriv from sigma so if i have sigma is d dog a , then sigma star includ string like just the empti string epsilon , a dog thur , two word string like a dog , the dog and so on , basic ani string that you can form through a finit sequenc of these word and sigma where we also includ the empti string sigma epsilon .
okai .
so sn ha got to be a sentenc , for exampl , the man sleep run on the sum sigma .
and then each intermedi si , for i is equal to two to n , is deriv from si minu <num> .
when kick the left most termin x in it i minu <num> and replac it by some beta where x goe to beta is a rule in r .
so here's an exampl , the deriv and i will show go with thi exampl in much detail in next line .
so we start off with s , the first string , the next string in thi deriv is np vp , s is taken and replac by np vp .
now take thi np and replac it by d n , and so on and so on .
let me actual go to an exampl will , where i'll illustr thi a littl bit more carefulli .
okai , so we alwai start in an in a deriv with the distinguish start symbol , for exampl s .
and then each step we're go to choos some rule under the grammar .
so we might for exampl choos s goe to np vp .
and in thi case we replac s with the right hand side of thi rule .
the nv goe to vp .
so , the critic idea and the deriv is that each point we take the left most non termin in the current string and we find some rule where that non termin we write is the sequenc of non termin and we simpli replac it .
so the next term that we ar go modifi is np , so we pick some rule from the grammar , for exampl , np goe to the n , replac np with n and now we have thi n vp again , i pick the leftmost element .
dt in thi case .
i pick some rule .
in thi case dt goe to there .
and now i have the n vp .
and we can keep go like thi .
and final , we end up with a sequenc of word .
okai , so a complet deriv alwai end up with a string where everi word in that string is a word in a languag , everi element of thi string is a member of sigma , okai , so the context free grammar will defin a set of va , valid deriv where ani deriv is valid if it start with s , end with some sequenc of word , and goe through thi process where at everi point we replac the left most known termin us some rule in the context free grammar .
it's veri us to repres these deriv as pars tree .
so thi pars tree , which i've shown you here , is noth more than a represent of the deriv .
so we have s at the veri root of the tree .
notic we chose an s goe to np vp as the first rule and that's reflect with the fact we have s domin np follow by vp .
assum we have np domin determin noun becaus that's the next rule we , we show here and so you can go through each of these rule us and see that it ha that it correspond to some sub fragment within thi tree .
okai , so given a context free grammar , we have a set of valid deriv under the grammar .
and thi set can be infinit .
in fact in almost in everi , almost all interest case , it's go to be an infinit set of possibl deriv .
so a coupl more definit , or on more definit , so we sai a string s so a sentenc , so thi could for exampl be , the dog laugh .
we sai a string s in the languag defin by the cfg if there is at least on deriv that yield s .
okai , so a cfg defin a languag .
a languag is a set of string , and a string is in the languag if there's at least on pars tree for the string .
critic , a veri interest properti is that , some string in the languag mai actual have more than on possibl deriv and thi lead directli to the problem of ambigu .
let me givb you an exampl of thi .
so here's a veri simpl sentenc .
he drove down the street in the car .
and here i have on possibl pars tree for thi particular sentenc .
but , under most grammar for english , there will also be a second possibl pars tree , which i've shown you here .
so let's go back and forth between these two .
in thi particular pass tree , thi preposit phrase is seen lower in the tree , and in particular , i have a rule , np goe to np preposit phrase at thi , low level in the tree .
wherea , if i go back to the previou pass tree , you'll see again we have thi preposit phrase , in the car but it's a littl higher in the tree .
in , in particular we have a rule , vp goe to vp preposit phrase .
so basic there is some ambigu about whether thi preposit phrase is seen high in the tree , attach to thi vp .
or if it's seen lower in the tree , attach to the noun phrase down here .
and that actual correspond to two differ possibl interpret of thi sentenc .
so under thi pars tree , in the car is modifi thi entir verb phrase down the street .
and thi basic correspond to by far the most like interpret , which is that i'm do the drive in the car .
let's look at the second pars tree , though .
we have , in thi case , in the car modifi street .
and so there is a possibl , although highli implaus interpret , where the street that i'm drive down is actual locat in the car .
so i have a street in the car .
you could imagin some crazi world where that would happen .
and that's what thi attatch correspond to .
thi is where the preposit phrase in the car modifi street .
and so we have a street in the car .
so , just to recap , critic , with an underli context free grammar , you mai find there ar multipl deriv for a particular sentenc .
and those multipl deriv veri often correspond to differ interpret of the underli sentenc .
so let's illustr thi issu of ambigu , with the follow sentenc , which is a quit short and rather innocu look sentenc .
she announc a program to promot safeti in truck and van .
and to a human there's realli onli <num> plausibl interpret for the sentenc .
it's realli not ambigu .
but as we'll see , there ar actual sever .
okai , so in thi next portion of the class i want to give a veri brief sketch of the syntax of english .
so we're actual go to develop a rel simpl context free grammar that captur some realli import construct and rule in english .
but i want to emphas here that we're realli just go to scratch the surfac build a full fact grammar for english is a pretti formid task and there ar mani , mani , mani issu that we won't cover .
just to emphas how formid a task build a grammar of english might be , thi is a book i brought up the product detail from amazon .
it's a book call a comprehens grammar of the english languag , it's a veri famou book about english grammar .
it is close to <num> page in length .
it weigh almost five pound .
it's a larg book .
it is ten inch on thi dimens , <num> inch on thi dimens , and over two inch thick .
and so thi book tri to document the grammar of english , and it is huge .
in spite of the size of thi book , if you were to take a randomli drawn sentenc from , sai , a newspap or from a book you were read , it's quit like , li , quit like you would come across a construct that wasn't cover in thi book .
end of transcript . so some sai it's a monument task to build the grammar of english .
but let's proce with thi idea of build up , a first attempt to the context grammar that cover some realli import contract okai .
so firstli let's talk about some veri basic part of speech .
these part of speech convent ar taken from someth call the brown corpu .
thi is a corpu from the earli <num>'s where actual a group of research took a pretti divers set of sentenc from a , a wide varieti of genr in english and annot their part of speech .
so thi is quit like the first part of speech taken corpu .
and the tag convent have been us in mani other corpu sinc that .
so if we look at noun , we actual sub divid these noun into few differ sub type , so we have singular noun , we us the tag nn for singular noun plural and noun .
we us the tag nn for plural noun .
these ar thing like telescop , hous , or build .
singular noun ar thing like man , dog , or park .
and final proper noun , we us nnp .
thi is the part of speech tag for a proper noun .
these ar name , thing like smith , gate , and ibm .
almost alwai capit word in english .
move on , a second veri import part of speech is dt , which stand for determin .
so determin ar word like the .
where a , or some , or everi .
determin usual come befor noun and so i can have string like the man or a man , or some man , where i see a determin befor a noun .
and then final we have adject , so jj is us to refer to an adject .
an adject could be a word like red , green , larg , or idealist .
and adject often come between determin and noun .
so i can have string like the red telescop , would be on exampl where i have a determin .
then an adject , then a noun .
okai , so i'm next go to start to build up a simpl grammar for what ar call noun phrase .
and the noun phrase ha thi categori np .
and let me show you how these rule work to give us sever differ noun phrase in thi languag .
okai .
so the rule over here have part of speech rewrit as variou word , for exampl , thi rule sai that the part of speech nn , rememb that's a singular noun , can be a box or we can have nn rewrit as car or mechan or pigeon .
we have two word for determin .
dt can be rewrit us there , or a .
and then i have a few adject over here ; fast , metal , idealist , clai and we have jj , thi adject symbol rewrit as each of those word .
okai .
so let's see how we can construct phrase of type np .
let's start with a veri simpl on .
so , i can us thi rule here to sai that the noun phrase consist of a determin follow by a categori call an n bar .
we refer to thi as n bar , it's an n with a bar abov it .
and we'll see soon what role thi plai .
a determin can rewrit for exampl as the , an n bar can rewrit in variou wai , we'll come to these other rule later but for now , let's just us thi first simplest rule , where an n bar rewrit as a singular noun and then final the nn , the singular noun can rewrit then for exampl as car .
so that's the first veri simpl exampl of a singular noun phrase correspond to their car , and it ha thi kind of intern structur .
so a noun phrase will gener consist of some main noun .
with some materi befor it , some word often call premodifi , which come befor that word .
in thi case we have on modifi , which is the determin there .
and under thi grammar , i've shown you here , a noun phrase is alwai compos of a determin follow by a categori , call an n bar .
okai so let's try a second exampl .
so , again , i have np goe to n bar .
sorri , determin and bar .
and now i'm go to make us of thi rule .
which sai an n bar can be form by an adject follow by an n bar .
so i us thi rule , and similarli , i can us n ball goe to n .
and then i can sai jj goe to fast and nn goe to car .
okai , so what do i have here ?
i have a structur for the noun phrase , the fast car .
and notic that critic i've us thi rule which sai that i can creat an n bar by an adject , follow by anoth n bar .
okai .
we can go a step further .
and it's import .
we'll see how thi rule n bar goe to jj n bar is recurs , in that it can be repeatedli appli .
so let's give a third structur .
so again i have thi rule at the top .
but now i'm go to appli thi adject rule twice , and so i can have thing like , the fast red car .
okai , and you can see how you could repeatedli appli thi adject rule .
so you can get multipl adject befor a , a noun , which , a rule modifi to that noun .
okai .
so that's an exampl of how we can us thi rule here .
let's move on a littl , a littl bit .
and let's now look at how thi rule can be , appli .
so i have a rule sai an n bar can be form by a singular noun , follow by an n bar .
so , let's give some exampl .
so again , i can have np goe to determin and bar at the top of thi .
we have there , and now i'm go to us n bar goe to nn n bar .
so that's where i've us thi rule here .
and here's an exampl of how thi rule might be us if i have a car factori .
thi is a factori that make car .
you notic that i have car act as a premodifi to factori .
so it's almost in the same posit as an adject but thi is definit a noun .
but it's a rather similar construct to the on we just saw with adject .
okai .
move on , let's final look at thi rule here which sai that an n bar can actual be form by an n bar follow by anoth n bar .
let me give you on exampl of how thi rule can be appli .
so again at the top we alwai have np goe to determin n bar , and now i'm go to make us of thi rule .
so mayb again have factori down here , okai , but now i'm go to make us of thi rule .
just let me write the structur and then we can go over it .
so what i have here is a string that sai the fast car factori .
okai , so thi is a factori that make fast car .
and now i have thi entir end bar .
fast car , a pre modifi factori .
so i've replac .
car here with thi entir pre modifi , fast car .
okai , and that itself ha the categori of n bar .
so you can see how thi n bar categori is be us as an intermedi categori within these noun phrase .
we alwai see n bar follow a disseminar .
and , in addit , n bar can be us further down within these tree .
okai , so next we're go to look at preposit , and what ar call preposit phase , and we'll see how these plai a role , within a grammar .
so the brown corpu us the tag i am to refer to what's call a preposit .
and a preposit is , a word such as of or in or out or besid or as .
a preposit can usual go in front of a noun phrase , so i can sai .
for exampl , of the man .
thi would be a preposit phrase .
or in the room would be anoth preposit phrase .
okai .
so , given the idea of preposit , let's see how we can incorpor these in the grammar .
and what i've shown you here is the same grammar i had on the previou slide .
except i'm go to add a coupl of rule .
so firstli .
i am go to have a rule that sai a pp , thi is a proport phrase can be form from an in follow by an amfer and i have variou rule specifi within in the proport can be word such as in , or under , or of , or on , or with , or as and so on thei ar probabl roughli i think a hundr proposit in english .
that's a rough rule of magnitud .
so that's the first rule sai , proposit phrase can be form by proposit form by noun phrase , so for exampl i can have the preposit phrase such as the follow , follow so in , np .
so incident i'll sometim us thi triangl notat .
to mean that there is some sub tree under the grammar which i haven't fulli specifi just becaus it would be veri tediou to write out all their inter , intermedi structur in thi np .
what i would realli have here is an applic of rule such as determin n bar , n box n , and n goe to room .
but thi is us shorthand when you want to hide the detail .
so , a preposit phrase can be form from the tag in , follow by np , and that mean it can be form by a preposit follow by some noun phrase .
so let's go a littl bit further with these rule and see how we can us them .
so we take a game we can creat a proposit phrase from in for by an np .
and so the np could , for exampl , be the car .
and then i'm go to us thi rule here to sai that i can form an n bar .
. . by an n bar , follow by a pp .
and then final , i'll us thi rule again at the veri top , np goe to determinor , n bar .
and so i can have someth like , the dog in the car . ok .
so what's go on here , now i have some noun , dog , and actual , i have a post modifi to thi now , thi preposit phrase come after the noun and modifi it , and actual specifi where the dog is in the car in thi case .
so thi rule n bar to n bar pp sai i can now take a preposit phase and us the preposit phrase as a post modifi to thi particular n bar .
and so i get thi full noun phrase now , so the noun phrase is thi full sequenc of word , the dog in the car .
and it's worth note that again these rule ar go to be recurs , so i can build up larger sequenc of preposit phrase modifi .
so i can sai , for exampl , the dog , okai , and again , notic that i have us thi rule n bar goe to n bar , pp and then i have i n goe to n .
thi time , i am actual go to specifi the intern structur of thi noun phrase , so .
so it's the dog in the park in the citi .
ooph , the citi , sorri , okai the citi is the noun phrase again i've us thi triangl notat to hide the full structur under here .
so , what do i have ?
i have an entir noun phrase which is the dog in the park in the citi and notic that i've now recurs appli thi preposit phrase .
so i have park in the citi as a n bar here .
i've us there to creat to put there in front of thi n bar to form a noun phrase .
and then i have a second preposit phrase in the park , in the citi .
and that itself modifi dog .
and you can see how you can get entir chain of preposit phrase mod , modifi each other in , in the wai i've just shown .
so the next set of categori we're go to look at ar verb , verb phrase , and sentenc .
so our first critic observ is that verb in english can be subcategor into distinct subtyp .
which have rather differ properti , so i'm go to us the symbol vi to refer to intransit verb .
vt to refer to transit verb , and vd to refer to ditransit verb .
and so , for exampl , an intransit verb would be sleep , or walk , or laugh .
a transit verb would be someth like see , or saw , or like .
ditransit would be gave .
and we'll have variou rule in the grammar express thi .
so for exampl , vi can go for sleep .
it might be on rule in the grammar , or vi goe to walk .
we would have vt goe to see .
and similar entri for saw and like .
and we would have vd goe to gave , for exampl .
now , given these differ verb type .
we have differ wai of construct what's call a verb phrase .
so let's look at thi first on .
so veri phrase can just be , an intransit verb .
so it can get a structur like the follow it's a vp .
and then vi underneath it and then sleep .
okai ?
a second exampl here would be the follow .
so vp goer to vt , and then i could , for exampl , have c's .
and , notic that a transit verb alwai preced a noun phrase to form a verb phrase .
so verb phrase can be made up of a transit verb , follow by a noun phrase .
so i can have , see the dog , as anoth verb phrase here .
and final , if we look at thi ditransit rule , i can have a verb phrase , which is someth like the follow .
so .
vd gave .
so now i have <num> noun phrase follow the verb .
and the first <num> might be the dog , the ball .
okai ?
so notic that a verb phrase is gener made up of some verb .
follow by <num> or more noun phrase .
or at least the verb phrase we've seen here have thi form .
intransit verb there ar zero noun phrase , transit there's on , ditransit there ar two differ noun phrase .
okai , so that's first simpl set of rule for vp and actual we'll come back to a few more a littl later in a few more slide .
the next thing we're go to look at is a rule which can construct a sentenc .
so thi rule sai that the symbol s , which stand for sentenc , can be form by a noun phrase .
follow by a verb phrase .
so actual i can us thi rule to construct sentenc , like the man sleep .
so notic i have s goe to np vp here is the man , and then vp goe to sleep .
similarli , i could have an np here , so again thi would be the man so , the man see that dog , and we have here we could have the man gave the dog a ball .
okai , so we're start to see the structur now of entir sentenc .
top down in s we write an np versu follow by vp .
thi np is critic the subject in each case .
ye , thi is the subject , thi is the subject , thi is the subject and then the verb phrase gener some verb follow by zero more noun phrase .
zero in the transit case , on in the transit case , two in the direct transit case .
okai so we construct mani sentenc now given these basic .
let's add on more rule to thi grammar concern verb phrase .
and thi sai that a verb phrase can be itself compos of a verb phrase follow by a preposit phrase .
so notic we see preposit phrase crop up again .
earlier we had seen a rule which said n bar can rewrit as an n bar follow by pp .
now we have a rule which sai a vp can be form by vp follow by a pp .
okai , so let me give you some exampl .
so , if we appli thi rule we can have sleep , and thi preposit phrase would have some structur .
sleep in the car for exampl .
okai ?
we'll give you anoth exampl .
we could have thi vp , us a ditransit verb so , you know , gave the dog , a ball .
and thi preposit phrase could , for exampl , sai on wednesdai .
that would be anoth preposit phrase .
okai .
so , we see thi basic structur .
we have vp form by vp , follow by anoth preposit phrase .
thi preposit phrase gener add some inform , for exampl , about the locat of , the verb , or , sorri , the event describ by the verb or the time of the event describ by the verb phrase and so on and so on .
now , it's worth note that thi is again a recurs rule and that i can appli it , multipl time .
so i could have anoth applic of thi rule up here . . .
end of transcript . and have anoth proposit phrase .
so it's good for exampl , sai , gave the dog a bowl on wednesdai in the car , where thi second preposit phrase add yet more inform to the underli event describ by thi verb .
okai .
move on .
we've seen how to construct s the sentenc categori .
now we're go to add anoth part of speech , which i'll us comp for .
so comp is go to rewrit word for exampl , like that .
that's the on complemiz , complement we'll focu on now , the word that .
and thi give a new categori , which we'll call sbar .
so we have a rule sai an sbar can be form by a complement follow by s .
and so for exampl we could have an sbar with the follow structur .
so that's go to be an s , and again , i'll hide the detail of the pars tree under here .
thi s , for exampl , could be the man sleep .
okai .
so for import structur in english , you'll frequent see , as we'll see subordin claus in sentenc form by a word like that follow by some sentnec .
so let's extend the grammar and see where these exactli fit in .
and for thi we'll actual need some , some more verb type .
so rememb earlier we saw intransit , ditransit , transit verb .
i'm run out of name so i'll just call these v5 , v6 and v7 .
so v5 is a new part of speech and can be reserv for word like said or report .
so we would have rule like thi , v6 is go to be reserv for verb like told .
and v7 is reserv for actual a quit unusu word , which is bet .
we'll see why it's unusu in a second .
and the new vp rule start to us thi sbar , so , if i have thi string here said that the man sleep , there's actual a thi can actual be a vp .
it's compos of v type five , thi said follow by an sbar .
so it critic know that it's possibl with these type of word said and report form and phrase by that verb follow by an sbar , okai ?
so if we look at thi rule thi sai that a verb of type six can be follow by an np and an sbar to form a vp .
and that's a phrase like told the dog that the mechan like the pigeon .
okai so , thi is an sbar .
it's form by the word that which is complement g and follow by a sentenc .
and thi is an np .
and told can actual take two argument .
firstli in np and secondli in sbar to form a verb phrase .
so thi is who's be told , and thi is what is be told .
the final rule sai that a vp can be compos of a v of type seven follow by two known phrase , an sbar and v7 for exampl might be best .
thi is unusu in that i think there ar veri , veri few verb which take three or more argument in english .
i think three is mayb the most .
let's give an exampl .
so i can sai bet the pigeon so thi is an mp , <num> .
thi is also an mp .
and then thi is also an s bar .
okai ?
so that's an applic of thi rule here .
thi is a via type <num> .
and thi entir phrase here can be a verb phrase .
so ne , next let's talk a littl bit about coordin , anoth part of speech .
so cc is a word such as and , or .
and we have variou rule that make us of these coordin .
and notic that each of these rule ha a form where we have some non termin left hand side compos of two instanc of the same non termin with a coordin .
and so , i can creat phrase like the follow .
so the man and the dog .
so that would be an instanc of thi rule us the np .
and notic that thi rule basic glue togeth two noun phrase with a coordin in between them to conjoin these two phrase to form a larger phrase .
and thi coordin type of rule can be us for all of these differ phrase type .
so , i can for exampl sai sleep and like the dog .
so both of these ar verb phrase , both of these sub segment ar verb phrase and i can final us thi rule , it sai the vp rewrit a vp , cc , vp to join these two thing togeth .
so that's it for the simpl grammar i wa go to show you .
but i want to emphas again , that we've realli , realli , just scratch the surfac , we've just gotten start .
hopefulli though , that's given you an illustr of how we might build up a grammar for english .
let me just mention a few veri import thing that we've complet miss in thi grammar .
the grammar complet fail to captur .
on veri import , properti in languag is what's call agreement .
so , if we look at these two sentenc , we have a subject and we have verb .
and critic in english and mani , mani other languag , there's thi properti where the subject and verb have to agre .
so if i have a pleural noun , for exampl dog , i have on form of the verb , which is laugh .
if i have a singular noun , dog , i have a differ form of the verb .
so there's some notion of agreement between the , the noun and the subject and the main verb at the sentenc .
and the grammar i've just shown you ha complet fail to captur that constraint .
a second veri interest phenomenon , is wh movement which ha led to all kind of interest realiz about the structur of languag .
so here's an exampl .
if i sai the dog that the cat like , like is a transit verb .
and in some sens there's a gap here , which is fill by the dog .
the dog is realli the object .
the dog is the thing that's be like here .
so you can think of the dog as have move out of thi posit .
we can think of variou other account , but there's definit some kind of hole here which is fill by thi phrase earlier .
thi is call wh movement .
anoth exampl of someth we've miss , you know we have activ versu passiv sentenc .
we can sai the dog saw the cat versu the cat wa seen by the dog .
and it's realli critic to somehow relat these two sentenc .
identifi that thei , in some sens , come from the same thing or thei have veri , veri close relat , relat mean , perhap even the same mean .
so , if you're interest in read more , i , i strongli recommend , pick up an introductori textbook on , syntax and linguist .
and on book i'd highli recommend is thi , which , give a veri , nice introduct to , formal grammar and syntax of languag .
okai , so in the last segment of the class , we saw context free grammar .
and we saw how ambigu is a veri sever problem in pars natur languag .
thi is the problem where a given sentenc can have sever differ possibl pars tree under a grammar .
in thi next segment we're go to describ probabilist context free grammar , often abbrevi as pcfg .
so pcfg augment context free grammar by ad a probabl to each rule in the grammar .
and thei therebi assign a probabl to everi possibl pars tree under the grammar .
and as we'll see , thei allow us a veri direct attack on the ambigu problem .
in their simplest form , thei actual perform rather poorli as a pars model .
but we'll see in subsequ lectur in the class , that with just a few refin , thei can actual form veri effect model .
so pcfg ar a simpl model , thei're a veri old model , but thei're a crucial model that form the basi of mani , mani , pars model now us in natur languag process .
so we'll first introduc the basic formal , pcfg .
and then we'll talk about a crucial algorithm , which is call the cky algorithm .
and thi is again , actual a dynam program algorithm , which will allow us to us pcfg to pars sentenc .
so here is a pcfg and you'll see that it basic look exactli the same as a context free grammar , but we now have probabl assign to each rule in the grammar .
so here i have a set of context free rule .
and we'll , again , assum that s is the start symbol under the context free grammar .
but now , in red , i've shown a probabl for each rule .
so s goe to np vp , for exampl , ha probabl <num> .
vp goe to vi ha probabl <num> .
and so on and so on .
these probabl have on critic properti , which is the follow if i take ani non termin in the grammar , so let's take dp for exampl , there're a number of possibl wai .
of expand that non termin .
so vp can be expand in three differ wai here .
it can be written here as vi , or vtnt , or vppp .
three possibl .
and notic that the <num> probabl associ with these <num> differ option sum to on .
so <num> plu <num> plu <num> is equal to <num> .
and thi is the kei constraint on the probabl in the pcfg .
if i look at ani non termin , we , must have these probabl sum to <num> .
here's anoth exampl .
mp can be write in two differ wai .
either as a duh , duh , determin follow by a noun , or a noun phrase follow by a preposit phrase .
these two probabl sum to on .
so these probabl have a clear interpret , which is the follow .
thei ar the condit probabl of , condit on a particular nontermin we have multipl differ wai of write that nontermin .
we now have a , a distribut over those differ option , over those differ wai of rewrit that particular nontermin .
okai so then we can do the follow , so let me just illustr thi definit down here .
thi text down here .
let's take a particular into thi ground , pretti simpl so notic that everyon of the rule i'm us here is in thi grammar .
so that's about it , pars tree under the underli context free grammar .
we're now go to assign a probil to thi entir pars tree , which is simpli a product of probailti for these differ rule .
so if i look at the first rule , s goe to np vp , that ha probabl <num> .
and then np goe to determin nn , that ha probabl <num> . <num> .
determin goe to the ha probabl <num> , and so to calcul the probabl of an entir tree , i just multipli togeth the probabl for differ rule .
so vp goe to vi ha probabl <num> , and vi goe to sleep ha probabl <num> .
okai .
so that's the final express for the probabl of thi particular pars tree .
so , more abstractli , if a tree ha rule alpha <num> goe to beta <num> , alpha <num> goe to beta <num> , up to alpha n goe to beta n .
thi is the left hand side thi is the non termin for each rule .
thi is the sequenc of non termin on the right hand side .
then the probabl for the tree is the product of these paramet , so i write q , so for exampl q with vp goe to vt and p is equal to zero point four and it's grammar .
so each rule ha a paramet , q of that rule which is some probabl .
the probabl for an entir tree is the product of these q term , just as i illustr here .
so on us intuit behind ptfg is the follow .
we can now think of top down stochast process under which we can sampl paus tree or deriv under a ptfg . .
so , recal that s is ar start symbol so deriv alwai start with s .
now at each point in the deriv i'm again go to pick the left most non termim in my current deriv , s , in thi case and i'm go to us some rule to expand that symbol .
so let's sai i pick thi rule here .
s goe to np vp .
and we can think of thi as a probabilist process .
where we choos thi rule with the probabl under the grammar .
so in fact , there's onli <num> wai of expand s as np vp .
and so with probabl on , we're alwai go to rewrit thi as np vp under the grammar i show you .
on the previou slide .
next step in the process , we take the leftmost non termin np , and you can again think of a stochast process where we choos the rule expand np from the differ possibl us the distribut under the grammar .
so in thi case we might choos np goe to determin nn , so notic nn is now replac by these two word .
write these two symbol and that ha probabl <num> .
now , we pick dt and we choos a rule from the grammar .
thi actual rewrit as though it probabl on , and so on and so on .
so thi probabilist top down process is go to termin when i end up with the sequenc of word .
so now at thi point i have no non termin left .
the phars tree underl thi of cours , is s over np .
p .
it's go to be someth like thi .
and the probabl is go to be the product of these differ term .
so you , you can in fact , onc you have a probabilist context free grammar .
sampl deriv from that , context free grammar .
in thi sort of top down process .
so you can us a probablist context free grammar to gener pars free .
okai , so some crucial properti of pcfg .
firstli , as we've said , thei assign a probabl to everi possibl pars tree allow by the underli context free grammar .
so , to calcul the probabl of a pars tree , i just look at the rule in that pars tree and multipli togeth the differ probabl .
most crucial for our purpos , we have the follow .
so sai i have a sentenc s .
so let's sai , for exampl , it's the dog saw the man with the telescop .
so thi is our sentenc , s .
thi sentenc mai have sever differ pars tree under the underli context free grammar .
so sai we have a coupl of differ pars tree , which i'll just sketch here , under the cfg .
end of transcript . and as we said , thi is the ambigu problem .
the problem is that our grammar is gener more than <num> pars tree .
so i'll cool thi entir set of pars tree .
t of s .
okai ?
so , t of s is the set of possibl pars tree , for thi input sentenc .
now , critic , now i have probabl on these rule in the grammar .
i can calcul the probabl of each of these pars tree .
so i might , for exampl , calcul the probabl of thi on as <num> . <num> , and mayb thi on as <num> . <num> .
and so now i get thi addit inform where each pars tree in the grammar .
each pars tree for thi particular sentenc ha a differ probabl .
and that give a rank on the differ pars tree in term of their probabl or likelihood .
and so , given thi rank i can for exampl simpli output from my model , or from my pars , at the highest probabl tree out of the model .
so i now have a wai of choos between differ pars tree .
and so , the pars method we'll look at , we'll first somehow learn the paramet in pcfg from datar , and then given a new sentenc , thei will search for the pars tree that ha the highest probabl under the grammar .
and if the pcfg doe a good job of model the probabl of differ possibl pars tree , you will end up with an accur parser that , more often than not , resovl ambigu that i .
just , describ to you in the previou lectur .
so , on last definit .
if we have the sentenc s , then the most like pars tree for that sentenc is , if we look at all tree and that set t of s from t of s is the set of possibl pars tree for s , we now just choos the highest probabl tree under our pcfg .
so the next thing we should discuss is how we can actual learn a pcfg from data .
and as i describ earlier in the cours , we have resourc avail call treebank , which ar extrem us for thi purpos .
so , an earli and veri famou exampl of a treebank is the penn wall street journal treebank .
and treebank consist of sentenc , here we have a <num> word sentenc togeth with their underli pars tree and these pars tree ar actual annot by hand .
so for exampl , in the case of the penn wsj treebank , a group of peopl in the earli <num>'s at univers of pennsylvania got togeth and came up with a set of convent base on linguist theori for the form of these structur , and went through and annot <num> , <num> sentenc .
that's close to a million word of data .
so we actual have exampl pars tree from which we can learn the rule and paramet of the pcfg .
the penn wsga treebank wa on of the earliest exampl but by now we actual have mani differ resourc in mani differ languag of thi form .
so onc we have a treebank , learn a pcfg is actual extrem straight forward , almost trivial .
so there're two thing we realli need to learn .
on is , the set of underli rule , in the context free grammar the probabilist context free grammar so , we might for exampl learn a few rule like thi .
and the other thing we need to learn is , is paramet associ with these rule for exampl <num> and <num> and so on .
in term of the rule in the pcfg that we learn , we simpli take all rule seen in the treebank .
so , learn , quot , learn , the context free grammar is simpli a matter of read the , off the rule in the treebank , read off the context free rule .
how do we estim the paramet ?
so , rememb , q of some rule alpha goe to beta is the probabl associ with that rule .
we're again , go to make us of maximum likelhood estim , which again , have a veri simpl and intuit form .
and so to estim the paramet for sum rule alpha goe to beta .
we take a ratio so on the denomin we have the number of time we've seen alpha , and on the numer we have the number of time we've seen the entir rule .
so for exampl qml of vp goe to vt , np , would simpli be count of vp goe to vt , np divid by count of vp .
where these count ar taken veri directli from the exampl tree in the tree bank .
there ar variou guarante for these kind of estim .
thi is on import on .
so if the data we're look at , if the treebank is actual gener , gener by some underli pcfg .
you could show that as the train data size get larger and larger .
these paramat estim will get closer and closer to the true underli probabl in the pcfg gener the data .
and that lead fairli , fairli directli to properti that the distribut over entir pars tree defin by our pcfg that we learn converg to the correct the correct underli distribut under under the pcfg which is gener our trade data .
bottom line , though .
given a tree bank , it's veri easi to learn a pcfg , we simpli read off the rule from the tree bank .
and then we calcul these maximum like hood estim , which amount essenti just to count , just to count the number of time you've seen non termin .
and count number of time we've seen entir rule .
so , i just want to talk about on final technic properti of pcfg .
thi goe back to work by booth and thompson from the earli <num>'s .
so if we step back and think about what we've done , it's , it's realli rather remark .
so if we take a given cfg , there will be some set of well form pars tree under that cfg , so we have some set of pars tree .
these ar quit complex structur , and in fact for mani cfg thi set will be infinit , so actual for the cfg i show you earlier there ar actual an infinit set of possibl tree .
we now have a wai of calcul a probabl for each tree under the cfg .
so for exampl thi might be <num> . <num> .
i have <num> . <num> , <num> . <num> , and so on , and so on .
and we've done thi by simpli assign a probabl to everi rule in the cfg , and then for a given tree we just multipli it togeth , the , the , the rule probabl within that tree to get thi probabl .
so for thi to defin a correct distribut over possibl pars tree , these propabl have to sum to <num> .
so we're attempt to defin a distribut over an infinit set and an infinit set of quit complex structur .
so booth and thompson give condit on the rule probabl , under which we get a proper distribut over tree .
the first on is exactli what i show you earlier , and it's by far the most import , and that is that if we take ani non termin , for exampl vp .
and we look at all of the rule with that non termin on the left hand side for exampl we might have the follow .
then these rule probabl have to sum to <num> , okai so thi would be valid for exampl that's the first condit .
and that's realli the onli <num> you , you realli need to worri about i'll just briefli mention though that there ar some other condit which ar rather technic .
and we'll go over them veri quickli .
well thei ar somewhat interest .
so let me give an exampl of a grammar , which satisfi thi first condit , but actual doe not defin the distribut of the possibl tree .
so the rule in thi grammar ar veri simpl , s goe to ss , or s goe to a .
and so we'll gener pars tree like thi or thi on right here and so on .
and in fact , you can see there ar an infinit set of possibl pars tree under thi grammar .
and , let's sai i choos probabl <num> for thi rule , probabl <num> for thi rule .
so , thi satisfi the condit that the rule probabl sum to <num> .
but actual , for ani finit tree , thi is go to get probabl <num> .
'caus it ha s goe to a in it .
thi is go to get probabl <num> , 'caus it ha s goe to a in it .
and so on , so actual it's go to assign propabl <num> to ani finit tree and it will fail to defin a distribut over possibl tree .
okai , so that's a simpl exampl of a grammar that satisfi condit on , but is ill form in some sens .
um . . .
the second exampl which i'll just briefli mention is a littl bit more surpris you can actual show that on a certain set with these rule paramet .
so , if i sai <num> for thi , and <num> for thi , you can actual show that thi grammar also fail to defin a well form distribut of the tree .
you can show that the sum of probabl for the set of finit size tree actual sum to less than on for thi particular case .
and intuit what's go on here is the grammar's split too quickli and it ha probabl less than on of actual produc a finit length pars tree .
okai , but i just want to mention that veri briefli .
it's sort of curiou .
in practic , thi is never realli a practic concern but it's just worth have at the back of your mind .
okai , so , so far i have describ the basic formula and underli pcfg , and i've describ how you can learn a pcfg from a tree bank , from a set of exampl tree .
in the last segment of thi lectur i want to talk about pars with a pcfg .
so that is the problem of take a , sentenc as input .
so for exampl , the man saw the dog with the telescop .
and find the most probabl tree under the pcfg .
ok .
so there is again a rather dumb , kind of brute forc method for do thi .
which would be the follow .
so given thi input sentenc , i could somehow enumer , brute forc , all of the possibl pars tree for that sentenc , under the pcfg .
so imagin i had an algorithm that simpli list all of the possibl past tree for thi input sentenc , so that will be step on .
in the second step , i will just calcul the probabl .
of each of these tree .
and i would choos the highest probabl tree as the output from the parser .
so that is a kind of brute forc method where i simpli enumer all of the tree , calcul the probabl for each tree , return the highest score tree .
there is a veri clear problem with thi method , which is the follow .
the number of possibl pars tree for a sentenc can be extrem larg .
and it's actual veri easi to come up with grammar where the number of possibl pars tree for the sentenc grow exponenti quickli with respect to the length of the sentenc .
so brute search realli becom unfeas for thi kind of grammar there's simpli too mani pri tree to search through .
howev , there is rather a beauti solut which again reli on dynam program .
i'm go to show you how we can actual effici find the highest probabl tree under pcfg , without have to enumer , brute forc , everi possibl tree under the grammar .
and as i said , thi will again us dynam program , in the same wai that we saw the dynam program alrogithm , for hidden markov model avoid thi problem with brute forc search .
so in the pars algorithm i'm go to describ to you , we'll actual assum that pcfg is in someth call chomski normal form , which mean that it ha a set of restrict on the rule of the pcfg .
so context free grammar is in chomski normal form if it consist of the follow .
so again we have some set of non termin symbol .
we have some set of termin symbol or word in the grammar .
we have a distinguish start symbol .
and now the rule in the grammar ar astrict to take on of two form .
so firstli we can have a rule of the form x goe to y1 y2 .
where x , y1 , and y2 ar all non termin .
so , for exampl , vp goe to vt np would be a perfectli valid rule , becaus of all three of these thing ar non termin .
and i have two children .
i alwai have to have two children under thi definit .
similarli s goe to np vp .
is again about the valid rule in chomski normal form .
the second type of rule is of the form x goe to y where x is in nontermin and y is a termin symbol .
so for exampl , vt goe to soar is a perfectli valid rule , or determin goe to and so on , so rule of thi phone have a non termin on the left hand side and a word on the right hand side .
so a cfg in would chomski normal form would have a set of rule like thi with associ probabl okai .
so the paus algorithm i'm go to describ will assum that our pcfg ha , ha rule of thi form .
now that might at first glanc seem to be a veri big restrict .
but it turn out that it isn't in the sens that you can take ani pcfg and convert it to an equival pcfg .
in chomski normal form .
i don't want to go into the full detail of thi becaus it's rather labori .
but i'll just give you a sketch of the kind of trick we can us to take a pcfg in gener form and convert to it to a chomski normal form pcfg .
so let me give you on exampl .
so let's sai , i have a pcfg , which includ the follow rule .
thi vp goe to sai vt np , proposit phrase .
okai .
so let's sai , for the sake of argument , we have the rule .
the awkward thing about thi rule is that it ha <num> non termin on the right hand side .
and so it , it violat thi restrict on the rule .
so the solut is essenti to convert thi to a .
sequenc of rule in chomski normal form .
let me show you how thi work .
so i'm actual go to introduc a new symbol in the grammar .
so , i'll write , my first rule is vp goe to vt np preposit phrase <num> .
so thi is a new symbol i've introduc vt np and then i have a second rule which sai that vt hyphen np goe to vt follow by an np with probabl on .
so you can see how i basic split thi rule with thee non termin on the right hand side into two separ rule , each which ar in chomski normal form .
so , if under the previou grammar i'd have some pars tree , where i would have someth like thi and i'd have some structur below these differ symbol like that .
in the new pars tree i essenti have thi intermedi non termin .
so , the structur would look someth like thi .
okai .
so , i'm sorri that should be a attach the preposit phrase , yep .
okai .
so by do thi convers , onc i've done thi convers , i can run the pars algorithm with the new pcfg with the rule convert in thi wai .
and i'll recov tree like thi and then it's straight forward to map back to the origin format of the rule just by remov these intermedi nontermin .
okai , so short stori is , for grammar which is not in chomski normal form i can us variou method to convert it to a grammar in chomski normal form .
of cours i haven't told you how to deal with unari rule .
thing like vp goe to vi , with probabl <num> , or someth .
but there ar similar trick you can us for these kind of unari rule .
so , let's now describ the dynam program algorithm , which will take a pcfg in chomski normal form .
and also a sentenc , s , or some sentenc like the dog saw the cat .
so , those , those ar go to be the two input to the algorithm and it's go to return the highest score for highest probabl for ani tree under the grammar .
so , for now , i'll focu on thi problem .
thi is the problem of simpli comput the highest probabl for ani tree under the grammar for thi particular sentenc .
rememb , t of is the set of possibl pars tree for our input sentenc .
of cours , what we realli want to calcul is the arg max , which is the tree , that actual achiev thi max .
we'll come back to that later .
first , we'll just consid thi problem of comput the maximum probabl for ani tree .
okai , so i'm go to us n to refer to the number of word in the sentenc .
i'll refer to wi to be i th word in the sentenc .
so , i have w1 , w2 , right the wai up to wn , that's the input sentenc .
i have n as the set of non termin in the grammar befor , and i have some start symbol s in the grammar .
okai , so we're again , go to make us of dynam program , and i'll again us pi  to refer to a dynam program tabl .
and pi  is go to have three indic .
so , x is a non termin .
i is go to be a first index in the rang <num> , <num> , up to n .
and j is also go to be some valu in thi field <num> , <num> up to n .
and we alwai have i less than or equal to j , okai ?
so , we onli consid entri pi  ijx , where i is listen to j .
we'll see why in just a second .
so , thi , and i'll give you an exampl in a moment , but let me give the definit .
thi is go to be defin as the maximum probabl of a constitu with non termin x span word i to j invclus .
okai , so sai i have some sentenc w1 , w2 , w3 , w4 , w<num> , w6 , and i look at pi  of <num> <num> np .
what doe that correspond to ?
so , if i think about the symbol np .
there ar go to be mani , mani differ wai , potenti , that an np can have a pars tree underneath it which span word <num> through <num> inclus .
everi on of those pars tree will have a probabl , which is simpli the product of the rule probabl within that pars tree .
and pi  <num> , <num> , np is go to be the highest probabl for ani pars tree with np span those word <num> through <num> .
given that definit , rememb that thi is our goal , is to find the highest score for ani pars tree .
well then , by thi definit , pi  <num> ns is go to be the highest score pars tree with s at it root span word <num> through n inclus .
and so , if we can calcul thi valu , we have essenti solv the problem .
so , what we're go to see is that we can actual calcul these pi  valu veri effici us a dynam program method , and critic us a recurs definit of these pi  valu .
where we initi build up the pi  valu for small segment in the sentenc , and we gradual get bigger and bigger until we have the pi  valu for the entir sentenc .
so again , just to re emphas what thi definit mean , here's a concret exampl .
so , if i have <num> , <num> , <num> , <num> , <num> , <num> , <num> , <num> and we just number the , the word in the sentenc here .
let's sai i have pi  <num> , <num> , vp .
so , that mean i'm go to consid all vp , which span word <num> to <num> inclus .
and in particular , there might be at least a coupl of vp here .
on where the telescop is modifi saw , instead of the telescop is be us to see the man , and on where the man ha the telescop .
those , those will , in gener , correspond to two differ pars tree .
and pi  <num> , <num> , vp is go to be the highest probabl for ani of those differ wai for reach vp , span these differ word .
okai , so next , we're go to consid the recurs definit of these pi  valu which will actual drive the dynam program algorithm .
okai , so first of all , the base case of thi recurs .
so for ani i in the rang <num> to n , for ani non termin x , i can defin pi  i , i , x to simpli be the paramet q of x goe to wi where i defin q of x goe to wi equal <num> , if thi rule is not seen in the grammar .
so , let me give you a concret exampl .
so , if we have the dog laugh , for exampl .
and i want to comput pi  , so thi is word <num> , <num> , <num> .
i want to comput pi  of <num> , <num> nn .
thi is go to be the highest probabl of ani pars tree with and n , as it root , span word <num> to <num> inclus .
there's onli on wai of do thi , there's onli on possibl pars tree , that is the pars tree with a singl rule .
and n goe to dog , so thi is go to add valu with the probabl of nn goe to dog .
so , that's a veri simpl case .
and if there is no such rule in the grammar , we just defin thi be equal to zero , reflect the fact that the maximum probabl is then zero of ani , of ani tree span the singl word dog with nn as it root .
so , that's the base case of the dynam program .
the recurs definit is more involv , and i'll first give the definit .
and then on the next slide , i'll give , an exampl of , and try to describ why thi recurs definit is correct .
okai , so thi is for ani case where we have i in the rang <num> to n , and j in the rang i plu <num> to n , and we have some non termin x .
actual , thi should realli be n minu <num> .
sorri about that .
so here we have the case where i is strictli less than j .
so , we have some span , for exampl in thi particular sentenc , we might have pi  of <num> to np , for exampl .
okai .
so here's the definit .
so , i have a max .
i'm go to have a max firstli over all the wai of rewrit x in the grammar .
so , thi is a max over all product of the form x goe to yz in the rule set .
rememb , our grammar is in chomski normal form , so we can just consid rule which ar binari where we have two children .
and then , we have some variabl s , which can take ani valu in the rang i through to j minu <num> .
thi is what would be call the split point .
we will see why shortli .
and so , i'm go to search over these two choic , the choic of rule and the choic of the valu for s .
and then , i'm go to multipli in q , thi is the probabl for thi rule that i have chosen , and then i have pi  of i , s , y , and y is here , and by s plu <num> , j , z .
z is here .
and so , thi definit is recurs becaus thi valu of pi  is calcul base on other valu of pi  .
in particular , it's base on valu of pi  which ar base on shorter segment .
so , the length form i to s or from s plu <num> to j is smaller than the length from i to j .
okai , so that's rather abstract .
let me give you an exampl illustr why thi definit is actual correct .
so , here's the exampl .
let's just number these word .
and let's sai , for the sake of argument , we want to calcul pi  <num> , <num> , vp .
so , that's go to be the maximum probabl for ani tree with vp , at it's , at it's root , span word <num> to <num> inclus .
so firstli , i'm go to write down how thi definit plai out , and then we'll justifi it .
let's sai for the sake of argument , i have two rule expand vp in my grammar .
on of them is vp goe to vt np .
and the , other on is vp goe to vp proposit phrase , sai for the sake of argument , these have probabl <num> and <num> respect .
so i'm go to search over two differ thing , i'm go to search over all possibl rule in the grammar .
now , i'm go to search over all valu for thi valu of s , and that actual mean i'm go to to search over s in the rang .
so , i is <num> in thi point <num> , <num> , <num> , up to j minu <num> .
j is <num> , so that's go to be up to <num> .
so let's go through these option so i can comput q of vp goe to vt , np time pi  <num> , <num> , vt time pi  <num> , <num> , np .
so , that's on option , so i can calcul thi valu .
so thi is the case where i choos thi rule here .
and i choos the split point equal to <num> , and so pi  is i , s <num> , <num> , vt , and pi  s plu <num> , j is <num> , <num> and z is np .
anoth option is q vp goe to vt np time pi  <num> , <num> vt time pi  <num> , <num> np , and so on and so on .
final , up to q , vp goe to vt np time pi  <num> , <num> , vt time pi  <num> , <num> , np .
so , here i've fix thi rule and i've search through all these valu for s .
and then , i can do a similar thing where i have q of vp goe to vp time pi  <num> , <num> vp time pi  <num> , <num> , preposit phrase .
so , that's where i've chosen thi rule .
the split point is <num> right the wai through to q of vp goe to vp pp time pi  of <num> , <num> , vp time pi  <num> , <num> , preposit phrase .
okai , so i'm go to search through all of these differ possibl .
i'm go to search over all possibl rule choic expand vp .
and i'm also go to search through all possibl split point .
so , each of these product is go to have a differ valu , and i'll simpli take my final valu of pi  of <num> , <num> , vp as the max of all these differ valu .
and as we will see when we actual implement thi algorithm , we're go to have to be care in term of the order in which we calcul these pi  valu .
we'll calcul pi  valu bottom up , i . e . , for smaller segment , befor we get to larger segment , which will ensur that these pi  valu at lower level ar calcul befor thi final pi  valu .
so let's next give some justif for why thi recurs method for calcul these pi  valu is correct .
and again i'll us thi sentenc as an exampl , and i'll us pi  <num> <num> vp as an exampl .
so , we're try to find the highest probabl for ani vp span these word three through eight inclus , and i'll assum our grammar ha these two rule .
and in thi case , we're go to search through these two rule .
and we're go to search for the split point , in the rang of valu three through seven .
so the basic intuit is the follow .
so if i have word three , four , five , six , seven , eight .
if i think of ani vp span these word , then i have to make some choic of rule .
for exampl , it might be vp preposit phrase , and i have to make some choic of split point .
so , let's take s equal <num> for exampl , what doe the split point mean ?
that mean that i'm consid the case where the vp span with three to five and the preposit phrase span with six through eight .
so given i choos thi rule vp goe to vp preposit phrase and i choos thi split point , ie .
i choos thi vp under here to span word three through five preposit phrase to span word six through eight .
the highest probabl for ani tree which make those choic is go to be q vp goe to vp , preposit phase .
becaus that's the probabl for thi rule .
and then , as i'll argu in a moment , pi  of <num> , <num> , vp time pi  .
of six , eight preposit phrase so where did these two term come from ?
thei come from the observ that for thi to be the highest probabl tree with thi choic of rule and thi choic of split point .
i have to us the highest probabl tree under thi vp .
and that thi pi  term is go to correspond to the highest probabl for ani tree under thi vp here .
and similarli i have to us the highest probabl for ani tree underneath thi preposit phrase .
that's where thi pi  valu come from .
so to calcul the highest probabl , for a vp span all of these word , with thi choic of rule , and thi choic of split point , thi is the calcul i'm go to carri out .
now , search over these differ option , the fact that thi max is over the choic of rule and the choic of split point disreflect the fact that we're go to search through all differ composit , decomposit of thi form .
we're go to , to search for all differ choic of rule and all differ choic of split point will therebi find the singl best wai of reach a vp span word three through eight inclus .
so that essenti the justif for why we can calcul these pi  valu us the recurs .
the definit that i've shown you here .
so here's the final algorithm , which put all of these ideal togeth .
so the input to the , to the algorithm is a sentenc , so that's a sequenc of n word , x1 through xn .
and in addit , we have a pcfg .
which consist of non termin , termin symbol , a start symbol , rule , and a set of paramet .
and we're assum of cours that thi pcfg is in chomski normal form .
so the first thing we do is implement the base case of the recurs .
so for i in <num> to n , for each non termin , we defin pi  i , i x .
to be q of q goe to xi , if that rule occur in the grammar , and zero otherwis .
that's the base case i show you earlier .
in the main loop of the algorithm , we implement the recurs definit i just show you .
and the onli thing we need to be care about is to fill in these pie valu for smaller segment , befor we get to larger segment .
and that's what .
these thi loop here is do .
so l is essenti go to be the length of the of the segment that we're fill in .
we go for i caus g <num> to n minu l , and we said j equal i plu <num> .
so , if you go through thi .
we're firstli go to set l equal <num> , and we're go to try i equal <num> , j equal <num> .
i equal <num> , j equal <num> .
i equal <num> , j equal <num> .
and so on , the secondli on we try l equal <num> .
we come back to thi .
we try wa <num> , j equal <num> , i equal <num> , j equal <num> .
so notic here we have segment length of <num> word .
here we haev segment of lenght , <num> word .
and so all thi is do is just make sure that we fill in the shorter segment befor the longer segment and that mean when we calcul pi  i j we guarante to have these pi  valu lower down fill in .
so thi is just appli the recurs definit i show you .
and the onli other thing is that , rememb these pi  valu ar just go to store the maximum valu for the probabl for ani subtre root that x span word i through j , but we realli want to recov the tree that achiev that max .
and we do thi by store back pointer in a veri similar wai to the algorithm we saw for hmm .
so in addit to store pi  i j x , i have bp for back pointer i j x , which is the arg max .
so thi record which rule and which split point actual achiev thi maximum valu .
onc you've fill in all of these valu it's straightforward to us the back pointer to actual trace back the paus tree that is the highest probabl tree under the grammar .
so lastli , let's talk about the run time , the time complex to get thi algorithm .
and it's actual cubic in the number of word in the input sentenc .
so rememb n is the number of word , and it's also cubic in the number of non termin in the grammar .
so thi is the final run time of thi algorithm .
let me explain how we arriv at thi number .
if you consid thi far in to be , the algorithm , there ar order n squar choic for i j and that's the start point and the end point for the pie valu we're calcul so we have basic order n squar choic for these two valu .
and then at each point we have , n possibl valu for x .
and for each valu of x we consid all possibl rule expand x .
and we have , at most , n squar possibl rule , becaus there's n choic for y and there's n choic for z .
and then s is also , as at most , littl n valu , put us in the rang of i to j minu on .
and i and j between on and .
okai .
so we have , in , thi impli , final , we have n cube time , big n cube time the final complex of the algorithm .
so , again , what i need to emphas here is , thi is wai , wai better than brute forc search .
rememb , it's easi to come up with the grammar where the number of possibl pars tree for an input sentenc is .
exponenti in the size of the input , and the length of that sentenc .
and so we now have polynomi time algorithm .
which is cubic in the , the length of the sentenc .
and also cubic in the number of non termin in the grammar .
so , to summar .
what we've seen in thi lectur of the cours is that pcfg augment cfg by simpli introduc a probabl of each rule in the grammar .
thei there by assign a probabl to everi possibl pars tree under the grammar where the probabl is just calcul as the product for probabl for the rule in the tree .
to build a parser base on a pcfg , you go through the follow step .
so firstli , you learn a pcfg from a treebank .
and as we saw , that's realli quit trivial , which is read off the rule from the treebank , and then comput maximum like estim base on simpl count .
and secondli given a new test data sentenc we can us thi dynam program algorithm the cky algorithm to comput the highest probabl tree for the sentenc on the pcfg .
so , that's essenti it , what i'm go to describ nest is actual we ar go to look at some weak of pcfg .
we're go to give some argument for why thei ar realli rather poor model for languag .
when thei were initi tri in the '90s when tree bank first becam avail .
the result were disappoint .
but then we'll see there ar some fairli simpl wai to augment pcfg in wai which make them much more sc , effect pars model .
and inde , mani of the state of the art model for pars nowadai ar directli base on the idea of pcfg you've seen in thi lectur .
okai , so in the last segment of the cours we saw probabilist context free grammar , pcfg .
we gave basic def , basic definit of pcfg , we saw how to estim a pcfg from a treebank , and we saw how to appli a pcfg to a new test sentenc us the dynam program algorithm , the cky algorithm to recov the most like pars tree for a given sentenc .
so , let me just give some histor background .
the first treebank were creat in the earli 1990s .
so , the earli 1990s wa the first time we had data of thi form from which we could learn , for exampl , the pcfg .
and when these resourc becom avail , it wa veri natur for peopl to consid pcfg .
pcfg have actual been around for a long time , thei've been known sinc at least the 1960s .
and so , peopl immedi , immedi appli pcfg basic in the wai i show you in the last lectur , but the perform wa realli quit poor .
roughli <num> accuraci , at least on the wall street journal treebank i wa talk about , where thi accuraci is the accuraci in recov basic subpart of pars tree , basic , basic constitu within pars tree .
so , if you look at pars tree with thi kind of accuraci , thei ar realli , realli quit poor .
pcfg were quit a disappoint .
modern parser perform consider better .
round again , for the wall street journal dataset , i wa talk about low 90s in term of accuraci , on english at least .
so , what i'm go to do in thi next segment of the cours is describ some properti of pcfg which realli explain thi low number .
two critic weak .
on is lack of sensit to lexic inform .
and two is lack of sensit to structur frequenc .
but we'll then go on to formal them as such as lexic , pcfg , which build veri directli on idea from pcfg and yet have much more essenti have state of the art perform .
so , we'll see that while raw vanilla pcfg perform veri poorli , it's possibl with some refin to pcfg to build a modern parser with state of the art perform .
okai , so let's start talk about these weak , take each of these two thing in turn .
so , let's first talk about thi problem of lack of sensit to lexic inform and we'll first look at the independ assumpt made in a pcfg and see how thei lead to real issu in thi , in thi respect .
so , here's a veri , veri simpl tree .
and as we saw , the probabl of thi tree is go to be a product of term , on q paramet for each rule that we see within the tree .
and what is strike about pcfg is that thei make veri , veri strong independ assumpt .
in particular , if we look at the choic of ani particular word in the tree , then onc we condit on the non termin abov that word , i . e . , the part of speech , the assumpt in the pcfg is that the choic of thi word is condition independ on everyth els in the tree onc i condit on the part of speech .
so , we're basic make the assumpt that the part of speech carri all the inform you could probabl , possibl need about the ident of thi word under that part of speech .
so , that is an extrem strong independ assumpt to make the assumpt that thi word is independ of everyth els in the tree onc i condit on thi non termin .
and in particular , thi word is independ of the other word in the sentenc .
and thi is an extrem strong assumpt , and it's a veri bad assumpt , for natur languag .
so , let's look at a coupl of exampl where thi independ assumpt , lead to real problem .
and we're go to first start with a case of preposit phrase attach ambigu .
and so , the sentenc here is , worker dump sack into a bin .
and there's a preposit phrase here , into have been and there ar two possibl attach , henc two possibl pars tree .
in thi first pars tree , which is actual correct , the preposit phrase attach to the verb phase , dump sack .
in the second pars tree here , the preposit phrase attach to thi noun phrase , sack .
okai .
so now , if we look at thi exampl more close , we can list the context free rule which is seen in the two tree that i just show you on the previou side .
so , thi is the list the list of rule seen in the first pars tree and thi is the list of rule seen in the second pars tree .
and the first thing to note is that mani of these rule ar actual ident .
in fact , the onli wai in which these two pars tree differ , is in the choic of a rule here , vp goe to pp preposit phase , thi wa actual the vp attach of the preposit phrase .
and , here we have np goe to np preposit phase .
so , if you think about the wai we calcul the probabl of each of these tree , we simpli multipli togeth the probabl of the individu rule .
and so , the decis as to which of these two tree is more probabl is go to come down to just these singl paramet correspond to these two rule which ar differ .
so essenti , if the q paramet for np goe to np preposit phrase is greater than q of vp goe to vp preposit phrase , then thi tree will win , otherwis , thi tree will win .
so , the entir decis between these two tree come down to these two paramet , and the attach , attach decis is basic made complet independ of the word in thi particular strain .
so , if we look at preposit phrase attach , thi is a spectacularli bad thing to do .
and let me give you a , a bit of context here .
so , let's look at the <num> main word in thi attach decis .
so dump sack into bin , so thi is a verb , thi is the first noun , so we call thi n1 , thi is the preposit and thi is n2 .
so , statist speak , if you make a decis without ani knowledg of the word you alwai go for a noun phrase attach or you alwai go for a verb phrase attach .
you might get around <num> accuraci in thi attach probabl problem .
so , you might get around <num> of the attach correct .
that reflect the fact that , in fact , there ar pretti much the same frequenc of noun phrase attach versu verb phrase attach .
mai , mayb a veri , veri slight bia toward on or the other .
if howev , you look at the four word involv in the attach decis , you can get up to around <num> accuraci , which is still not perfect which is , but it's consider better .
it's not perfect becaus preposit phrase attach ambigu is still a veri , veri difficult problem .
but we can , nevertheless , do much , much better than the accuraci here .
thi goe back to work in the earli dai of statist pars , where peopl consid just thi isol problem .
given four word , can i predict whether we attach to the noun or the verb and thei us variou machin learn method for thi , and thei us supervis data .
so , exampl like thi , with a label .
so , a label , in thi case , would be the verb label becaus thi is a verb attach .
you would come collect a few thousand exampl like thi and then see if you could us machin learn to make the predict .
if you , in other word complet , do realli rather badli if you build a classifi that actual look at the word , you can do much , much better .
anywai , bottom line pcfg make thi decis without ani refer to lexic inform , it's complet independ of the word , and we know that thi is a , a veri sub optim decis in thi case .
here's the second exampl , thi is a case of coordin ambigu .
so here , i have a phrase , dog in hous and cat , and there's an ambigu as to what thi noun phrase cat is coordin with .
so , in thi first exampl , it's coordin with dog and hous .
and in the second exampl , it is coordin with just hous .
so , anoth classic exampl of ambigu , these kind of coordin ambigu come up everywher .
again , we can plai thi game , where we simpli list the rule in the two analys that i've just shown you .
and in thi case , we see the follow .
so , here is the set of rule in the first analysi .
here is the set of rule in the second analysi .
and the rule ar actual ident , we have exactli the same set of rule in the two pars tree that i just show you .
and so , becaus of thi , thei have to have ident probabl under ani pcfg .
so , there is simpli a tie , in thi case , between the two differ pars tree .
so , even though the pars tree appli the rule in differ order , that's how we get differ analysi , the set of rule in two pars tree is the same and so the probabl in the two pars tree ha to be the same .
and the pcfg complet fail to displai a prefer from on pars tree or the other , and in particular , it complet ignor the lexic inform .
so , that's a coupl of exampl where ignor lexic inform lead to real problem .
there ar mani , mani other .
but let me go on to the second caus of problem with pcfg , and thi is a failur to model structur prefer .
thi is a phenomenon call close attach that i'm go to show you on thi slide .
so let's assum we have some sequenc of word noun , preposit , noun , preposit , noun .
for exampl , we might have presid of a compani in africa .
and again , we have a preposit phrase attach ambigu here becaus thi preposit phrase , in africa , could attach to the most recent noun , compani , or it could attach to thi more distant noun , presid .
so , we either have a compani in africa or , or we have a presid of africa , that's the ambigu .
and here , we have these two structur , thi is the close attach , so i should have said , thi attach where the preposit phrase attach to the closest possibl attach point , is call close attach .
and in thi second exampl , we have attach to the you can verifi the both of these pars tree have exactli the same set of context free rule and therefor again , receiv ident probabl under a pcfg .
and so the pcfg fail to distinguish between these two thing .
but if you look at the statist , thi first structur , close attach , is significantli more frequent than the second structur .
it actual occur about twice as often as the second kind of structur .
so , even befor you look at the word , there's a fairli signific structur bia for these kind of close attach as oppos to these further attach .
thi close attach prefer becom even more pronounc if we look at exampl involv attach to verb .
and thi is actual an exampl sentenc which i'd shown you earlier in the class .
so , the ambigu here is that the preposit phrase by bill can either attach to shot , thi is the close attach or it can attach to believ .
so , in the shot interpret bill wa do the shoot .
in the believ interpret bill believ that john ha been shot .
okai , so there's two possibl analys .
human , i think , would have a veri strong prefer for the shoot analysi .
thei might a , might not even see the believ analysi in thi case .
if you look at a pcfg , you will see that these two analys again have ident set of rule .
and so the pcfg again is go to assign the same probabl to both of these analys .
howev , if you look at thi kind of case where a preposit phrase ha two differ verb which it can attach to , the close attach is , at least in the wall street journal data , we , we have , about <num> time more , more like .
so , there is a veri , veri signific statist prefer for the close attach here , which the pcfg complet fail to captur .
okai , so i've outlin some weak of pcfg .
lack of sensit , lexic inform failur to model these kind of structur prefer .
in the next segment of thi cours , we're go to look at refin to pcfg , which fix mani of these problem and lead to much , much more accur pars model .
