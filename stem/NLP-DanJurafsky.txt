hi , i'm dan jurafski , and chri man and i ar veri happi to welcom you to our cours on natur languag process .
thi is a particularli excit time to be work on natur languag process .
the vast amount of data on the web and social media have made it possibl to build fantast new applic .
let's look at on of them .
question answer .
you mai know that ibm's watson won the jeopardi challeng on februari sixteen , <num> .
answer question like william wilkinson's book inspir thi author's most famou novel .
and you mai know that the answer is bram stoker who famous wrote dracula .
anoth import task is inform extract .
for exampl , imagin that i have the follow email from my colleagu chri about schedul a meet .
we'd like softwar to automat notic that there ar date , like tomorrow ; time , like ten to eleven <num> ; in a room , like gate <num> ; extract those inform , creat a new calendar entri , and then popul a calendar with thi kind of structur inform , with the event , date , start , and end , for a calendar program .
and modern email and calendar program ar capabl of do thi from text .
anoth applic of thi kind of inform extract , involv sentiment analysi .
imagin that you're , interest in camera and you're read a lot of review of camera on the web , so here's a bunch of , bunch of review .
we'd like to automat determin , from the review , that what peopl care about in camera , ar particular attribut .
if thei're bui a camera , thei want to know if it ha good zoom or afford , or size and weight .
so , you want to automat determin those attribut .
and then we'd like to automat , for ani particular attribut , determin how the review felt about those attribut .
for exampl , if a review said nice and compact to carri , that's a posit sentiment , and here's anoth posit exampl .
but a , but a phrase like flimsi is a neg sentiment .
so we'd like to automat detect for each sentenc what the sentiment is , and then aggreg for each featur for , sai , presum for afford , so we might decid that thi camera , the review realli like the flash .
but thei weren't so happi about the eas of us .
we might measur the posit and neg sentiment .
about each attribut and then aggreg those .
machin translat is anoth import new applic and machin translat can be fulli automat .
so for exampl , we might have a sourc sentenc in chines and here's stanford's phrasal mt system translat that into english .
but mt can also be us to help human translat .
so here we might have an arab text and the human translat translat it into english might need some help from the mt system , for exampl , a collect of possibl next word that the mt system can build automat and help the human translat .
let's look at the state of the art in languag technolog .
like everi field , nlp's divid up into specialti and sub specialti .
a number of these problem ar pretti close to solv .
so , for exampl , spam detect , while it's veri hard to complet detect spam in our email box , we don't have , <num> percent spam , and that's becaus spam detect is a rel , easi classif task .
a coupl of import compon task , part of speech tag and name entiti tag .
we'll talk about those , later in the cours .
and those work at pretti high accuraci .
we're gonna get <num> percent accuraci in part of speech tag , and we'll see how that's import for pars .
in other task , we're make good progress .
not as commerci not as complet solv but , there ar system out there that ar , that ar be us .
so we talk about sentiment analysi the task of decid , thumb up or thumb down on a sentenc or a product .
compon technolog like word sens disambigu decid if we're talk about a rodent or a comput mous when peopl talk about mous in a search .
we'll talk about parc which is good enough now to be us in lot of applic , and machin translat usabl on the web .
a number of applic howev ar still quit hard .
so for exampl , answer hard question like how effect is thi medicin in treat that diseas , by look at the web or by summar inform we know is quit hard .
similarli , while we made some progress on , decid that , the sentenc xyz compani acquir abc compani yesterdai mean someth similar to abc ha been taken over by xyz .
the gener problem of detect that two phrase or sentenc mean the same thing the paraphras task still quit hard .
even harder is the task of summar , read a number of , let's sai , new articl that sai that the oh the dow jone is up or the s p500 ha jump , and hous price rose , and aggreg that to give user inform , like , in summari , the economi is good .
and final , on of the hardest task in natur languag process carri on a complet human machin commun in dialogu .
so , here's a simpl exampl ask about what movi is plai when and bui movi ticket , and you can get applic that do that todai .
but the gener problem of understand everyth the user might ask for , and return a sensibl respons , is quit difficult .
why is natur languag process so difficult ?
on cute exampl ar the kind of , ambigu problem that ar call crash blossom .
so , ambigu is ani case where a surfac form might have multipl interpret .
a crash blossom is the name for a kind of headlin that ha two mean , and the ambigu caus , a humor interpret .
so , read thi first headlin , violinist link to jal crash blossom . you might think that the main verb is link and the violinist is be link to what .
he's be link to japan airlin's crash blossom .
well , what ar crash blossom ?
well thi headlin gave the name to thi phenomenon becaus the actual interpret that the headlin writer intend , the main verb wa blossom .
who doe the blossom , the violinist , and thi fact about be link to ja crash wa a modifi of violinist .
similar kind of syntact ambigu .
so here teacher strike idl kid , the writer intend the main verb to be idl .
the strike caus the kid to be idl , but , of cours , the humor interpret is that the teacher is strike .
strike is the verb .
and we have a teacher .
strike idl kid .
anoth import kind of ambigu , is word sens ambigu .
so in our third exampl , red tape hold up new bridg , the writer intend hold up , to mean someth like delai .
call that sens on of hold up .
but the amus interpret is the second sens of hold up , which we might write down as to support .
and now , we get the interpret that liter red tape , as oppos to bureaucrat red tape , is actual support a bridg .
and , we can see lot of other kind of , ambigu in these actual headlin .
now , it turn out that it's not just amus headlin that have ambigu .
ambigu is pervas throughout natur languag text .
let's look at a sensibl , non ambigu look headlin from the new york time .
so the headlin shorten it here a bit , is fed rais interest rate , bui that seem unambigu .
we have a verb here , a vital parser rais .
what get rais ?
a noun phrase , a vital role to announc here interest rate .
and we have a verb phase , so rais interest rate and then we have the fed .
make a littl noun phrase .
and then we'll sai , thi is a sentenc that ha a noun phrase , fed , and a verb phrase , rais .
and what get rais is interest rate .
so , thi is call a phrase structur pars .
we'll talk about that , later in the cours , phrase structur .
so , we could also write a depend pars .
so , we sai the head verb , rais , ha an argument which is fed , and ha anoth depend , which is rate .
and , rate ha anoth , itself ha a depend , interest .
so , we can see the main verb is rais .
well , anoth interpret of the veri same sentenc , on that peopl don't see but that parser see right awai , is that it's not rais that's the main verb of the sentenc , but interest .
somebodi interest someth , and , that someth that get interest is rate .
and what is interest these rate , well .
it's fed rais , rais by the fed .
so it a complet differ sentenc with a differ interpret that someth is interest , the rate , whatev that could mean , and it seem an unlik interpret for peopl .
but of cours , for a parser , thi is a perfectli reason interpret that we have to learn how to rule out .
in fact , the sentenc can get even more difficult .
thi is , the actual headlin wa some , somewhat longer so we had fed rais interest rate half a percent .
here we could imagin that rate is the verb and now we have what is read fed rais interest .
the interest in feder rais .
ar rate , half a percent , so we might have a , a depend structur like thi .
so again , interest .
rate .
the rais ar what do the interest and the fed is a modifi of rais .
so , whether with our , phrase structur pars , or depend pars , and even more so as we add more word when get more and more ambigu , that have to be solv in order to build a pars , for each sentenc .
now , the format of the cours you're go to have in video quizz and most lectur will includ a littl quiz .
and thei're there just to check basic understand .
thei're simpl multipl choic question .
you can retak them if you get them wrong .
let's see on right now .
a number of other thing make natur languag understand difficult .
on of them is the non standard english that we frequent see in , text like twitter feed , where we have , capit and , unusu spell of word , and hash tag and user id's and so on .
so , all of our , parser and part of speech tagger that we're gonna make us of ar often train on veri clean newspap text english but , the actual english in the , in the wild .
will caus us a lot of problem .
we'll have a lot of segment problem for exampl if we see that the string y o r k dash ani w as part as new york new haven , how do we know , the correct segment is new york ?
and new haven .
so the new york , new haven railroad .
and not someth like .
york dash new .
thi word here is not a word like in dash law .
we have to solv the segment problem correctli .
we have problem with idiom , and with , new word that haven't be seen befor .
and , we'll also have problem with entiti name , like the movi , a bug's life , which ha english word in it , and so it's often difficult to know where the movi name start and end .
and thi come up veri often in biologi .
where we have gene and protein name with english word .
the task of natur understand is veri difficult .
what tool do we need ?
well , we need knowledg about languag , knowledg about the world and a wai to combin these knowledg sourc .
so gener the wai we do thi is to us probabilist model that ar built from languag data .
so , for exampl , if we see the word maison , in french , we ar veri like to translat that as the word hous in english .
on the other hand if we see the word avoc all in french , we ar veri unlik to translat that as the gener avocado .
and train these probabilist model in gener can be veri hard .
but it turn out that we can do an approxim job of probabilist model with rough text featur and we'll introduc those rough to , text featur as we go on .
so our goal in the class is teach kei theori and method for statist natur languag process .
we'll talk about the viterbi algorithm , niev base , and maxen classifi .
we'll introduc n gram languag model and statist parc .
we'll talk about the invert index and tfidf and vector model of mean that ar import in inform retriev .
and we'll do thi for practic , robust , real world applic .
we'll talk about inform extract , about spell correct , about inform retriev .
the skill you'll need for the task , you'll need simpl linear algebra so you should know what a factor is and what a matrix is , you should have some basic probabl theori , and you need to know how to program an either job over python becaus there'll be weekli program assign , you know have your choic of languag .
we're veri happi to welcom you to our cours on natur languag process and we look forward to see you in follow lectur .
welcom back .
we've introduc the idea of inform extract , and on of it core task , name entiti extract .
now we'd like to turn to a second task of inform extract .
the task of extract relat or relat extract .
consid the follow sentenc from a compani report .
intern busi machin wa incorpor in the state of new york on june sixteenth and so on .
we'd like to extract a rel complex relat about compani found .
we'd like to know that there , that there wa a found event and that the compani wa ibm , that the locat wa in new york , that there wa a date , that the compani wa origin name ctr and so on .
so all thi kind of fact about thi larg structur relat .
that's the gener task of inform extract as we've defin it .
but we're gonna defin a simpler task of relat extract which we're gonna call the task of extract relat tripl .
so , instead of these complex event we'll extract thing like found year , ibm , <num> .
so , a simpl relat between a predic and two argument .
ibm wa found in the year <num> , found year ibm , <num> .
and the same with locat , foundat locat ibm , new york .
so , a seri of individu tripl , and that'll be our task , extract these tripl from text .
so , for exampl imagin the follow .
we complet a page about stanford univers .
take a sentenc like the leland stanford junior univers , commonli refer to as stanford univers , and so on .
from these fragment of sentenc , we'd like to extract relat like the follow , that stanford is locat in california .
and we get that from locat in stanford , california , or found in <num> .
found the univers in <num> and the founder wa leland stanford and here we have leland stanford .
so from thi kind of text , our job is to extract individu relat between the entiti and the text .
relat extract is import for all sort of applic .
ani time we need some kind structur knowledg and the knowledg origin is in the form of text , it's more easi for an applic to get knowledg from a structur databas .
so we'd like to extract these textual fact into structur form and that could be new structur databas or we could be ad word to current databas , like the word net , the sourc that we'll talk about later or freebas or ddp that we'll talk about later todai .
these kind of relat ar quit us , for exampl , in task like question answer .
so imagin the follow question that's modifi from a question that wa ask on the jeopardi show .
the granddaught of which actor star in the movi , et ?
and in order to answer thi question , we'd like to know that the , relat in the question includ act in et .
so we want to know who act in et .
and we have to know that someon is an actor , and that someon is somebodi els's granddaught .
so these kind of relat will help , both in what's be ask in the question , and then find the answer in the raw text .
that's all great , but what relat should we be extract ?
well , on set of relat come from the autom contact extract , or ac task , where seventeen relat group into six class were defin .
so , for exampl , we had the class of person social relat .
so we had famili , so , that's gonna be relat to a , a parent or a brother , relat between peopl .
or we had physic locat , someth as locat in some place or locat near some place , or .
or affili with some organ .
i wanna be the founder of an organ or the owner or some member of an organ .
or i might be the creator of some artifact .
the manufactur or inventor of an artifact .
or i might be a corpor subsidiari .
or i might be a geograph sub part of some other entiti .
so these ar all the kind of relat that ar in a's .
and so , for exampl we might have the physic locat in relat and that might hold between a person like he and some kind of geopolit entiti like the state of tennesse .
or a part whole subsidiari that might hold between two organ , the first organ the parent compani of the second organ .
or , we talk about famili relat , so , again , a hold between two peopl , that a wife and a husband ar relat by the famili relat .
or , you might have the founder relat , that hold between a person and an organ .
so here , we have a relat between steve job and appl .
so , these ar the kind of relat that we extract in a , in a , us the set of relat .
of cours there's differ set of relat for everi possibl task .
so for bio medic inform extract we might wanna us the uml , the unifi medic languag system which defin entiti and relat as well .
so for exampl we have entiti like injuri or physiolog function and thei can be relat by relat like disrupt or locat of .
so , for exampl , we might have some pharmacolog substanc , some drug .
and that might caus some patholog function might caus some diseas or caus some , some problem , or we might have some other patholog , sorri , excus me , pharmacolog substanc , or we might have some other pharmacolog substanc that instead treat some patholog function .
so , we have lot of differ entiti , in thi case <num> of them in thi dataset and <num> relat that can hold between them .
now for each applic we might have a differ set of relat that might be us .
so for exampl , the sentenc doppler echo choreographi can be us to diagnos , left anterior descend arteri stenosi .
we might extract from that sentenc , a relat , of , diagnos , diagnosi between thi techniqu and thi stenosi .
now there ar lot of exist data basi of relat that have been extract from public sourc like wikipedia for exampl .
so to take the that same stanford univers wikipedia page you mai notic over on the right here a littl structur part of the wikipedia page that's call the wikipedia info box .
if you look in more detail at what we have there .
we have in fact a set of relat , so we have type privat , presid john hennessi , locat stanford and so on .
so each of these kind of relat name and the valu of the relat .
and if you look at the sourc for thi we can see the actual info box .
and here for exampl we can see that the citi of the univers is stanford , the state is california and the motto die luft der freiheit weht and so on .
and so we can extract these relat directli out of the info box state , their relat state , their relat model hold between stanford and california ar stanford in the motto .
these databas that draw from wikipedia tend to repres the inform in an order call resourc descript framework tripl or rdf tripl .
an rdf tripl is just a subject , some sort of predic and an object .
so the predic is what we're call a relat , so locat would be a predic in an rdf .
so we have golden gate park locat san francisco , so a locat relat between golden gate park and san francisco .
and databas like dbpedia , which draw on the wikipedia info box to creat these larg databas of tripl , in fact ha a billion of these tripl <num> million of which come from , should sai million , come from english wikipedia .
and you know , other databas like , like free base .
we have a lot of these kind of relat .
so a common relat is the nation of a person mention in wikipedia , or their profess .
or classif of a biolog entiti .
or which locat ar insid which other locat .
that pari is in franc , and so on .
we can also extract ontolog relat .
we'll talk about thi more when we talk about , thesaurus like wordnet .
but everi kind of relat databas includ , the word call isa or hypernym or subsumpt relat .
so we know that a giraff is a kind of rumin which is a kind of ungul , which is a kind of mammal and so on .
so these isa or hypernym relat ar veri import .
and we can think of those as the same kind of relat , extract them with similar kind of method .
there's a specif type of thi kind of relat where the , we have an instanc , and a particular .
entiti like san francisco which is an instanc of a citi .
so we can have a relationship between class , the class giraff is a sub type of the class rumin .
or we can have an instanc san francisco , which is an instanc of a class like citi .
these relat can be extract from text and we can us them to augment or creat new databas that affirm .
how do we build these relat extractor ?
there's a number of method , like all inform extract , as we saw with the ident tag , we can hand write pattern .
we'll talk about hand written pattern for extract relat .
we can do supervis machin learn and then there's a number of popular method for do semi supervis and unsupervis learn for extract relat and we'll talk about those as well .
we've seen that relat extract is an import compon of inform extract at least for question answer and for build larg knowledg base from text .
perhap the simplest wai of do relat extract is us hand built pattern .
let's look at the intuit .
here's a sentenc .
agar is a substanc prepar from a mixtur of red alga such as gelidium for laboratori or industri us .
perhap you like me i didn't know what the word gelidium meant .
but i can learn what it mean from thi sentenc .
and the wai we know is that we see thi phrase red alga such as gelidium .
so that tell us that gelidium is a kind of red alga .
and thi intuit from an earli paper by marti hurst suggest that there's in fact a lot of pattern that can be us to suggest that two entiti ar in thi is a or relat .
so for exampl , know if i saw red alga such as gelidium , that tell us that gelidium is a kind of red alga , so x is a kind of y .
i might see other pattern , then , like , gelidium or other red alga , or gelidium and other red alga or , red alga includ gelidium .
in each case , i have textual pattern , hold between two entiti .
that ? s a strong queue , that what i have here , is a particular relat , between the two entiti .
and hirsch show that these kind of pattern were abl to learn the isa relat between , new term such as bombard and dong , and bold .
or learn about author name .
or learn countri name and so on .
that could be ad to a databas .
now the same kind of intuit about us hand written rule to learn relat can be us for the rich relat that we've been look at , richer than just isa or hypenem .
and the intuit for learn these more .
which ar rule ar that these relationship often between specif entiti .
so the locat locat in relat often hold between an organ and a locat , where the found relat hold between a person and a organ .
so in addit to just come up with clever string pattern that might indic a relat , we can also us fact about the actual entiti that were involv and that will help us learn which kind of relat we might have .
and we're gonna start with the bentlei tag and that's gonna help us do relat extract .
now mani of these aren't quit enough and to see thi let's look at a coupl of exampl imagin i have an entiti drug and an entiti diseas , what kind of relat can hold between those entiti ?
well , a lot , we could , a drug could cure a diseas , it could prevent a diseas , it could even caus a diseas .
and that's true for ani kind of entiti between a person and their organ .
the person could be the founder of the organ .
there could be an investor in the organ , a member of the organ , an employe of , of the organ , it could be the presid of the organ .
so ani kind of relat mai hold between these entiti although there ar constraint so the person can't cure , can't be the cure or the diseas caus or other kind of thing .
there's there ar constraint from these entiti but lot of other relat meet those constraint .
and so , we're gonna combin these two intuit us a pattern and us the name entiti .
so , we can combin these two intuit , us of the name entiti and us of specif pattern to extract richer relat .
consid the question , who hold what offic in what organ .
so we'd like to extract from a sentenc like , georg marshal , secretari of state of the unit state , that a person , georg marshal .
plai the role or had the , had the offic of secretari of state in an organ in the unit state govern .
and so we have an entiti person an entiti posit and an entiti organ and here's a patter , person comma .
posit of organ that might extract from a sentenc like thi .
the posit of relat between a person and an organ .
and we might need to have anoth pattern for sentenc of a differ form .
so here we have , person truman appoint marshal secretari of state , so we might have person , appoint , or some other word like name , or chose , and so on .
person .
posit and we might have an option of preposit in between so truman appoint marshal as secretari of state will be an option there or just truman appoint marshal secretari of state .
so again a combin of pattern with mayb list of possibl word that we might get from a thesauru or some kind of thing and the name entiti of the , of the entiti .
or find anoth pattern .
georg marshal wa name us secretari of state .
so again person mayb a form of word to be and option follow by again on of these name or appoint word and mayb an option preposit and then the organ and the posit .
so by combin the name entiti type with the word or perhap part of speech in between we can creat pattern that extract particular relat .
now an advantag of the pattern extract algorithm for do relat extract , is that these human pattern tend to be high precis .
if you look at a particular domain and you write rule and the rule ar usual pretti high precis , and thei can be tailor to these veri specif domain if you have a particular relat extract task when you write rule , thei can , thei can get thi high precis .
the minus ar that human pattern tend to be low recal .
often , you don't think of a particular wai somebodi might have thought to word a certain sentenc in free text that express a relat and it's also a lot of work to do thi , especi if you have a lot of relat .
you're gonna have to come up with veri specif pattern for each relat and for each domain in thi relat and it would be nice to also get a higher accuraci than you can get from these relat .
so , while hand built pattern ar us method for relat extract , we're gonna look at other method like supervis and unsupervis method as well .
so handwritten rule with nument pattern and other kind of word and part of speech in between ar a good wai to extract relat .
thei have some problem of accuraci and have gener to new domain .
and so we're go to also introduc other method of relat extract like supervis and unsupervis learn .
supervis machin learn is an import wai to do relat extract .
the algorithm work as follow .
we choos some set of relat we'd like to extract .
we choos some set of entiti we'd like to extract the relationship in between .
onc it presum we have some name entiti tagger that can tag those entiti .
and now we find some data and we label it .
so , we choos some repres corpu , we run our name entiti tagger and label the entiti , or we label them by hand if it's small .
and now , by hand , we'll , we label the relationship between each entiti .
so all the relat we're interest in , we're gonna label all of them in the corpu .
and now we break our corpu in a train develop test like we have done in the past for all of our classif task .
and now we train our classifi in our train set , and then we test it in our develop and test set .
for effici reason , we often modifi thi algorithm slightli .
we first find all pair of name entiti , usual occur in the same sentenc or right near each other .
and we build on classifi which just make a ye no decis .
ar these two entiti relat in some wai ?
and if so , we then run to a second classifi , which classifi the relat .
so why don't we build two classifi instead of on ?
usual , if we're , we have a lot of data , thi simpl classifi that sai , these thing ar probabl relat in some wai , can be run veri quickli , can be train fast , and run fast .
we can train in a lot of data .
and that will elimin most pair , becaus most entiti in most sentenc ar probabl not , in , in whatev relat we're look for .
and then we can us distinct featur set specif to decid if two thing ar relat to each other at all , and decid if thei're in a particular relat .
so again we might us the relat for exampl from the autom content extract or ac task .
rememb we had six med relat and seventeen sub type of those relat and given that set of relat .
our task is to classifi the relat between two entiti in a sentenc .
so , imagin thi sentenc , american airlin a unit of amr , immedi match the move , spokesman tim wagner said .
so , we have two entiti , american airlin and tim wagner .
and our task is to decid what the relationship is between those two entiti .
and it might be famili or citizen or employ or it might be noth .
i might be unrel .
or it could be subsidiari or founder or venter , and so on .
so , what ar the featur we're gonna us for thi task ?
and let's for now imagin that we're do , just the task of decid what the relationship is between the two .
so here's the sentenc again .
we have two mention .
mention on is american airlin , and mention two is tim wagner .
so , on import featur is the head word of the two mention .
so , the head word of american airlin is airlin .
so we'll talk more about head word when we get to pars .
but the , s , in thi case , airl , the , american airlin is a kind of airlin .
and the head word of tim wagner will be wagner .
and so , airlin and wagner might be us featur .
and we can creat a new featur which is just the two combin togeth and sometim that's gonna be us becaus we're gonna see the two head togeth often enough that featur might actual tell us some inform .
so we have three featur so far , airlin , wagner , airlin wagner .
you might throw in a bag of word or even a bag of bigram that ar themselv .
so the word american , the word airlin , the word tim , the word wagner ar all word that occur in the and the bigram american airlin .
and the bigram tim wagner occur in the two mention .
and we might pick word or bigram that ar in particular posit to the left and right of the two mention .
so , for exampl , the word befor mention two , so we'll call thi word minu on with respect to mention two , is the word spokesman .
and the word after mention two .
so we'll call thi on with respect to mention two is the word said , or after mention on , if we're count punctuat our first word there is a comma , if we're not count punctuat our first word is a .
and the word befor american airlin is nil , there's no word befor american airlin .
so we can have these word that ar specif , at specif posit befor and after each mention .
and we can have the word that ar in between the two mention .
so , for exampl , thi between region , so a unit of amr immedi match the word spokesman between american airlin and tim wagner .
we can throw in a bag of those word .
so a , amr , of , immedi , all thi sort of thing .
or in fact , if we have enough comput power , we can throw in bag of bi gram as well , so all pair of word between the two entiti .
we've alreadi said that name entiti type , veri import for relat extract .
so , i wanna know that the first entiti is an organ .
so , american airlin an organ , the second mention , tim wagner is a person .
and i might creat a new featur just by concaten those two togeth .
so , a new featur call orgperson which is the concaten of the two name entiti type .
a featur is call orgperson .
and then we might add what's call the entiti level of the two mention .
so , the entiti level is whether an entiti is a name a nomin or a pronoun .
veri often what we have is name .
but we also get nomin and pronoun act as name entiti .
so these two ar both name , so american airlin is a name , and tim wagner is a name , but if thei were instead it or he then we would call thi a pronoun .
and if it wa a nomin like the compani , so not a proper noun , then we call that nomin .
so anoth featur we can us for each other two mention .
than we haven't talk yet about parsen but we can us lot of featur relat to the pars .
onc we pars the sentenc we can extract lot of us pars featur .
and just to give you the intuit without go into the detail of pars we could extract what's call a , a syntact chunk sequenc or a base chunk sequenc .
so there's a coupl of noun phrase follow by a preposit phrase .
so here we have a , a noun phrase and a noun phrase and a preposit phrase and a verb phrase and a noun phrase , and so on .
so thi sequenc of sympathet chunk .
we can actual run a parser and then .
flatten out the pars into what's call a constitu pass and we'll talk about how these work later , but basic thi is sai that we see the pars ha a noun phrase , who's parent is a noun phrase , who's parent is a sentenc , who's parent is anoth sentenc and so on .
so it's a wai of take out complex pars stream , flatten it out .
and we can have a depend path for exampl we can sai that the verb said ha an argument which is wagner and an argument which is match and match ha an argument which is airlin .
so ani of these kind of thing can be us as pars featur for relat extract .
and final we can us gazett and trigger word featur .
so a trigger word .
it's just a list of term of term that might be us in thi particular domain .
so , for exampl , kinship term ar obvious us for just have if we have the famili relat .
so a word like parent of wife or husband or grandpar ar obvious word that ar gonna help in find a famili relat .
and we can get these from onlin data base like the wordnet thesauru or other place .
and a gazett featur is a list of us geograph or geopolit word .
we might have a countri name list in a gazett or other kind of kind of sub entiti like name of river or lake or state or .
citi and so on .
that's gonna help us know that san francisco is in california , and california is in the unit state , and so on .
and we often , when we talk about featur we might , for exampl , for detect name entiti like person name , have a countri name list isn't as us .
but have a list of common person name in whatev languag we're work in , might be a veri us featur .
and so we often call those featur , even though a name list isn't realli a , a , it's a list of name .
but sometim , we , we us the word to mean ani long list of us proper noun that might help us in do name entiti extract .
so in summari , for our sentenc , american airlin , a unit of amr immedi match the move spokesman , tim wagner said , we might have a whole a seri of featur .
so we might have the entiti type of the first .
mention be org , and the second on be person , and the head of the first on be airlin , and the head of the second on be wagner .
and thi concaten type featur whose valu is org purs .
and then the bag of word of all of the word between the two entiti , and the word befor entiti on , which , there isn't on , so that would be none or nil .
and the word after entiti two , which is said .
and then all the variou pars featur that we talk about .
and we combin all these featur and , and we extract them from our train set , we extract them from our test set , and we do standard classif .
and of cours , you can us ani classifi you like .
we talk about the .
we've talk about the max n classifi and the naiv base classifi .
there's other classifi like svm and whatev you like .
and in each case you train your classifi on the train set .
you extract all these featur for decid you relat , train that classifi on the train set , and then you tune ani of your hyperparamet on the and then test on your unseen test set .
like other kind of classif , supervis relat extract is evalu with the precis to recal and f1 .
so just as we saw with other kind of classif , the precis is the number of correctli extract relat over the total number of relat extract and the recal is the number of correctli extract relat over the total number of true gold relat .
so when we test that , that's hand label .
for the correct relat , we can comput the precis to recal and , again , the balanc , the f1 is 2pr over p r .
so in summari .
supervis relat extract let us get high accuraci , if we have enough hand label data and if the test set is , in the , the same domain as the train set .
the minus of supervis relat extract ar the expens of make a larg train set and the gener problem of supervis model , which is that thei don't gener well to differ genr .
so if we know that we're go to run our system on a similar genr to what we have in train , then supervis is a good approach .
if we're worri that the test set's go to be veri differ that the train set , and we have to abl to be veri robust in differ genr then we're probabl go to need to unsupervis or semi supervis method .
so supervis relat extract an import wai to do relat extract in case where we're gonna afford to label a train set and we think our test domain is gonna be veri similar to that train domain .
some of the most popular reason algorithm for relat extract ar semi supervis or unsupervis algorithm .
let's look at them .
what happen if you don't have a larg train set , but what you have instead ar mayb just a coupl of seed exampl , or mayb you have a coupl of high precis pattern ?
can you us those seed to do someth us ?
in the seed base or bootstrap approach to relat extract , we us the seed to directli learn how to popul a relat .
the intuit of the seed base method from hersh <num> is to gather a set of seed pair that have a relat .
and then iter the follow .
we find sentenc with these pair .
we look at the context between or around the pair , look at the word befor or after , or in the relat itself .
and we gener thi context to creat some pattern .
we us the pattern to search for more pair .
if we're on the web , we're search the web .
if we're in a larg corpu , we look in thi corpu .
we find more pair , and now we just iter .
we , we take , these pair .
we find sentenc that have them .
we find more pattern , we gener the pattern and so on , in thi iter loop .
so suppos we're look for where famou author ar buri , and we know that mark twain is buri in elmira new york .
so we might start with that singl c tupl .
we might grap or googl on the web for all environ to that c tupl .
so we might find sentenc like these mark twain is buri in elmira , the grave of mark twain is in elmira , elmira is mark twain's final rest place and then we take out the actual entiti and creat littl variabl .
so we learn that x is buri in y is a pattern or the grave of x's and y's ar pattern or y's x's final rest place .
so we learn these kind of pattern we take those pattern grap from mark tupl and then we iter .
thi approach wa first appli in <num> by sergi brinn , who look at the task of extract author book pair .
so we might have author like isaac asimov and the robot of dawn and william shakespear and the comedi of error .
so , first we might find instanc , so imagin for comedi of error we happen to find these four instanc the comedi of error by william shakespear wa , the comedi of error by william shakespear is , the comedi of error wa on of william shakespear's earliest attempt , and so on .
and now we extract pattern from these , and on wai that . . .
to do thi is to group all these pattern by what's in the middl .
so these two both have comma by in the middl .
and now we take the longest .
common prefix or suffix of the , of the befor and after part .
so both of these have noth befor .
and both of these have a comma after .
so we'll extract a pattern sai we have an x follow by comma by , and then a y , follow by a comma .
or in these two pattern , thei both have comma on up in the middl and thei both have noth befor , and thei both have an's afterward .
we can extract the pattern x and then comma on of and then y and then's .
and again , we iter , now that we've got some new pattern and we find new seed that match that pattern and can , and we continu iter .
the brin approach , wa improv in the snowbal algorithm .
similar itera of algorithm and similar kind of group instanc to extract pattern and the extra intuit of snowbal wa the requir that x and y be name entiti .
so in the , in the algorithm , x and y could be and string .
now we're gonna add the intuit that each of those thing have to be a particular name entiti .
that's go to help us , becaus we know we have a relat between organ and locat .
and , again , we extract word in between or befor or after the two pattern .
and the algorithm also comput a confid valu for each pattern , so that's kind of a new intuit .
anoth semi supervis algorithm extend both of these idea by combin the booth trap algorithm that we saw with the seed base method , with the supervis learn algorithm we saw of a previou lectur .
so instead of five seed in a distant supervis algorithm , we take a larg databas and we get a huge number of seed exampl .
now from those huge number of seed we have a big databas .
we could have hundr of thousand of exampl .
we creat lot and lot of featur and now instead of iter .
we simpli take all of those featur and build a big supervis classifi , supervis by all these fact that we know ar true in our larg databas .
so , like supervis classif , in distanc supervis we ar learn a classifi with lot of featur , that's supervis by the detail , hand creat knowledg in some databas , but we're not have to do the complex , expand of pattern that we saw in the c base method .
but like unsupervis classif , distanc supervis let you us lot and lot of unlabel data and it's not sensit to the genr issu that you might have in supervis classif .
so let's see how it work .
for each relat .
let's see .
we're try to distract the born in relat .
we go to each tupl in some big databas of the bornin relat , we have lot of bornin relat .
edward hubbl , born in marshfield , albert einstein , born in ulm .
we find sentenc in some larg corpu , let's sai we're us the web , that have both entiti , and here's a bunch of sentenc we might find , hubbl wa born in marshfield , einstein born <num> , ulm , hubbl's birthplac in marshfield , and so on .
lot's of sentenc .
and now from all of those sentenc , for all of those differ entiti we extract frequent featur , so we might pars the sentenc , we might just us the word in between .
we might have some , we might have part of speech tag , all sort of thing , we extract lot amount of featur .
and now , we take all those featur and do exactli what we did for supervis classif .
we have a ton of featur .
and we have a larg train set .
and now we just train a supervis classifi .
we'll need , like ani supervis classifi , we need exampl in the train set of posit and neg instanc .
we extract our posit instanc from what we've seen in the databas .
so person , a particular person albert einstein born in is a posit instanc .
so from our supervis classifi we can get a probabl of the born in relat for a particular data point and now what we can condit that on all sort of featur we can extract from each sentenc , so a huge number of featur .
most recent there's been a number of algorithm for do unsupervis relat ex , extract .
often call open inform extract .
where the goal is to extract relat from the web with no train data and no realist relat .
we just go to the web and pull inform out .
and here's the , the algorithm .
call the text runner algorithm .
thei first us .
pars data to train a classifi to decid if a particular relat is trustworthi or not .
and so thei have a small amount of pars data where thei can us veri expens pars featur to decid that a subject and a verb and an object ar in a , like to be in a relat .
train a classifi that can do that ani relat .
and now , thei walk through the veri larg corpu , let's sai it's the web , in a singl pass , and thei just extract ani relat between np .
and we keep them if the trustworthi classifi sai thi is like to be a relat between the two entiti .
and then we rank these relat base on redund .
if we .
relat occur and a lot of time between <num>'s and <num>'s in differ websit when we guess thi is a real relat .
and thi open inform extract algorithm extract relat like , fci special in softwar develop or tesla invent coil transform , and so on , where we can extract a virtual infinit number of possibl relat between ani entiti .
all we have to do is see them often enough time on the web .
how do we evalu the semi supervis and the unsupervis relat extract algorithm .
so sinc , all of these ar extract total new relat from the web or from some larg corpu , there is no gold set of correct instanc .
we can't pre label the web , with all the relat that ar on it .
if we could do that , we'd be done , and that mean that we can't comput precis , becaus we don't know which of the new relat we've extract ar correct , and we can't complet , comput recal , becaus we don't know which on we miss .
so what do we do ?
we can comput an approxim precis .
and we do thi by draw a random sampl of relat from the output of the system and we just check the precis of those manual .
so we take some random sampl , we pull out some the relat in that sampl and we measur how mani of those were correct and that will tell us an estim of the precis of the system .
and we can also do that at differ level of recal .
so we could take let's sai our , our relat ar rank and we've got a probabl .
there's probabl a rank .
we could take the top <num> , <num> by that rank , comput the precis for some sampl of <num> of those , or take the top <num> , <num> , comput the precis for some sampl of <num> of those and so on , and get the precis at differ level of recal .
so in each case we're take a random sampl .
but there's no wai to evalu the actual recal , the complet recal of the system without label the entir databas which , of cours , we can't do .
now we're gonna talk about text process .
the most basic and fundament tool we have for text process is the regular express .
and regular express is a formal languag for specifi text string .
so let's suppos that we're look for woodchuck in a text document , woodchuck can be express in a number of wai .
we could have a singular woodchuck , we could have the plural s at the end .
we could have a capit letter at the begin , or a lower case , and ani combin of these .
so we're gonna need tool to deal with thi problem .
so the simplest , fundament tool in a regular express is the disjunct .
the squar bracket in a regular express pattern mean ani letter insid these squar bracket .
so .
lowercas w , capit w .
squar bracket mean either a lowercas w or a capit w .
so , we can combin that with woodchuck , to match lowercas or uppercas woodchuck .
and , similarli with digit , on , two , three , four and so on .
<num> .
zero match ani digit .
now that wa kinda annoi to write .
so , we'd like to do instead , is have littl rang .
the rang zero through dash nine so squar bracket zero dash nine mean ani charact insid that rang .
and the rang a z , mean ani charact , between a , a capit letter , between a and z .
so let's see if we can see how that work .
so here's an exampl of a red x , a littl tool we're go to us for regular express search , and we have here a littl text from dr .
seuss .
we look , then we saw him step on the mat .
we look and we saw him , the cat in the hat .
and let's try our , our , disjunct , so we can have , the capit w and the lower case w .
and x , excus me , a capit w and a lower case w .
and that's gonna match , as you can see , the capit w's and the lower case w's just fine .
or we could have all the e's and all the m's .
and that's gonna match all the e's and the m's .
or in our rang , we can have all the capit letter .
here's all the capit letter be match .
we can have all the lower case letter , there's a lot of lower case letter there or we can match all of the alphanumer charact , think for a second how to match all of the alphanumer charact .
we can have .
or we can simpli match some of the non alphanumer charact .
we can have space , an exclam point in our squar bracket and that is gonna match , as you can see , some of the non alphabet charact .
okai , so let's go on .
anoth kind of thing we might wanna do in our regular express is negat in our di junction .
we might wanna sai we don't want some kind of , set of letter .
so for exampl , we might wanna sai , not .
a capit letter .
and we can do that by sai , carrot , a through z , in our squar bracket .
carrot , when it occur right after the squar bracket , mean not .
carrot a through z , not a capit letter .
caret a , littl a .
mean neither a capit a or a littl a .
and carrot , e , carrot , mean not an e , and not a carrot .
so you can see that the carrot , when it occur right after the squar bracket , mean not .
but later on mean simpli just a carrot .
so let's take a look at that .
so we can try , find all of the non capit letter .
here's all the non capit letter .
how ? bout all the non exclam point ?
most thing , and the non alpha numer .
sorri , it didn't , non alphabet .
and there's just the space and exclam point , as you can see .
how bout look for a carrot ?
ani carrot in here ?
there ar none .
so there ar no carrot , noth match .
anoth type of disjunct which can be us for longer string is the pipe symbol , sometim call or , or pipe , or just disjunct .
so groundhog or woodchuck can be , can , will mean either the string groundhog or the string wood , woodchuck .
so we can us the pipe symbol sometim for the same thing as the squar bracket , so a pipe b , pipe c .
it's the same as squar bracket abc , and we can combin these thing .
we can combin the squar bracket in the pipe so we can have groundhogg woodchuck but us our squar bracket for express capit at the begin .
and we can see that in our , in our littl exampl .
we can have look or step .
and sure enough , there , the word look and step ar both highlight .
or we can have distinct of just random thing , thei don't have to be word .
we can have all of the at .
excus me , all of the at , and all of the and , ani random string is fine .
final , there's set of special charact that ar veri import in regular express .
the question mark mean that the previou charact is option .
so the question mark after thi u here , i mean , will match the word color , with or without the u .
with , without the u , with the u .
then there ar the two clean oper name for steven clean .
star match zero or more of the previou charact .
so here is the star .
it match zero or more os .
so we have on o follow by zero or other os .
so there's the initi o and zero other os .
and then our h ! .
here's our initi o follow by on o and then the h , and so on .
two , three , and so on .
sometim , more simpl , we can have the , the clean plu , so , that mean on or more of the previou charact .
so , there's our o follow by the plu , mean on or more o .
so there's on o , there's two o's , three o's , and so on .
and the dot , is a special charact mean ani charact , so beg . n can match ? begin' , 'begun' , 'beg3n' , it match anyth .
and final two special charact . . .
the caret match the begin of the line .
so caret , capit a through z match a capit letter at the begin of the line .
the dollar sign match the end of a line .
so a through z dollar match the end of a line , like the capit letter at the end of the line .
and then if we want to talk about a period , sinc period ar a special charact , we have to escap them .
back slash period mean a period .
so a period by itself mean ani charact .
back slash period mean a real period .
let's go look at some of these .
so here's the letter o .
here's zero or , it's , like , make , let's make it on or more o first .
here's on or more o .
so there's <num> o over here , and two o's over here .
and , and now let look at , at begin and end of line .
here is capit letter at the begin of a line .
here's .
capit letter at the end of a line .
oh , there aren't ani .
here is punctuat at the end of a line .
there's all the exclam point at the end of a line .
here's all the period .
rememb we have to backslash our period .
and , if we didn't backslash the period , we would get all the charact , 'caus period match everyth .
all right , let's do on more exampl .
let's look at , thi littl sentenc here , the other on there , the blith on .
let's , let's walk through how to search for word .
let find the word , all the word the in thi littl passag .
so think for yourself how you would do thi .
well , the simplest thing you might do is just type the t h e .
and , that doe a good job of find thi , thi the here .
let's find thi the and that the .
but it miss these two the .
it also find some other thing .
let's fix the first problem .
how do we not onli get the the in the middl but those capit the at the begin , well we're go to us our ar disjunct .
and sure enough , that correctli now match the two , begin of line .
the , the .
but you notic that our pattern , although it now captur someth it miss befor , it still captur thing it shouldn't be captur , other .
there and blith .
so , we need to augment our pattern .
so , how ar we go to augment our pattern .
we realli want the when it's , there's not in alphabet charact around .
we need a space or punctuat or someth non alphabet .
let's just sai non alphabet afterward .
great .
that get's rid of other and there .
doesn't solv blith becaus blith ha an alphabet charact befor it .
so , let's go fix blith .
by sai non alphabet befor ad ether .
there we go .
now we've found all of our , all of our the's .
so .
we , we look for the .
we notic it miss capit exampl , so we , we ad some .
we made our pattern more , more , ri more , expans .
we increas the yield of our pattern .
but that incorrectli return more thing .
so then we , we need to make the pattern more precis .
by , by specifi more thing thi process .
and , that we went through , is base on fix two kind of error .
on is match string we shouldn't have match .
we match there , we match other .
so that's try to , that's solv the problem of fals posit , or thei're call type on error .
we were max thing we shouldn't match .
and the other thing we went through is , to solv the problem of not match thing we should have match .
so we miss those capit the .
and that's deal with the problem of fals neg , or type two error .
and it turn out in nat , in natur languag process , we're constantli deal with these two class of error .
so reduc the error rate in ani applic , and we're gonna see thi again and again , in thi cours , involv two antagonist effort .
we're increas the accuraci or precis , which help us minim those fals posit .
or we're increas our coverag , or technic call recal , minim our fals neg .
so in summari , regular express plai a surprisingli larg role in text process .
and the sophist sequenc of regular express that we've seen veri simpl version of ar often the first model for almost ani text process task .
for harder task , we're often go to be us , and we'll introduc these , these machin learn classifi that ar much more power .
but it turn out even then , regular express ar us as featur in the classifi and ar veri us at captur gener .
so you're go to be return again and again to regular express .
word token is an import part of text process , everi natur languag process text ha to normal the text in some wai , we start by segment or token the word off and we often have to normal the format of each of the word and as part of thi process we're gonna have to break out sentenc from the text .
so let start by talk about thi kind of word token .
how mani word ar there in a sentenc ?
here's a sentenc , i do main , mainli busi data process .
how mani word ar in that sentenc ?
it's a complic question .
there's a word like is a word ?
or how about the cut off main of mainli ?
so we call thing like main a fragment .
we call thing like uh fill paus .
so for certain applic we might want to be count these , if we're deal with speech synthesi or speech recognit , or , or correct thing .
what about cat and cat ?
we talk about the cat in the hat .
so we defin the term lemma .
two word ar the same lemma if thei have the same stem , the same part of speech , the roughli the same word sens .
so cat and cat ar both noun , thei have similar mean .
we could , we sai that cat and cat ar the same lemma , so the same word in that sens .
we defin the term word form to mean the full inflect surfac form , so cat and cat by that definit of word ar differ word .
thei're differ word form .
so we're gonna us differ definit depend on our goal .
so let's look at an exampl sentenc .
thei lai back on the san francisco grass and look at the star , and their , so on , and let's ask how mani word there ar in thi sentenc .
so count for yourself .
we can defin word in a coupl wai .
word type , how mani vocabulari element there ar .
how mani uniqu word there ar , and word token .
how mani instanc of that particular type there ar in run text .
so , how mani token do we have in , in here ?
well , should be easi to count on , two , three , four , five and so on .
so if we count san and francisco separ , we end up with fifteen .
if we count san francisco as on token , we end up with fourteen .
so even the definit of , of a word depend a littl bit on what we're gonna do with our space .
how about type , count for yourself .
well , it's thirteen type , again depend on how we count .
so we have multipl copi .
the word the , there's the and the again it depend if we count san francisco as a on word or two and rememb our lemma .
we might decid that thei and their and sinc thei ar the same lemma although a differ word form , we might want to count them as the same type depend again on our goal .
in gener , we're gonna be refer to the number of token , which come up whenev we're count thing with capit n .
and we'll us capit v to mean the vocabulari , the set of differ type and we'll us set notat so the cardin of the set v is the size of the vocabulari , although sometim for simplif , we'll just us capit v to mean the vocabulari size when it's not ambigu .
so how mani word and token and type ar there in the kind of data set that we look at in natur languag process ?
well , let's look at a coupl of these .
da , data set of text ar call corpora .
and here's three import corpora .
the switchboard corpu of phone convers ha <num> million word token .
and there's <num> , <num> word type in those <num> million word .
shakespear ha just under a million word token .
shakespear is quit a small corpu .
he wrote , <num> , <num> word in hi lifetim .
and in that less than a million word , he actual us <num> , <num> distinct word .
so he had a veri , veri broad vocabulari famous .
and if you look at a veri huge corpu , the googl n gram corpu that ha a trillion differ token , a veri larg number of word , there's .
thirteen million type , so how mani word ar there in english ?
well , if you look at convers , <num> , <num> differ word .
if you look at shakespear , <num> , <num> word .
and if you combin the two , probabl somewher , not quit the sum of the two , but some larger number .
but if you look at the googl engram , we have thirteen million .
and of cours , some of those ar probabl url , and .
and email address , but even if you elimin all of those , the number of word in a languag is veri larg , mayb there's a million word of english and if fact .
church and gale have , suggest the size of the vocabulari grow greater than , with the squar root of the number of token as you get n token .
the squar root of n more vocabulari on it .
so vocabulari keep grow and grow and it's name and other kind of thing that contribut to thi grow in vocabulari .
we ar gonna introduc some standard unix tool that ar us for text process .
so i have here a corpu of shakespear , shakespear's complet work , you can see here's the sonnet and it goe onto a place .
so let's start by extract all the word in the corpu .
so we ar gonna do thi us the tr program .
all right , so the tr program take .
charact , and it map everi instanc of that charact into anoth charact .
and we specifi tr c , which mean compliment .
so it mean , take everi charact that's not of these charact .
and turn it into thi charact .
so in thi case , it's take everi non alphabet charact , and turn it into a carriag return .
so we're gonna replac all the period and comma in the space in shakespear with new line .
so we're gonna creat on line , on word per line in thi wai , so let's look at that .
so there's , we've know turn the sonnet into on word per line .
and now .
we're gonna sort those word , to let us look at the uniqu word type , so let's do that .
and you can see , here's all the as , there's a lot of them and it occur a lot in shakespear .
and , that's , thi is a veri bore wai to look through all of shakespear , we don't wanna do thi .
so let's , instead .
us the program uniq , and the program uniq will take that sort file and tell us for each uniqu type , the count of time that it occur .
so , let's try that .
so , here we have all the word in shakespear , with a count along the left .
thi is the product of the uniqu program , and we can walk through .
so we know that in shakespear the word achiev with a capitol a occur onc , the word achil appear <num> time , the word acquaint six time and so on .
so that's interest .
but , it would be nice if we didn't have to just look at these word in alphabet order .
but if we could look at them in frequenc order .
so let's take thi same , list of word , and now , resort it by frequenc .
so now we have them .
the most frequent word in shakespear is the word 'the' , follow by the word 'i' , follow by the word 'and' , then we have the actual account in shakespear .
so that , here is all the lexicon of shakespear sort in a frequenc order .
there ar some problem , on is that the word 'and' occur twice becaus we didn't map our uppercas word to lowercas word .
so let's , let's fix the map of case first .
so let's try that again .
we're gonna map all of the uppercas letter to lower case letter .
and shakespear .
and we're gonna pipe that .
to anoth instanc of the t r program .
which replac all of the non alphabet with new line and now we're gonna do our sort as we did befor we're go to be uniqu to find all the individu type uniq c tell us the actual account and then we're gonna sort again , n mean numer and r mean start from the highest on and then we'll look at those , so let do that .
all right , so now we've solv the problem of the and , so now we onli have lowercas and we don't have our uppercas and appear .
but we have anoth problem .
we have thi d here .
why is the word d or the word s .
why ar thei so frequent in shakespear ?
we also have to decid a standard that we're gonna need for .
is our word , so for exampl if our input is finland , apostroph s , capit .
how we gonna token finland's depend on , on our goal .
so we might choos to keep all the apostroph in and then we have finland apostroph s , we might choos to replac all the apostroph with noth , we might choos to elimin all the apostroph s es .
similarli we might choos to expand the what're to what ar .
and the i'm s to i am s becaus if we're for exampl look for all the case of i for some sentiment analysi task .
or if we're look the case of negat for some , some task we might want to turn isn't to is not .
how about hewlett packard .
we have to decid whether a word like hewlett packard ar go to be repres .
or n .
or with a space .
the same is true with phrase like state of the art .
we'll have to decid for word like lowercas , should thei have a dash , no dash at all .
should thei have a space .
we talk about the issu of san francisco .
and then issu with period becom a huge issu .
we have to decid if we're gonna repres mph , leav the period in and then all of our algorithm that us period for split thing ar gonna have to be sensit to thi .
the issu of token becom even more complic in other languag .
we have the french phrase l'ensembl .
for the l apostroph to be a separ word , and if so do we turn it into , into the full articl le or do we just keep it as l apostroph , or just an l by itself .
we'd like it to match thi same , the same word ensembl , even if a differ articl occur befor it .
so we're go to want to break them up for some reason but then we're stuck with these sort of non word .
so anoth issu that we have to deal with .
in german .
the long noun ar not segment as thei ar in english .
so , a word like life insur compani employe in english would be segment up .
in german , we're gonna get into these veri long phrase , but spell as a singl word .
so if , for german task , like inform retriev , we're gonna need to do , like compound split .
in chines and japanes , we have a differ problem .
there's no space at all between the word .
so here's and we've shown you the , the origin chines sentenc here .
and now here's the sentenc segment out so here sharapova now live in us , and so on and so on .
in english we segment out , in chines we don't .
so if we wanna do natur languag process on chines in the applic we need to break thing up into word and so we'll need some wai of do that .
similarli in japanes we have the problem that there's no space between word and we have the problem that there ar multipl alphabet that ar intermingl .
there is the katakana alphabet .
there's the hiragana alphabet .
there ar kanji , which ar like the chines charact .
and there's romaji , the roman letter .
anoth complic issu that ha to be dealt with in token japanes .
the word token in chines is a common research problem that ha to be address when do ani kind of chines natur languag process .
and the , charact in chines repres a singl syllabl , often a singl morphem .
and the averag word is about <num> charact long .
so a word , it ha to be broken up into approxim two or three charact .
and there ar lot of complic algorithm .
for thi , but there wa a standard , a baselin segment algorithm call the max match , the maximum match algorithm , also call the greedi algorithm .
so let's look at max match as an algorithm .
we're given a word list of chines .
so a vocabulari of chines , a dictionari , and a string .
we'll start a pointer at the begin of the string .
we'll find the longest word in the dictionari that match the string , so far , start at the pointer .
we'll move the pointer over the word in the string and then we'll go back and move on from the next word .
let's just see an exampl of that work , i'm gonna pick an english exampl , it's easier to think about .
we'll take the phrase , imagin english wa written like chines with no space , we'll have a phrase like , the cat in the hat all ran bun togeth .
and we have a dictionari that ha word like .
the and , and cat .
so we look at thi and we sai what's the longest word in our dictionari ?
that match the begin and the longest word in our dictionari is the , becaus thec is not a word , and theca is not a word , and so on .
so we'll start with the , and now we've gotten to here , and then we sai what's the longest word start with c , and the longest word is cat .
so now we sai , what's the longest word , start with the i , and so on .
and we do a good job .
how about the phrase , the tabl down there .
we've taken the space out of the tabl down there .
what's our segment of a segment algorithm gonna do with the tabl down there ?
think a littl for yourself .
you mai think that what it's gonna do is produc the tabl down there .
but there's a problem , english ha a lot of long word .
english ha the word theta for the variabl .
and so , instead of , the tabl down there , we're gonna get theta , right after that , bled .
and then own and then there .
so we're gonna get theta bled on there .
so max match is in fact not a gener good algorithm for thi kind of pseudo english , english without space , becaus english ha these veri long word and short word all mix togeth .
but sinc chines in gener ha rel consist word length thi work veri well for , for chines .
and it turn out the at modern problemist segment algorithm work even better .
so that's the end of our section on word token .
onc we've segment out word , or token them , we need to normal them , and stem them .
so normal mean differ thing .
for inform retriev , for exampl , we requir that the index text and the queri term have to have the same form .
so we wanna match u . , s . , a . , to usa .
if somebodi ask a question or a queri with , with on of them .
and the answer ha the other , we want them to match .
so it's like implicitli defin some kind of equival class of term .
we might do thi by alwai delet period , for exampl , it might take to have a rule that take u . , s . , a .
to usa .
an altern is some kind of asymmetr expans .
so , for exampl , let's sai it's , we're do inform retriev .
if i enter the term window , i might wanna search for window or window or ani , ani , ani morpholog variant of the word window .
but , if i enter capit w window , i might onli wanna search for capit w window , 'caus the person's presum look for the product and not the part of your hous .
i mean thi is a potenti more power algorithm but less effici and much more complic .
so in gener , we us symmetr and rel simpl expans .
so for exampl , in inform retriev , we gener remov , reduc all letter to lowercas sinc user tend to us lowercas and with some small except .
so for exampl , if we see uppercas in the middl of a sentenc like gener motor , we might want to keep the case .
and thi matter for distinguish the verb fed from the feder reserv bank with a capit f , ar more a group like sail , the stanford artifici intellig lab from the verb sail .
and it turn out that for sentiment analysi or machin translat or inform extract .
case is in fact veri help .
so it wa a big differ between u s and to us .
we also often want to do lemmat , so we're reduc our inflect or variant form to their base form .
so word like am and ar and is will get lemmat into be car car , car , car's , and so on , get lemmat to car .
so a phrase like the boi's car ar differ color should get lemmat to the boi car be differ color .
so in gener the task of limat is find the correct dictionari head word form for a word form that you ar given and of cours thi is veri import for all sort of applic particularli machin translat .
where for exampl if you have a spanish verb like quiero i want or quier you want .
you it's veri import to know if thi is the same lemma as querer , the verb to want .
so , thi gener topic of look at part of word lead us to morpholog .
and morpholog is the studi of morphem .
and a morphem is the smallest unit that make up a word .
we usual distinguish two kind of morphem .
stem that's the core mean bear unit in a word .
and affix , affix is the bit and piec that adher to the stem , often thei have grammat function .
so on thi , i , particular slide in fact , stem is a stem , and s is an affix .
the word affix , just to confus us , is a stem , and the es is the affix , so there's an affix .
there's an s , and there's an s , and mean ful , there's anoth affix , and so on .
so stem is the task of take off these affix to reduc term to their stem .
and it's particularli , histor deriv from inform retriev , although it's us in all sort of applic .
we us the word 'stem' when we specif mean a kind of a crude chop off of , of affix , and thi is of cours a languag depend kind of process , so the english word , autom , autom , automat , autom , we'd like them all to refer to the same stem , automat .
so stem is like a , simplifi version of lemmat where we pick a prefix of the word , us that to repres the word , and we , we , chop off all suffix , that ar relev , lead to that stem .
so here's an exampl , littl text for exampl , compress and compress ar both accept as equival to compress , that's the text .
and if we stem that text , here's the result output so you can see that , that we've lost the e on exampl , and compress and compress have both turn into compress and here we us ar .
we could have us be as our represent of ar , but thi particular exampl we us ar and so on .
the , the simplest algorithm , the most commonli us on for simpl stem of english is the porter algorithm .
and the porter's algorithm is a seri of , an iter seri of simpl rule , simpl , replac rule .
so , for exampl , on set of rule , rule step 1a , take string like .
s s e s and replac them with s s .
so in a word like caress , it chop off the e s at the caress , or a rule that take i e s to i .
chop off the , i in poni , and levi poni .
we're gonna us poni with an i , as the represent in porter stem of poni .
and the rule oper in order , so that , at thi point , if there's ani ss's left thei stai as an s .
but ani other ss's get delet at thi point .
so the s of cat is delet , while the ss of caress is kept .
similarli in step 1b , we might , we remov all of the ing and the ed , so we want to cross off the ing of walk and the ed of plaster , but we specifi the rule veri carefulli , that in the porter's stemmer onli word with a vowel get their ing remov , and that's becaus a world like sing , onli word where there's a vow , an addit vowel befor the ing .
so a world like sing , which ha no extra vowel , sing onli ha on vowel , the vowel in ing , stai as sing .
but walk ing , which ha .
a vowel , and in addit a vowel befor the ing is allow to delet the suffix .
so , if a word ha a vowel follow by ing , the ing is delet .
and there's lot of other such rule .
so , ation turn into at .
so , we can cross off all of relat and the tion and end up with relat .
and izer to iz and so we cross out the r and so on .
and the road get even more complic so as you get to veri long stem your go to remov the al from reviv and the abl and so on .
let's look again at thi rule that strip aim and practic us the unix tool that we saw in the last section .
to look at morpholog in a corpu .
so rememb , why ar we strip the ing's onli if there's a vowel preced the ing ?
here is the rule .
and rememb we said that in a word like walk , we have a vowel befor the ing and so it's okai to remov the ing .
in a word like sing , there is no vowel befor , there's no letter at all befor that s .
there's no previou vowel and so sing , the rule doesn't appli .
and let do a littl .
search four word end in ing , in shakespear .
so were gonna first take all of shakespear and turn all the non alphabet charact .
oop .
turn charact into new line , so we're gonna get on word per line , then we're gonna .
translat all of the upper case to lower case .
so we're deal with combin all the upper case and lower case word .
and now let's grep out , grep .
is a program for , for , find ani line that contain a regular express in a file .
veri us unix program .
so we're gonna look for the regular express , ing .
and , so we'll find all .
word end in i n g and let's sort them .
and then we'll just take on copi of each and count them and then sort them by the count .
and let's see what word we find of end in i n g in shakespear .
and what we see is lot of these word ar not word that in fact we would like to , to , to remov the ing from so , we have word like king and noth and thing and ring and someth and sing and anyth and spring .
so thi , thi is a lot of word , a lot of veri frequent word , in fact that it would be a bad idea to remov the ing .
if we remov the ing from king we'd get k and so on , remov the ing from spring and we get sp .
so let's modifi our rule that we did , instead of sai grep for all word end in ing , let's just go back and chang that to grep for all word with a vowel , we'll just make it be a e i o u , simul our vowel with just the vowel letter and we need some wai to sai there's a vowel and that anyth can happen in between , follow by the ing .
well , how do we sai anyth ?
dot mean ani charact , star mean zero more of those , and now let's look at , at what word we get back from that pattern .
and now , sinc we specifi that the word ha to start with a vowel , we've done a much better job of find two syllabl word where the ing , in fact , is suppos to be strip off .
so there ar still some problemat word , like noth and someth .
we don't wanna noth , and someth , and anyth .
but otherwis .
mayb not cun but otherwis we've done a pretti good job of make the rule a littl bit better .
and so there's a littl explan of , how the porter stemmer work and then how we can actual us our unix tool to do a littl corpu linguist to , to help write rule of thi kind .
so that's a simpl exampl of morpholog .
it turn out that in some languag much more complex morpholog is necessari , and turkish is the famou exampl of thi .
so here's a word in turkish which i am , i won't be abl to pronounc for you which mean behav as if you were among those whom we could not civil .
so i see it's the kind of thing your mother sai to you when , when you've been particularli naughti .
and in turkish thi is on word .
so it's a veri long word with a lot of , stem .
we have the , the , the civil stem , and an affix mean becom , and , and affix mean caus , and an affix mean not abl , and so on .
so in languag like turkish , and as we saw earlier , for , the veri long noun in , german , we're gonna have to do , richer and more complex morphem segment .
so as we've seen word token and now we've seen that word will have to normal and stem to map them to a normal form .
our final discuss in basic text process is segment out sentenc from run text .
so how ar we go to segment out sentenc ?
thing that end in exclam point or question mark , that's realli great becaus those ar rel unambigu clue that we've gotten to the end of a sentenc .
period , unfortun , ar quit ambigu .
if you think about it , a period can be a sentenc boundari .
but , period ar also us for abbrevi like , inc .
or dr . , thei're us for number like , point <num> , or four point three .
so , we can't assum that a period is the end of a sentenc .
so what we need to do , to solv the period problem is build ourselv a classifi .
we're go to build ourselv a binari classifi .
look at a period .
and simpli make that binari ye no decis ; am i at the end of a sentenc , am i not at the end of a sentenc .
and to make thi classifi , we could us handwritten rule , we could us regular express , we could build machin learn classifi .
the simplest kind of classifi for thi is a decis tree .
so here's a simpl decis tree for decid .
whether a word is at the end of a sentenc or not .
so a decis tree is a simpl if then procedur that ask a question and branch base on the answer to the question .
so we sai , am i in a piec of text that ha a lot of blank line after me .
well if so , then i'm probabl at an end of sentenc .
what if there's no blank line after me ?
well , is my final punctuat a question mark or an exclam point ?
if so , well then i'm still probabl end of sentenc .
well if not that is my final punctuat a period .
if it's not , well i'm , i'm not an end of sentenc .
but if i am a period , well then it depend .
if i'm on some long list of abbrevi like the word etc then i'm probabl not an end of sentenc i'm just a period mark an abbrevi like doctor or etc .
but if i'm not an abbrevi then i'm end of sentenc .
so here's a decis tree .
and you could imagin arbitrarili sophist decis tree featur that we could us .
so on thing we could us is the case or , it's call the word shape of the word with a period .
am i an uppercas word ?
am i a lowercas word ?
am i all cap ?
so uppercas mean the first letter is uppercas , lower mean it's lowercas , cap mean it's all cap .
am i a number ?
ani of these kind of word shape featur can give us inform .
an all cap word is veri like to be a .
and a abbrevi .
we can look at the word , with the abbrevi , with the period , we can look at the word after the period , if , the next word start with a capit letter then i'm like to be the begin , a period that end a sentenc becaus the next word start with a capit letter .
and we can look at lot of numer featur .
so we can look at , am i a long word ?
or a short word ?
so abbrevi tend to be rel short , acronym tend to be veri short .
and i can us veri sophist featur , so i can sai let look at the , the , word i'm look at right now .
take thi word and ask .
in a corpu that i alreadi know where the sentenc boundari ar , how often is thi word occur with a period at the end of a sentenc ?
is thi the kind of word that end a sentenc ?
is thi the kind of word , for exampl , that tend to start a sentenc ?
is thi the word that .
thi phrase for exampl the word , the , after a period thei ar like to be a capit the , after a period veri like to start a sentenc with space in between .
so thi will have a high probabl of be a start of a sentenc and we can us these kind of featur depend on condit of each of the word again to help us in decid what is or isn't a end of sentenc period .
now a decis tree is just an if , then and els statement .
so , the , the that , that's just a .
the definit of what a decis tree is .
the interest research is choos the featur .
so we've seen a number of featur you might pick for thi particular task .
in gener , the structur of a decis tree is not , is often too hard to build by hand .
in gener , hand build of decis tree is possibl onli for veri simpl featur or simpl domain .
you might build a simpl decis tree with six or seven rule like thi for some simpl task .
but it's veri hard to do for numer featur becaus .
you have to pick the thresh hold for each of the numer featur .
i am pick up probabl as on of my featur .
i've gotta have a question in the decis tree , is thi probabl greater than some thresh hold data or not and i've got to set all those data and so gener we us machin learn that learn the structur of the tree and learn thing like the thresh hold for each of the question that we're ask .
nonetheless the question in a decis tree , we can think of them as the kind of featur that could be exploit by ani other classifi , whether it's linguist regress or svm or neural net , some of the classifi we'll talk about later .
so thi , thi intuit that we can build a classifi , we can deriv featur that ar good predictor of whether a period is act as an end of sentenc or not , and then we can put these featur into ani kind of classifi , hold for , whatev classifi we're go to be us .
welcom back .
in thi lectur we begin to talk about word mean , and how to decid if two word have the same mean .
let's look at some fundament .
you mai rememb from an earlier lectur we talk about the differ between lemma and word form .
a lemma is a citat form it ha the same stem , the same part of speech , same rough semant .
where a word form is the inflect word as it appear in text .
so for exampl , the word form bank ha a lemma bank , the word form sung ha a lemma sing , and thi is of cours true for all languag .
the , in spanish word duerm ha the lemma dormir .
the you sleep and to sleep .
now each lemma can have multipl mean .
so , for exampl , the word , the lemma bank we can have a sentenc like a bank can hold the invest , or , the east bank .
so here we have some kind of financi institut and here some kind of slope land .
so we call thi differ sens , we sai a sens or a word sens is a discret represent of the aspect of a word mean .
and so we wanna sai that lemma bank here ha two sens or at least two sens sens on thi financi institut , and sens two , thi slope land .
the word homonymi mean the situat in which word share a form but have distinct unrel mean .
we call the two word homonym , homonym , homonymi .
so bank on , financi institut and bank two , slope land ar homonym .
similarli , bat on , a club for hit a ball , and bat two , a nocturn fly mammal , ar homonym .
now homonym come in two form .
thei can be homograph that's word that ar spell the same whether or not thei ar pronounc the same or differ so bank and bat ar homograph .
or thei can be homophon .
so , write and right sound the same , but thei're spell differ .
and so sometim homonym ar both homograph and homophon , so bank and bat ar both homograph and homophon , sometim thei're onli pronounc the same but spell differ .
sometim thei're onli spell the same but pronounc differ .
homonymi caus a number of problem for applic in natur languag process .
so if you ar do inform retriev and you ask about bat care , we don't know if you have a pet fly mammal or if you're a basebal fan .
and machin translat , a similar problem .
if you sai the english word bat , in spanish you might mean the fly mammal or again you might mean a basebal club .
text to speech , the instrument , the string instrument call a bass is pronounc differ than the fish call a bass .
so if we're go to read a word out loud in text to speech we have to know which word we ar talk about .
so consid these two sens of the word bank the bank wa construct in <num> out of local red brick , and , i withdrew my monei from the bank .
ar those the same sens ?
you might call sens two a financi institut .
so i withdrew my monei from thi financi institut and sens on someth about the build .
so here thi , when i sai thi bank wa construct i realli mean the build associ with the bank .
so these two sens , the financi institut and the build belong to the financi institut , ar relat .
so when the two sens of the homonym word ar relat we call the word polysem .
so a polysem word ha a relat , multipl relat mean and most non rare word have multipl mean and so polysemi is quit common .
when the mean between relat word ar systemat , we call thi systemat polysemi or sometim metonymi .
so lot of polysemi is systemat , so for exampl word like school , univers , hospit , all of these have the same polysemi we saw for bank .
thei can mean the institut or thei can mean the build .
so we sai there's a systemat relationship between build and organ .
there's other kind of systemat polysemi .
so we might refer to an author , jane austen wrote emma .
or her work , i love jane austen , mean i love the book that she write .
or jane austen is store on that shelf over there .
tree .
we might talk about the blossom of plum , or the fruit , the plum i at yesterdai , and again , a systemat polysemi between author and work of the author , or tree and their fruit .
so lot of kind of systemat polysemi like thi .
how do we know when a word ha more than on sens ?
on test we often us is the zeugma test .
consid these two sens of the word serv which flight serv breakfast , and doe lufthansa serv philadelphia , where here we mean give a meal to a passeng and here we mean an airlin fli to a particular citi .
and the zeugma test sai let's form a sentenc us both sens of the word serv and see how bad it sound .
so , doe lufthansa serv breakfast and san jose ?
well that sound funni .
and so sinc thi conjunct sound weird , we sai these ar two differ sens of serv .
we sai two word ar synonym if thei have the same mean , in some or all context .
so a nut can be call hazelnut by some peopl , and filbert by other and it mean the same thing .
or a couch and a sofa , or the word big and larg or automobil and car .
so we sai two lexem ar synonym if thei can be substitut for each other in all situat and if that's the case we sai thei have the same proposit mean .
now , as it happen , there ar veri few , or perhap no exampl of perfect synonymi .
even if mani aspect of mean of two word ar , ar ident , thei still mai not preserv their accept depend on polit or slang or regist or genr .
so while water and h2o technic mean the same thing , in practic we're not gonna us h2o to describ our beauti dai in the mountain look at a run stream .
and even big and larg , which seem like thei might be synonym , have differ mean in differ situat .
let's look at that .
so , we might sai , how big is that plane ?
or , would i be fly on a larg or a small plane ?
and it sound like the sens of big and larg ar the same there .
thei seem like synonym .
thei both refer to some , to a measur of size , and were larg on some , some scale of size .
but consid these sentenc .
miss nelson becam a kind of big sister to benjamin .
miss nelson becam a kind of larg sister to benjamin .
now here , big and larg mean veri differ thing .
big sister mean older sister here , and larg sister simpli can't have that mean .
it mean a physic larger sister , which is a funni kind of mean .
and so we want to sai that big here , big ha a sens that larg simpli lack .
big ha a sens , there's a sens of big that mean older or grown up and larg just lack thi sens .
so sometim , even if two word seem to have veri similar mean , some of the sens mai be differ between the two word .
and that tell us that synonymi is realli a relat between sens rather than , than a relationship between word .
so we sai that on of the sens of big , the on mean physic larg and on of the sens of larg , mean physic larg , it's hard to defin , isn't it ?
both of those sens , those sens ar in a synonymi relationship , but not the entir word .
anoth import relationship between word sens is antonymi .
so , antonym ar sens that ar opposit with respect to on featur of mean , and otherwis veri similar .
so we have , dark and light , short and long , fast and slow .
all these ar word which ar , have veri similar mean .
short and long both mean i'm a properti of length , and short just mean that i'm at thi end of the scale , and long mean i'm at the other end of the scale .
so more formal we sai antonym defin a binari opposit or thei can be at opposit end of the scale .
so two kind of antonymi .
so in long and short , and fast and slow , we have thi binari opposit .
or thei can be revers , so in a verb like to rise or to fall , or , or the word up and down .
thei're not at differ end of the scale , thei're differ direct .
so we sai thei're revers direct .
so two kind of antonymi .
again , relationship between sens , specif kind of relationship between sens .
and perhap the most import relationship between sens that we'll see , is that of hyponymi and hypernymi .
so we sai that on sens is a hyponym of anoth if the first sens is more specif or denot a subclass of the other .
so we sai that car is a hyponym of vehicl , mango is a hyponym of fruit , so anoth word for hyponym might be subclass or subordin .
and convers , hypernym , or superordin .
i try to rememb thi by sai that hyper is super .
here we have , vehicl is a hypernym of car , mean it's a supertyp of car ; car is a subtyp of vehicl , vehicl is a s , superordin of car .
fruit is a hypernym of mango .
and so i've given you some exampl here , with the superordin , like vehicl , and the subordin , like car .
so hypernym , or hyponym , hypernym , hyponym , or superordin , subordin will be synonym here .
now , more formal , we sai that the class denot by the superordin or hypernym , extension includ the class denot by the hyponym .
we can defin hyp , hyponymi also by entail .
we sai a sens a of a hyponym , excus me , a sens a is a hyponym of sens b if be an a entail be a b .
and hyponymi is usual transit , so if a is a hyponym of b and b is a hyponym of c , that entail that a is a hyponym of c .
and anoth common name for thi hyponymi hierarchi is the is a hierarchi .
intuit we sai a is a b , or a is a b sometim without the dash .
or we also sometim sai that b subsum a .
and we'll see all of thi term is a , hyponymi , subsumpt , hierarchi , as we proce through the cours .
we often make a distinct between hyponym and instanc , and wordnet , the thesauru we'll talk about it in a bit , ha both class and instanc .
an instanc is an individu , a proper noun that is a uniqu entiti .
and so we sai san francisco , that instanc , is an instanc of the class citi .
so instanc is a relationship between an individu and a class .
citi is a class and so we can sai citi is a hyponym of municip or locat or other kind of , of class .
so , hyponomi , or subsumpt is a relationship between a class and anoth class .
instanc is a relationship between an individu and a class .
these ar some of the fundament definit we'll need in talk about extract word mean .
a thesauru is on of the most import repositori for word mean .
in thi lectur , we'll talk about wordnet , on of the most import thesauri .
thesauri or ontolog ar us for all sort of applic inform extract or retriev , question answer , lot of bioinformat applic , machin translat .
thei plai a role in all sort of place .
on of the most commonli us on , wordnet , is a hierarch organ lexic databas .
it's an onlin thesauru , but also it ha some aspect of a dictionari .
and , wordnet itself is in english , but there's lot of other languag that ar either avail or under develop .
and in wordnet itself we have about , over <num> , <num> noun , and over <num> , <num> verb , and also adject and adverb , and for each of those , we're gonna have definit and ontolog relat .
so for exampl take the word bass or bass , in wordnet .
we have noun and we also have verb .
so , here's a list of noun , each of them , each of the sens , and adject .
so , we have <num> , <num> , <num> , <num> , <num> , <num> , <num> , <num> sens of bass or bass , in wordnet .
so , sea bass or fresh water bass .
or bass voic or basso , and so on .
and in wordnet a sens is defin by what's call a synset or synonym set .
that's the set of near synonym , similar word , that instanti a sens , or a concept .
and also we have a gloss .
so for exampl , take chump .
as a noun we have the gloss , a person who is gullibl and easi to take advantag of .
and thi sens of chump is share by nine word .
so , we have chump on , and fool two , so the second sens of fool , and the ninth sens of the word mark , and the first sens of the word fall gui and so on .
so , each of these sens ha the same gloss .
now it's not everi sens of those word .
so , for exampl gull ha anoth sens , sens two , which is a kind of aquat bird .
so , a synset is set of near synonym , mean particular sens of word that all have a similar mean .
and here's the wordnet hypernym hierarchi for bass or bass .
thi is on sens of bass , an adult male singer with the lowest voic .
and a , you see that a bass is a kind of singer , which is a kind of musician , which is a kind of perform , a kind of person , a kind of organ , all the wai up to physic entiti and entiti .
so with the hyp , hypernym hierarchi goe all the wai to the top .
now , wordnet ha a lot of relat .
we've talk about relat befor .
so we have , hypernym and hyponym .
but wordnet also ha relationship like member .
so the , the relat ha member .
so , for exampl , a group like faculti ha member like professor .
it ha instanc , we talk about instanc .
so the relationship between compos in gener , and the on compos , bach .
or the on compos johann sebastian bach , of cours there's more than on bach .
and then we have meronym and holonym .
so part whole relationship between individu , so a tabl ha a part , which is a leg and a cours ha a part which is a meal and so on .
so those ar the relationship between part and whole .
so lot of these relationship occur in wordnet , and antonymi as well .
now , you can look on the web for wordnet , and here it is .
so we can look for the word bass or bass .
and sure enough , there it is .
there's our sens of bass and we can click on on of them .
let's click on on of the fish .
there we go and now we can ask for the whole set of inherit hypernym and there's a bass is a kind of , or bass excus me is a kind of freshwat fish , a kind of seafood , a kind of food , all the wai up , again all the wai down here to entiti at the bottom .
so here's the whole hypernym or superordin hierarchi .
now there ar standard librari for access word wordnet .
there ar python librari and there ar java librari and we've given you some pointer here .
anoth import thesauru or ontolog is mesh , the medic subject head thesauru from the nation librari of medicin .
so thi ha almost a coupl hundr thousand entri correspond to a lot of head .
so here's on , hemoglobin and it ha a set of what ar call entri term hemoglobin , ferrou hemoglobin , and i don't even know how to pronounc that word .
and these ar the syn that correspond to the synset in wordnet .
thi is the set of synonym , for defin hemoglobin .
and then we have a gloss as well .
so here's the mesh hierarchi , here ar the top , sixteen top level categori .
so we might wanna look insid on of them .
let's sai chemic and drug , and insid those we might wanna look insid amino acid .
and if we do that we can look insid protein , and there's hemoglobin .
so there's the hierarchi for mesh .
and , thi ontolog is us to provid synonym or what ar call entri term in the mesh terminolog ; provid hypernym from the hierarchi just like wordnet .
and mesh is us for index journal articl in the nation librari of medicin's bibliograph databas , medlin pubm , where there's twenti million journal articl , where the articl ar hand assign ten or twenti mesh term .
so a us ontolog .
so we've seen wordnet and mesh , two veri commonli us thesauri or ontolog .
an import place to get represent of word mean and word relat like hyponomi and synonomi .
hi again .
let's turn to how to comput whether two word ar similar or not .
and we'll start in thi section with method that us a thesauru to comput word similar .
so where synonymi wa a binari relat , two word ar either synonym or thei're not , we often want a , a looser definit of similar or distanc , that sai two word ar more similar if thei share more featur of mean , mean , without requir that thei be absolut synonym .
and similar , just like synonymi , is a relationship between sens , not between word .
so we don't sai that the word bank is similar to the word slope .
we sai sens on of bank is similar to the word fund , actual sens three of fund , but it's actual sens two of bank is similar to slope .
rememb we had two sens of bank .
so even though technic similar is a properti of sens , we'll find wai to comput similar over word as well either by take the max similar between sens or sum or variou wai .
now word similar plai a role in lot of applic .
you might want to know if two word ar similar to grab a set of synonym or similar word , for the queri , if someon ask in inform retriev , or for the answer in question answer .
lot of time the translat of a word , you'll have to look for similar word to help find a translat .
essai grade , where student write an essai and you need to know what the similar word ar to the correct word for the concept and so on .
thi come up a lot .
and , in all of our applic for word similar , we'll , we often distinguish , technic , word similar from word related .
so a similar word is a near synonym , but a relat word can be relat in ani wai .
so car and bicycl might be similar .
mayb thei're not synonym , becaus car and automobil ar synonym .
car and bicycl ar not quit so close , close , so there're similar .
but car and gasolin , clearli relat .
gasolin is someth that goe with car but it's not similar to car in some wai .
so we're gener here look for similar , but occasion some of the algorithm will give you instead word that ar relat .
and that might or might not be us depend upon the applic .
there ar two class of similar algorithm .
thesauru base algorithm , we'll talk about in thi section , ar word nearbi in the hypernym hierarchi .
or do word have similar gloss .
we'll us the hierarchi or the gloss as wai of defin similar .
and distribut algorithm that we'll talk about in the next section .
do word have similar distribut context .
so the simplest of the thesauru base similar algorithm is call path base similar and i'm give you a littl pictur here of the wordnet hierarchi .
and here we sai two concept , two sens or synset , we'll call them concept for now , ar similar if thei're near each other in thi thesauru hierarchi .
by near each other we mean have a short path between them and we'll defin path length somewhat unusu .
we'll sai a concept ha a path length on to themselv and two to their nearest neighbor and so on .
so the word , the concept nickel ha a path length on to nickel , two to coin and three to dime becaus it goe on for nickel , two to coin , three to dime .
and the path length between nickel and coinag is similarli three and to , all the wai up to monei is six to coin to coinag to currenc to medium of exchang down to monei and so on .
nickel is even further from richter scale , goe all the wai up to standard and then down to richter scale .
so the path length formal is on plu the number of edg in the shortest path in the hypernym graph between the sens node c1 and c2 .
now we can us , we can turn a path length which is a distanc metric into simpath , a similar metric , simpli by take on over the distanc .
so , we'll take the path length and invert it and we get a similar metric .
and we can turn the sens base metric , simpath is a metric of similar between two sens or concept , c1 or c2 , and turn it into a metric between word by take the maximum similar among pair of sens .
so for all sens of word on and all sens of word two , i take the similar between each of those word on sens and each of those word two sens and i take the maximum similar between those pair .
and that's the similar between the word .
so return to sens base similar , we've got our metric now for path base similar , on over the path length .
so between nickel and coin , we have a path length of <num> <num> <num> .
rememb , we're ad on to the number of edg .
so it's <num> <num> , or . <num> .
between fund and budget , similarli , on over two or . <num> .
between nickel and currenc , we have on , two , three edg .
so on over <num> <num> or four is . <num> .
between nickel and monei , now we have <num> <num> , or six , distanc on over six .
similarli between coinag and richter scale , a similar of on over six .
or for that matter , between nickel and standard , also on over six or . <num> .
now there's a problem with thi basic path base similar , which is that we assum that everi link repres a uniform distanc .
nickel to monei somehow seem to us that it ought to be closer than nickel to standard .
and that's becaus node that ar veri high in the hierarchi ar veri abstract .
and we'd like a metric that sai that node whose onli link is go all the wai up to the top of the hierarchi , those ar probabl not veri similar word .
and to do that , we'd like to repres the cost of each edg independ .
the most common solut to thi problem is the us of inform content similar metric , first propos by resnik in <num> .
and these defin a concept p of c , a probabl of a concept c , as the probabl that a randomli select word in a corpu is an instanc of that concept .
and formal , what i mean is , there's a distinct random variabl that rang over word associ with each concept .
so everi node in a hierarchi ha thi random variabl .
and for ani , for each , for that concept everi observ noun is either a member of that concept with probabl p c , or not a member of that concept , with probabl on minu p c .
so everi word is a member of the root node , which might be call entiti or it might be call someth els , in differ version of wordnet or your own hierarchi .
so that mean that the probabl of whatev the root node is is on .
and the probabl of node right below the , the root node ar go to be veri high .
and the lower you get in the hierarchi the lower the probabl .
let's see how that work .
so here's a littl piec of a hierarchi , we've got entiti here .
there's actual someth abov thi hierarchi in the top , and then we have entiti , then we have down to geolog format and then some leaf note , hill , ridg , grotto , coast , and so on .
and we're gonna train inform content similar , first by train a probabl p c , and everi instanc of a word like hill count toward the frequenc of all of it parent , of itself obvious , but also natur elev , geolog format , all the wai up to entiti .
so if we defin the concept word of c , word of c is the set of all word that ar children of node c .
i should sai plu c itself as well .
so , the word of natur elev ar hill , ridg , and natur elev itself .
the word of geolog format ar hill , ridg , grotto , coast , shore , cave , natur elev , and geolog format itself .
and now we can take for ani concept we sum over all the word of that concept , so itself and all of it children , sum the count of all those word and then normal by the total number of word in the corpu .
and that tell us the probabl of the concept .
so the probabl that a random word will be an instanc of that concept .
onc we've comput these probabl , we can associ them with a hierarchi .
so here's probabl comput by dekang lin , and so now we can sai that the concept coast ha probabl . <num> , while the further up we go in the hierarchi up to entiti , we have a probabl of . <num> .
and whatev in thi particular version of wordnet wa abov entiti will have a probabl of on .
now that we have probabl , we just need two more thing .
we'll defin the inform content of a concept as the neg log probabl of that concept .
so we're just follow inform theoret definit of inform there .
and we'll defin the lowest common subsum of a node , as the lowest node in the hierarchi that subsum both of them .
veri natur .
so the lowest common subsum of hill and coast , think about that .
geolog format .
the lowest common subsum of coast and shore .
shore .
now , how , how ar we go to us thi inform content , as a similar metric ?
there ar a number of method .
the resnik method , we sai that the similar between two word is relat to how much inform thei have in common .
the more thei have in common , the more similar thei ar .
for resnik , we just sai , what's in common between two word , is the inform content of their lowest common subsum .
if i have two concept , what's in common between them is the thing that thei share as their inherit thing in common .
so if i just measur the amount of inform in that , that is in fact what thei have in common .
so the neg log probabl of that least common subsum , that's their similar .
so we'll defin that metric , the resnik similar metric thi wai .
an altern metric for deal with inform theoret similar is the lin metric .
as with resnik , the more two thing have in common , the more similar thei ar .
but now the new intuit , the more differ between a and b , the less similar thei ar .
and we measur common by introduc a predic common , which is a proposit state the common between a and b .
and ic , the informa , amount of inform contain in that proposit .
and to measur the differ between a and b , we sai that the total descript of a and b , the sum of everyth we know about them , is the sum of the common plu the differ .
so to get the differ we can take the descript and subtract out the common .
so roughli speak , a and b ar more similar if ic of common is high and ic of descript is low .
so the lin similar between two concept a and b is higher when thei have more in common , less when there's a lot of other thing about them that thei don't have in common .
and lin modifi resnik in defin the inform content of the common of the two as twice the inform of their lowest common subsum .
and so , given two concept in a heirarchi , the lin similar is two time the log probabl of their least common subsum over the sum of the log probabl of the two concept , the total of the descript that we know about the two concept .
let's look at an exampl in our small sampl hierarchi .
we want to know the lin similar between the concept hill and coast and we look at their lowest common subsum , geolog format .
we take twice the log probabl of that , divid by the sum of the log probabl of the two item , hill and coast and that give us for the lin similar between hill and coast as . <num> .
now a final thesauru base similar metric is call the lesk algorithm after michael lesk who invent it .
it's often call lesk or extend lesk .
and thi method , instead of us the hierarchi , look at the gloss of the word in the dictionari or thesauru .
and the intuit is that two concept ar similar if their gloss contain similar word .
so the two wordnet word , draw paper and decal , have lot of similar word .
paper that especi prepar for us in draft , transfer design from especi prepar paper , blah , blah , blah .
and we have here , the word special prepar ar in common , and paper in both definit .
and so from each n word phrase that's in both gloss , the lesk algorithm add a score of n squar .
so paper is in both gloss .
that's a length on , so we'll add a score of on .
special prepar is of length two , so we'll add a score of two squar , or four .
so thi , the total lesk similar between draw paper and decal is five .
and in fact , with most version of lesk similar , we don't just look at the gloss of the two word , we look at the gloss of the word , their hypernym , their hyponym , and so on , and we add all that up .
so it's a sum , over all the word , or sometim a max over all the word , of their similar .
so , in summari , we've seen three class of thesauru base similar .
path length similar , where two word ar similar if there's a short path between them in the hierarchi .
inform theoret similar .
we've seen two method , resnik and lin , and there's a third on call jiang conrath .
so these ar the inform theoret similar where we're look at the least common subsum of a node , measur it probabl , turn that a , into an inform measur .
and we've seen lesk similar , where given two concept we take the gloss of them , and comput the overlap in word with thi kind of weight that we talk about .
or we might just look at the gloss not just of the word , but of some word in relat to those concept like their hyponym or hypernym , and so on .
and we sum all that up over a specifi set of relat , and that give us the lesk similar , or extend lesk similar .
there ar lot of librari for comput these variou thesauru base method , in python nltk ha method , and there ar python base tool like wordnet similar .
and there's even a nice web base interfac so that you can check out the similar between two word base on differ meth , method and you can go take a look at all of these .
we evalu similar , like mani other nlp algorithm , in two differ wai .
we can do intrins evalu where we look at the metric itself and sai how similar is the , the number thi metric give to what human would give on some similar task .
so i get a similar metric for two word and then i get human to give me a number .
how similar ar these two word ?
and i compar those .
so that's intrins evalu .
more , more function is extrins or task base , end to end evalu where i have some applic , i put my similar metric in the applic and i see how well it improv the applic .
and that applic could be word sens disambigu , or spell error correct or essai grade .
a common simpl extrins test that's us is take the test of english as a foreign languag , or toefl , multipl choic vocabulari test .
so here we have question like levi is closest in mean to which of these four word ?
and we can simpli take our similar metric , comput the similar between levi and impos , levi and believ , levi and request , levi and correl .
and see if our metric return the right answer , the most similar word , or the correct answer is impos .
so thesauru base method for word similar ar a us wai of tell if two word ar similar .
thei're veri function in languag like english , where were we have lot of thesaurus , either for gener text in wordnet , or for medic text in mesh .
thei work less well when we're work in particular genr , where we might not have the right inform in the thesauru , or in languag for which thesauru , thesaurus ar not as avail .
and so for those applic we'll turn to the distribut method that we'll see next .
all right , let's turn to the second import method for comput word similar distribut similar .
now , thesauri have problem for comput similar .
we don't alwai have a thesauru for a particular languag , and even when we do , thesauri have a problem with recal .
so , word ar miss , phrase ar miss , connect between sens mai be miss , and in gener thesauri don't work as well for verb or adject , which have less structur hyponymi relat .
so for all these reason we often us distribut model of mean , often call vector space model of mean , and these tend to give you higher recal than hand built thesauri , although thei might have lower precis .
the intuit of these distribut model of mean come from earli linguist work .
so for exampl , zellig harri in <num> said , oculist and ey doctor occur in almost the same environ .
so if a and b have almost ident environ , we sai thei're synonym .
and firth back in '<num> said you shall know a word by the compani it keep .
so here's an exampl .
we have a bunch of sentenc about tesg ino .
nida sai a bottl of tesg ino's on the tabl .
everybodi like tesg ino .
tesg ino make you drunk .
we make tesg ino out of corn .
so from these context word for a human it's veri easi to guess that tesg ino mean some kind of alcohol beverag .
some kind of beer made out of corn .
so the intuit for an algorithm is two word ar similar if thei ar just surround by similar word , veri simpl idea .
let's see how to make that work .
rememb the term document matrix we saw inform retriev .
so each cell in the term document matrix wa the count of a term t in a document d .
so we call it the term frequenc of t in d .
and we thought about it , what meant to be a document wa to be a count vector .
so a column in thi term document matrix .
so the document as you like it , the shakespear plai is a count vector over lot of word .
i'm show just four word battl , soldier , fool , clown .
so as you like it is a count vector .
so two document ar similar if their vector were similar .
so juliu caesar , high count for battl and soldier , low count for fool and clown .
similarli , henri the fifth , high count for battl and soldier , low count for fool and clown .
so we , so we saw that juliu caesar and henri the fifth , if thei were a queri and a document , or two document , thei were similar by thi vector similar method .
and we're just gonna us the same exact intuit for decid if two word ar similar .
so , look at the word in a term document matrix .
and now a word is a count vector .
and two word ar similar if their vector ar similar .
so fool ha high count in the document as you like it , and the document twelfth night .
and low count in juliu caesar and henri the fifth .
clown ha high count in as you like it and twelfth night , and low count in juliu caesar and henri the fifth .
so we sai that fool and clown ar similar .
but probabl battl and fool ar not similar , becaus battl ha high count here and low count here , but fool ha the opposit .
it ha high count here and low count here .
so the intuit of word similar , distribut word similar , is instead of us entir document , like we us for inform retriev , let's us smaller context .
we could us a paragraph , or we could us just a window of ten word .
and now we defin a word by a vector over these context count , for whatev thi context is .
so let's suppos we us context of ten word to the left , and ten word to the right .
here's a sampl exampl i've grab from the brown corpu .
so here's some word .
the word apricot , the word pineappl , the word digit , and the word inform .
and i've shown you , for each of them , just on set of context .
ten word befor apricot , ten word after apricot , from on of the us of apricot , and on , on document in the corpu .
and here's ten word befor and after pineappl and so on .
so from the variou document , exampl i have grab from the brown corpu , some exampl for each of these word , look like these exampl , i can comput littl count and i can build myself the term context matrix .
so here's the term context matrix for these four word apricot , pineappl , digit , inform .
thei don't ever appear with the word aardvark but the word digit and inform have the word comput within ten word of them , twice for digit , and onc for inform , or the word pinch occur with apricot and pineappl , and the word sugar occur with them wherea the word data and the word result tend to occur with the word digit and the word inform .
now , again , we sai , two word ar similar in mean if their context vector ar similar .
so , apricot ha a on for pinch and a on for sugar .
pineappl also ha a on for pinch and a on for sugar , and zero for comput and data .
so that tell us that probabl , these word ar similar .
wherea , digit and inform , their count occur in other word , like comput , data , and result .
so thei're probabl similar as well .
simpl intuit , just like what we saw in inform retriev for compar document .
but now we're compar word , and us a reduc context .
now for the term document matrix for inform retriev , we us tf idf weight , we didn't us raw count .
we us variou kind of weight .
sometim tf , sometim idf , sometim both .
now for the term context matrix it's veri common to us a version of pointwis mutual inform call posit pointwis mutual inform , so let's look at that .
pointwis mutual inform is an inform theoret method that sai , do event x and y occur more often than if thei were independ .
so the pointwis mutual inform between two thing , x and y , is the probabl of the two occur togeth , divid by the probabl of the two , the product of the probabl of the two independ , and we take the log of that .
so , you can see that , if the two thing occur more often than you would expect by chanc , more often than you expect by independ , then the numer will be much higher than the denomin .
so between two word , the pointwis mutual inform is the log of the probabl of the two word occur togeth , time the product of the two word occur independ .
and posit pmi simpli replac all the neg valu with zero .
so , imagin that we have a matrix f , our term context matrix , we'll call it f for frequenc , and we've got word label by , row label by word , and we have column label by context , which could be a context word , so we saw for exampl the context word aardvark , or comput or data or pinch or so on , and we're gonna take our , here's our count and each of these , each of these count is the , is f sub ij , the frequenc of row i column j .
so , we're go to turn those into probabl first .
so we'll sai that the probabl of a word i and j occur togeth , the joint probabl of a word i and a context j is the frequenc with which thei appear , normal by the sum of all , the frequenc of all word in all context .
so we sum over the entir matrix .
that's the , the denomin n .
the probabl of a word , that's a row i<i> , is the count of all , all of the , all of the context< i> that that word occur in .
so we sum over all possibl context , and we sum all the count for that word in those context , normal by n .
and the probabl of a context is the sum over all word that that context occur in , the c , those count , again normal .
and we take these probabl , the probabl of a word and a context occur togeth time the probabl of a wor , over the probabl of a word time the probabl of a context .
we take that log , and that's our pmi .
and our posit pmi is , if it's less than zero , we replac it with zero .
so let's see how that work in practic .
we've got our , a littl , i made a simpler littl matrix of count here for work through our exampl , so , the word digit occur in the context comput twice , in the context data onc , in the context result onc , and never for pinch or sugar .
okai , we saw befor .
and again , our probabl of , the joint probabl of a word and a context , is the frequenc with which the word occur in the context normal by the total n , the sum over of all count for all word and all context .
let's first comput n .
n is the sum of all these thing .
two , three , four , ten , eleven , twelv , thirteen , fourteen , <num> , <num> .
so n is nineteen .
we've got nineteen .
our , our denomin ar gonna be nineteen for all of our variou probabl .
so now , the probabl , the joint probabl of the word be inform in the context data .
so word is inform , in the context data .
we've got our f sub ij is six , and our n is nineteen , so we have <num> 19th's or . <num> .
so that's the joint probabl of inform and data .
now , we need to comput the probabl of word and the probabl of context .
so the probabl of a word , we just sum a row over all context that that word can occur in .
so the word inform occur eleven time .
onc in thi context plu six plu four .
so we have a total of eleven time over again , n of <num> , or . <num> .
so that's the probabl of a word .
and we do the same thing for context .
we sum over all word that that context occur with over those count and normal .
so for the context data , we have a on plu a six .
so we have seven over nineteen or . <num> .
so if we do thi comput for all of our exampl , we'll get thi littl matrix .
here we have the , in here we have the joint probabl , all the joint probabl .
here we have the margin , the probabl of the word , and the probabl of the context .
so again here's our <num> . <num> and that's the probabl of data .
and here's our <num> . <num> , that's the probabl of the word inform .
and there's our <num> . <num> .
good .
so now we're readi to comput pmi .
recal that pmi is the log base two of the joint over the two margin .
so that's go to be our . <num> divid by our . <num> time . <num> .
and then we'll take the log of that , so that's . <num> , or . <num> us full precis .
and we can see here that , if we take thi log , that we get posit number for comput be link with digit , and inform with data , and pineappl with sugar , and so on .
and we've replac ani of the time we got neg number , sinc thi is posit pmi , we replac the neg number with zero .
now the problem with pmi is that it's weight toward infrequ event .
and there's a a love survei paper by turnei and pentel that walk through some of the wai that can allevi thi .
but it turn out that veri simpl method like laplac smooth can actual help .
let's look at how thi work with add two smooth .
so here i've just ad <num> to all the count , so we just have simpl add two smooth , and recomput our probabl tabl again with our margin for the context and the word .
and now what i've shown you here is our posit pmi tabl .
here's our origin tabl for no smooth .
and here we ar with add two smooth , and what you'll notic is that , in the origin tabl without smooth , the link between apricot and pineappl and sugar is veri high .
we get a veri high pmi despit the fact , i don't know if you rememb that the count were just on .
wherea data for which we had a lot more evid , count of six and four , the link between inform and data , or inform and result , had much lower pmi valu .
ad two affect the lower count more than the higher count , and we can see that the larger count haven't chang veri much but we've discount those excess high pmi valu , and now we have much more reason relat number for our pmi .
so that's the first half of our introduct to distribut similar .
and we'll turn in the second half to how to us cosin metric to actual comput the similar .
all right now , in the previou segment we saw how to comput ppmi , posit pointwis mutual inform .
in thi segment we'll see how to take those valu and comput similar between word .
first let's talk about a differ kind of context than just word context .
a common wai to defin context is to us syntax .
and thi again relat back to earli linguist argument that the mean ha to do with the restrict of combin of entiti grammat .
so in other word , two word ar similar if thei have similar pars context .
and we haven't talk a lot about pars but to give you the intuit .
the word duti and the word respons , can both be modifi by similar adject .
so you can have addit duti , administr duti , assum duti or addit respons , administr respons , assum respons .
and thei can also be object of similar verb .
assert duti , assign a duti , assum a duti , assign a respons , assum a respons .
so it's not just that thei have similar word around them , but that their grammat context can be similar .
thei have similar pars context .
and we can captur thi by us co occurr vector that ar base on syntact depend .
so we sai that the context instead of be count of word with previou ten word or follow ten word , the context instead ar , how mani time i have a particular word as my subject or how mani time i have a particular word as my adjectiv modifi .
so here's an exampl from dekang lin .
we have the word sell and we sai , how often wa thi the subject of the verb absorb ?
well , onc .
how often wa it the subject of the verb adapt ?
how about the subject of the verb behav ?
how about the preposit object of the verb insid ?
so we can get our count for each of these context .
and now our vector is determin not by the count of word that occur within ten word of me , but count of time i occur in a particular grammat relat with a context .
and just as we saw with word count , we can us pmi or ppmi to our depend relat .
so the intuit come from earli work by don hindl .
if i count in a corpu , and i pars the corpu and i see that the verb drink ha the object it three time and it ha i drink anyth three time , i drink wine twice , i drink liquid twice and so on .
well , drink it , or drink anyth , ar in fact more common than drink wine .
but we'd like to sai that wine is a more drinkabl thing than it .
if i found wine occur a lot with a verb , two differ verb , i would think that those verb ar probabl similar .
more than if i found it occur as the object of the two verb .
and if i comput the pmi's , the pmi between the object of the verb and the verb drink , now i see that wine and tea and liquid have a higher pmi than it or anyth .
so if i sort by pmi , now i see that tea and liquid and wine ar the most associ word to be the object of the verb drink .
so pmi us for noun associ , just for word in the context , excus me , for word associ of word in the context and also for associ for depend relat .
all right .
now we've seen how to comput the term context or word context matrix , how to weight it with pmi , and we talk about comput in two wai , base on just word that ar in my neighborhood of ten word , or whether i'm in a particular syntact or grammat relationship with word .
now we're readi to us those to comput the actual similar .
and the cosin metric we're go to us just the same cosin that we saw from inform retriev .
so rememb we had a dot product , we said that the cosin similar between two vector , two vector indic the count of word , is just the dot product between the similar is the dot product between the two word normal by the length of the two vector .
so the dot product v w over length of v time length of w .
or , we could comput , think of it as comput separ unit vector , the normal v by it length , normal w by it length to get unit vector and just multipli them togeth ; or we can comput the whole thing out .
so here's our dot product for each dimens of v and each dimens of w we multipli the valu togeth and then we normal by the squar root of the sum squar valu to get the length of the vector .
and , now , let's sai we're do ppmi so v sub i is the ppmi valu for word v in context i , and w sub i is the ppmi valu for word w in context i .
and , rememb that cosin as a metric .
if two vector ar orthogon , thei're gonna have a cosin of zero .
if thei point in opposit direct , thei'll have a cosin of minu on .
if thei point in the same direct , thei'll have a cosin of plu on .
and it turn out that raw frequenc or ppmi ar non neg .
thei're alwai zero or greater .
so that mean that the cosin rang is alwai zero to on ; we're alwai on thi part of the , of the slope .
so cosin as a similar metric , if we us ppmi weight count , we're gonna get , or raw frequenc for that matter , we're gonna get a number between zero and on .
so let's comput , us cosin to comput similar .
and i've taken a littl subset of the exampl we saw earlier .
so we have apricot , digit , and inform .
and we have the context vector .
we have larg , data , and comput .
and i'm just go to us count here instead of ppmi valu , just for , for make the exampl more simpl to see , but in real life of cours , we'd us ppmi .
so the cosin between apricot and inform is the dot product .
so , from apricot to inform we have on time on , plu zero time six , plu zero time on .
or on plu zero plu zero .
over the squar root of the length of apricot , on squar plu zero squar plu zero squar , over the length of inform , on squar plu six squar plu on squar .
and , that's gonna be on over the squar root of <num> or . <num> .
and similarli the cosin between digit and inform .
we have from digit to inform , we have zero time on plu on time six plu two time on .
so that's gonna be zero plu six plu two over the squar root of the length of digit so that's , the length of digit i'm sorri .
so that's the squar root of zero squar plu on squar plu two squar .
so , root of zero plu on plu four and then the length of inform , the same as we saw befor , and now we get . <num> .
and , similarli for apricot and digit .
now the dot product between apricot and digit , on time zero , zero time on , zero time two is zero , so we're gonna get a similar of zero .
there ar a number of possibl similar metric , besid cosin , we can us the jaccard method , that we saw earlier in inform retriev , the dice metric , or there's a famili of inform theoret method like jensen shen and diverg that can be also us for similar , but cosin is probabl the most popular .
similar for distribut method is evalu just the same as it is for thesauru base method .
either intrins evalu , where we comput it correl with some human number , human word similar number , or extrins we pick some task like take the toefl exam or detect spell error , where we can comput some error rate and now we see if our similar metric result in a better error rate .
in summari , distribut similar metric us a method of associ , like ppmi , and a metric for similar , like cosin , to give you a similar between two word .
and distribut and thesauru base method both veri us in decid if two word ar similar for ani nlp applic .
welcom back .
todai we're gonna look at question answer , on of the oldest applic in natur languag process .
some of the first nlp task built on punch card system in the earli <num>'s includ the simmon et al system from <num> .
let's look at that .
so that answer question like , what do worm eat ?
by take the question , pars it into a depend format , so we have a relationship between eat and worm , and eat and what .
and then look for a match depend sentenc somewher in a larg databas of answer .
so thei had sentenc like worm eat grass , and bird eat worm , and hors with worm eat grass , and so on .
and the idea is that thi depend tree let , worm eat what , match worm eat grass , and even , grass is eaten by worm , but not hors eat grass .
or bird eat worm .
so the idea of answer a question by find a sentenc that look like the question but answer it is an old intuit that we're gonna see underli mani modern system as well .
you mai know that last year , ibm's watson won the jeopardi challeng , answer question about , for exampl , thi bram stoker novel that wa inspir by william wilkinson's book .
so the novel of cours wa dracula , and the author wa bram stoker .
and you mai also know about appl's siri , anoth question answer applic .
so you can ask question like do i need an umbrella tomorrow in san francisco , and it will check the weather for you and tell you which dai it's gonna rain .
anoth question answer applic , wolfram alpha , you can ask question like , how mani calori ar in two slice of banana cream pie ?
and not onli will it give the answer , but it'll give you , littl variabl that it let , it'll tell you the semant of the answer .
so we're look for an amount of pie that's two slice of banana cream type .
and we're ask about the total calori , and there's the answer , <num> calori .
so these kind of question ar factoid question .
so question like how mani calori ar there in two slice of appl pie , or who wrote the univers declar of human right , or what is the averag ag of the onset of autism .
these kind of question ; pretti simpl question , can be answer by a simpl fact , often a name entiti .
modern system also deal with complex or narr question .
so , in children with an acut febril ill , what is the efficaci of acetaminophen in reduc fever ?
or what do scholar think about jefferson's posit on deal with pirat ?
so these , these complex question ar gener answer more in research system .
wherea , factoid question answer is a wide us commerci applic .
what ar these kind of factoid question ?
so thei can be question about locat .
so the louvr be locat in pari , franc .
or , what an abbrevi mean .
or the name of the raven of odin .
name of a currenc , or an ingredi , or an instrument or a phone number .
so , thei're all simpl answer often with a , with a singl phrase or name entiti .
there ar two main paradigm for question answer .
the ir base approach , which we'll spend a lot of time on todai .
and that's pioneer in the annual trec evalu , and us in commerci system like ibm's and googl's .
and the ir base approach is go find the answer in some string on the web .
the knowledg base approach is build an answer from understand a pars of the question .
and hybrid approach take a combin of these approach .
so we might us some onlin databas and we might us some inform retriev approach to find sentenc and build answer .
and most modern system i would sai ar some kind of hybrid of knowledg and inform retriev .
a few of them , like perhap wolfram alpha , ar pure knowledg base , but most system us a littl bit of both .
so simpl web search actual answer a lot of question alreadi .
and that's realli the intuit underli the ir base approach to question answer that we'll talk about the most .
so if i ask into googl , what ar the name of odin's raven , the first page i get ha the answer .
and in the snippet that googl return , there is the answer .
so it's in the titl of thi page and it talk about the name of the re , raven .
other kind of simpl question like where is the louvr museum locat , ar also answer by method like googl .
and here in fact googl appli modern question answer techniqu to give you the , it best guess for the locat by it pull out of semi structur resourc like wikipedia or answer . com .
so , let's see how thi factoid question answer algorithm work .
the gener algorithm for ir base factoid question answer start with a question , and begin by extract inform from the question itself .
and the two most import and common thing to extract from the question ar a queri that's go to be sent to an ir engin , and the type of the answer that tell us what kind of name entiti we're look for .
in advanc , we take a whole lot of document , we index them , so that when we have a queri we can return a whole lot of document .
from those document we extract passag , so part of those document .
and then , those passag ar then process , in answer process , look for the type of the answer that we know we're look for .
and then , then return an actual answer .
so that's the gener process , and we'll walk through the piec todai .
so , anoth wai of look at the same set of process , three big part of the algorithm .
question process , so we're detect the question type , formul the queri .
passag retriev , given these queri , retriev document and break them into piec .
and then answer process , extract answer from the text snippet , and rank those candid us evid from the text and from other kind of sourc .
so by contrast , the knowledg base approach , which we'll talk about less todai , build a pure semant represent of the queri , and thi is true of siri or wolfram alpha , where thei're gonna come up with a semant represent languag for question that thei understand .
and you might pick some sub domain that you're abl to build a perfect semant represent for time , date , locat , scientif question , mathemat question .
and from thi semant , we can then map to structur databas or structur resourc , geospati databas , or ontolog , or restaur review system ; these thing where we can build up pure semant of the approach .
hybrid approach , and ibm watson is a good exampl of thi , but lot of other do thi now , build some shallow semant represent of the queri , so thei do some process of the queri , us ir method to gener mani candid answer , but then us these more knowledg base approach to us spatial databas or tempor reason to score the candid .
so , ir method to find possibl candid , knowledg base method to score them .
so question answer is both on of the oldest topic in natur languag process and also on of the newest and most excit research area with commerci potenti .
the first step in ani kind of question answer is answer type detect and queri formul .
so go back to our box diagram here , our flow chart .
we have a question , and the first thing we're go to do in question process is understand what's be ask here .
there ar as mani as five thing that we normal extract from a question .
perhap the most import is the answer type .
thi is a name entiti , a person or a place or a date , that tell us what we're look for in thi factoid question .
queri formul , equal import , that's the set of word we're gonna send the ir engin , that tell us what to look for , what passag ar like to have the answer .
and then other kind of thing depend on the system , mai also be extract .
so question type classif , we wanna know for exampl , is thi a definit question .
we might have to find the definit in the dictionari , or build on ourselv .
is it a math question ?
we might wanna answer that with mathemat directli , rather than , than go to find text snippet .
or a list question , we're look for list of thing .
so we might wanna look for list sourc .
and there ar other kind of thing like focu detect , so find the question word that ar be replac by the answer .
or relat extract , find all the relat between entiti in the question .
we talk about relat extract earlier .
and these ar us by some system and not by other .
so here's an exampl of the kind of thing we might extract from a question .
thi is from a jeopardi kind of question .
so , thei're the two state you could be re enter , if you're cross florida's northern border .
so here we're look for a state , a u . s .
state , so the answer type is a state .
what we might want to send to the ir engin is , two state , border , florida , and north .
these word ar gonna be good word to find passag that might have the answer .
the focu , the thing that we're try to answer , is two state .
and the relat that we might extract , the relat that express what the answer is , is , the answer is some unknown thing , which is in the border's relat with florida and north .
so , someth that's north of florida .
so if we had databas that express geograph inform like what state border what state , we might look for thi relat in those databas .
so , the first step , answer type detect , here we just need name entiti .
so if i see a question like who found virgin airlin , the answer type is person .
if i see a question like , what canadian citi ha the largest popul , the answer is a citi .
rel simpl .
and for that we need an answer type taxonomi or a type of name entiti that might be us in question answer .
so for exampl here's the taxonomi from li and roth .
thei talk about six coars class .
we might ask about abbrevi or entiti , descript of entiti , peopl , place , and number .
you know , the kind of thing you might have .
and insid those , we might have more specif class .
so we might be ask about a citi or a countri or a mountain as a type of locat , or we could be ask about a person or a group of peopl , and we could be ask about all sort of differ kind of entiti .
and here's their answer type taxonomi show that , some of the , some of the individu subtyp of the major six entiti type .
so a numer question could be a date question , or a distanc question , or a percent question , and so on .
ani of these kind of type that we might wanna find .
and here , it's just show you , it's in a small font , so you can come back and look at it later .
but , the kind of answer type that ar in the li and roth taxonomi .
so we might have currenc or diseas or food , or instrument and so on .
and some exampl of each of these .
or for locat , we might have mountain or state , and all sort of thing for number .
speed and size and temperatur and so on .
the jeopardi system also us a lot of answer type and thei did a nice analysi , look at <num> , <num> answer type insid <num> , <num> question and what thei found wa , that the most frequent <num> answer type cover less than half of the data .
here's the <num> most frequent jeopardi answer type .
you can see in jeopardi you tend to be ask about peopl , or movi or countri or citi , and so on .
here's more peopl , author and so on .
but still , there's a pretti broad distribut of possibl answer type , so we're gonna need a larg number of answer type detector to realli get a larg set of question for someth like jeopardi or even for easier factoid question .
how do we do answer type detect ?
like almost everyth we've seen so far , we can do hand written rule , we can do machin learn or we can do hybrid of the two .
and we're go to see all three of these in modern system .
so for exampl , for some kind of answer we might do veri well with regular express rule .
so , if we see who , and then a form of to be , and then a name entiti person , then our guess is that we're ask , thi is a person question .
or if , or if we see in an answer , person and then year dash year , we know that thi is , thi a , that we're look here for a person .
in other kind of rule , we're gonna be us the question head word .
and the head word is gener the head of the first noun phrase after the wh word .
so , here we have wh word which , and then the head word is citi .
or here we have wh word what and here's the noun phrase , the state flower of california and it head is flower .
so , thi head word often will tell us a lot about the answer type .
thi tell us we're look for a citi and thi tell us we're look for a flower .
but more often rather than simpli write handwritten rule we treat the problem as machin learn .
so we defin a taxonomi of question type .
we've seen some alreadi .
we annot train data for each question type and now we'll train classifi us some rich featur which might includ those handwritten rule we've talk about to decid what question type a question is .
what's the answer type of a particular question .
and the featur we're go to us for these answer type ar the word or phrase in the answer , the part of speech tag , we might us the head word we talk about , name entiti , semant relat word .
and all of these kind of regular express we can hand write for particular question pattern .
the next step in factoid qa .
we've done , now we've done answer type detect .
the next step is queri formul .
how do we decid what word to send to the ir engin to return document and then passag .
and on well known keyword select algorithm from moldovan et al us a number of heurist , each of which tell you which keyword in the question might be import keyword to put in the , in the queri .
so , if we see word in quotat , that's a veri like word that peopl ar look for .
we , and that , and anoth import thing to look for is proper name .
and then other kind of nomin .
and mayb less import , verb .
and then , even less import , adverb .
and mayb all other word mayb even less import .
so we have a rank of what word ar import to put in the queri .
so for exampl here's a slide from mihai surdeanu .
so we have a question like , who coin the term cyberspac , in hi novel neuromanc .
well we might throw out some stop word .
so who and the and in and hi .
and now we might sai , well cyperspac and neuromanc ar in quot .
those ar probabl thing we definit be put in the queri .
term and novel ar , ar nomin , and we saw that nomin ar an import thing .
we rank them as rank four .
and now a verb might be rank seven .
so we might sai , extract from thi that we have two term that ar extrem like to want to be in the queri and someth that's a littl less like , coupl less like thing and even less like .
so now we can form the queri in variou wai .
we could try just send the thing of rank on .
or we could send , go until we have enough queri word to make a long queri , or we can send the thing that rank on , see how mani queri we get back , and if there's not enough , add in more queri term and so on .
so what we do with these rank of queri depend on exactli what databas were queri , whether it's the web or a smaller databas and so on .
so we've seen the first step in factoid base , ir base factoid question answer .
which is extract an answer type from the question and extract the queri term that we're gonna send to the ir engin .
the next step in question answer is passag retriev and answer extract .
look at our flowchart again , we've now process the question , we've decid what queri to send to the ir engin .
and our job now is to do document retriev , find all document that have those queri word , extract some document and pull passag out of those that might have relev answer fragment in them .
so we'll have three step retriev document from the ir engin , segment the document into shorter unit , often we just us paragraph break for that , and then passag rank .
so re rank the passag depend on how well thei ar like to contain an answer to the question .
so , it's thi third step , passag rank , that i want to talk about .
we've seen ir alreadi , and the segment is rel simpl .
we might us paragraph or similar kind of thing .
so the hard part realli is i've got a whole lot of passag that came back from break up these document into piec .
which on contain the answer ?
so , the kind of featur that get us for passag rank , and again we might us a rule base classifi , we might us supervis machin learn for thi , we might ask , how mani name entiti of the answer type occur in thi passag ?
if i'm ask about a person or , or about a date , how mani peopl or date ar in thi passag ?
if none , i've got a bad passag .
how mani of the queri word occur in thi passag ?
so , i know that thei occur in the document but , i wanna know how mani of them also occur in thi particular passag .
instead of word we might look at entir n gram , we might look at how close these queri keyword occur to each other in the passag .
if i've got two or three keyword right next to each other i'm probabl on to someth .
and relat to that i can take the longest sequenc of question word and ask how long that is .
that's anoth featur i can us , and of cours the document that had the passag , we alreadi rank that , and the rank of that document might , itself , be a us featur .
so we can throw all these thing togeth .
so onc i've done thi step .
so now we've , we've got fact about the question and we've retriev and rank a bunch of answer passag .
the last step is pull the answer out of the passag .
so answer process .
so the first thing to do here in answer extract is to run a name entiti tagger on the passag .
so if we have an answer type we've gotta have a name entiti tagger that detect that answer type .
so we have a , if we know we ar look for a citi that's no help if we don't have a tagger that can find citi in raw text .
so thi could be again a full name entiti tagger or some simpl regular express or some hybrid .
and then our job is to return the string with the right type .
so for exampl if we have a person question , who is the prime minist of india .
we're go to want to be abl to detect answer in passag , so that if i had thi passag , mannohan singh , prime minist of india , i want to know that's a person , like to be the answer .
similarli if i have a length question , then a passag that contain thi length , is like to be an answer .
so i'm gonna wanna be abl to pull out the , the thi length and thi person .
now the problem happen when a passag contain multipl candid answer of the correct name entiti type .
so here's a question , who wa queen victoria's second son ?
and we know we're look for a person , and here's a love passag , the mari biscuit is name after mari and so on .
but thi passag ha a whole lot of name entiti in it .
it ha a whole lot of peopl .
it ha czar alexand ii , and it ha alfr , and it ha queen victoria , and it ha princ albert .
so , alfr is in fact the answer to the question .
alfr is the second son of queen victoria .
so , decid which of these name entiti is the correct answer , now we're , we're in thi machin learn problem where we need lot of featur that will tell us which of the name entiti is the correct , like to be the correct answer to extract .
and we can us machin learn and lot of rich featur for rank these candid answer .
so , again , a candid is a good candid if it ha a phrase that ha the correct answer type .
so if , if i'm in a , if i'm in a if i've got a good , a name that's the right kind of name entiti , that's a good sign .
i can write regular express .
i can measur the number of question keyword .
i can look at the distanc again .
so all these factor that we can us for passag , we can us for answer as well .
a good answer candid is in apposit to question term .
it might be there's an apposit claus .
it might be follow by some kind of punctuat .
we might look again for the longest sequenc of question term .
so all these same featur can now be us for rank candid answer .
so in ibm watson the answer is actual score by a whole lot of rich knowledg sourc , more than <num> compon .
and thei us unstructur text , thei us semi structur text .
thei might us knowledg from tripl store , from these relat extract we talk about earlier .
and each of them might give a score to a , to a possibl answer from their own knowledg .
so we might us geospati knowledg that we know , let's sai from a geospati databas , that california is southwest of montana .
that might help us in decid if california is a good answer to a question involv southwest of montana .
we can extract tempor relationship .
we can look at how reliabl the sourc passag is .
we can do full pars and get logic form and so on .
we'll talk a littl bit later about how all these knowledg sourc might be us .
onc we've pick an answer we need to decid how good it is .
if we're just return on answer , we can just us accuraci .
doe the answer match some gold label correct answer for that question ?
but often we return multipl answer , as we do in gener in inform retriev .
and in that case we us a metric call mean reciproc rank .
what we do is for each queri , we're gonna return a rank list of m candid answer , and the score of that queri is on over the rank of the first right answer .
so if there's , m is five , and we give five answer , and onli the third on is the first on that's correct , the score of that queri is <num> 3rd .
and now we take the mean of those rank all over our n queri , and that's the mean reciproc rank .
so that's the final step in the standard ir base algorithm for factoid question answer .
rich knowledg can also be us in modern question answer system .
and the most commonli us form of knowledg is relat .
and so relat extract , which we talk about in a previou lectur , is veri import in question answer .
and the relat can occur in the , in , in pull out the answer or in analyz the question , or in both .
so for exampl , we might have databas of relat , tripl store that we might draw from wikipedia infobox and might be store in place like dbpedia and freebas and these , these variou databas .
and we can learn fact like born in ; emma goldman ; june <num> , <num> .
so we can learn the birth date of somebodi or that cao xue qin wrote dream of the red chamber , my favorit novel .
so thi author of relat , thi born in relat ar the kind of thing that we might be abl to extract from these tripl store , to answer question about emma goldman's birth date , or , or the , the name of the author of dream of the red chamber .
and , do that requir that we know what relat wa be ask for in the question .
and so we're gonna need to also extract relat from question .
so if i ask a question like , whose granddaught star in e . t . , i need to know that thi question ha a relat like , someon act in e . t . , and someon els ha that someon as a granddaught .
so these ar just the kind of relat , that , if we could extract from both the question and the answer , then we can do some match on our tripl store and help find the answer .
and that's gonna be us by most of the modern question answer system .
anoth kind of rich us of knowledg is tempor reason from again , from these relat databas but also from obituari or biograph dictionari .
so a question that ibm's watson face in <num> he took a job as a tax collector in andalusia .
and the candid answer here ar thoreau and cervant .
and we'd like to know that thoreau wa a veri bad answer , becaus thoreau wa born in <num> , couldn't have been aliv in <num> to take a job , but cervant is at least a possibl answer becaus he wa aliv in <num> .
so these kind of tempor databas would let us do reason about when peopl ar born and di , and therefor , when thei can , can , be an answer to a possibl question .
geospati knowledg , question about contain or direction or border , it can help answer lot of question , so it can help us know that beij is a good answer for sai , asian citi , but new york is not a good answer for asian citi .
or that california is southwest of montana .
and so for exampl , on of these databas geonam . org let you type in , i've type in palo alto here .
and you can pull out all sort of fact from these kind of databas .
so it popul , it elev , the latitud and longitud , and the fact that it's part of the unit state , and part of california , and in santa clara counti , and so on .
mani question answer system ar simpli on shot question .
you ask a question , you get an answer .
other system , like siri is on of these , carri , carri on a full convers with the user , and there context in convers matter more .
so for exampl , in a siri convers , you might sai , book a tabl at il fornaio at seven o'clock with my mom .
that might be sentenc on , and sentenc two might be , also send her an email remind .
so that her , know that her , mean my mom is the kind of thing that matter if you're go to have multipl sentenc question .
so thi co refer , resolv dialogu co refer , know that her refer to my mom , is the kind of thing we need to do in a question answer system if we have a full convers , multipl sentenc in a row .
similarli , you might be abl to ask clarif question .
if a user sai chicago pizza a system like siri might ask , do you mean pizza restaur in chicago or chicago style pizza ?
so we might have an ambigu and the system resolv the ambigu by ask a follow up question to the user .
so these ar the kind of complic that occur when our question answer system ha to be abl to deal with follow on question and dialogu in gener .
knowledg like relat extract is veri import in question answer .
and depend on the question answer system , we're go to see other kind of knowledg , like geospati , and tempor , and dialogu and convers knowledg , plai a role as well .
now there ar some kind of advanc question that don't tend to be answer in modern commerci system , but ar part of modern research system .
and let's turn to those now .
imagin the follow harder question .
what is water spinach ?
and the answer we'd like to give , is realli a full paragraph , that talk about water spinach , and it histori , and name for it in differ languag and so on .
so we'd like to be abl to build that kind of question not by just look it up on the web , but by merg togeth possibl snippet we might get from differ sourc .
or we might get hard medic question like in children with an acut ill what's the efficaci of a particular medicin and an answer there might be again a summari of thing we might see from a particular pubm paper .
we might wanna read a sentenc and decid , read a document and extract sentenc and we can sai where thei came from and how much we believ them .
these ar the kind of harder thing that ar import in the research literatur , but haven't yet made it into commerci , modern commerci system .
and for answer these harder question , we often us an approach that's call queri focus summar .
and queri focus summar mean that we're appli a natur languag summar algorithm .
we're go to summar multipl document , pull inform from multipl document .
but it's queri focus .
it's not just a summari of everyth that's in those document .
it's the part that ar relev for a queri .
and there ar two kind of algorithm for queri focus summar .
on of them , what we might call the bottom up snippet method , we find a set of relev document , extract inform from them , bottom up , us tfidf , decid how mani s , how sentenc ar relev by their tfidf score , other kind of score .
and then we combin and modifi the sentenc into an answer .
and what i'm gonna talk about todai is a second method which you might think of as an inform extract method .
and here we build specif answer that work for differ question type .
so we might have definit question or biographi question or certain medic question and for each of those we might decid what constitut a good definit question or a good biographi question or a good medic question .
and we're gonna build specif answer for those .
so for exampl we know that a good biographi contain a person's birth and death , date , it contain how famou thei ar , what thei're famou for , their educ , their nation , and these kind of thing .
wherea a good definit contain what's often call a genu or hypernym , so we know .
what is a hajj ?
it's a type of ritual .
and a good medic answer contain the problem that the medicin is design to , to , to solv , the intervent , what is thi drug or intervent we're gonna us and then the outcom , the result of the studi describ thi problem .
so for these three kind of answer , we might know that we need to extract for a definit question , we've got to build a genu detector and a speci extractor , subtyp extractor .
wherea for a biographi question , we need to build a data extractor and a nation extractor .
while for a drug efficaci question , we have to extract the popul the studi wa run on or what the problem or intervent ar and so on .
and i've given you exampl here of the kind of sentenc we need to extract for these kind of answer .
so here's a sampl architectur for a complex question answer system from blair goldensohn et al .
here you might have a , a question like what is the hajj , and we might specifi that we'd like to search twenti document and extract an answer of about eight sentenc .
so we'll pull , do document retriev , pull lot of relev document .
and now we're go to build classifi whose job is to pull out genu speci sentenc .
so the hajj is a pilgrimag , the hajj is a mileston , the hajj is a pillar and so on and a classifi that pull on other non , non kind of definit sentenc .
and we're gonna cluster these and order these and produc as the respons an answer of about eight sentenc , that summar a good respons to the question .
so advanc question answer algorithm us in the laboratori either us inform extract method or us bottom up cluster method to combin inform from lot of document to creat a set of sentenc that answer our question .
but more commonli commerci system ar base on factoid answer .
so either us knowledg base or more often us inform retriev techniqu to find sentenc that contain an answer to the question extract and rank the answer and then present it to the user .
let's now turn to the us of summar in question answer .
the goal of summar is to produc an abridg version of the text that contain inform that's import to or relev for a user , a particular user need .
and so that might mean abstract ani kind of document or articl , summar email thread or meet , simplifi text , we'll talk about compress sentenc .
and we can talk about summar in two wai .
singl document summar .
given a singl document , we're gonna produc an abstract or an outlin , perhap a headlin , a veri short summari of a document .
in multipl document summar , we're given a group of document , presum relat document , and our job is to produc the gist of these document .
so if we have some new stori on the same event , we might summar the event by pick inform from differ document .
and that could be true also for ani kind of set of web page that ar focus on some topic or question .
now we distinguish between gener summar and queri focus summar .
gener summar is what i've talk about so far .
we have some document , or some set of document , and our job is just to build a summari .
queri focus summar is summar a document with respect to the inform need the user express by their queri .
so you can think of queri focus summar as just anoth kind of complex question answer .
we're answer a question by summar a document that ha the inform in it to construct the answer that the user need .
now we've all seen queri focus summari , becaus that's what search engin us to show you inform about a page .
so for exampl , googl will give you <num> charact about <num> word for each page as a summari of the page .
so if i've ask a question to googl , what is die br cke , i get , let's sai these three url back .
we have that titl of the url , the url , and then some text .
and that text , that snippet , is some kind of summari of the page that help the user understand what's in the page .
so we can think of summar in gener , of singl document , as gener thing like snippet .
and we're go to talk about the more gener task other than specif just snippet for web search , but you can think of these snippet as a characterist exampl .
now where a snippet is the answer to a question by take inform from a singl document , we often want the answer from multipl document .
so creat answer to complex question that involv summar from multipl document , we're gonna creat a cohes answer that combin inform from each of these document .
that's multipl document question answer .
now we can think of two wai of do summar .
in extract summar we creat a summari by take particular word or phrase that ar in the document and build a summari just out of those word or phrase .
and a snippet from a search engin like googl is an extract summari in thi wai .
by contrast an abstract summari is on in which we creat our own word , differ word than ar in the text , to summar the content of the text .
and , we're go to be talk complet about extract summar todai .
abstract summar is an import research goal , but veri difficult .
and on thing to think about whenev we're talk about summar from a document , whether it's a web page or anyth els , is what our baselin ar for summar .
and good writer often put their idea right at the begin , in the titl or in the first sentenc .
so a simpl baselin , whenev we're measur ani kind of summar algorithm is just take the first sentenc .
so , for exampl , in thi googl queri what is die bru cke , if we take the output from thi first hit and we look at thi sentenc die bru cke wa a group of german expressionist artist form in dresden and so on .
let's look at thi page where the snippet come from .
and we can see that thi snippet is realli just the first <num> charact of thi page .
so sometim the best snippet is realli just the begin .
and so we're go to us that not onli as a baselin but later we'll see that that's a featur we could us in differ kind of summar algorithm .
we've introduc the task of natur languag gener , produc shorter abstract or summari or even headlin from a document .
and we've talk about queri focus summar , where we us summar to answer a particular question .
we build a summari of a document that is specif design to answer a question pose by the user .
and we've talk about singl and multipl document summar .
and now we'll see the detail of how to do each of these task in the further lectur .
let's now see how to gener these snippet or other summari from singl document .
now here's an exampl of a snippet , thi is again come from googl .
i've ask a question , wa cast metal movabl type invent in korea ?
and googl's give me three littl snippet with the answer and you can see that it's bold face case where word in my queri occur in the snippet .
and you can see the us of dot , dot , dot in the snippet tell you that it's combin piec from differ page , differ , excus me , differ place in the page .
so let's see how these kind of snippet and other kind of summari base on singl document ar built .
you can think of ani summar algorithm as have three stage .
the first stage is content select extract the sentenc that we need from the document .
so we have some document as input , and we're gonna need to extract sentenc .
so we might segment our sentenc off .
mayb we us sentenc with period , full sentenc .
mayb we us some kind of move window .
so we've extract some kind of littl piec , littl sentenc , and now , from thi segment set of sentenc we want to pick the on that ar import so i have mark those with littl , littl black dot here .
we pick some set of extract sentenc and our next task inform order is we ar gonna decid what order the sentenc go in .
so we have some now order set of import sentenc and final we might do some modif to the sentenc ; perhap we're gonna simplifi them or someth els .
so that's sentenc realiz and the result of these four step is our summari .
now , the most basic summar algorithm , the on that come up a lot , realli onli us on of these three step , the content select step .
so in the simplest possibl algorithm we don't worri about what order the sentenc come in and we don't modifi the sentenc at all .
we simpli segment our document into sentenc or mayb their window .
we pick the import on and we leav them in the same order thei came in .
so we're gonna us what we call document order with the origin sentenc .
so thi is a veri simpl baselin for summar and on that most web base snippet gener algorithm certainli us .
the most commonli us algorithm for content select date back to the veri earliest paper in the field , from <num> , it's pretti excit that these idea came out so earli .
and the intuit's realli veri simpl .
choos sentenc that have salient or inform word .
well , what's that mean ?
well , you've seen tfidf .
that's a wai of pick word that ar particularli frequent , and then don't contain word that occur in all document .
so that's on wai we might defin salienc or inform .
turn out in summar we tend to us anoth approach .
the log likelihood ratio or , sometim call topic signatur approach .
and , that differ from tfidf in two wai .
on , we us a slightli differ statist for pick , for weight each of the word .
and second , instead of pick all the word , we'll choos onli the word whose weight is abov some threshold , the veri salient word .
now , log likelihood ratio give us a statist call lambda .
i'm not gonna go into detail , but , thei're in some love paper .
and we're gonna choos all word for who the valu of two log lambda is greater than in thi cutoff of ten .
so that give us a threshold for which we can pick word that ar particularli salient by thi statist .
so we're gonna weight everi word .
and the weight of word i is gonna be on , if the word is especi associ with that document , mean , occur more time in that document than in the background corpu by some threshold .
otherwis , we're gonna , we're gonna give it a weight of zero .
and again , for detail about how to comput the log likelihood and intuit about the statist , you can see thi love ted dun paper , or the lin and hovi paper that propos us it for summar .
now , we want to modifi thi algorithm for deal with queri focus summar .
again , we're not interest so much in pure summar in todai's lectur , but how to us summar techniqu for question answer .
so thi is topic signatur base .
topic signatur mean , pick the word that ar particularli associ with a , with a document .
content select , choos the sentenc where we've got queri , all right ?
and so we're gonna modifi the algorithm veri slightli .
we're gonna choos word that ar inform , either by log likelihood ratio , or word that happen to appear in the queri .
so , we're go to weight everi word in a document .
we're go to give it a weight of on if it meet the log likelihood threshold , it pass the threshold of about ten .
we're go to give it a weight of on , also , if that word happen to appear in the queri or question , and otherwis we're go to give the word a weight of zero .
and these weight ar veri simpl , on , on zero you could imagin learn more complex weight and some research ha gone into come up with veri power wai to learn detail weight .
but on , on zero work pretti well , it turn out .
and now we're just go to weigh a sentenc or perhap it's a window , if we don't have actual sentenc .
we're go to weigh it by the weight of the word , so we're just go to sum over all the word in our sentenc of the weight of the word and then we're go to take the averag .
now the content select algorithm we just describ is unsupervis .
we didn't have ani label train set of which of summari to learn weight from or thing like that .
so that's an altern approach supervis content select .
so now if we had a label train set where for each document we had a good summari and we had an align for everi sentenc in the summari we knew what sentenc it came from in the document , we had the match sentenc .
now we could extract all sort of featur .
we could extract the posit of the sentenc in the document .
first sentenc ar veri like to be good summari sentenc .
how long it is .
we can have all the featur we had befor , word inform and thing like that .
we can have other kind of featur base on discours inform that we might have .
and we might associ everi sentenc with some vector of featur , and now we can just train a binari classifi .
shall i put thi sentenc in summari ?
ye or no .
and it might learn weight for all these featur and ani other featur we can come up with .
and the algorithm , thi sound good but in practic it turn out to be veri hard to get label train data of thi type .
when peopl actual write abstract for sentenc thei're not alwai , the author don't alwai us exact word and phrase and certainli don't us entir sentenc that come from the document , so find perfectli label abstract with extract from the document is hard .
it's hard to do the align becaus thei don't pick entir sentenc , thei mai be pick word or phrase or chunk .
it's hard to figur out where those word came from , even when thei did pick them from the document .
and it turn out , surprisingli perhap , that the perform is simpli not much better than unsupervis algorithm .
so in practic unsupervis content select , just us log likelihood ratio , or other simpl measur of how salient or inform a word and henc a sentenc is , ar the most common method for content select .
so we've seen how to gener summari from a singl document and the baselin algorithm we pick is simpli come up with a simpl statist wai to find a sentenc that is veri inform by look for inform word and we talk about the log likelihood ratio as an import wai of find these sentenc .
we talk earlier about evalu question answer .
if we have the answer to a factoid question we can simpli evalu it by see if the factoid the system return is the correct factoid .
now summari can't be evalu that wai becaus we can't have a singl perfect summari for ani document .
so we'll introduc a differ algorithm call roug .
roug stand for recal orient understudi for gist evalu , propos by lin and hovi .
and , here's the idea ; it's is an intrins me , metric for evalu summari , we're gonna ask is thi summari good as a summari .
not in some other extrins applic , but just as a summari .
and it's base on a metric call bleu , or blue , that defin origin for machin translat .
and it's not as good , roug is not as good as us , as us human to sai , did thi summari answer the user's question ?
so if we can afford that , we'll certainli hire user , and have them test to see if an answer answer a user's question .
but roug is veri conveni for test while we're build our system .
and it work as follow .
we're given a document d and let's sai we've got our summar and it produc an automat summari .
and thi can be a queri focus summari , so we mayb know about the , the queri that the user ask or even for gener summar .
now we have n human produc a set of refer summari of thi document .
again , in queri focus summar , thei look at the queri and write their summari .
in gener summar , thei just write their summari .
so we have a set of summari , on , two , three , four human summari , of our document d .
and now the system produc anoth summari , call that x .
so we have our automat summari and our four human summari and now we just ask , what percentag of the bigram from these human summari occur in x ?
obvious thei won't all occur in x .
a good summari will contain a lot of the bigram that occur in some of these human summari .
so by count the percentag you get an intuit for what's a good summari .
and there ar variou version of roug , unigram roug , bigram roug , there's also other version that , that talk about length in differ wai .
we'll introduc just on roug two , bigram roug , which work pretti well and it's just ask , out of all the bigram , in all the sentenc , in all the refer summari , take the count of those bigram .
and notic that out of all those bigram look at , again , in each sentenc in thi summari , for each bigram ask what's it minimum count in the , the summari produc by our system and the human summari .
so it's ask how mani bigram occur both in our system and in the human summari .
so that will give us , out of all bigram how mani occur both in our , in the summari and in the human refer .
let's look at an exampl .
here i have made up three human summari and a system summari , so here's our human three summari and our system answer to a question .
let's comput roug , so the numer we want to know how mani bigram that occur in these human summari , how mani of those bigram also occur in our system answer .
we can walk through , well water spinach .
that's in our answer .
here it is here .
and spinach is , and is a .
and in thi summari water spinach and spinach is , and is a , and in thi third summari again , water spinach and spinach is and is a , but also commonli eaten and leaf veget and of asia .
so if we add all that togeth , we have three from thi first summari and three from the second summari and six from the third .
and how mani total bigram ar there in the human summari ?
well , you can count them yourself .
there ar ten in the first exampl up here , nine in here , nine in here .
so we have <num> <num> <num> <num> <num> <num> or a roug score of . <num> .
so we've introduc roug , an algorithm for evalu summari , whether thei're gener or queri focus , by look at how mani of the bigram , or n gram in gener , in a human summari occur in our machin gener summari , and a better summari is on that overlap more with the human summari .
now roug doesn't work as well as have human actual answer the question , did thi answer provid the inform the user ask for .
but , that can be veri expens , and so roug can provid a fast to run and conveni intrins metric that we can us to test our system , and then , at the end we can us human to see how well it realli did .
let's conclud thi section by look at more advanc research system that try to answer much more complex question .
so consid the follow definit question , what is water spinach and i've given a potenti long answer to the question what is water spinach .
could we gener answer to definit question like thi , if let's sai thei didn't occur in a good place on the web and want to gener them ourselv .
or take a medic question .
what is the efficaci of a particular therapi for a particular diseas ?
you might wanna give an answer .
thi on's from a particular paper in the pubm databas that answer thi question .
so difficult , hard to answer question that might involv summar on or more document .
there's in fact a competit on answer such complex question .
and i've given you some simplifi version of some of those question .
how is compost made and us for garden ?
what caus train wreck ?
what can be done to prevent them ?
what's the human toll in death or injuri of tropic storm in recent year ?
so these ar the kind of question that to answer them , you'd want to read a lot of document automat , pull inform from them , and summar thi inform .
so it's a veri difficult task , answer these harder question is the task of queri focus multipl document summar .
and there ar two standard algorithm for thi .
what we might call the bottom up or snippet style method is just like we saw for singl document summar .
we ar go to find a set of relev document first .
now we ar go to extract inform sentenc from the document , and then mayb we will do some order and modif .
so we ar just go to grab sentenc from document and mix them togeth .
the top down or inform extract method , we ar go to build specif answer for differ question type .
we will build an answer for definit question and on for biographi question , and so on , by extract particular inform need for those question .
and we will see both of these in thi littl lectur .
so here's the snippet base method .
the bottom up method for queri focus multi document summar .
we're gonna start as , as again by grab all the sentenc from now multipl document .
not just on document .
so we have a set of sentenc .
and we're gonna modifi them .
it's common to do sentenc simplif .
we'll take a sentenc , i'll show you in a second , and we'll simplifi them in variou wai .
and we'll have lot of differ simplifi version of sentenc .
so now we've got lot of famili of cloud of sentenc .
and we're gonna appli , the log likelihood ratio test , and other wa , other method of pull good sentenc from thi set of sentenc .
we now have a set of extract sentenc , the littl black dot i've mark here .
and we're gonna order those extract sentenc , as we talk about for singl document summar .
and we're gonna modifi them in variou wai to produc the realiz sentenc .
we often simplifi sentenc to get rid of unimport detail that would make the summari too long .
and the , on of the most common wai to do thi involv pars .
we pars the sentenc , then we have hand written rule base on the pars tree that suggest which modifi ar better to prune .
so , for exampl , apposit ar the kind of thing you might prune , so rajam , an artist who wa live at the time in philadelphia .
we can elimin that .
or attribut claus , intern observ said tuesdai blablabla .
well we care more , more about what thei said , that rebel agre to talk .
that's an import fact and we can delet mayb the attribut claus in our summari .
preposit phrase especi those without name entiti turn out to be the kind of thing that we can delet and keep a summari more concis .
so , the , the increas to a sustain number , mayb a shorter summari would just sai increas .
and adverbi at the begin , like for exampl , or on the other hand or as a matter of fact .
thing that matter in a lo nger document bur aren't go to be appropri in a small abstract or a small summari .
so , the simplest method is take write variou rule of thi kind .
and each of these paper give you anoth , some interest set of rule that you can us to simplifi the sentenc .
so onc we've done thi , we have a larg set of sentenc includ the origin sentenc and the on with the apposit delet , the on with the pp delet and so on .
we have a whole bunch of differ sentenc .
the origin and variou shorten version .
what do we do next ?
well now we'd like to select from these veri redund set of sentenc just the on to put in the summari .
and thei're redund for two reason .
thei come from multipl document that might talk about the same event .
and we've ad these simplifi sentenc , so there ar all sort of redund there .
on iter method for content select , given these redund sentenc , is call mmr , maxim margin relev .
and the idea is as follow , we're go to iter or greedili choos the best sentenc from our set of sentenc to insert in our summari so far and we want the best sentenc to satisfi to satisfi two properti .
we want it to maxim relev to the user's queri so we might do someth simpl like just insur it ha high cosin similar to the queri .
and we want it to be novel , mean we want it to be minim redund with the summari so far .
if we've just put in a sentenc , we don't want to put in a variant of that sentenc that's mostli the same .
and we want to measur that by just measur it cosin similar to the summari in choos sentenc that have low cosin similar .
so , on version of mmr might be , we'll have a lambda for weight these two factor .
we want a sentenc that ha a high similar with the queri .
and which ha a low , we'll subtract out the similar of the sentenc with it most similar sentenc that's in the summari so far .
so we'll pick sentenc that don't look like ani of the sentenc in the summari so far but do look like the queri and we'll add those sentenc in iter and we'll stop by some criterion , perhap when we've just achiev our desir length .
now , we're gonna wanna combin the intuit of log likelihood ratio , pick inform sentenc , and mmr , choos non redund sentenc .
so on of the mani wai we might combin these intuit is to start by score everi sentenc , base on log likelihood ratio , us the word that either occur more than we expect by chanc , or word that occur in the queri .
and now , start by includ the sentenc with the highest log likelihood ratio score in the summari .
and now start iter ad into that summari , other sentenc that ar not redund with the sentenc with the summari so far .
and when we had singl document summari we could pick document order as our order for the sentenc in the summari .
that's a littl harder to do if our sentenc come from multipl document .
so there's variou thing we can do .
we can if we're summar new we can do chronolog order and look at the date that the articl came out and order the sentenc in order of , of actual time .
we can choos coher base order .
we can put sentenc near each other if thei have similar mean .
so we could look at the cosin between the two sentenc and sentenc that have high cosin we could put next to each other .
or perhap we could look at the entiti discuss by the sentenc and if two sentenc talk about the same entiti , we could put them near each other .
or we could even do some kind of fanci method where we look at the sourc document and look at the actual semant topic that happen in the document and we could order tho , pick an order and appli that to our output sentenc .
so variou wai we can do inform order .
so that's the bottom up or snippet base approach to do queri focus multi document summar .
we grab a whole bunch of sentenc , we rank them by log likelihood ratio with mmr and form our summari that wai and we order them by some method .
the altern method , the inform extract method , is us when we have a particular kind of question that we wanna , we know what kind of thing we expect to put in the answer .
so we know , for exampl , that a good biographi contain a birth and death , mayb why thei're famou , their educ and nation , and so on .
wherea a good definit contain what's call a genu , or hypernym sentenc .
the hajj is , what is it ?
it's a type of ritual .
or a medic answer about a drug's us , you want to talk about the problem , the medic condit .
you want to talk about what is thi intervent , thi drug or procedur and what's the outcom .
so , know these type of question we can know the type of thing we might want to see in the answer .
so , we could then build a littl detector ; let's sai that find genu sentenc in the document .
find me a genu sentenc , find me a speci sentenc , find the date thi person wa born or di , or wa educ .
find where thei went to school .
or if we're talk about a drug , who wa the studi run on , and what is the actual intervent and so on .
so we could build classifi to extract these kind of inform .
so let's look at an exampl of a system from blair goldensohn that doe thi kind of complex question answer for definit question .
we have a question , what is the hajj ?
and we specifi let's sai that we ar go to look at twenti document to pull our summari out and we want a summari of length eight sentenc long .
so we ar go to extract all of our document , we ar go to pull out , we sai thi is a definit question , so we know we need a genu speci sentenc , so we run our classifi and find all of our genu speci sentenc , in all of these document , and now we find the other sentenc and we're gonna for each of the piec of inform we need , build littl cluster of sentenc , and us your log likelihood ratio and mmr and other approach to decid which sentenc to pick , and then we're gonna past these into a definit .
so we've talk about two method for answer these complex difficult question that requir look at multipl document .
we talk about the bottom up snippet base method that us log likelihood ratio and mmr to choos just the right non redund inform sentenc from lot of document .
we've talk about an inform extract style method in which we build separ templat for each question type and pull out particular attribut that ar import thing to put in the answer , for that question .
of cours research on these harder question that requir look at multipl document is realli just begin .
and thi is gonna be an import and excit area for us to follow in the futur to see where their research goe .
let's begin our discuss of minimum edit distanc by defin minimum edit distanc .
minimum at a distanc is a wai of solv the problem of string similar , how similar ar two string .
so let's pick a particular exampl .
spell correct , the user type graff , what do thei realli mean ?
and on wai of operation thi question is ask which of the follow word is closer to the letter that thei type .
graph , graft , grail , or giraff .
the problem of string similar come up also in comput biologi , where we have sequenc of nucleotid .
a , c , g , t we're try to align .
and a good align should be abl to tell us that two particular sequenc , perhap from two sampl line up in a certain wai with some amount of error .
and thi idea of string similar or sequenc similar come up from machin translat , from inform extract , from speech recognit , come up everywher .
so let's defin edit distanc .
the minimum edit distanc between two string is the minimum number of edit oper , insert , delet , and substitut , that ar need to transform on into the other .
and we gener us these three , edit oper , insert , delet , and substitut , you can imagin more complic transposit and , and long distanc movement , but we tend to avoid those .
so for exampl we have the string intent and the string execut .
here's an align show that mani of the letter line up .
and with some substitut and then there ar some gap where a gap here line up with a letter c in execut and a gap in execut line up with a letter i in intent and so on .
so we can think about thi align as have a set of oper that gener the align .
so here the to turn intent into execut we have to delet d for delet an i , we substitut an n for an e .
substitut a t for an x .
insert a c .
substitut an n for u .
and the rest of the letter , the e , the t , i , o , n ar all the same .
so the edit distanc if each oper is on , is five .
there's five .
we had to do five thing to turn intent into execut .
if substitut cost two , thi is call levenshtein distanc .
in levenshtein distanc , insert and delet cost on , but substitut cost two , and now the distanc between these two string is now eight .
in comput biologi we've seen sequenc of base , and our job is to figur out that thi a align to thi a and thi g to thi g and , and , mayb thi t a t and c a c and so on .
so , thi c to here so we have some , so we can see that there's some kind of insert there and we can repres , again , that align between charact by show thi align string of symbol .
so the task is , given two sequenc align each letter to a letter in a gap , that's our task in biologi .
edit distanc come up all over the place .
in machin translat , for exampl , we'd like to measur how well a machin translat system doe .
so , let's suppos our machin translat system repres some sentenc , mayb translat from chines .
as the spokesman said , the senior advisor wa shot dead .
and some human , expert translat said it should have been spokesman confirm senior govern advisor wa shot .
so we can measur the differ between these two by sai how mani word chang .
confirm wa , wa substitut with said .
word that were insert .
and the and word and dead .
and word that were delet govern .
so it , we have measur how good our machin translat is by compar it to human .
similarli , in task like the entiti extract , you know , if ibm inc .
and ibm ar the same entiti , or if stanford univers presid john hennessi is the same entiti as stanford presid john hennessi , then we can do that by us at a distanc to notic that these ar veri similar , with on word differ , and here , on word differ .
and so , by measur the number of word differ , we can , improv our accuraci at the invent extract , and , and other kind of task .
all right .
so how ar we go to find thi minimum at a distanc ?
our intuit for thi algorithm is to search for a path .
by path we mean a sequenc of edit from the start string to the final string .
so we'll start with an initi state , so that will be the word we're transform .
we'll have a set of oper insert , delet , substitut , and we'll have a goal state , that's the word we're try to get to .
and , final , we'll have a cost for the path of get there , and that' the thing we're try to minim .
so that's the number of x .
that's our path cost .
so , for exampl from intent here's a piec of the path .
from intent we could delet a letter and end up with ntention .
we could insert a letter and end up with e eintent or we could substitut a letter and end up with entent .
so that's , these ar piec along a path that would go from intent .
all the wai off the screen here into all the possibl wai you can transform intent into someth .
so , thi space of all possibl sequenc is enorm .
so we can't afford to navig naiv in thi , in thi sequenc .
and the intuit of solv thi problem of lot of possibl sequenc is some of these path wind up at the same state .
so , we don't have to keep track of everi wai of transform on string into anoth if the second piec of the set ar ident .
all we have to keep .
is the shortest path to everi revisit state .
so let's look at an exampl of how thi work , we ar gonna to defin minimum edit distanc formal as , for two string .
you have string x of length n and string y of length m .
we'll defin a distanc matrix , d i , j .
and that will be the edit distanc between the first i charact , on through i , of x and the first j charact , on through j , of string y .
so that's the , that's what's in that's what's defin by d i , j .
and so , the distanc between the entir two string is gonna be d n , m becaus the string ar of length n and m .
so that's our definit of minimum ad distanc .
how ar we go to comput interim at a distanc ?
the standard algorithm is with dynam program .
down program is a tabular method of comput .
and what we're gonna do , is we're gonna comput d , the distanc between two string x and y , x of length n , y of length m , by combin solut to sub problem .
and combin solut to sub problem , is the intuit of all dynam program algorithm .
the intuit is , is veri simpl .
we're gonna for small prefix of length i of string x , and j of string y , we'll comput the differ between those string and we'll comput our larger distanc for larger string base on those previous comput smaller valu .
in other word , we're go to comput the distanc i j between prefix of string x of length i and prefix of string .
y of length j for all i and j , and we'll end up in the end with the di , the distanc .
so let's look at the actual equat .
here's the equat for defin the minimum at a distanc .
and i've given you levenshtein distanc which is the distanc when there is a cost of on for insert , on for delet , and two for substitut .
so let's look at the initi condit .
so the , we first sai that .
ani charact in x , so thi is the x string for it , for the i charact is an x string .
the distanc between those and the null string in y is the cost of delet each of those charact .
so the cost of those is , is the length of the string .
we're delet each charact .
and similarli for insert all the charact into y .
to creat the string y .
the distanc between the null string x and the string of y is just a length of y .
the insert cost of y .
and then , we'll do the recurr relat .
so , walk through string x and walk through string y .
we'll have that the distanc in ani particular cell of our matrix is go to be the minimum wai of get to that cell from three previou cell .
if we go from the string i , that's on shorter so we're delet on more thing in i .
to make a j .
or we're insert on thing into j to make it longer .
or we're substitut between the previou string i of length i , x of length i minu on and y of length j minu on .
we're ad in a new charact .
if it's the same in both string we have a cost of zero .
if it's differ we have the substitut cost of two .
and then at the end , the distanc between the two string is simpli the it's simpli the , the dfnm , the upper right corner of the matrix .
so here's our tabl .
and we can fill in each element of the tabl from us thi equat that tell us the delet cost , the insert cost and the substitut cost .
so let's do that .
i put the equat up here in the corner .
so we want to know what's the distanc between the null string of intent and the null string of execut , obvious zero .
the null string i to the string noth is still the cost of delet an i .
that's on .
so now let's try to comput what the cost of convert in to e .
while intuit we expect it's go to be a delet and a substitut .
so let's see if that work out .
all right , so the , element in thi cell is the minimum of three valu .
it's thi distanc plu on , thi distanc plu on , or thi distanc plu either two if i and zero ar differ , or zero if thei ar the same .
well , thei're differ , so it's the minimum of i <num> , which is two .
on <num> which is two , or zero two which is two , so we have two .
so we're gonna write two in thi similarli , if we wanna know the distanc between i , n and e , it's the minimum distanc , of i n to noth , plu on .
so two <num> or three .
or , the differ distanc between i and e plu the cost of ad in that n or three .
or the cost of have just an i and ad in that n to e substitut which is two or three .
so again , we have three here .
we have a two and we have a three .
and if we continu along thi manner , again , in each case look at the previou cell and us thi equat over here , we'll slowli end up with .
so , and if we continu along thi , in thi manner , we're gonna end up with the follow complet tabl .
so .
everi cell in thi tabl , let's take thi cell , tell you the cost of sub , of ca , of at a distanc , of edit the string int and turn it into the string ex and that mean that thi valu here in the upper right corner is the cost , the at a distanc , between intent and execut , the cost of turn intent into execut and we see the valu eight , which we earlier said wa the , the levenstein distanc .
so we had levenstein distanc equal eight .
know the edit distanc between two string is import but it turn out not to be suffici .
we often need someth more which is the align between two string .
we wanna know which symbol in string x correspond to which symbol in string y and thi is gonna be import for ani applic we have of for often from spell check to machin translat even in comput biologi .
the wai we comput thi align is we keep a back trace .
a back trace is simpli a pointer when we enter each cell in the matrix that tell us where we came from and when we reach the .
and the upper right corner of our matrix .
we can us that pointer an then trace back through all the pointer to read off the align .
let's see how thi work in practic .
again , i've given you the equat for each cell , in edit distanc .
and , if we put in some of our valu that we saw earlier , i'll start by put in some valu , so .
all right .
so we can ask , how did we get to thi valu two ?
two is , we pick the minimum of three valu .
we could either take , so two is the distanc .
thi two here is the distanc between the string i and the string e .
and we got that by sai , it's either the align between noth and e , plu the insert of an extra i .
so that's distanc of on plu on is two .
or zero plu two is two , or on plu on is two .
so we had three differ valu .
so if we were ask which of , which minimum path did we come from .
realli thei're all the same .
we could have come from ani of them .
and that's go to be true for thi valu of three as well .
yeah .
we comput it as the minimum of two plu on , on plu two , or two plu on .
so thi could have come from here , here , or here .
and similarli , that's go to be true .
i didn't work out the , arithmet for you , but it's go to be true for thi cell too .
you can work it out for yourself .
here we have a distant , distant , differ .
so , the distanc between int and e , we could comput that by take the distanc .
what it cost us to , to convert i n t e to noth , and then add anoth insert for e .
and that would be , that would be silli becaus four plu on is five , and there's a cheaper wai to get from i n t e to e , and that is that it cost us noth to match thi e to that e .
so , our previou align between i n t and noth , we , we can add zero from three to get a three , so .
the minimum path for thi three came from that three .
so while in some case a cell came from mani place .
in thi case it came from thi previou three .
so we're go to do thi for everi cell in the arrai .
and the result will look someth like thi where we have for everi cell everi place it could have come from .
and you'll see that in a lot of case ani path could have work , so thi six could have come from ani place .
but , crucial , thi final align , thi eight that tell us the final , edit distanc between intent and execut .
our traceback tell us it came from the , the best align between intentio and executio , which came from the best align , from intensi , from executi and so on .
and so , we can trace back thi align , and get ourselv , align that tell us that thi n .
match thi n match thi o match thi o and so on but mayb here we have an insert , rather than a clean line up .
comput the back trace veri simpl .
we take our same minimum edit algorithm that we have seen and here i have label the case for you .
so when we ar look at a cell we're either delet , insert or substitut and we simpli add pointer .
so in a case where we ar insert we point left and in a case where we ar delet we point down and in a case where i am substitut we point diagon .
i have shown you that arrow on the previou slide .
so we can look at thi distanc matrix and think about the path from the origin here .
to the , the end of the matrix .
and ani non decreas path that goe from the origin to the point nm , correspond to some align of the two sequenc .
an optim align , then , is compos of optim sub sequenc , and that's the idea that make it , it possibl to us dynam program for thi task .
so , the result of our back trace ar , two string and then , the align between them .
so we , we'll know which , which thing line up exactli , which thing line up with substitut , and then , when we should have insert or delet .
what's the perform of thi algorithm ?
in time it's order nm becaus our back , our distanc matrix is of size nm , and we're fill in each cell on time .
the same is true for space .
and in the backtrac we have to on the , in the worst case go for , if we had n delet and m insert we'd have to go n plu m .
we'd have to touch n plu m cell but not more than that .
so that's our backtrac algorithm for comput align .
at a distanc can also we wait .
why would we add wait to the comput of at a distanc .
think about particular applic .
in spell correct it's obviou that some letter ar more like to be mistyp than other .
while in biologi , becaus of the constraint of the subject matter , some kind of delet or insert ar more like than other .
so , for exampl in spell , here's a confus matrix for spell error , so if you look at thi confus matrix you can see that e is veri like to be confus with a or o and e .
so vowel tend to be confus .
but it's veri unlik to con , to confus a and b .
so a's ar confus with e's and i's and o's and u's , and so on .
so thei're , thei're the kind of spell error peopl make , have systemat to them .
so , not just , confus vowel with vowel .
but also , the fact of the keyboard mean that you're like to make error either us the homolog finger on the other hand or us nearbi keystrok .
so the constraint of the domain , in thi case we're talk about spell , or mayb we're talk about biologi ar gonna make some edit , some edit more like by other .
so we're go to repres thi by modifi the algorithm slightli to add weight .
so , in levenshtein distanc we have the cost of on for insert and on for delet , and two for substitut .
in weight minimum edit distanc , we simpli add a special cost that we have to look up .
so the initi instead of ad on for each delet , we have the actual cost of delet ; instead of ad on for each insert , we add the cost of each insert .
and so on for the in the recurr relat we're gonna add a special cost .
delet of x of i .
how much doe it cost to delet that particular charact ?
how much doe it cost to insert that charact ?
how much doe it cost to substitut that charact ?
and we're gonna end up with the , termin condit .
so , we're just gonna add separ littl tabl , look up tabl , that will tell us what the delet , insert and substitut cost ar .
by the wai , where did the name dynam program come from ?
here's some quot from richard bellman's autobiographi .
bellman wa the on who invent dynam program .
and amusingli , he tell us that he came up with the , name for dynam program realli as a public relat move to make an algorithm sound excit .
so thi is the , mayb on of the first algorithm that wa , that wa name , in a brand wai to make an algorithm sound excit .
so there's our , in summari , algorithm for weight minimum at a distanc .
there ar a number of advanc variant of minimum edit distanc that plai a special role in comput biologi .
so we're call a comput biologi when we're align a sequenc of nucleotid or sometim protein .
and our job is to take two string like thi and produc an align like thi .
so , in biologi thi is import for a number of .
reason .
we can be find region in the genom .
we could be discov function of gene .
we could be look for evolutionari thing by compar differ speci .
thi is also import for assembl fragment of dna sequenc .
we're go to be try to assembl fragment and we want to look for overlap piec .
we'll talk about overlap piec and find some match between them ; find that these two piec match .
and we're compar individu and look for mutat , find place where there ar similar and differ .
in gener , in natur languag process we talk about distanc , string edit distanc and minimum edit distanc .
so we're minim distanc and we're comput weight for thing .
in comput biologi , we're talk about similar or maxim similar , so we're ask how similar two thing ar , so we're try to maxim someth and we gener talk about score rather than weight .
so in competit biologi , the , standard minimum at a distanc algorithm that we just look at , is call needleman wunsch .
and i've shown you the algorithm here , but it's the same , the same thing that we saw befor , although in gener , we're just go to keep , we'll us d to mean the cost of insert and delet , and we'll have a littl s valu for the substitut , the posit or neg valu of substitut thing .
and in gener in biologi we'll talk about posit cost for thing that match , a posit valu for thing that match and a cost for thing , for delet and insert .
so here's the , needleman wunsch matrix .
and , notic that , as oppos to what we did in natur languag process in gener , in comput biologi , we put the origin at the upper left .
so let's , let's first look at some of the variant that ar import in comput biologi .
so on is case where it's possibl to have unlimit gap at the begin and end of the string and thi happen exactli when we have two littl snip of d and a and we know that the end point of on might overlap with end of anoth but there might be someth els go on in other place .
so here is on long sequenc and here's anoth long sequenc but it's just thi piec .
of , of thi sequenc , and thi piec of thi that might overlap .
so we don't wanna penal the fact that there's other thing go on befor here or after here .
so we'd like to modifi the algorithm so it doesn't penal gap at the end .
and in fact there can be variou differ kind of overlap of thi , of thi sort .
thi might happen when we ar , when we ar do sequenc and we have overlap read or it might be that we ar look for a piec of a gene insid anoth piec and so we have a subset piec insid a larger piec .
so the variant of the dynam program algorithm that we us for overlap detect , the overlap detect variant , we'll just make a few small chang in the algorithm .
so first , we just chang the initi so that .
it doesn't cost us anyth to start from a long string and delet everyth or insert everyth .
so us to be that we had a , we had i , i star d here and we had j star d here and we've gotten rid of those becaus it's allow ourselv to start at a path at a random point we ar here at the intersect .
so we're , we're .
allow ourselv to start at zero cost here and not be penal for not match all these thing up until here .
so again we're look for edg overlap .
and now , for our , termin condit , we're gonna look for the , start from , not from the upper right corner , becaus we're allow a match not to go all the wai to the edg , but we'll find the , the place along the final column , or the final row , where we have the maximum valu , and we'll trace back from there .
so , in thi case , our maximum valu is here , in thi column , and we'll trace back from there .
a similar extens of the needleman wunsch , or the standard dynam program algorithm for string at a distanc , is the local align problem .
so here's the local align problem .
we have two string x of length m and y of length n and we want to find two sub string who's similar is maximum , so imagin that here is x and here is y we would like to add up thi and these two sting , we would like to find these two these sub string c , c , c g , g , g that's the largest similar sub string .
so it's veri similar to the overlap detect variant that we saw except not onli do we allow ourselv not to ignor previous unalign sequenc at the begin and end , but also anywher .
so we can basic have our maximum align be somewher in the middl as it is here .
so in order to , in order to modifi the needleman wunsch algorithm to allow ani kind of local align , the new version is now call the smith waterman algorithm .
and we're first go to allow , as we did for the overlap detect variant .
allow them the initi condit to be zero both for x and y so we don't penal ourselv for initi string .
and now we're gonna make on more modif which is that , in each cell when we're look at the possibl place we could come from to choos the align , we're gonna not onli pick , the maximum of the three previou cell .
but we're also go to add a maximum of zero .
so we're go to let ourselv , sens in an , biologi we're talk about maxim similar .
when thing get veri differ and we get veri neg score we're just go to start all over again from zero .
allow myself to just throw awai region that don't align at all .
the termin condit of the smith waterman algorithm , depend on what we're look for .
if we just want the best local align we'll pick the place that's , that's , maximum in the entir arrai and we'll trace back from there .
if we want all of the local align that score greater than some threshold t then mayb we'll find some place that's greater than t , find all those place and trace back all of them .
now thi get's complic by the fact that there can be overlap local align , so here we might have two align like thi and it might be that thei actual overlap trace back , so there can be some complic here .
but if you want the best local align that's actual much easier .
so here's an exampl of local align .
so let's , let's imagin that we're , we're , we're get on posit point everi time two symbol match , and a neg point for ani delet , insert or substitut .
and then let's look for all the local align between these two string , a t c a t and a t , t a and a t c .
and if we fill in the matrix , thei're gonna be , start with zero everywher , becaus we're do local align .
we see , two , if we then look for region , cell that have a maximum distanc to trace back from , we see two of these cell .
so , on of them correspond to the align a . t . c . a . t to a . t . t . a . t . , so we have four string that match , on mi match , so that's gonna be a distanc of three , and the other on of them , over here , correspond , to , the align between a . t . c .
and a . t . c .
where we have three , match symbol .
so those ar some of the more advanc varianc of edit distanc that we see in comput biologi .
todai we're gonna introduc the topic of languag model , on of the most import topic in natur languag process .
the goal of languag model is to assign a probabl to a sentenc .
why would we want to assign a probabl to a sentenc ?
thi come up in all sort of applic .
in machin translat , for exampl , we'd like to be abl to distinguish between good and bad translat by their probabl .
so , high wind tonight might be a better translat than larg wind tonight becaus high and wind go togeth well .
in spell correct , we see a phrase like fifteen minuet from my hous .
that's more like to be a mistak from minut .
and on piec of inform that let us decid that is that fifteen minut from is a much more like phrase than fifteen minuet from .
and in speech recognit , a phrase like i saw a van is much more like than a phrase that sound phonet similar , ey aw of an .
but it's much less like to have that sequenc of word .
and it turn out languag model plai a role in summar , and question answer , realli everywher .
so the goal of a languag model is to comput the probabl of a sentenc or a sequenc of word .
so given some sequenc of word w1 through wn , we're gonna comput their probabl p of w , and we'll us capit w to mean a sequenc from w1 to wn .
now , thi is relat to the task of comput the probabl of an upcom word , so p of w5 given w1 through w4 is veri relat to the task of comput p w1 , w2 , w3 , w4 , w5 .
a model that comput either of these thing , either p w , capit w , mean a string , the joint probabl of the whole string , or the condit probabl of the last word given the previou word , either of those , we call that a languag model .
now it might have better to call thi the grammar .
i mean technic what thi is , is tell us someth about how good these word fit togeth .
and we normal us the word grammar for that , but it turn out that the word languag model , and often we'll see the acronym lm , is standard , so we're gonna go with that .
so , how ar we go to comput thi joint probabl ?
we want to comput , let's sai the probabl of the phrase it water is so transpar that , thi littl part of a sentenc .
and the intuit for how languag model work is that you're go to reli on the chain rule of probabl .
and just to remind you about the chain rule of probabl .
let's think about the definit of condit probabl .
so p of a given b equal p of a comma b over p of b .
and we can rewrit that , so p of a given b time p of b equal p of a comma b , or turn it around , p of a comma b equal p of a given b i'll make sure it's a given time p of b .
and then we could gener thi to more variabl so the joint probabl of a whole sequenc a b c d is the probabl of a , time b given a , time c condit on a and b , time d condit on a b c .
so thi is the chain rule .
in a more gener form of the chain rule we have here the probabl of ani , joint probabl of ani sequenc of variabl is the first , time the condit of the second and the first , time the third condit of the first two , up until the last condit on the first n minu on .
all right , the chain rule .
so now , the chain rule can be appli to comput the joint probabl of word in a sentenc .
so let's suppos we have our phrase , it water is so transpar .
by the chain rule , the probabl of that sequenc is the probabl of it time the probabl of water given it , time the probabl of is given it water , time the probabl of so given it water is , and final time the probabl of transpar given it water is so .
or , more formal , the probabl , joint probabl of a sequenc of word is the product over all i of the probabl of each word time the prefix up until that word .
how ar we gonna estim these probabl ?
could we just count and divid ?
we often comput probabl by count and divid .
so , the probabl of the given it water is so transpar that , we could just count how mani time it water is so transpar that the occur and divid by the number of time it water is so transpar occur .
so we could divid thi by thi .
and get a probabl .
we can't do that .
and the reason we can't do that is there's just far too mani possibl sentenc for us to ever estim these .
there's no wai we could get enough data to see all the count of all possibl sentenc of english .
so what we do instead is , we appli a simplifi assumpt call the markov assumpt , name for andrei markov .
and the markov assumpt suggest that we estim the probabl of the given it water is so transpar that just by comput instead the probabl of the given the word that , or the veri last that , that mean the last word in the sequenc .
or mayb we comput the probabl of the given it water is so transpar that given just the last two word , so the given transpar that .
that's the markov assumpt .
let's just look at the previou or mayb the coupl previou word rather than in the entir context .
more formal , the markov assumpt sai the probabl of a sequenc of word is the product for each word of the condit probabl of that word , given some prefix of the last few word .
so , in other word , in the chain rule product of all the probabl we're multipli togeth , we estim the probabl of w , given the entir prefix from on to i <num> by a simpler to comput probabl w given just the last few word .
the simplest case of a markov model is call the unigram model .
in the unigram model , we simpli estim the probabl of a whole sequenc of word by the product of probabl of individu word , unigram .
and if we gener sentenc by randomli pick word , you can see that it would look like a word salad .
so here's some automat gener sentenc gener by dan klein , and you can see that the word fifth , the word an , the word of thi doesn't look like a sentenc at all .
it's just a random sequenc of word thrift , did , eighti , said .
that's the properti of the unigram model .
word ar independ in thi model .
slightli more intellig is a bi gram model where we condit on the singl previou word .
so again , we estim the probabl of a word given the entir prefix from the begin to the previou word , just by the previou word .
so now if we us that and gener random sentenc from a bigram model , the sentenc look a littl bit more like english .
still , someth's wrong with them clearli .
outsid , new , car , well , new car look pretti good .
car park is pretti good .
park lot .
but togeth , outsid new car park lot of the agreement reach that's not english .
so even the bigram model , by give up thi condit that english ha , we're simplifi the abil to model , to model what's go on in a languag .
now we can extend the n gram model further to trigram , that's <num> gram .
or <num> gram or <num> gram .
but in gener , it's clear that n gram model is an insuffici model of languag .
and the reason is that languag ha long distanc depend .
so if i want to , sai , predict the comput which i had just put into the machin room on the fifth floor , and i hadn't seen thi next word , and i want to sai , what's my likelihood of the next word ?
and i condit it just on the previou word , floor , i'd be veri unlucki to guess crash .
but realli , the crash is the main verb of the sentenc , and comput is the subject , the head of the subject noun phrase .
so , if we know comput wa the subject , we're much more like to guess crash .
so , these kind of long distanc depend mean that in the limit of realli good model of predict english word , we'll have to take into account lot of long distanc inform .
but it turn out that in practic , we can often get awai with these n gram model , becaus the local inform , especi as we get up to trigram and <num> gram , will turn out to be just constrain enough that it in most case it'll solv our problem for us .
how do we estim these n gram probabl ?
let's look at bigram probabl .
the maximum likelihood estim for a bigram probabl , the probabl of a word i , given the previou word i <num> , we just estim by count .
we count , how mani time did word i <num> and i occur togeth ?
and divid it by how mani time word i <num> occur .
so it's like sai , of all the time that we saw word i <num> , how mani time wa it follow by word i ?
and we'll us the notat count sometim .
and sometim , we'll , for simplif , we'll just refer to a c , we'll us c for count ?
so , the joint count of word i <num> and i , divid by the count of word i <num> .
so let's walk through an exampl .
so we have again our equat , the probabl of word i given the word i <num> is the maximum like the estim is just .
we'll write , i'll write mle .
estim by the maximum likelihood estim as , as thi count over thi count .
and let's look at our , let's look at , let's make up a corpu .
so , here's our simpl corpu borrow by dr .
seuss .
so , we have three sentenc , each on start with a special token start up sentenc , and end with a special token end of sentenc .
and thei're veri short sentenc .
i am sam , sam i am , i do not like green egg and ham .
let's comput some languag model probabl from thi small corpu .
so , first probabl of i given the start symbol , so that's comput as count of start symbol comma 'i' with a big i over count of start symbol .
so 'i' follow the start symbol twice on , two , and the start symbol occur three time on , two , three .
so that probabl is two third or . <num> .
so that's the probabl of 'i' given start .
and you can see common exampl of lot of differ probabl here .
so for exampl , let's just pick anoth on at random .
the probabl of 'sam' given the word am , how mani time doe 'am' , 'sam' occur ?
it occur onc .
so that's on .
and the denomin is , how mani time doe 'am' occur ?
and that occur twice .
so that's gonna be on over two .
so here's our probabl of 'sam' given 'am' , and so on .
so let's look at a larger corpu in order to get some more realist count .
and the corpu we're us here wa collect from a dialog system that answer question about restaur in the citi of berkelei , california .
and here's the kind of sentenc that were in thi corpu can you tell me about ani good cantones restaur close by ? or , mid price thai food is what i'm look for . , tell me about chez paniss . now i hear some sentenc , and let's comput some n gram base on these sentenc .
so first , let's start with the raw bigram count from the small corpu of just under <num> , <num> sentenc .
so , what i'm show you here is a bigram count tabl , so here the word 'i' is follow by the word 'i' five time .
the word 'i' is follow by the word 'want' <num> time , the word 'want' is follow by the word 'to' <num> time , the word 'to' is follow by the word 'eat' <num> time and we've just put some sampl word up here , i pick i , want , to , eat , chines , food , lunch , spend , just to show you some word that might occur togeth in a sentenc and some other word and you can see a lot of these word , a lot of these probabl ar zero , a lot of these count , i'm sorri ar zero , becaus just happen in thi small data set that 'want' wa never follow by 'want' .
so that's thi zero here or 'chines' wa never follow by the word 'to' here .
okai .
so in order to turn these count into probabl all we have to do is normal by a unigram count .
becaus rememb the probabl of a word i given the word i minu on .
is the count of word i <num> .
word i over the count of word i <num> .
so , we need to divid these joint count of the two word , by the count of the previou word .
here's the uni gram count we're go to need to comput those probabl .
so , here's the count if 'i' , it's <num> <num> .
here's the count of 'eat' , it's <num> <num> .
and us the equat we can now comput the bi gram probabl .
the probabl for exampl of to given want .
how like is it , given that the previou word wa want that the next word is to .
and it's pretti like .
for exampl . <num> .
but notic that thing with count of zero still have probabl of zero .
lot of thing have zero here .
so now , that we've comput all of our bigram capabl , we can now estim the probabl of the sentenc , that's our goal for languag model .
simpli by multipli togeth all the compon probabl .
so the probabl of i want english food is probabl of 'i' given 'start' , time the probabl of 'want' given 'i' , time the probabl of 'english' given 'want' , 'food' given 'english' , 'start' end of sentenc given 'food' and so on .
what kind of knowledg ar express by these bigram probabl .
why , for exampl is the probabl of 'english' given 'want' lower than the probabl of 'chines' given 'want' .
well , probabl that's becaus chines food is more popular and more peopl ar gonna ask about it and so want chines is more like than want english , and that's a fact about the world .
it's a fact about cuisin , not so much a fact about english .
what about the probabl of 'to' given 'want' be so high ?
now that's a fact about grammar .
that's a fact that the verb 'want' , in english , requir an infinit after it .
so 'want' ha infinit , and that's a grammat fact .
so that's grammar , we'll put grammar .
and what about the probabl that 'want' , given the previou word is 'spend' , is zero ?
so that zero seem to be caus by a grammat fact 'spend' , 'want' is two verb in a row , that kind of verb doesn't seem to be grammat possibl in english .
so that zero is caus by a grammat disallow .
how about thi zero ?
what's the , why is the probabl of 'food' follow 'to' is zero ?
now here , thi zero is a conting zero .
thi , you could imagin a sentenc that ha 'to food' in it .
i'd like you to stop and think about a sentenc like that yourself .
good .
it just happen that such a sentenc never occur in the train data .
so thi is a conting row , where thi is a structur zero .
all right , let's move on .
in practic , we don't keep these probabl in the form of probabl , in fact we keep them in the form of log probabl and there ar two reason for thi , on is , we is can avoid underflow , if you think about it if you have a veri long sentenc and you're multipli togeth <num> or <num> or <num> , littl to tini probabl each of which is a veri small number less then zero when you multipli with so mani small number , you get a veri small number that often end up with arithmet underflow , in the comput .
and so , we want to avoid thi kind of underflow .
and it turn out that ad is faster than multipli anywai .
so , again , instead of multipli four probabl we'll , in gener , just add four log probabl .
then we're go to store our languag model in log .
there ar a number of publicli avail languag model toolkit .
on of them , srilm and you can download srilm .
anoth publicli avail resourc is the googl n gram releas , thi ha been out for over five year now .
googl releas a trillion word corpu .
over a trillion five gram and thirteen million uniqu word .
so a huge data set which , you can download and us for your , ani kind of n gram applic you'd like to us .
so here's an exampl , some of the data from the googl n gram releas .
some four gram count for four gram begin with serv as the and word begin with 'in' .
so thi is a veri big corpu , and , and you can see that serv as the indic occur <num> time on thi web corpu .
and you have more inform on the , googl web corpu there .
anoth publicli avail corpu is the googl book n gram corpu .
let's look at that .
so thi corpu let you plot count of word in googl book .
and a number of corpora ar avail for american english , british english , chines , french , and german , and , variou kind of corpora , which can all be download .
everi natur languag process tool ha to be evalu and languag model have to be evalu as well .
what doe it mean for tore languag model to be a good languag model ?
in gener , we sai a good languag model is on that is better at find the good sentenc and predict , and like them more than the bad sentenc .
or , more specif , we want to assign a higher probabl to real or perhap frequent observ sentenc than ungrammat or imposs or at least rare observ sentenc .
so , that's the goal of our evalu a languag model .
now we train the paramet of a languag model on a train set , and we test the model's perform on data that we haven't seen .
so we have some train data and some unseen data .
thi unseen data is call a test set .
and we want it to be someth that's not the same as our train set , total unus , we've never look at it befor .
and that will be a fair evalu of our model .
and then we'll need an evalu metric that tell you how well doe your model do on thi unseen test set .
so what ar these evalu model ?
the best evalu , best wai of compar ani two model , two languag model , a and b , is to put each model in a task .
so we're gonna build our spell corrector , or speech recogn or mt system .
whatev our applic is that us languag model .
we'll put our languag model in there and now we'll run our task , and we'll get some accuraci for the system run with model a , the system run with model b .
perhap that's how mani misspel word ar correct properli if we're do spell correct .
or how mani word ar translat correctli if we're do translat .
and now we just compar the accuraci of the two model , whichev model ha the higher accuraci is the , better languag model .
so thi is call extrins evalu , were us someth extern to the n gram model itself and look at our perform on that extern task .
the problem with thi kind of extrins evalu , it's also call invivo evalu .
and is but it's time consum in mani case thi could take dai or week for a modern machin translat system or modern speech recognit system run evalu can often be extrem slow .
so instead , what we sometim us is an intrins evalu .
someth that's about , intrins about languag model themselv and not about ani particular applic .
and the most common intrins evalu is call , perplex .
now perplex happen to be a bad approxim to an extrins evalu unless it turn out that the test data look a lot like the train data .
so gener perplex is us onli in pilot experi , but it doe help to think about the problem and it's a us tool as long as we also us extrins evalu as well .
so let's think about the intuit of perplex .
and like mani idea in languag model thi date back to claud shannon .
so , so shannon propos among mani other thing a game about word predict .
how well can we predict the next word .
so , for exampl , we've seen sentenc like , i alwai order pizza with chees and .
and our job is to predict the next word .
so from thi first sentenc , we might sai , well , a good languag model might guess that we ar like to have mushroom and like to have pepperoni and mayb less like to have anchovi becaus anchovi ar somewhat less popular than mushroom .
and veri unlik to put fri rice on our pizza .
and extrem unlik , let's sai , to have and , and .
although peopl , i guess , do sai and , and .
after , after the word and .
and so the how well the model predict the , the actual word that occur is , is the intui , is , is how good the model is .
so a model .
on a sentenc , on a sentenc like the thirti third presid of the us we know the next word is veri like to be jfk or john or kennedi or some word like that .
so thi is a veri predict case .
here we have , i saw anyth could come next .
so , in some case we're gonna do , be , be much better predict the next word , in some case veri much wors .
but a good languag model on averag should do better than a bad languag model .
now it turn out that unigram ar veri bad at thi game , and if you think for a second you'll realiz why .
so in summari , a better model of text , a better languag model , is on that assign a higher probabl , assign a higher probabl to whatev word actual occur .
if you can guess right , the next word , you ar a good languag model .
so the best languag model is on that best predict an unseen test set , or assign , on averag , the highest probabl of a sentenc to all the sentenc that it see .
if i've seen thi , if i see thi test set and i assign , give me a new test set , and i assign a probabl to each of those sentenc , the better languag model is the on that sai , oh , i knew that sentenc wa come and assign it a veri high probabl .
now the perplex , thi new metric we're go to be us , is the probabl of the test set normal by the number of word .
so we'll take , let's sai our test set is , is a long sentenc , n , n word long .
so we'll take thi n word sentenc , we'll take it probabl , and we'll take the on over , that , we'll take the rou , the rout .
so , and we'll take the invers of it .
so , it's , it's a wai of normal for the length of the probabl .
so , it's , it's , take thi long sentenc , take the , probabl the whole sentenc , and normal by the number of word , becaus obvious long sentenc , the longer the sentenc , the less probabl it's go to be .
so , we want some normal factor so we can compar test set of differ length .
so that's the enthru of on over , the perplex of a string of word , w , is the enthru of on over the probabl of the string of word .
so , thi parenthes should be here .
so , by the chain rule .
that's the , the probabl of thi string of word on through n is the probabl overal i sorri i'm sorri the product overal i of the probabl of each word given the entir prefix befor hand .
and so we've just , by the chain rule , replac the , probabl of a long sequenc with the product of the probabl of each word , given it prefix .
and then , for bigram , by our mark of approxim to the chain rule , we can sai that the , probabl , we've replac the probabl of a sequenc of word with the product of a bunch of bigram .
so the perplex of a string of word is the rout of the product of , of , n , by gram probabl multipli togeth and invert , so it's just a function of the probabl of the sentenc .
so becaus of thi invers minim perplex is the same as maxim probabl .
there is anoth intuit for perplex also base on shannon and , and thi exampl come from josh goodman and thi , the second intuit for perplex reli on the idea that perplex is the averag , relat to the averag branch factor .
perplex at ani point in a , in a sentenc is on averag , how mani thing can occur next ?
and we'll see later thi is relat to the probabl of the upcom thing , relat to the entropi of the upcom thing .
but roughli speak if i had ten possibl word that can come next and thei were all equal probabl my perplex will be ten .
so for exampl if i'm recogn the ten digit .
then the perplex of the task is ten .
there's ten possibl thing that could come next and i can't decid between them .
if i have to repr , to recogn am i build a speech recogn for , for a , a , a , switchboard phone servic and i have to recogn <num> , <num> name .
then the perplex of the name is <num> , <num> if thei're all equal like .
but suppos a system ha to repres , ha to recogn , let's sai again a phone , switchboard phone oper , automat phone oper .
ha to recogn the word oper and that occur a on fourth of the time .
the word sale that occur a quarter of the time or the word technic support , that occur a on fourth of the time .
and then with on over <num> , <num> time each , anoth <num> , <num> name occur .
so now we have to take the weight averag of all these possibl of what could occur , to comput , on averag , how like is ani on word to occur and now the perplex is <num> .
so the perplex again is the weight equival branch factor .
so let examin thi new kind of perplex , the weight equival branch factor , and show that it ? s the as thi invert normal probabl metric .
so let ? s take a sentenc contain random digit , what ? s the perplex of thi sentenc accord to a model , that assign equal probabl to each digit ?
so we'll see perplex of thi sentenc , thi string of digit let make it , let make it .
, it doesn't matter how long it is .
so we have a bunch of digit .
and the probabl of thi bunch of digit , we'll call them digit on , digit two , through digit n .
the perplex by our first metric is neg on .
is , is the probabl of thi sequenc to the neg on over n .
and sinc we've said that each of these word ha probabl <num> <num> , on tenth .
and we're assum a unigram probabl .
so that's the probabl of on tenth x on tenth x on tenth x on tenth , and so on .
so that's the prob , that's on tenth .
to the n .
becaus there's , there were n word to the neg on over n .
and as that we can see over here that's equal to the n's cancel we get on tenth of the minu on or we get ten so by think about perplex as the normal probabl of a long string .
we can sort of see the intuit that the averag branch factor by normal for the length .
we're sort of ask how mani thing can occur each time wait by their probabl .
all right .
now , so perplex in gener the lower the perplex the better of the model .
so , for exampl here's , here's where a train set train on <num> million word test on , on <num> million word from the newspap the wall street journal and a unigram model ha a perplex of <num> .
a bi gram model ha a much lower , much more accur perplex of <num> and try gram model ha an even lower per , perplex .
so , perplex sinc it's model someth like averag branch factor or averag predict the lower you get .
the better you ar at predict the , the model , the actual data that occur .
we saw earlier that lot of time probabl or count of bigram or trigram would be zero .
what do we do in these case ?
let's think about thi by start with the , what's call the shannon visual method .
and thi is what shannon propos to , to visual the actual endgram that you've built , by maxim like estim .
so , here's the , the method .
we choos a random bigram , accord to it probabl .
so , here's a bigram .
with start as the first word an then ani other word accord to it's probabl .
roll a die and pick whichev come up .
so let's sai we pick i as veri like first word , so we pick star i as our first bio gram .
now which wa anoth random bio gram that start with that word w we just gener and , and whose next word is chosen accord to it's probabl .
so now we pick want , then we pick i want and now we go on until we happen to chose the end of sentenc .
so , i want to , to eat , eat chines , chines food , food is how we go .
so now we bring these word togeth and we've gener a sentenc .
so the shannon visual method can show us a lot of thing about the that we've built .
so for exampl .
here's a grammar languag model train on shakespear and gener random sentenc .
so here's some unigram sentenc , everi enter now sever so let .
hill he late speak or .
not veri good sentenc , how about our bigram ?
why doe stand forth thy canopi forsooth ?
he is thi hit the king henri , live king , follow .
oh , thi is better , it's begin to sound like shakespear .
how about thi on ?
inde , the duke , and had a veri good friend .
well , that sound pretti good .
sweet princ falstaff shall die .
and now let's look at the quadro it cannot be but so .
will you not tell me who i am ?
that sound veri good .
now shakespear produc <num> , <num> word at , with a vocabulari of <num> , <num> .
and it turn out that in those <num> , <num> word he produc about <num> , <num> word , he produc about <num> , <num> differ bigram type or differ word , uniqu pair of word .
but that's <num> , <num> out of <num> , <num> squar or out of <num> , <num> million possibl bigram .
so if we multipli that out <num> . <num> percent of the possibl bigram were never seen , thei also were gonna have zero entri in the bigram tabl .
vast number of <num>'s .
so , that's just bigram .
quadrigram ar even wors .
so the reason why those quadrigram look like shakespear is becaus those were actual shakespear sentenc .
becaus follow ani particular quadrigram realli onli on possibl word could occur with such a small corpu as shakespear .
and we can see that if we look at a differ corpu like the wall street journal , it's not shakespear .
so , for exampl , here's some trigram sentenc from the wall street journal .
thei also point to <num> billion from <num> , three percent of the rate of interest store , as mexico and brazil and market condit .
eh , sound like the wall street journal .
but here's two corpora of english , both rea , you know , both of reason size corpora , million of word , or at least , a million word .
no overlap at all in the shakespear sentenc and the wall street journal sentenc .
so what's' the lesson from thi ?
on lesson is the parallel of overfit .
n gram onli work well for word predict if the test corpu look like the train corpu .
if you test on shakespear and your , but you train on the wall street journal , you're not go to predict word veri well .
so in real life , thi just doesn't happen .
so we'd like to train robust model that do a better job of gener .
and i wanna talk about on kind of gener , which is deal with zero .
so by zero , i mean , thing that never occur in the train set , but do occur in the test set .
so let's look at some zero .
imagin that in the train set , we had phrase like deni the alleg , deni the report , deni the claim , deni the request .
and we never saw deni the offer , so the probabl , base on maximum likelihood estim of offer , given deni is zero .
now , we go to a test set , and we see there is a sentenc deni the offer , and deni the loan .
what's the probabl of those sequenc , deni the offer and deni the loan gonna be ?
well , the probabl is gonna be , becaus our , we've train our probabl on our train set , we're gonna do a veri bad job .
if we're a speech recogn , we'll never recogn thi phrase .
if we're a machin translat , we'll refus to translat into thi phrase .
we're gonna claim thi phrase is just not good english .
so , thi is a , thi is a big problem we need to solv .
so bi gram with zero probabl valu .
mean that we're gonna assign zero probabl to the test set .
and so we can never comput perplex , we can't divid by zero .
how do we deal with bi gram with zero probabl .
the simplest idea is call add on smooth .
and let's look at a pictur that give us the intuit of smooth in gener from dan klein .
so suppos in our train data we saw deni the alleg , deni the report , deni the claim , deni the request .
and so we've comput probabl .
there wa seven total thing follow deni the and we can get our probabl of everyth , of each of these thing .
but we would like to sai deni the effort might occur , deni the outcom might occur .
so we'd like to steal some probabl mass and save it for thing we might not see later .
so thi is our train data .
and thi is the maximum likelihood count , so these thing occur after these never occur .
we'd like to steal a littl , a littl probabl mask from each of these word and put that probabl mask on to all other possibl word or some set of word , so that the zero go awai .
and the simplest wai of do thi is call add on estim or lepla smooth .
and the idea is veri simpl .
we pretend we saw each word on more time than we actual did .
we just add on to all the count .
so if our maximum likelihood estim .
is the count of the bigram divid by the count of the count of the unigram .
or add on estim is the count of the bigram plu on over the count of the unigram plu v we have to add v here in the denomin , becaus we're ad on to everi word that follow word i minu on .
so , our denomin is increas , not just by the total count of time that someth happen to i minu on , wasn't the previou thing that follow it , but each on of those got increment by on , and there were v of them , so we have to add v to the denomin .
thi is the add on , estim , probabl estim .
i keep us the term maximum likelihood estim , and let's just remind you what that mean .
the maximum likelihood estim of some paramet of some model from a train set is the on that maxim the likelihood of the train set , given the model .
so we have some train set , and we're gonna , a maximum likelihood estim that let us learn a model from a train set , is the on that make that train set most like .
what do we mean by thi ?
suppos the word bagel occur <num> time in the corpu of a million word .
and .
i ask .
what's the probabl that a random word from some other text will be bagel ?
well , the maximum estim from our corpu is <num> over <num> , <num> , <num> , or . <num> .
now thi could be a bad estim for that other corpu .
who know what of the other corpu bagel occur <num> time per <num> , <num> , <num> or some other probabl .
but thi estim is the on that make it most like the bagel will occur <num> time in <num> , <num> , <num> word corpu , which is what it did occur in our train corpu .
so we're maxim the likelihood of our train data .
so an add on smooth and ani kind of smooth is a non maximum likelihood estim , becaus we're chang the count from what thei occur in our train data to hope to gener better .
so if we go back to our berklei restaur project and we add on to all of our account , here's our la plaz smooth bigram count and with all those <num>'s that we had have becom <num>'s and everyth els ha on ad to it .
so now we can comput the bi gram probabl from those count and just us the laplac add on smooth equat that we saw earlier and now we got all of our laplac , their add on smooth bi gram .
so we have again the probabl of two given on that is . <num> and now all of those zero have turn into iii . <num> , . <num> and so on .
now we can also take those probabl and reconstitut the count as if we had seen thing the number of time that we would have to see to get those add on probabl natur .
so we take our probabl and we re estim the origin count as if thei were the number that would have given us these probabl .
and we ask , what ar those reconstitut count look like .
how much of my , ha our add on smooth chang our probabl ?
so , here's reconstitut count .
so , we have i wa .
it's follow by want <num> time or chines is follow by food <num> time .
these ar reconstitut count .
and let's compar them to the origin count .
so , up here , here on the top we have the origin count and here we have our reconstitut count , and i want you to notic that there's a huge chang .
so in our origin count , two follow want <num> time .
in our smooth count , two follow on onli <num> time .
so it's , it's , almost a third sma , a third the si , th , smaller .
three time smaller .
or , chines food occur <num> time in our origin count and onli <num> , in our reconstitut count .
so , that the , add on smooth ha made massiv chang to our account .
and sometim chang a factor of ten , the origin count , in order to steal that origin probabl mass to give to all those massiv number of zero that had to be assign probabl .
in other word add on estim is a veri blunt instrument .
it's , it make veri big chang in the count in order to get these probabl mast to assign to thi massiv number of <num>'s .
and so in practic we don't actual us add on smooth for n gram .
we have better method .
we do us add on smooth for other kind of natur languag process model .
so add on smooth for exampl is us in text classif or in similar kind of domain where the number of <num>'s isn't so enorm .
descript , languag code en , dir ltr , name english , metadata , note , resourc_uri api<num> partner video xarksnczro0x languag en subtitl , site_url http www . amara . org video xarksnczro0x en <num> , sub_format json , subtitl end <num> , meta new_paragraph true , posit <num> , start <num> , text let's talk about case where we need to<br>interpol be , between or back off from , end <num> , meta new_paragraph fals , posit <num> , start <num> , text on languag model to anoth on .
and<br>we'll also touch on the web todai .
these , end <num> , meta new_paragraph fals , posit <num> , start <num> , text ar case where it help to us less<br>context rather than more .
and the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text intuit is , suppos that , you have a , a<br>veri confid trigram .
you've seen a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text trigram a veri larg number of time . <br>you're veri confid thi trigram is a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text good estim .
well we should us the<br>trigram .
but suppos you onli saw it onc . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text well mayb you don't realli trust that<br>trigram .
so you might want to back off and , end <num> , meta new_paragraph fals , posit <num> , start <num> , text us the bigram instead .
and mayb you<br>haven't seen a bigram either .
you might , end <num> , meta new_paragraph fals , posit <num> , start <num> , text back off to the unigram .
so he idea of<br>back off is , sometim if you don't have , end <num> , meta new_paragraph fals , posit <num> , start <num> , text a , a larg count or a trustworthi evid<br>for a larger order enneagram , we might , end <num> , meta new_paragraph fals , posit <num> , start <num> , text back off to a smaller on .
a relat idea<br>is interpol .
interpol sai , well , end <num> , meta new_paragraph fals , posit <num> , start <num> , text sometim the trigram mai not be us<br>and in that case , if we mix trigram and , end <num> , meta new_paragraph fals , posit <num> , start <num> , text bigram and unigram , well then we mai get<br>more inform from the unigram and , end <num> , meta new_paragraph fals , posit <num> , start <num> , text bigram but other time trigram will be<br>more us and so interpol suggest , end <num> , meta new_paragraph fals , posit <num> , start <num> , text that we just mix all three all the time<br>and , and , and get the benefit of all of , end <num> , meta new_paragraph fals , posit <num> , start <num> , text them and it turn out in practic that<br>interpol work better than back off . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so most of the time in languag model , <br>we'll be deal with interpol .
there , end <num> , meta new_paragraph fals , posit <num> , start <num> , text ar two kind of interpol .
simpl<br>linear interpol , we have , our , end <num> , meta new_paragraph fals , posit <num> , start <num> , text unigram , our bigram , and our trigram .
and<br>we simpli add them togeth with three , end <num> , meta new_paragraph fals , posit <num> , start <num> , text weight . . .
lambda on , lambda two , and<br>lambda three .
the lambda just sum to on , end <num> , meta new_paragraph fals , posit <num> , start <num> , text to make thi a probabl .
and , and , and , <br>and we can comput our new probabl . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we'll call it p hat of a word given the<br>previou two word , by , by interpol , end <num> , meta new_paragraph fals , posit <num> , start <num> , text these three , languag model .
we can do<br>someth slightli more complic .
we , end <num> , meta new_paragraph fals , posit <num> , start <num> , text can condit our lambda on the context . <br>so we can sai , still mix our trigram or , end <num> , meta new_paragraph fals , posit <num> , start <num> , text bigram with a unigram but now , the lambda<br>ar depend on what the previou two , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word were .
so we can train even a richer<br>and , and more complex context condit , end <num> , meta new_paragraph fals , posit <num> , start <num> , text for decid how to mix our trigram and<br>our bigram and our unigram .
so , where do , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the lambda come from ?
the normal wai to<br>set lambda is to us a held out corpu . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so we've talk befor about have a<br>train corpu .
here's our train , end <num> , meta new_paragraph fals , posit <num> , start <num> , text corpu and our test corpu .
a held out<br>corpu is yet anoth piec that we set , end <num> , meta new_paragraph fals , posit <num> , start <num> , text out , set asid from our data .
and we us a<br>held out corpu .
sometim we , we us , a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text held out corpu call a dev set .
a<br>develop set , or other kind of held , end <num> , meta new_paragraph fals , posit <num> , start <num> , text out data .
we us them to set<br>metaparamet and check for thing .
so in , end <num> , meta new_paragraph fals , posit <num> , start <num> , text thi , we can us the held out corpu to<br>set our lambda .
and the idea is , we're , end <num> , meta new_paragraph fals , posit <num> , start <num> , text gonna choos lambda which maxim the<br>likelihood of thi held out data .
so , end <num> , meta new_paragraph fals , posit <num> , start <num> , text here's what we do .
we take our train<br>data , and we train some enneagram .
now , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we sai , which lambda would i us to<br>interpol those enneagram , such that , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text it give me the highest probabl of<br>thi held out data .
so we , we ask , find , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the set of probabl , such that the<br>log probabl of the actual word that , end <num> , meta new_paragraph fals , posit <num> , start <num> , text occur in the held out data ar highest . <br>now we've talk about case where there , end <num> , meta new_paragraph fals , posit <num> , start <num> , text is zero , so we haven't seen some bi gram<br>befor and we have to replac that zero , end <num> , meta new_paragraph fals , posit <num> , start <num> , text count with some other count , that's<br>smooth .
but what do we do if the actual , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word itself ha never been seen befor . <br>now sometim that doesn't happen .
in , end <num> , meta new_paragraph fals , posit <num> , start <num> , text task where , let's sai a menu base task , <br>where we're , where we have a thick set of , end <num> , meta new_paragraph fals , posit <num> , start <num> , text command , then no other word can ever be<br>said .
our vocabulari is fix , and we have , end <num> , meta new_paragraph fals , posit <num> , start <num> , text a , what's call a close vocabulari task . <br>but , lot of time , languag model is , end <num> , meta new_paragraph fals , posit <num> , start <num> , text appli in case where we don't know , ani<br>word could be us and it could be word , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we've never seen in our train set .
so<br>we call these word oov or out of , end <num> , meta new_paragraph fals , posit <num> , start <num> , text vocabulari word .
and on wai of deal<br>with out of vocabulari word is as , end <num> , meta new_paragraph fals , posit <num> , start <num> , text follow , we creat a special token call<br>unk .
and the wai we train unk , end <num> , meta new_paragraph fals , posit <num> , start <num> , text probabl is we creat a fix<br>lexicon .
so we take our train data and , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we first decid which , we hold out a few<br>word , the veri rare word or the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text unimport word , and we take all those<br>word and we chang those word to unk . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text now we train the probabl of unk like<br>a normal , ani normal word .
so we have our , end <num> , meta new_paragraph fals , posit <num> , start <num> , text corpu , our train corpu .
it ha word , <br>word , word , and it ha a realli low , end <num> , meta new_paragraph fals , posit <num> , start <num> , text probabl word , word , word , word , and<br>we'll take that word and we'll chang it , end <num> , meta new_paragraph fals , posit <num> , start <num> , text to unk .
and now we train out bigram word , <br>word , word unk word , word , word as just as , end <num> , meta new_paragraph fals , posit <num> , start <num> , text if unk had been a word in there and now at<br>decod time if you see a new word you , end <num> , meta new_paragraph fals , posit <num> , start <num> , text haven't seen you replac that word with<br>unk and treat it like get it , it bigram , end <num> , meta new_paragraph fals , posit <num> , start <num> , text probabl and it trigram<br>probabl from the unk word in the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text train set .
anoth import issu in m<br>gram ha to do with web scale or veri , end <num> , meta new_paragraph fals , posit <num> , start <num> , text larg m gram .
so we introduc the googl<br>m gram corpu earlier .
how do we deal , end <num> , meta new_paragraph fals , posit <num> , start <num> , text with comput probabl in such larg<br>space ?
so , on answer is prune .
we onli , end <num> , meta new_paragraph fals , posit <num> , start <num> , text store n gram that have a veri larg<br>count .
so for exampl the veri high order , end <num> , meta new_paragraph fals , posit <num> , start <num> , text n gram we might want to remov all of<br>those singleton .
all of the thing with , end <num> , meta new_paragraph fals , posit <num> , start <num> , text count on becaus by ziff's law there's<br>gonna be a lot of those singleton count . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and we can also us other kind of more<br>sophist version of thi .
we don't , end <num> , meta new_paragraph fals , posit <num> , start <num> , text just remov thing with count we actual<br>us comput the perplex , end <num> , meta new_paragraph fals , posit <num> , start <num> , text on a test set and remov count that ar<br>contribut less to the probabl on a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text particular held out set .
so that's<br>prune .
we can do a number of other , end <num> , meta new_paragraph fals , posit <num> , start <num> , text effici thing .
we can us effici<br>data structur like tri .
we can us , end <num> , meta new_paragraph fals , posit <num> , start <num> , text approxim languag model which ar veri<br>effici but ar not guarante to give , end <num> , meta new_paragraph fals , posit <num> , start <num> , text you the exact same probabl .
we can , we<br>have to do effici thing like don't , end <num> , meta new_paragraph fals , posit <num> , start <num> , text store the actual string but just store<br>index .
we can us huffman code and , end <num> , meta new_paragraph fals , posit <num> , start <num> , text often instead of store our probabl<br>as these big <num> byte float , we might just , end <num> , meta new_paragraph fals , posit <num> , start <num> , text do some kind of quantiz and just<br>store a small number of bit for our , end <num> , meta new_paragraph fals , posit <num> , start <num> , text probabl .
what about<br>smooth for web scale enneagram .
most , end <num> , meta new_paragraph fals , posit <num> , start <num> , text popular smooth method for these veri<br>larg enneagram is an algorithm call , end <num> , meta new_paragraph fals , posit <num> , start <num> , text stupid back off .
stupid back off is call<br>stupid becaus it's veri simpl but it , end <num> , meta new_paragraph fals , posit <num> , start <num> , text work well at the veri larg scale .
and<br>the fact it's been shown to work as well , end <num> , meta new_paragraph fals , posit <num> , start <num> , text as ani more complic algorithm when you<br>have veri larg amount of data .
i mean , end <num> , meta new_paragraph fals , posit <num> , start <num> , text intuit of stupid back off is if i wanna<br>comput the stupid back off probabl of , end <num> , meta new_paragraph fals , posit <num> , start <num> , text a word given some previou set of word .
i<br>us the maximum likelihood estim and , end <num> , meta new_paragraph fals , posit <num> , start <num> , text thi is the count of the word divid by<br>the count of the prefix , if that count is , end <num> , meta new_paragraph fals , posit <num> , start <num> , text greater than zero , and if not , i just back<br>off to the , to the probabl of the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text previou .
the lower order n gram prefix<br>with some constant weight , so it's if , if , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the trigram would sai occur i just us<br>the count of the trigram , if it doesn't i , end <num> , meta new_paragraph fals , posit <num> , start <num> , text take the bigram probabl and multipli<br>it by point four and just us that .
and , end <num> , meta new_paragraph fals , posit <num> , start <num> , text then when i get down to the unigram if i<br>don't have anyth at all i just us the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text unigram , i just us the , the unigram<br>probabl , so , we call thi s .
instead , end <num> , meta new_paragraph fals , posit <num> , start <num> , text of p , becaus stupid back off doesn't<br>produc probabl becaus to produc , end <num> , meta new_paragraph fals , posit <num> , start <num> , text probabl we would actual have to<br>us variou clever kind of wait a back , end <num> , meta new_paragraph fals , posit <num> , start <num> , text off algorithm ha to discount thi<br>probabl to leav some mass left over , end <num> , meta new_paragraph fals , posit <num> , start <num> , text to us the bigram probabl . <br>otherwis , we're gonna end up with number , end <num> , meta new_paragraph fals , posit <num> , start <num> , text that ar greater than on , and we won't<br>have probabl .
but , but , so stupid , end <num> , meta new_paragraph fals , posit <num> , start <num> , text back off produc someth like score , <br>or , or , rather than , than , probabl . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text but it turn out that thi , thi work<br>quit well .
so , in summari , for smooth , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so far , add on smooth is okai for text<br>categor , but it's not recommend , end <num> , meta new_paragraph fals , posit <num> , start <num> , text for languag model .
the most commonli<br>us method we'll discuss in the advanc , end <num> , meta new_paragraph fals , posit <num> , start <num> , text section of thi week is , the , the<br> nye algorithm , or the extend , end <num> , meta new_paragraph fals , posit <num> , start <num> , text interpol nye algorithm . <br>but for veri larg enneagram , like , end <num> , meta new_paragraph fals , posit <num> , start <num> , text situat where you're us the web , <br>simplist algorithm like stupid back off , end <num> , meta new_paragraph fals , posit <num> , start <num> , text actual work quit well .
how about<br>advanc languag model issu ?
recent , end <num> , meta new_paragraph fals , posit <num> , start <num> , text research ha focus on thing like<br>discrimin model .
so here , the idea , end <num> , meta new_paragraph fals , posit <num> , start <num> , text is , pick the enneagram weight .
instead of<br>pick them to fit some train data , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text whether it's maximum likelihood estim<br>or smooth .
instead , choos your enneagram , end <num> , meta new_paragraph fals , posit <num> , start <num> , text weight that improv some task .
so we'll<br>pick a , whatev task we're do , machin , end <num> , meta new_paragraph fals , posit <num> , start <num> , text translat or speech recognit , and<br>choos whatev enneagram weight make , end <num> , meta new_paragraph fals , posit <num> , start <num> , text that task more like .
anoth thing we<br>can do is instead of just us anagram , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we can us parser .
and we'll see the us<br>of parser and statist parser later , end <num> , meta new_paragraph fals , posit <num> , start <num> , text in the cours .
or we can us cach<br>model .
in a cach model we assum that , end <num> , meta new_paragraph fals , posit <num> , start <num> , text a word that's been us recent is more<br>like to appear again .
so the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text probabl , the cach probabl of a<br>word , given some histori , we mix the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text probabl of the word .
with some<br>function of the histori , how , like , how , end <num> , meta new_paragraph fals , posit <num> , start <num> , text much , how often the word occur in the<br>histori with , and we , we weight those two , end <num> , meta new_paragraph fals , posit <num> , start <num> , text probabl togeth .
it turn out that<br>cach model don't work in certain , end <num> , meta new_paragraph fals , posit <num> , start <num> , text situat , and in particularli , thei<br>perform poorli for speech recognit .
we're readi to talk now about advanc method of smooth .
rememb the add on smooth that we had earlier .
in add on smooth we add on to the numer and v to the denomin .
then we saw a gener of that , at k smooth , where we ad k to the numer , and kv to the denomin .
and we can modifi that slightli , we can creat a new version , where we simpli replac , introduc a new term , a new variabl m kv .
and now we have a new wai of write add k smooth that is go to be a help wai of write it .
and the reason is , let's see on the next slide , that when we write .
write it thi wai , we can that what we're do is ad to everi .
by gram we ar ad a constant that's relat to on over the vocabulari size .
and instead of do that we could add a constant relat to the uni gram probabl of the word that ar back off to .
so the uni gram prior smooth algorithm is a is an extens to add k that sai instead of us on over v as our . . .
to add to everi ad some function of on over v to everi bigram count , let's add someth about the unigram probabl .
so , realli , unigram prior is a kind of interpol .
it's , it's a variant of interpol where we're ad the count and , in some function of the unigram probabl to the bigram count .
nonetheless , although unigram prior smooth work well , it still doesn't work well enough to be us for languag model .
instead , the intuit us by mani smooth algorithm , good ture smooth , kneser nei smooth , witten bell smooth , is to us the count of thing we've seen onc to estim the count of thing we've never seen .
the goal of a smooth algorithm is to replac those unseen zero with someth els .
and all these algorithm sai , look at the thing you've seen onc .
thing that you saw onc befor ar just like thing that you haven't seen yet .
and then you're gonna seem them onc in the test set .
so to see how thi intuit work , we're gonna introduc some notat .
and we're gonna , the notat we're gonna introduc is , big n , sub c .
and that will mean the frequenc of frequenc c , mean how mani thing occur with frequenc c .
how big is the bin of thing that occur with frequenc c .
and that's hard to , to , it's not veri intuit .
so let's look at some intuit .
so let's take a littl sentenc .
sam i am , i am sam .
i do not eat .
and let's just look at the unigram count in there .
so we have i occur three time , sam occur twice , and do , not , and eat occur onc each time .
so what is n sub on ?
how mani thing occur on time ?
well , here thei ar , there ar three of them .
three differ word type occur on time .
so n sub on is three .
how'bout , how mani thing occur two time ?
well , there's two of those .
so n sub two is two .
and how'bout thing that occur three time , well onli on of those happen .
so n sum three is on .
all right , so now that we have the intuit about how to think about frequenc of frequenc , let's appli thi to get the intuit for good ture smooth .
imagin you're fish , thi is a scenario invent by josh goodman , and you've caught ten carp , three perch , two whitefish , on trout , on salmon , and on eel .
i don't know what kind of river or stream or ocean thi could be , but nonetheless , you've caught eighteen fish .
and i , we want to estim how like is it the next speci is trout .
and thi is like word .
we have mayb a word that's occur ten time or three , or two , or on .
we want to know how like ar these 1s to occur again .
well , there's been eighteen fish .
the trout's occur on time out of eighteen .
so , the probabl ought to be on out of eighteen .
but now , let's ask how like is it that the next speci is a new speci , catfish or bass , some speci that we haven't seen befor .
someth that occur zero time .
well the good ture intuit sai let's us our estim of thing we saw onc to estim these new thing we've never seen befor .
so what's our estim of thing onc ?
our estim of thing onc is drawn from n sub on , how mani thing occur onc ?
well , what's n sub on ?
n sub on is three .
so out of the eighteen thing we saw , three of them were new , were thing that onli occur on time .
so let's us three out of eighteen as our estim for thing that , that we've never seen befor .
we're go to us our estim of thing , our count of thing that we've seen onc as our estim of thing that we've never seen befor .
we're go to reserv some probabl mass for all those unseen thing .
well now , if we do that , if we us three out of eighteen as our estim for all the unseen thing we could possibl see , how like is it the next speci is trout ?
well i alreadi ask you that question .
but befor i said on over eighteen , but that can't be true anymor .
it must be less than on over eighteen becaus we've us some of our probabl mass for the , from the origin eighteen fish .
we've save some of that for these new fish that we've never seen befor .
we've remov <num> <num> of our probabl mass and so we now have to , have to discount all of our probabl for the other fish downward a littl bit .
how ar we gonna estim what thi discount factor is ?
how much should we reduc all of these count .
here's the , equat for good ture .
here's the answer to that question .
good ture tell us that the probabl for thing that we've never seen befor , p star for thing with zero frequenc , is exactli what we us on the previou slide .
n sub on , the count of thing that have occur with frequenc on , over n .
so it's just a , that's , we saw three out of eighteen wa our number .
well , then , what do you do with , with thing that didn't occur with zero frequenc and for that we us the second part of the good ture equat , which sai the new count , c star , the good ture count , is go to be n sub c plu on divid n sub c , time c plu on .
so , let's just work that out , and we'll , we'll , we'll , give you an intuit for why thi is in a second , let's work out the , work it out in an exampl first .
so , unseen fish , let's sai it's bass or catfish we haven't seen befor , in the train set , the maximum likelihood probabl , estim probabl is zero .
we didn't see thi in the train set out of zero word , so it's zero out of eighteen or it's zero .
but smooth , we're gonna us the new good tour probabl .
and that sai it' n1 out of n , n1 is three .
we saw three thing onc on a previou slide out of eighteen thing .
and so the new probabl is go to be <num> <num> .
what about for someth we've seen onc , like trout ?
how ar we gonna re estim the trout ?
well , the maximum likelihood estim tell us that there wa the count of on .
and so the maximum likelihood probabl is on over eighteen .
but the new good tour formula here sai the count of trout should be c , c is on , c <num> , so two time n sub two over n sub on .
and that's go to be two x on third , becaus n sub two is on from the previou slide , and n sub on is three , and two x on third , so two third .
so our good ture probabl take our c star from trout .
and , and , and divid it by the eighteen thing we've seen so it's two third slash eighteen or <num> <num> .
so instead of the count of thing we saw onc befor we had on over eighteen and now we've drop it to two third .
two third over eighteen .
so we've discount our probabl from <num> <num> to onli two third of an eighteenth , and we've us that extra discount probabl mass to account for the zero thing we've never seen befor .
let's look at the nice intuit for good ture develop by herman nei and hi colleagu .
imagin the train set , thi is of size c , and thi would be a train set with word in it .
thi is a word , thi is anoth word , here's anoth word , here's anoth word .
and now let's , we're go to hold out iter word from thi train set .
let's first take on word , that first word there , the blue word , and we'll just write it over here .
and now we'll think about the train set without that word .
that's got c minu on word .
and thi on held out word over here , the blue word .
and now let's do the same thing with a differ word .
let's take out let's sai the second word .
so we still have c minu on , if we includ thi gui .
c minu on word left in train and then on more word over here in the held out set .
and we'll do thi c time so each time we'll pull out on word .
so we pull out word on by on and what we've creat is a held out set that's of size c , but each word in it wa creat from a train set that wa miss that word , a train set of size c <num> minu that word .
so imagin each of these word and their correspond train set .
and we can look at a pictur develop by dan klein to think about thi intuit .
and here we've just turn those held out and train set on their side .
so i still have of length c , but i've now written them vertic .
and now let's , think about thi intuit .
i've got c train set , each on of size c <num> and then i have a , each on ha a held out set of size on .
and let's try to answer the question , what fraction of held out word , ar unseen in train .
well .
these word n sub zero , the word unseen in train .
each word that's unseen in train occur on time in the origin train set , befor we remov , we took out each of our held out data .
if there wa a word that occur onc in train , so it's in n sub on .
occur onc in train , and we take it out of it train set , leav c minu on word , then that word occur zero time in it train set , the new train set without that word .
so the word , the held out word , n subzero of them , those n subzero word , were the word that were n sub on in their origin train set , befor you remov them .
so .
if we wanna know how mani word ar unseen in train , it's the word that occur on time in the origin train set , or n on over c .
well correspondingli if we want to know , let me clear that up , what fraction of word ar seen k time in train , let's pick a k , perhap there will be two , so we pick n sub two , then the number of thing that occur two time in our held out set is the number of thing that occur three time in the origin train , befor we remov on , on copi of each of those word , so now thei occur onli twice .
so we need to think if we wanna know how mani word occur k time in train to estim that , it's realli the word that occur k <num> time in our origin train set .
and then we're gonna wanna we're gonna wanna multipli that by the number of word that occur , each of those word occur in k plu on time so k plu on were occurr of the n sub k plu on bin each of which ha n sub k plu on word in it and we'll express it as a fraction out of the total word c , rememb the total word were c .
so that's the fraction of held out word seen k time in train .
and , that mean in the futur , we expect k <num> time n sub k <num> over c of the word to be those with train count k .
and sinc there're n sub k word with train count k , we wanna , thi , thi fraction , thi probabl , we wanna distribut that over n sub k word .
so each of those n sub k word is gonna occur with probabl k plu on time n sub k <num> over c over n sub k , becaus we're distribut it over those word .
and that mean that the expect count would be multipli back by c again to turn from a fraction back into a count .
the expect count of word that occur with train count k , k sub star , is k plu on time the ratio of n sub k plu on over n .
sub k .
so on thing we talk about .
we alwai comput the count n sub k from n sub k plu on , but what do we do for word that ar in fact the largest set , the k plu the largest number ?
let's sai that the word the is in fact the word that occur most frequent in the corpu .
we don't have a more frequent word to estim from .
so for larg k , thi good ture estim doesn't work well becaus there ar lot of word that mai never have occur , let's sai <num> , <num> time , or even <num> , <num> time .
we're go to have some gap and so we'll have some zero .
so mayb the word the .
and thi is some other word , of , and there's a miss word in here , and there's miss word here .
so we can't alwai us the n <num> word to do the estim .
and a simpl replac for that , in fact an algorithm call simpl good ture , is after the count get unreli , after the first , you know , first few count , we just replac our estim with some kind of a best fit power law .
so we don't actual us good ture with each of these higher order number .
we just us them for the lower bin .
so let's look at the result good ture number from on exampl .
so here's number from , a church and gale experi , where thei us <num> million word of ap newswir .
here's the , just to remind you , here's the good ture equat .
so the count c star is c <num> time nc <num> over nc .
so here's the origin count , c .
and here ar all , here's the <num>'s now replac by the good ture estim with a littl extra probabl mass from , from n sub on .
here's the <num>'s , the <num>'s all turn into . <num> .
all the thing that occur with count two , now occur with count <num> . <num> .
all the thing with count three occur with count <num> . <num> .
so each of our count ha been discount .
each of these count have been discount to a lower number to leav some room for the thing with zero count .
and the last thing i'm gonna leav you on is ask you , what's the relationship between each of these count , the origin count c and these count c star .
do you notic ani gener relationship ?
let is talk about kneser nei smooth , on of the most sophist form of smooth , but also on with a beauti and eleg intuit .
rememb that from good ture , we talk about the c<i> is , < i> the discount count you end up from good ture , and we discount each of the count the count of on wa discount to point four , and the count of two discount to <num> . <num> and so on in order to save mass to replac the zero count with some low number .
and if you look at the actual valu of these count <num> . <num> for nine and <num> . <num> for eight you ll notic that in a veri larg number of case , the discount count ha a veri close relationship with the origin count .
it is realli the origin count minu . <num> , or somewhat close to that .
so , in practic what good ture often doe is produc a fix small discount from the count .
and that intuit , that of a fix small discount , can be appli directli .
when we do thi , we call thi absolut discount , and absolut discount is a popular kind of smooth .
and here we ar show you absolut discount interpol .
and again , the intuit is just we ll save some time and have to comput all those complic good ture number and we ll just subtract . <num> , or mayb it will be a differ discount valu for differ corpora .
now here is the equat for absolut discount .
so we ar do diagram again .
so the probabl , absolut discount , of a word , given the previou word , will be some discount bigram , interpol with some interpol weight , with the unigram probabl .
so we have a unigram probabl p w , and then the bigram probabl , and we just subtract a fix amount , let is sai it is . <num> , from the count .
and otherwis comput the bigram probabl in the normal wai .
so , we have a discount bigram probabl , mix with some weight , which i shall talk later about how to set thi weight , with a unigram .
and mayb we might keep a coupl of extra valu of d for count on and two .
count on and two , we saw on a previou slide , weren t quit subtract . <num> , so we can model thi more carefulli by have separ count for those .
but the problem with absolut discount is the unigram probabl itself .
and i want to talk about chang the unigram probabl .
and that is the fundament intuit of kneser nei .
so in kneser nei smooth , the idea is keep that same interpol that we saw in absolut discount , but us a better estim of probabl of the lower unigram .
and the intuit for that , we can go back and look at the classic shannon game .
rememb , in the shannon game we ar predict a word from previou word , so we see a sentenc , i can t see without my read <u><u><u><u> . < u>< u>< u>< u> what is the most like next word ?
well , glass seem pretti like .
well , how about instead the word francisco ?
well , that seem veri unlik in thi situat .
and yet francisco , as just a unigram , is more common than glass .
but the reason why francisco seem like a bad thing after read , on intuit we might be abl to get is that francisco alwai follow san or veri often follow san .
so while francisco is veri frequent , it is frequent in the context of the word san .
now , unigram in an interpol model , where we ar mix a unigram and a bigram , ar specif us thei ar veri help just in case where we haven t seen a bigram .
so it is unfortun that just in the case where we haven t seen the bigram read francisco , we ar trust francisco is unigram weight which is just where we shouldn t trust it .
so instead of us the probabl of w , how like is a word , our intuit is go to be when we ar back off to someth we should instead us the continu probabl .
we ar go to call it p continu of a word , how like is the word to appear as a novel continu .
well , how do we measur novel continu ?
well , for each word we ll just count the number of bigram type it complet .
how mani differ bigram doe it creat by appear after anoth word .
in other word , each bigram type is a novel continu the first time we see thi new bigram .
in other word , the continu probabl is go to be proport to the cardin of thi set , the number of word of preced word , i minu on , that occur with our word .
so , how mani word occur befor thi word in a bigram .
how mani preced word ar there .
that will be the cardin of that set , that is a number we would like our continu probabl to be proport to .
so how mani time doe w appear as a novel continu ?
we need to turn that into a probabl .
so we just divid by the total number of word bigram type .
so , of all word bigram that occur more than zero time , what is the cardin of that set ?
how mani differ word bigram type ar there , and we ar just go to divid the two to get a probabl of continu , of all the number of word bigram type how mani of those have w as a novel continu .
now it turn out that there is an altern metaphor for kneser nei with the same equat so again we can see the numer as the total number of word type that preced w , how mani word type can w follow and we ar go to normal it by the number of word that could preced all word .
so , thi sum over all word of the number of word type that can preced the word .
and these two ar the same the number of thi denomin and the denomin we saw on the previou slide ar the same becaus the number of possibl bigram type is the same as the number of word type that can preced all word sum over all word .
if you think about that for a second , you ll realiz that is true .
so in other word , with thi kind of kneser nei model , a frequent word like francisco that occur onli in on context like san will have a low continu probabl .
so if we put togeth the intuit of absolut discount with the kneser nei probabl for the lower order n gram , we have the kneser nei smooth algorithm .
so , for the bigram itself we just have absolut discount we take the bigram count , we subtract some d discount , and i ve just shown here that we take the max of that and zero becaus , obvious , if the discount happen to be higher than the probabl we don t want a neg probabl .
and we ar just gonna interpol that with thi same continu probabl that we just saw , p continu of w sub i .
and , the lambda , now let is talk about how to comput that lambda .
the lambda is gonna take all that probabl mass from all those normal discount that we took out of these higher order probabl , and us those to weight how much probabl we should assign to the unigram .
we ar gonna combin those .
so that lambda is the amount of the discount weight divid by the denomin there .
so , it is the normal discount .
and then , we ar gonna multipli that by the total number of word type that can follow thi context , w<u>i minu on . < u> in other word , how mani differ word type did we discount or how mani time did we appli thi normal discount .
and we multipli those togeth and we know how much probabl mass total we can now assign to the continu of the word .
now , thi is the bigram formul for kneser nei .
now in thi slide , we ar show you the gener recurs formul for n gram in gener , and here we have to make a slight chang to deal with all the higher order n gram .
so here we ar just show the kneser nei probabl of a word given the prefix of the word .
and , just like kneser nei we saw befor , we ar just interpol a higher order n gram which is discount with a lambda weight and a lower order probabl .
but now we need to distinguish between the veri first top level time that we us a count and these lower order count .
so we ar go to us the actual count for the veri highest order bigram , and we ar go to us the continu valu that we just defin earlier for all the lower order probabl .
so we ll defin thi new thing count kneser nei of dot which will mean actual count .
thi will be actual count , let is sai we ar do trigram .
for the trigram , and then when we recurs and have the kneser nei probabl for the lower order thing .
when we get down to the bigram and unigram , we ll be us the continu count , that is again the singl word context that we defin earlier .
so kneser nei smooth , a veri excel algorithm .
it is veri commonli us in speech recognit and machin translat and yet it ha a veri beauti and eleg intuit and i hope you appreci it .
descript , languag code en , dir ltr , name english , metadata , note , resourc_uri api<num> partner video vbacifslgnqw languag en subtitl , site_url http www . amara . org video vbacifslgnqw en <num> , sub_format json , subtitl end <num> , meta new_paragraph true , posit <num> , start <num> , text todai we're gonna talk about spell<br>correct .
lot of applic make us , end <num> , meta new_paragraph fals , posit <num> , start <num> , text of spell correct .
for exampl , word<br>process , almost ani modern word , end <num> , meta new_paragraph fals , posit <num> , start <num> , text processor will take a misspel word like<br>compon with an a and give you , end <num> , meta new_paragraph fals , posit <num> , start <num> , text suggest like compon with an e and<br>automat replac it for you .
modern , end <num> , meta new_paragraph fals , posit <num> , start <num> , text search engin will not onli flag an<br>error .
so , languag spell without a u , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text here .
but , give you , the result , as if<br>you had spell the word correctli .
and , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text modern phone addition will<br>automat find misspel word .
here , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text i type l a y r , and it replac it<br>automat , or suggest a replac , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text with late .
we can distinguish a number of<br>separ task and spell correct . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text on is the detect of the error itself . <br>and then the correct of the error onc , end <num> , meta new_paragraph fals , posit <num> , start <num> , text you've found it .
and we can think about<br>differ kind of correct .
we might , end <num> , meta new_paragraph fals , posit <num> , start <num> , text automat correct an error if we're<br>posit that the error that we know the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text right answer for the error .
so h t e is a<br>veri common misspel for the , and so , end <num> , meta new_paragraph fals , posit <num> , start <num> , text mani word processor automat correct<br>h t e .
we might suggest a singl , end <num> , meta new_paragraph fals , posit <num> , start <num> , text correct if we're , there's onli on veri<br>like correct , or we might suggest a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text whole list of correct and let the user<br>pick from among them .
we distinguish two , end <num> , meta new_paragraph fals , posit <num> , start <num> , text differ class of spell error .
non<br>word error ar error in which the , what , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the user type is not a word of english . <br>so g r a f f e a misspel let's sai for , end <num> , meta new_paragraph fals , posit <num> , start <num> , text giraff is not a word of english .
by<br>contrast , real word error .
ar error in , end <num> , meta new_paragraph fals , posit <num> , start <num> , text which then the result .
<br>misspel is actual a word of english , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and that make them somewhat harder to<br>detect .
and we can break up real word , end <num> , meta new_paragraph fals , posit <num> , start <num> , text error into on produc by realli<br>typograph process .
these were meant , end <num> , meta new_paragraph fals , posit <num> , start <num> , text to type three .
and type let's<br>sai .
or cognit error , where the user , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text meant to type a word like and<br>instead type a homophon of a , of the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word , or u<num><num>ct o o u<num><num>d instead of<br> and in both case what , what's , end <num> , meta new_paragraph fals , posit <num> , start <num> , text produc is a real word of english , but by<br>model the differ between these , end <num> , meta new_paragraph fals , posit <num> , start <num> , text kind of error , we might come up with<br>better wai of fix them both .
how , end <num> , meta new_paragraph fals , posit <num> , start <num> , text common ar spell error ?
depend a lot<br>on the task .
so in web queri , spell , end <num> , meta new_paragraph fals , posit <num> , start <num> , text error ar extrem common .
so<br>practic on in four word in a web , end <num> , meta new_paragraph fals , posit <num> , start <num> , text queri ar like to be misspel .
but in<br>web process task on phone it's much , end <num> , meta new_paragraph fals , posit <num> , start <num> , text harder to get an accur number .
so<br>there's been a number of studi and most , end <num> , meta new_paragraph fals , posit <num> , start <num> , text of these studi ar done by retyp .
you<br>give the user a passag to type and then , end <num> , meta new_paragraph fals , posit <num> , start <num> , text you measur how well thei , thei type it . <br>and , of cours , that's not quit the same , end <num> , meta new_paragraph fals , posit <num> , start <num> , text user's natur write messag or<br>type .
nonetheless if you ask user to , end <num> , meta new_paragraph fals , posit <num> , start <num> , text retyp and you don't let them us the<br>backspac kei , thei make about thirteen , end <num> , meta new_paragraph fals , posit <num> , start <num> , text percent of the word , thirteen percent of<br>the word ar in error .
so indic that , end <num> , meta new_paragraph fals , posit <num> , start <num> , text if , that a lot of word .
thei correct<br>themselv with the backspac .
if you let , end <num> , meta new_paragraph fals , posit <num> , start <num> , text them correct , now we're try to<br>experi on a , on a p d a style phone , end <num> , meta new_paragraph fals , posit <num> , start <num> , text site , organ , thei'll correct about<br>seven percent of the word themselv . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text thei'll still leav about two percent of<br>the word uncorrect , on the organ . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and , similar number on peopl do<br>retyp on a regular keyboard .
so , the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text number ar about two percent where peopl<br>type .
and probabl a much higher number , end <num> , meta new_paragraph fals , posit <num> , start <num> , text for web queri and probabl a much higher<br>number for peopl text .
ar the kind of , end <num> , meta new_paragraph fals , posit <num> , start <num> , text spell , spell error that<br>we see .
how do we detect non word spell , end <num> , meta new_paragraph fals , posit <num> , start <num> , text error .
the tradit wai is just to us<br>a larg dictionari .
ani word not in the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text dictionari is an error .
and , the larger<br>the dictionari , it turn out the better , end <num> , meta new_paragraph fals , posit <num> , start <num> , text thi work .
for correct these non word<br>spell error , we gener a set of , end <num> , meta new_paragraph fals , posit <num> , start <num> , text candid that's real word that ar<br>similar to the error .
and then we pick , end <num> , meta new_paragraph fals , posit <num> , start <num> , text whichev on is best .
and we'll talk<br>about the noisi channel probabl model , end <num> , meta new_paragraph fals , posit <num> , start <num> , text of how to do that .
and it's also relat<br>to anoth method call the shortest , end <num> , meta new_paragraph fals , posit <num> , start <num> , text weight distanc myth .
so we<br>find the word that ar not in the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text dictionari .
for each on , we gener a<br>set of candid .
those ar go to be , end <num> , meta new_paragraph fals , posit <num> , start <num> , text real word that ar similar , we'll talk<br>about what similar mean , to that error , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and then we'll pick the best on .
for real<br>word spell error , the algorithm is , end <num> , meta new_paragraph fals , posit <num> , start <num> , text quit similar .
again , for each word we<br>gener a candid set .
but now we do , end <num> , meta new_paragraph fals , posit <num> , start <num> , text thi for everi word in a sentenc , not<br>just the word that ar not in some , end <num> , meta new_paragraph fals , posit <num> , start <num> , text dictionari .
so real word spell error<br>correct , we don't us a dictionari , end <num> , meta new_paragraph fals , posit <num> , start <num> , text becaus of cours the error ar in a<br>dictionari .
so that wouldn't help .
so , for , end <num> , meta new_paragraph fals , posit <num> , start <num> , text everi word , we gener a candid set . <br>so we might find candid word with , end <num> , meta new_paragraph fals , posit <num> , start <num> , text similar pronunci , we might find<br>candid word with similar spell , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and depend on the algorithm , exactli . <br>and it's veri import that we're gonna , end <num> , meta new_paragraph fals , posit <num> , start <num> , text includ the word itself , in the candid<br>set , becaus the everi word might be a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text misspel of some other real word , or it<br>might be the correct word .
in fact , most , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word ar probabl correct .
so , for each<br>candid set of each possibl error , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we're gonna includ the word itself .
and<br>most of the time , in fact , we're gonna , end <num> , meta new_paragraph fals , posit <num> , start <num> , text pick that .
and again , how we pick the<br>word we might us the noisi channel , end <num> , meta new_paragraph fals , posit <num> , start <num> , text model .
descript , languag code en , dir ltr , name english , metadata , note , resourc_uri api<num> partner video <num>gk<num>ptphqvzc languag en subtitl , site_url http www . amara . org video <num>gk<num>ptphqvzc en <num> , sub_format json , subtitl end <num> , meta new_paragraph true , posit <num> , start <num> , text let u<num><num>s introduc the noisi channel model of spell . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the intuit of the noisi channel u<num><num> , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and it come up throughout natur languag process u<num><num> , end <num> , meta new_paragraph fals , posit <num> , start <num> , text is that we have some origin signal u<num><num> , end <num> , meta new_paragraph fals , posit <num> , start <num> , text let u<num><num>s sai it u<num><num>s a word u<num><num> , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and we imagin that it goe through some channel . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and the idea wa origin invent for speech , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text where if you talk into a tube , end <num> , meta new_paragraph fals , posit <num> , start <num> , text or you go over some kind of telecommun line , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and the word is distort . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and so what come out from the origin word is some noisi word . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and we u<num><num>ve repres that here with a weird font . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text but , in the spell case we imagin that , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text u<num>coh , somebodi mistyp the word ! u<num>d , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so the channel is the typewrit or the person type or the keyboard , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and at the end , you u<num><num>ve got a misspel version of the word . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and our goal in the noisi channel model , end <num> , meta new_paragraph fals , posit <num> , start <num> , text is to take that output of that noisi process , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and by model how thi channel work , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we build a model u<num><num> probabilist model u<num><num>of the channel . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we can run all possibl origin word through that channel , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and see which on look the most like the noisi word . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so the decod will take a bunch of hypothes for each on , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text run it through the channel , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text just run hypothesi two through the channel , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text run hypothesi three through the channel , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and we see which word look the most like thi noisi word , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and we pick that , end <num> , meta new_paragraph fals , posit <num> , start <num> , text as the origin hypothesi for the word that start out . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so let u<num><num>s look at that . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text first we u<num><num>ll introduc some probabl , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and then we u<num><num>ll look at some exampl . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the noisi channel is a probabilist model . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text our goal given an observ x of some misspel u<num><num> , end <num> , meta new_paragraph fals , posit <num> , start <num> , text some word we u<num><num>ve seen , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text some surfac thing we u<num><num>ve seen , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text some observ x u<num><num> , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we u<num><num>d like to find w , the correct word . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and we u<num><num>re go to model that probabilist , end <num> , meta new_paragraph fals , posit <num> , start <num> , text by sai we u<num><num>re look the best word , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the word that we u<num><num>d like to replac our misspel with , end <num> , meta new_paragraph fals , posit <num> , start <num> , text is that word out of the vocabulari that maxim a probabl . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text what probabl ? , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the probabl of the word given the misspel . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so what word , given that we u<num><num>ve seen some misspel ? , end <num> , meta new_paragraph fals , posit <num> , start <num> , text what u<num><num>s the most like word ? , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the most probabl posterior probabl word , given that misspel . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and we u<num><num>re go to us bay rule to replac that probabl . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so , the probabl of w given x , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we u<num><num>re go to replac that with p x w p w p x . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and so we can also elimin the denomin . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so whatev word maxim thi equat , end <num> , meta new_paragraph fals , posit <num> , start <num> , text will also maxim thi equat . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we u<num><num>re ask , given a misspel x , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text what u<num><num>s the most like word ? , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and sinc the formula for that probabl , end <num> , meta new_paragraph fals , posit <num> , start <num> , text includ the probabl of the word , the misspel x . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we u<num><num>re includ that probabl , end <num> , meta new_paragraph fals , posit <num> , start <num> , text in everi w that we u<num><num>re consid . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so if some w , sai w hypothesi on , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text ha a greater probabl than hypothesi two by thi equat , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text it u<num><num>ll also have a greater probabl by thi equat , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text becaus w is a constant . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text x is the misspel that we u<num><num>re try to decid , end <num> , meta new_paragraph fals , posit <num> , start <num> , text if w<num> or w<num> is a better hypothesi for it . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so that mean that the noisi channel model , end <num> , meta new_paragraph fals , posit <num> , start <num> , text come down to maxim the product of two factor , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the likelihood , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and the prior . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and we gener call thi term the languag model . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and you u<num><num>ve seen languag model befor , end <num> , meta new_paragraph fals , posit <num> , start <num> , text that u<num><num>s the probabl of the correct word , w . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and thi likelihood term , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we often call thi the channel model , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text or sometim the error model . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so we u<num><num>ve got two factor , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the languag model and the channel model . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and the intuit is that the languag model tell us , end <num> , meta new_paragraph fals , posit <num> , start <num> , text how like would thi word be to be a word , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text perhap in thi context , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text perhap by itself . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the channel model sai , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text well , if it wa that word , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text how like would it be to gener thi exact error ? , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so the channel model wa sort of model that noisi channel , end <num> , meta new_paragraph fals , posit <num> , start <num> , text that turn the correct word into the misspel . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text now thi noisi channel model for spell wa propos around <num> , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text independ at two separ laboratori . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and the us of speech recognit model like noisi channel , end <num> , meta new_paragraph fals , posit <num> , start <num> , text came into natur languag process right around then , end <num> , meta new_paragraph fals , posit <num> , start <num> , text mainli , although not exclus , becaus of the work at these two lab , end <num> , meta new_paragraph fals , posit <num> , start <num> , text at ibm and at at t bell lab . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and so the exampl we u<num><num>re go to take for the rest of thi exampl , end <num> , meta new_paragraph fals , posit <num> , start <num> , text come from these two import earli paper , end <num> , meta new_paragraph fals , posit <num> , start <num> , text by mai et al .
descript , languag code en , dir ltr , name english , metadata , note , resourc_uri api<num> partner video r<num>hmmjd<num>vjff languag en subtitl , site_url http www . amara . org video r<num>hmmjd<num>vjff en <num> , sub_format json , subtitl end <num> , meta new_paragraph true , posit <num> , start <num> , text we've seen how to correct non word of<br>english but what happen if the error , end <num> , meta new_paragraph fals , posit <num> , start <num> , text produc a real word ?
thi turn out to be<br>a veri common problem .
mayb between a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text quarter and a half of spell error<br>depend on the applic turn out to , end <num> , meta new_paragraph fals , posit <num> , start <num> , text be real word .
so in these exampl from , <br>from a classic paper by karen kukich .
the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word minut is misspel as minuet , <br>perfectli resembl english word .
the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word and is misspel as the word an , a<br>veri , a veri common english word .
leav as , end <num> , meta new_paragraph fals , posit <num> , start <num> , text lave , by as be , and so on .
so these ar , <br>not onli ar these english word , but some , end <num> , meta new_paragraph fals , posit <num> , start <num> , text of them ar quit common , and , and<br>frequent us english word .
so a much , end <num> , meta new_paragraph fals , posit <num> , start <num> , text tougher problem , solv the real word<br>spell error , task .
again what we're , end <num> , meta new_paragraph fals , posit <num> , start <num> , text gonna do for the real word spell error<br>is veri similar to what we did for the , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the non real word , we're gonna gener a<br>candid set , which includ the word , end <num> , meta new_paragraph fals , posit <num> , start <num> , text itself and all singl letter , <br>that produc english word and mai be , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we'll also in some version produc , <br>includ word that ar , sound , end <num> , meta new_paragraph fals , posit <num> , start <num> , text to like and give thi set<br>for each word .
we'll choos the best , end <num> , meta new_paragraph fals , posit <num> , start <num> , text candid .
either us the noisi channel<br>model or you can imagin we'll talk later , end <num> , meta new_paragraph fals , posit <num> , start <num> , text about more , complex model that us<br>classifi .
so let's look at that in , end <num> , meta new_paragraph fals , posit <num> , start <num> , text detail .
given a sentenc with word on<br>through word n .
we're go to gener , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text for each word , a set of candid .
so for<br>word on , we have the candid word on , end <num> , meta new_paragraph fals , posit <num> , start <num> , text itself , and then a bunch of vari , the<br>singl edit distanc neighbor of that , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word .
word on prime , word on doubl<br>prime , word on tripl prime , word two , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word two prime , word two doubl prime , <br>world two tripl prime , and so on for each , end <num> , meta new_paragraph fals , posit <num> , start <num> , text of the word .
so we have a whole lot of<br>candid for each of the word .
and now , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we're gonna choos the sequenc , capit<br>w , the sequenc of candid that , end <num> , meta new_paragraph fals , posit <num> , start <num> , text maxim , ha a maxim probabl .
in<br>other word , we might pick .
word on from , end <num> , meta new_paragraph fals , posit <num> , start <num> , text thi candid set .
and word two prime , <br>prime from thi candid set and word , end <num> , meta new_paragraph fals , posit <num> , start <num> , text three prime , prime , prime from thi<br>candid set and so on .
for each word , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we're gonna pick some candid , which<br>might be the word itself or some , end <num> , meta new_paragraph fals , posit <num> , start <num> , text correct of that word .
and we're gonna<br>pick the sequenc that is most like . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text let's look at an exampl of that .
we have<br>the imagin the three word , the mini , the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text mini sentenc , two of , thew , thew , thew . <br>so for each word , the word two , the word , end <num> , meta new_paragraph fals , posit <num> , start <num> , text of , and the word , we gener<br>potenti correct , each of which is a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word of english that is of<br>on .
so , i've shown some here .
so , two , end <num> , meta new_paragraph fals , posit <num> , start <num> , text could have been the word t o , if the word , <br>if the origin word two wa the error wa , end <num> , meta new_paragraph fals , posit <num> , start <num> , text an insert of a w .
or , it could have<br>been the word tao , where the error wa a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text substitut of a for w .
or , it could have<br>been the word too , substitut of an o by , end <num> , meta new_paragraph fals , posit <num> , start <num> , text a w .
or it could have been correctli , the<br>word two could be correct .
similarli of , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text could have been , the correct word could<br>have been off , and there wa a delet of , end <num> , meta new_paragraph fals , posit <num> , start <num> , text an f .
so , again , three candid off , on , <br>and includ the word of itself , and the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word few , which is a real word of english . <br>could have been the word through , and the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text r got delet , or the word thaw , or the<br>word the .
a veri common word , and , ew , a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text veri like error , it turn out .
becaus w<br>is right next to e in the keyboard , and so , end <num> , meta new_paragraph fals , posit <num> , start <num> , text on .
and so we have each of our candid<br>set .
and then we just wanna ask , of all , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the possibl set of sentenc .
produc<br>by path in thi , in thi graph .
so here's , end <num> , meta new_paragraph fals , posit <num> , start <num> , text on .
two of through .
here's anoth on . <br>two on thaw .
here's anoth on .
two of , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the .
and so on , for each of those , <br>possibl sentenc , what's the most like , end <num> , meta new_paragraph fals , posit <num> , start <num> , text on accord to the noisi channel .
we<br>pick the mo , excus me , the most probabl , end <num> , meta new_paragraph fals , posit <num> , start <num> , text on accord to the noisi channel .
and<br>hopefulli the , the noisi channel , a good , end <num> , meta new_paragraph fals , posit <num> , start <num> , text noisi channel model will predict the , will<br>pick the correct answer to of the , as our , end <num> , meta new_paragraph fals , posit <num> , start <num> , text most like sequenc here .
in practic , <br>for spell correct , we often make the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text simplif of we're onli see on<br>error , rather than let everi word have , end <num> , meta new_paragraph fals , posit <num> , start <num> , text a , have a possibl error in it .
in other<br>word .
the set of sequenc we consid is , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the sequenc in which onli on of the<br>word is an error .
and the rest of the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word were correct as type .
so here , word<br>on , word three , and word four were , end <num> , meta new_paragraph fals , posit <num> , start <num> , text correct as type , and it wa word two that<br>wa misspel , and we replac it by word , end <num> , meta new_paragraph fals , posit <num> , start <num> , text two , doubl prime , let's sai .
or , in thi<br>sequenc , it wa the word three that wa , end <num> , meta new_paragraph fals , posit <num> , start <num> , text misspel .
thew wa misspel as , the<br>wa misspel at thew and so here's the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text mi , the error and these , these three<br>word ar correct and so on .
and so thi , end <num> , meta new_paragraph fals , posit <num> , start <num> , text smaller set of possibl candid<br>sequenc .
so instead of have to , end <num> , meta new_paragraph fals , posit <num> , start <num> , text consid n squar possibl sequenc<br>we're just consid a constant time n , end <num> , meta new_paragraph fals , posit <num> , start <num> , text possibl sequenc .
from thi set now we<br>choos the sequenc that maxim , that , end <num> , meta new_paragraph fals , posit <num> , start <num> , text ha the maximum probabl that we<br>pick .
the , the most like , the most , end <num> , meta new_paragraph fals , posit <num> , start <num> , text probabl most condition , most probabl<br>set of sequenc , sequenc of candid . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text where do we get these<br>probabl ?
again , we can the languag , end <num> , meta new_paragraph fals , posit <num> , start <num> , text model .
just as we saw befor , we have our<br>unigram , we have our bigram .
we can us , end <num> , meta new_paragraph fals , posit <num> , start <num> , text whatev smooth method we'd like .
the<br>channel model's just the same , again , as , end <num> , meta new_paragraph fals , posit <num> , start <num> , text for the non word spell error<br>correct .
the onli differ is , we now , end <num> , meta new_paragraph fals , posit <num> , start <num> , text need a probabl of have no error . <br>becaus , of cours , we're assum that , end <num> , meta new_paragraph fals , posit <num> , start <num> , text onli on of the word is an error .
so , we<br>have , we have to have a probabl for , end <num> , meta new_paragraph fals , posit <num> , start <num> , text all those other word that ar not an<br>error .
we need to be abl to , to , decid , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text when we have an error , and when a word is , <br>in fact , correct .
mean that the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text probabl of the word itself , given the<br>word , is high , so , unlik to have an , end <num> , meta new_paragraph fals , posit <num> , start <num> , text error .
how do we comput thi probabl<br>of no error ?
what's the channel , end <num> , meta new_paragraph fals , posit <num> , start <num> , text probabl for a correctli type word ? <br>and thi obvious depend on the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text applic .
and so we might make the<br>assumpt , that , in a particular , end <num> , meta new_paragraph fals , posit <num> , start <num> , text applic , you know , on word on ten is<br>type wrong .
and that mean that the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text probabl of , of , a correctli type word<br>is . <num> .
or we might have , instead , the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text assumpt that on word in <num> is wrong . <br>and so now , the probabl of , of ani , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word be type correctli is . <num> .
so<br>there's our channel model probabl of a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word not chang .
let's assum that the<br>channel model of a task is ha a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text probabl of on in twenti of an error . <br>mean that <num> percent of the time a word , end <num> , meta new_paragraph fals , posit <num> , start <num> , text is correct as type .
so , here's an exampl<br>from peter norvig .
again we have the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text spell error , thew .
we want to know<br>whether it should be the word the .
the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word few , correct , that it wa correct as<br>type or thaw or through or three and so , end <num> , meta new_paragraph fals , posit <num> , start <num> , text on .
and again , for each on , we gener<br>our channel model and the other channel , end <num> , meta new_paragraph fals , posit <num> , start <num> , text model were exactli comput the same wai<br>as befor .
we have the probabl of a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text substitut of a , subset of a , a , be<br>substitut by a e or of an r be , end <num> , meta new_paragraph fals , posit <num> , start <num> , text delet after an h and so on that we can<br>comput just from our channel model .
well , end <num> , meta new_paragraph fals , posit <num> , start <num> , text here's our channel model probabl . <br>and again , we have our languag model , end <num> , meta new_paragraph fals , posit <num> , start <num> , text probabl , just as befor .
and these ar<br>exampl that pierr norbert comput from , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the googl anagram count .
and again we've<br>assum the channel model of a word not , end <num> , meta new_paragraph fals , posit <num> , start <num> , text chang , of the , of the error x be<br>gener by correct , correctli gener , end <num> , meta new_paragraph fals , posit <num> , start <num> , text by the word x .
and we can multipli these<br>togeth , multipli togeth the channel , end <num> , meta new_paragraph fals , posit <num> , start <num> , text model with the languag model and again<br>show you these multipli by ten to the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text ninth to make it easi to read .
you can see<br>that the word the .
is correctli chosen , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text veri high probabl as the misspel<br>of the word the by itself and in context , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and thi is us a unigram languag<br>model , if we're us a bigram or a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text trigram or even more like probabl to be<br>abl to distinguish when the word the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text realli wa the word the and when it wa<br>the word the .
descript , languag code en , dir ltr , name english , metadata , note , resourc_uri api<num> partner video <num>hlbldinibrd languag en subtitl , site_url http www . amara . org video <num>hlbldinibrd en <num> , sub_format json , subtitl end <num> , meta new_paragraph true , posit <num> , start <num> , text there ar a few more issu that come up<br>in spell correct that we wanna , end <num> , meta new_paragraph fals , posit <num> , start <num> , text includ in ani kind of state of the art<br>system .
on is hci issu , human comput , end <num> , meta new_paragraph fals , posit <num> , start <num> , text interact issu .
so if we're veri<br>confid in the correct for exampl , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we might wanna auto correct .
and so that<br>happen veri often as i talk about , end <num> , meta new_paragraph fals , posit <num> , start <num> , text earlier with the exampl hte which is a<br>veri common misspel of the .
if we're , end <num> , meta new_paragraph fals , posit <num> , start <num> , text slightli less confid , we might wanna<br>give a singl best correct .
but to the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text user to just , to , to sai ye or no to .
if<br>we're even less confid , we might wanna , end <num> , meta new_paragraph fals , posit <num> , start <num> , text give the user a whole list , and let them<br>pick from thi list .
and if we're just , end <num> , meta new_paragraph fals , posit <num> , start <num> , text unconfid at all but we're pretti sure<br>we saw an error , we just don't know how to , end <num> , meta new_paragraph fals , posit <num> , start <num> , text fix it then we might just flag what the<br>user type as an error .
so variou thing , end <num> , meta new_paragraph fals , posit <num> , start <num> , text again depend on our applic , and<br>depend on the probabl and the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text confid valu that we might gener . <br>in practic for almost all noisi channel , end <num> , meta new_paragraph fals , posit <num> , start <num> , text model .
even though we , we defin that<br>model as multipli a prior and a , and a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text likelihood and a error model .
in practic , <br>these two probabl ar comput from , end <num> , meta new_paragraph fals , posit <num> , start <num> , text with , make a lot of independ<br>assumpt about how mani error there , end <num> , meta new_paragraph fals , posit <num> , start <num> , text were , and , and the fact that the spell<br>is independ of neighbor word , and , end <num> , meta new_paragraph fals , posit <num> , start <num> , text these ar realli not true .
and the result<br>of these incorrect independ , end <num> , meta new_paragraph fals , posit <num> , start <num> , text assumpt mean that these two<br>probabl ar often not commensur . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so we do , in fact , is , instead of just<br>multipli these two , we weight them .
and , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the wai , sinc we're multipli<br>probabl , we weight them by rais , end <num> , meta new_paragraph fals , posit <num> , start <num> , text on of them to a power .
we can't , <br>obvious , multipli on of them by , end <num> , meta new_paragraph fals , posit <num> , start <num> , text someth .
so we , we weight them by<br>rais on of them to a power lambda .
and , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we learn thi lambda from some develop<br>test that we pick whatev lambda to , to , end <num> , meta new_paragraph fals , posit <num> , start <num> , text rais the , the languag multipl<br>probabl to such that the product is , end <num> , meta new_paragraph fals , posit <num> , start <num> , text more like to pick out just those error<br>that realli ar error .
and we us thi , end <num> , meta new_paragraph fals , posit <num> , start <num> , text weight of the , of the noisi channel<br>model in almost ani applic that we , end <num> , meta new_paragraph fals , posit <num> , start <num> , text see with a noisi channel model .
someth<br>els that's us in the state of the art , end <num> , meta new_paragraph fals , posit <num> , start <num> , text system is to us not just the spell<br>but the pronunci of the word to help , end <num> , meta new_paragraph fals , posit <num> , start <num> , text us find error .
so the metaphon system<br>which is us in , in gnu aspel instead of , end <num> , meta new_paragraph fals , posit <num> , start <num> , text just ask for candid that have a<br> a similar spell , ask for , end <num> , meta new_paragraph fals , posit <num> , start <num> , text candid that have a similar<br>pronunci .
and that's done by first , end <num> , meta new_paragraph fals , posit <num> , start <num> , text convert the misspel to a<br>pronunci .
and , and the metaphon is a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text simplifi pronunci system , that , a<br>set of rule that convert a word into a , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text someth approxim a pronunci . <br>and here's the rule that get us .
drop , end <num> , meta new_paragraph fals , posit <num> , start <num> , text duplic adjac letter , except for c . <br>if the word begin with kn , or gn , drop , end <num> , meta new_paragraph fals , posit <num> , start <num> , text that first letter .
drop b if it's after an<br>m and if it's at the end of the word and , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so on .
these ar drop variou silent<br>letter and variou rule like thi , end <num> , meta new_paragraph fals , posit <num> , start <num> , text convert the misspel into a kind of a<br>represent of the pronunci , as a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text singl vowel at the begin and then a<br>set of conson .
and then we find word , end <num> , meta new_paragraph fals , posit <num> , start <num> , text whose pronunci is nearbi the<br>misspel pronunci , so we've , end <num> , meta new_paragraph fals , posit <num> , start <num> , text convert all other word , into the , into<br>the metaphon pronunci .
find similar , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word and now .
we score the word by , by<br>some combin of two eta distanc .
how , end <num> , meta new_paragraph fals , posit <num> , start <num> , text like is the candid to be<br>orthograph chang into the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text misspel , so we'll us some kind of<br>channel model like thing .
and the same , end <num> , meta new_paragraph fals , posit <num> , start <num> , text thing with the pronunci .
how like<br>is the misspel to be pronounc like , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the candid .
so a metaphon system<br>doesn't us a languag model .
but us them , end <num> , meta new_paragraph fals , posit <num> , start <num> , text pronunci base kind of channel model . <br>and you can imagin also combin a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text pronunci base model with a noisi<br>channel model and modern model of the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text channel .
in the last decad or so allow a<br>number of kind of improv like thi . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so incorpor a pronunci compon<br>into the channel model is on , and we , end <num> , meta new_paragraph fals , posit <num> , start <num> , text might also want to allow richer edit .
so<br>not just singl letter edit , but kinda , end <num> , meta new_paragraph fals , posit <num> , start <num> , text edit like a p , ph be incorrectli type<br>as an f .
or , veri common error , it's not , end <num> , meta new_paragraph fals , posit <num> , start <num> , text that all e's ar mistakenli type as a's , <br>but that the sequenc ent is like to be , end <num> , meta new_paragraph fals , posit <num> , start <num> , text mistyp as ant .
so , a coupl of<br>differ improv that a state of the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text art system might have in the channel<br>model .
and in fact , we could consid a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text veri larg number of factor that could<br>influenc the probabl of a misspel , end <num> , meta new_paragraph fals , posit <num> , start <num> , text given a word the channel model .
so we've<br>talk about the sourc letter or the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text target letter .
and we've talk about , you<br>know , mayb on surround letter .
but we , end <num> , meta new_paragraph fals , posit <num> , start <num> , text could look at more surround letter , or<br>we could look at the posit in the word . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text mayb , some error happen in the middl of<br>the word , some error happen at the end . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text we might explicitli model the keyboard , <br>and talk about nearbi kei on the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text keyboard .
or homolog , we're like to<br>mistyp a , a , a word with our left hand , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text third finger , by us our right hand<br>third finger .
so , so a kei which is on the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text same finger on the altern hand is<br>homolog .
or , again , we might us , end <num> , meta new_paragraph fals , posit <num> , start <num> , text pronunci .
we might us these kind of<br>like morphem transform we talk , end <num> , meta new_paragraph fals , posit <num> , start <num> , text about , in the last slide .
lot of possibl<br>factor that could influenc the proba , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text thi channel model .
here's a pictur of<br>on of them , a keyboard .
so we might wanna , end <num> , meta new_paragraph fals , posit <num> , start <num> , text sai that r and w ar like mi , <br>mistyp for e and so on if we're on a , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text some kind of a , a phone keyboard .
so<br>combin all these differ factor is , end <num> , meta new_paragraph fals , posit <num> , start <num> , text often done with a classifi base model . <br>so the classifi base model is an , end <num> , meta new_paragraph fals , posit <num> , start <num> , text altern wai of do real word<br>spell correct .
and here we , instead , end <num> , meta new_paragraph fals , posit <num> , start <num> , text of just two model , a channel model and a<br>languag model , we might take those two , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and a number of other model and combin<br>them in a big classifi .
we'll talk about , end <num> , meta new_paragraph fals , posit <num> , start <num> , text classifi in the next lectur and so for<br>exampl if we had a specif pair like , end <num> , meta new_paragraph fals , posit <num> , start <num> , text whether and weather , commonli confus<br>real word confus .
we might look at , end <num> , meta new_paragraph fals , posit <num> , start <num> , text featur like , well is the word cloudi . <br>you know , window of plu or minu ten , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word , or am i follow by the word two in<br>some and then some verb .
so , if i , the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word , cloudi is , is nearbi me i'm probabl<br>the word weather .
if i'm follow by two , end <num> , meta new_paragraph fals , posit <num> , start <num> , text verb , i'm probabl the word whether , so , <br>whether to go , whether to sai , whether to , end <num> , meta new_paragraph fals , posit <num> , start <num> , text do , is probabl thi whether .
similarli if<br>i'm follow by or not , then i'm probabl , end <num> , meta new_paragraph fals , posit <num> , start <num> , text thi weather .
so each of these featur . <br>plu the languag model plu the channel , end <num> , meta new_paragraph fals , posit <num> , start <num> , text model could be combin into on<br>classifi that could make a decis , i , end <num> , meta new_paragraph fals , posit <num> , start <num> , text mean we might build separ classifi<br>for each possibl like pair of word .
so , end <num> , meta new_paragraph fals , posit <num> , start <num> , text in summari real word done in correct<br>can be done with the same noisi channel , end <num> , meta new_paragraph fals , posit <num> , start <num> , text algorithm that's us for non word<br>spell correct but we can also us a , end <num> , meta new_paragraph fals , posit <num> , start <num> , text classifi base approach .
in thi lectur , we'll introduc the topic of text classif and the naiv bay algorithm which is on of the most import wai of do text classif .
let's begin by look at some exampl of text classif applic .
here i've shown an email that i actual receiv the other dai .
how do i know that thi email is spam ?
take a look at the mail and think of some featur you might automat extract from thi email that tell you that it's spam .
you might notic the word great .
a mi , a misspel of great , so we have a typo here .
mayb you might notic import notic and mayb an exclam point .
it's pretti rare that univers put exclam point in their subject header .
you might notic that there's no , dan here , it's not address to me in particular , and we have undisclos recipi and there's no particular address .
and the url's a littl funni here , that's not a stanford url .
mayb the word excit .
each of these featur can be combin in a classifi give us some evid that we got a piec of spam .
anoth import text classif class is authorship attribut .
how do i know which author wrote which piec of text .
on of the most famou exampl of authorship attribut is the famou anonym essai call the federalist paper that were written at the begin of the histori of our countri in part to convinc the state of new york to ratifi the earli constitut .
and three author wrote variou number of the letter but twelv of the letter it wasn't clear which author wrote .
and in <num> the and in <num> mostel and wallac show that bayesian method were abl to distinguish which letter were written by madison and which letter were written by hamilton .
and the bayesian method that thei us in <num> , gave rise to the naiv base method that we're go to be talk about todai .
anoth text classif task is gender identif .
determin if an author is male or femal .
recent research in gender identif ha shown that we can look at the number of pronoun and other featur .
the number of determin , the number of noun phrase , ar , subtleti indic of the differ between male and femal writer .
femal writer tend to us more pronoun and male writer tend to us more fact and determin in their noun phrase .
and you can see from that , that , here we have a lot of pronoun .
and here we have a lot of determin and , and factual sentenc with , with the copula verb and so you might determin that thi in fact a male and thi is a femal and that would be correct .
thi is the author margaret drabbl and thi is the author anthoni grei .
anoth text classif task is sentiment analysi .
and on of the classic sentiment analysi task is movi review identif .
given a review , whether it's a movi or a product , can i tell whether thi review is posit or neg .
and although i'm gonna show you an exampl here for movi , thi can appli to ani product review for ani , ani product or servic you might find on the web .
so thi is actual a veri import commerci applic .
so , suppos we saw a review that said unbeliev disappoint .
well , that's clearli a neg review .
how'bout , full of zani charact and richli appli satir , posit .
how'bout , thi is the greatest screwbal comedi ever film .
we've got word like greatest , or greatest ever , that's veri posit .
how'bout , it wa pathet .
the worst part about it wa the box scene .
here , we've got evid like pathet and worst , and so on , to tell us that thi is , in fact , a neg review .
text classif often .
we also appli text classif to scientif articl .
for exampl , decid what the topic of a particular articl in a data base like line might be .
for exampl , we might have to decid , you know , automat index an articl which of variou subject , antagonist or blood suppli or drug therapi or epidemiolog , appli to ani particular articl that's written that's in our data base .
so , in summari text classif is the task of assign ani kind of topic categori to ani piec of text .
and that could be subject categori in , in , in some kind of onlin data base .
it could be detect spam .
it could be choos an author from a set of author , choos their gender or mayb it's their ag .
you want to find the young writer or old writer .
tell if a languag if a text wa written in on languag versu anoth languag .
and the import applic of sentiment .
all of these ar exampl of text classif .
let's defin the task of text classif .
we have as input a document d , and then a fix set of class a set c with j class c1 , c2 up 'til cj .
and our goal given thi document in the set of class is to predict a class c from that set of class .
our job is to take a document and assign a class to that document .
how do we do thi ?
the simplest possibl text classif method is to us hand return rule .
so for exampl if we're do spam detect , we might just have a list of , of bad email address or blacklist that these peopl ar probabl spammer .
or we might look for phrase like million of dollar , or you have been select .
these ar good indic that we have spam .
and if these rule ar carefulli refin by an expert , you can get high accuraci from hand written rule .
but in gener , build and maintain these rule is expens .
so , although hand code rule ar often us as part of a system of text classif we , we gener combin that with an import method for machin learn .
thi method in supervis machin learn .
so in supervis machin learn .
we have a document d , just as we did befor and a fix set of class , as we did befor , but we need on more thing now .
we need a train set of some la .
some document that have been hand label for their class .
so we have for document on , we know that it's in class on .
for document two it's in some other class .
mayb for document m we have a label for the class of document m .
so given .
the document , the set of class and the fix and the train set of hand label document .
the goal of machin learn is to produc a classifi and we'll be us gamma to refer to the classifi and gamma's a function that , given a new document will give us the class .
so , given a set of train label of document and class , we'll learn a classifi that map a document to a class .
there's lot of kind of machin learn classifi .
we're go to talk todai about naiv bay , but we'll see , we'll look later , in the cours , we'll talk about logist regress and we'll touch on other kind of classifi , like support vector machin or so call svm , neighbor , lot of other classifi .
no matter which classifi we us , the task of text classif is to take a document , it's text , other kind of featur and extract featur that repres the document and build the classifi that can tell us which class the document belong to .
the naiv bay algorithm is on of the most import algorithm for text classif .
the intuit of the naiv bay algorithm is realli quit simpl .
it's base on bay rule , which we'll see in a second , and it reli on a veri simpl represent of the document call the bag of word represent .
let's see the intuit of the bag of word represent .
imagin i have some document that sai , i love thi movi , it's sweet but with satir humor .
and so , and our job .
is to , is to build thi function gamma , which take the document and return a class .
the class could be posit .
or the class could be neg in case of , of sentiment analysi .
which is it a posit or neg ?
in order to solv thi task , on thing we might do is look at individu word in the document , like love or satir or great .
we might look at all of the word .
in some kind of text classif we're gonna look at all the word , we're gonna look at everi singl word .
in other case we'll look at just some subset .
if we were to look at a subset , we might imagin that the document look someth like thi .
it just look like it ha the word love , and the word satir , and the word great and all the other word have disappear .
whether we us a subset of word or all of the word in the document .
the bag of word represent lose all the inform about the order of the word in the document and all we repres about the document is the set of word that occur and their account .
so for exampl for the previou document , we might repres the document as just a vector of word great , love , recommend , laugh , happi .
and for each on account great occur twice , love occur twice , recommend occur onc .
and again , we can keep all of the word in the document , and we'll often do that or we can just keep some of the word in the document if we have an idea that some of the word ar particularli indic cue .
so the idea of the bag of word' represent is that we're gonna repres our document just by a list of word .
and there count , and throw awai everyth els about the document .
which order the word occur in , what font thei were in , anyth els , and our function will , our function gamma , our classifi , will take that represent , and assign us a class posit or neg .
and thi appli , i've shown it to you for the two class problem of sentiment analysi , posit or neg sentiment .
but thi appli for all sort of document classif task .
so i might have some document i need to classifi into a differ comput scienc topic becaus i'm build an onlin librari of comput scienc paper .
or i'm give advic on comput scienc topic .
so i have some text , some , some document here with word like parser or languag or label or translat and i wanna know which aspect of comput scienc it should go in so i can file my paper automat and a good text classifi should automat figur out that that's a , that's a natur languag process paper .
so that's the intuit of the naiv base classifi .
let's now formal the naiv base classifi .
in classif we have a document d and a class c .
and our goal is to comput the probabl for each class of it's proba , it's condit probabl given a document .
and we're gonna see that the we're gonna us thi probabl to pick the best class .
now , how do we comput the probabl of a class , given a document ?
by bay rule , thi is equal to the probabl of a document given the class , time the probabl of the class over the probabl of the document .
so let's see how to us that in the classifi .
the best class , the maximum class , the class that we're look for to assign thi document to is out of all class the on that maxim the probabl of that class given the document .
so we're look for the class whose probabl given the document is greatest .
by bay rule .
that's the same whichev class maxim probabl of c given d , also maxim thi equat the probabl of d given c , the probabl of the class over the probabl of the document .
and , as is tradit in classif .
whichev , document , whichev , excus me , whichev class , maxim thi equat , also maxim thi equat .
what we've done here is drop the denomin , cross out the denomin .
why is it okai ?
to cross out the denomin d .
probabl of d is how like the document is .
now if i give you a document and i sai which of these ten class of thi document belong to .
and if for each of these class , i'm comput the probabl of the document given the class , the probabl of the class , and the probabl of the document , the document , probabl the document is ident for all ten class .
for each class , on more time i have to comput the probabl of the document .
and that mean that if i'm compar ten thing , each of which is divid by probabl of the document , the probabl of the document is a constant , and i can elimin that .
so , the most like class camp is that class which maxim the product of two probabl , the probabl of the document given the class , we'll call that the likelihood , and the probabl of the class , we'll call that the prior .
prior probabl of the class .
so the most like class is the on that maxim the product of these two probabl .
the probabl of a class will turn out to be rel simpl to comput .
what do i mean by the probabl of a document given a class ?
what do i mean to sai , thi particular movi review wa , wa , how like isn't it , given the class posit ?
seem like a veri complic and confus thing to comput .
and on wai to operation that is to sai , let's repres the document by a whole set of , of featur , x on through x n .
so when i sai the probabl of a document given a class .
i'm gonna ; i'm gonna sai all that mean is the probabl of a vector of featur given a class .
p of d given c .
we're go to repres that probabl by the joint probabl of x1 , x2 up through xn given the class .
in other word , we're repres thi document d as a set of featur x1 through xn .
that still doesn't tell me how to comput thi probabl but , but it'll , it's , it's a start .
so let's talk about these two piec now .
how do i comput probabl of a class ?
well , realli that's just ask how often doe thi class occur .
ar posit review much more common than neg review ?
is madison a much more frequent author ?
comput the probabl of a class can be done just by count rel frequenc in some corpu or data set .
so the probabl of a class is rel easi to comput .
what about the likelihood of the document , of these featur in a document given the class ?
well there's a lot of paramet for thi probabl , there's , if , if there's n differ featur .
and each of them ha a certain length , that's a lot of paramet that have to be comput and we have to comput them on for each class .
so that's , that's far too mani paramet that we could possibl comput .
we can onli estim at thi number if we had a huge number of traine exampl and we easi don't have such an enorm amount of traine exampl .
so we ar gonna make some simplifi assumpt in an ibas classifi to make thi comput more possibl .
the first simplifi assumpt we're go to make is call the bag of word assumpt , and we're go to assum that the posit in the document doesn't matter .
so thi is what i gave you the intuit of a few slide ago .
the posit of the doc , of the word in the document , whether it's the first word or the seventh word or the on hundr and fiftieth word isn't go to matter .
all we care about is which , which word or which featur occur .
and the second thing we're gonna , second assumpt we're gonna make , is we're gonna assum that the differ featur x1 , x2 , x3 , that their probabl ar independ given the class .
so that the , the whether on featur occur given a class and whether anoth featur occur given a class , ar independ gonna be true .
and of cours thi is a , both of these assumpt ar incorrect simplifi assumpt .
thei're , thei're absolut wrong .
thei're , thei're , thei're terribl complet not true .
nonetheless , by make these simplifi , these incorrect simplifi assumpt , we can make our problem so much simpler that in practic , we're abl to solv the problem with a high degre of accuraci despit the simplif .
so the result of these two simplifi assumpt is we're gonna repres the probabl , the joint probabl , of a whole set of featur .
x1 through x1 condit on a class .
as the product of a whole bunch of independ probabl .
probabl of x1 given the class , probabl of x2 given the class , probabl of x3 given the class and so on up to probabl of xn given the class .
we're just go to multipli them all togeth .
we're not go to care about x1 wh , which posit it occur in .
all we care about is that it , it , it wa , it wa thi particular order featur and we're not go to care about the depend between x1 and x2 .
in other word .
in order to comput or simplifi our e base assumpt , to comput the most like class .
by multipli a likelihood .
the probabl of a whole , a whole joint string of featur , time a prior probabl of a class .
we're gonna simplifi that , and sai that the best class by the naiv bay assumpt is that class that maxim these the prior probabl of the class , so that's the same .
but now , more simpli , we're just gonna multipli , for everi featur in the set of featur , the probabl of that featur given the class .
much simpler equat .
so now look specif at text .
first let's look , we're gonna assum we're gonna look at all posit , all word posit in a text document .
so , we have a text document and it ha a , it ha <num> word in it , so for all , for posit of word number on , posit number two , posit number three , we're gonna take look at all the class , and for each class , we're gonna sai , what's the probabl of the class ?
and then , for each class , we're gonna walk through everi posit in the text , and for each posit , we're gonna look at the word in that posit and ask , what's it probabl given the class i'm look at ?
so , we'll do thi for , class on .
we'll comput p of class on , time the product over all the is of p of word i , given class on .
so we'll comput that , and then we'll do the same for class two .
for class two we'll comput p of class two .
and then the product overal posit i of the p , of word i , given class two .
and then we're go to pick whichev of these , two is the highest .
if thi is higher we're go to pick class two and assign to the document that thi is higher we'll assign class on to the document .
and of cours i've shown you thi with just two class but in gener thi is true for , for ani number of class .
so that's the formal of an ie base classifi .
how do we learn the perimet for naiv bay ?
the simplest wai of multinomi naiv bay model is to us maximum likelihood estim simplest us the frequenc of the data so for try to comput the prior probabl for particular document be in a class j we have the count of all the document , and out of those document how mani of the document will be in class j that's our prior , that random document will be in class j for the likelihood , the probabl of word wi given the class of j we count the number of time word<u>i occur in document of class j< u> and we normal by the total number of word in the class j , so the sum of all the word , and we normalis the total number of so all the word in the vocabulari of those word so out of all the document in the class j how mani of them ar the particular word that we ar look at so we ar go to comput the fraction of time of word wi appear among all the word of thi document of thi topic cj we ar go to do thi , by creat some kind of mega document for topic j by concaten all the document in thi topic togeth we us the frequenc of w in thi document so for sentiment analysi we might have document of posit , a mega document for all the posit document we ar just gonna concaten them altogeth into some big mega document and we might have doc neg and so on now i have been ly to you we don't in fact us maximum likelihood for naiv bay and the reason is the follow imag we ar look at the word 'fantast' we introduc the word 'fantast' , might have occur in the test set and happen not to appear in the train set in the topic posit so the probabl of fantast in the class posit in the train set by maximum likelihood , is the count of fantast occur in the posit normalis by the sum of all the count of those word in posit but fantast never occur , so that count is zero so the maximum likelihood estim for the likelihood of fantast given posit will be zero why is that bad ?
becaus these zero probabl can never be condit awai for we ar look for the most like class that's the argmax over all class of the prior time the likelihood and if on of the likelihood term in zero , then thi whole thing is zero and we will never gonna pick that class the solut , is veri simpl , add on smooth so here is the comput without smooth and add on smooth , we simpli add on to each of those count so we add on the count everi time the count is numer and everi time that count occur in the denomin , we add on to that too and we can re write that , into the form we have seen befor here we take the total number of that word in class c and we add the vocabulari size becaus we add on to everi vocabulari into the denomin so classic laplac add <num> smooth so let's walk through the calcul of thi paramet first , from the train corpu , we ar gonna extract a the vocabulari , a list of word next we ar gonna calcul the prior for each class j so for each class , we gonna get the list set of all document that have that class cj and the number of document in that set , divid by the total number will give us the prior , the probabl of that particular class now for the likelihood , now we ar gonna want comput the likelihood for everi wk given everi topic cj so first we ar gonna creat our mega document that can concaten all the document call text j now for each word wk in vocabulari , we ar gonna count the time of wk occur in the mega document text j , nk now the probabl the likelihood of word wk in class cj is just the add on smooth i have shown you , the add alpha smooth version , of the naiv bay algorithm so we have ad alpha , to nk , and in the denomin , we have the n , the total number of token in the class j what about unknown word ?
it turn out that naiv bay is a veri close relationship to languag model .
let's see how that is .
we'll start by look at the gener model for multinomi naiv bay .
so imagin i have a class , let's sai it's china .
and we're , imagin that we were randomli gener a document about china .
so we might start by sai well the first word except on is shanghai and the second word is and , and the third word is shung jung and the forth word is issu and the fifth word is bounc and so on .
we've just gener a littl document , a random littl document about china .
so , what thi gener model show you , is that each word is a independ gener word from a class .
gener with a certain probabl , we have a littl set of probabl we're keep for each word .
let's think about that .
now in gener naiv bay classifi can us all sort of featur url , email address .
we'll talk about that for spam detect .
but if in the previou slide , we just us the word featur and if you us all the word in the text , then it turn out that thi gener model for naiv bay give it a import similar to languag model .
naiv bay turn out to be a kind of languag model .
and in particular each naiv bay , each class in a naiv bay classifi , each class is a unigram languag model .
i mean , the wai we can think about that , is , each word in a naiv bay classifi , the likelihood term assign a word , the probabl of the word given the class .
and a sentenc in a naiv bay classifi , sinc we're multipli togeth a sentenc , or even , even a whole document .
sinc we're multipli togeth the probabl of all the word , we comput the probabl of a sentenc , given the class .
we're just multipli togeth all the word , the likelihood of all the word in the class .
so let's see how that work .
imagin that we have , the class posit .
and we have our likelihood , the likelihood of i given posit , that's p of i given posit , and we have p of love given posit , and we have p of thi given posit and so on .
and p of i given posit is . <num> and p of love given posit is . <num> and so on .
okai ?
so here's our naiv bay classifi .
well , we can think of that exactli as a languag model .
we have a sequenc of word .
i gener some word i , love , thi , fun , film .
and , and naiv bay is assign that sequenc of word a set of probabl , on for each word from the class , . <num> , . <num> , . <num> , and so on .
so that if we multipli all these togeth we can get a probabl of the sentenc .
so in naiv bay , each class is just a unigram languag model condit on the class .
so when we ask the question , which class assign a higher probabl to a document , it's like we're run two separ languag model .
so here i've shown you two separ languag model , the posit class , and the neg class , and each on ha separ probabl .
so here's probabl of i given neg , and here's probabl of i given posit .
i guess peopl ar more like to us the word i when thei don't like someth .
and , now , if we take a particular sentenc , i love thi fun film and we sai what probabl doe thi sequenc , what proba , what's the probabl of thi sequenc accord to our first model ?
and what's the probabl accord to our second model ?
each on assign a probabl for each word .
that's the naiv bay likelihood .
we can multipli them all togeth , and we can show that if we multipli them all togeth you can sort of see from inspect that the posit probabl multipli togeth ar gonna be , becom mimic of thi on and thi on ar gonna be much higher than the neg probabl .
so you can think of naiv bay .
as each class is a , is a separ class condit languag model .
we're gonna run each languag model to comput the likelihood of a test sentenc and we'll just pick whichev languag model ha the higher probabl as the class which is the more like class .
so that's the close relationship between naiv base and languag model .
let walk through a detail , work exampl in multi numer naiv base .
so for thi exampl , we put up here the equat for naiv base .
so the probabl of a class , is the number of document with that class , over the total number of document .
and the like hood of a word given a class , we'll just do simpl add onc move .
the count of the word go in that class .
divid by the count of all the word in that class .
and with add on smooth , and let's assum that we have four train document , on , two , three , four , and on test document .
and here ar the document , thei ar , thei ar simplifi , obvious document .
thi document ha onli three word .
chines , beij , chines .
and thi document ha three word , chines , chines , shanghai .
and so on .
and , let's sai we have just two class , chines and japanes and our job is to do asian , new topic classif .
so three of our train document ar in the , ar in the class china , and on of them is in the class japan .
and our question is , what's the class of thi test document ?
all right .
so let's do some comput .
first thing we're go to do is comput the primer .
so we need to comput p of c , code the class china , and p of j , of the class japan .
so p of c is how mani time the class chines occur in our train set .
three .
out of how mani total class ar there in our trade document ar there in our train set ?
four .
so three of the four train document .
so that's three out of four ar about china .
so our probabil , our prior probabl of a document be in the topic china is three out of four .
how about japan ?
well , there's onli on document in our train set about japan .
out of the four document .
so the probabl of japan is on quarter .
so we've seen how to comput our prior probabl here .
let's move on to the likelihood .
let's comput each of these probabl .
what's the probabl of the word chines , given the class c ?
what's the probabl of the word chines , given the class j ?
and so on .
all right , so , the probabl of chines is .
how mani time doe the word chines occur in our train set of document that ar in the class china ?
okai , so thi word chines , how mani time doe that occur in these three document ?
on , two , three , four , five .
so we have five , we have , add on so ad on and how mani total word ar there in our train set ?
on , two , three , four , five , six , seven , eight .
now in our chines class , and then we're gonna add the vocabulari size v and we're do add on smooth , so add the vocabulari size v and our vocabulari is six .
on , two , three , four , five , six .
so , that's five plu on eighth plu six , or <num> <num> , or three seventh .
and we'll do the same thing for the word tokyo .
tokyo doesn ? t occur at all in our three class , it ha a count of zero .
but we're do add onc move , so we will add on .
and that ? s the same denomin .
the same number of total word , and vocabulari size in our chines class .
so that would be on out of fourteen .
and the same exact thing is true for the word japan .
doesn ? t occur in our train set .
an in our , now , now let ? s turn to the class j for japan .
so now the word chines doe occur onc .
and i will do , so it's onc , poss on for add on smooth , and will divid by the count of the word in thi class , and there's three word in thi , in thi meta document of on document , and then again we have the vocabulari size , and so we've got two nine .
and we can comput the next two number the same wai .
so now , we have our prior and we have our condit probabl , we're readi to decid which class is more like for our test document .
so for our test document , we'll call that document five .
we need to comput our prior .
and our likelihood .
so our prior is three quarter .
the prior of be chines for a document five is three quarter .
that's the prior for chines time the word chines occur three time on , two , three and each on of those get the probabl .
probabl of chines given three .
so it's three seventh , three seventh , three seventh .
then the word tokyo occur .
that's got probabl on fourteenth .
then the word japan .
on fourth .
and we multipli all that togeth and we get approxim <num> . <num> .
notic that we sai that thi probabl is proport to thi product becaus we ar comput the argmax between these two class .
we ar comput the class which maxim the product of the two probabl .
but the actual probabl ha a denomin in it .
it would have to be divid by .
p of the document .
and we're just skip that part .
and so , we're actual not comput the probabl .
we're comput the numer of the probabl .
but sinc document five ha the same p of document five in both m , the chines and japanes class , we're just gonna not comput that .
so we have a proport to , and not an equal sign here .
good , and then for the probabl of document of class japanes , given document five , now we have the prior for document japanes and we multipli that by the probabl of chines given the class japanes .
so that's two ninth time two ninth time two ninth and then the probabl of tokyo , given japanes that's anoth two ninth and then japan , given japanes anoth two ninth .
and , so we get , a slightli lower probabl .
so , in thi exampl , our , our model would choos which class for thi document ?
you would choos .
chines , becaus . <num> is bigger than . <num> .
now we've talk about naiv bai , where we're us each word as a featur and we're us all of the word .
but in lot of exampl of naiv bai , we're gonna us richer featur than just word , and we're gonna us specif kind of word and other thing .
so , spam assassin in naiv bai classifi ar for spam detect , and it look for thing like , is the phrase gener viagra mention , is the phrase , onlin pharmaci mention , a littl regular express for million of dollar mention , is there a phrase , impress , and nearbi the word girl .
doe the firm have a lot of number in it , is the subject in all cap .
all sort of differ featur that we can us to combin and you can look at the spamassassin websit to see a common set of featur .
so in summari , naiv bay is actual not that naiv .
it's a veri fast algorithm , it ha low storag requir .
it's pretti robust to irrelev featur , thei tend to cancel each other out .
it work well in domain where you have lot equal import featur and thi turn out to be a problem for some other classifi in particular decis tree which have some advantag in numer domain , don't work well if lot of featur ar equal import .
if the independ assumpt hold .
if the assum independ is correct , then it turn out the naiv bay is in fact the optim classifi for a problem , of cours that's rare that these independ assumpt ar realli true .
but if thei happen to be true or close to true , it turn out naiv bay is in fact optim .
and in gener , naiv bay is just a good , depend baselin for text classif .
we've introduc precis and recal .
now , let's turn to the remain issu in the evalu of text classif .
a commonli us data set for text classif is the reuter data set .
it's got <num> , <num> document ; there ar standard train and test split .
the set ha <num> categori and thi is a class of multi valu classif becaus an articl can be in more than on categori .
so that mean we're go to be learn <num> separ classifi , each on make a binari distinct .
and the averag document ha about just over on class .
and here's some common categori with some number of train and test document .
so there's <num> train document about grain and <num> test document about grain .
and we have class like wheat and corn and interest and so on .
here's a sampl reuter document .
you can see it's about livestock and about hog , so it ha two topic and then here's the text .
so our task is given thi text to classifi thi document as about livestock and about hog .
the confus matrix is veri import for multiclass classif .
the confus matrix tell us , for ani pair of class , c1 and c2 , how mani document from c1 were incorrectli assign to c2 ?
here's a littl exampl .
we have some document about poultri or wheat or coffe , and here's their true class , number of document in these true class .
and here's what our classifi assign .
so , c sub <num> , <num> , thi <num> is document that were about wheat , but our classifi thought thei were about poultri .
so thi is a classifi that just love the chicken .
each cell of the classifi tell us how mani document of each class were classifi in the other class .
and that mean that the diagon of thi confus matrix give us the correct classif .
here , <num> document that we said were about the u . k .
ar in fact about the u . k . , and no document that we said were about wheat ar actual about wheat .
we can us the confus matrix to comput the same measur we've talk about , precis and recal .
let's start with recal .
recal , the fraction of document in class i that ar classifi correctli how mani of these class i document did we find .
so true posit , c sub i , i divid by the sum of the entir row .
let's go back and look at our tabl .
here's an entir row of document that ar actual about wheat .
and let's sai our true posit here ar zero .
so a veri , veri bad classifi about wheat .
we divid zero by the sum of all these number .
<num> <num> <num> is gonna give us our precis , our recal , excus me .
for precis , we're gonna ask , of the document that we return , so that's an entir column , of that column , how mani ar the document that we were correct about ?
of the document that we said were about wheat , how mani of them were truli about wheat ?
so the document about wheat divid by all the document that we said , the sum of all these document that we said were about wheat .
and then accuraci is just the fraction of document classifi correctli .
so it's the sum of these diagon entri divid by the sum of all of the entri in the confus matrix .
now , sinc we have more than on class we're gonna need a wai to combin the valu , the precis and recal valu we get from each class into on measur caus it's often us to have a singl measur .
and there's two standard wai to do thi .
in macroaverag we comput the perform , the precis or recal or f score , for each class , and then we averag them to get a averag valu .
so if we have <num> class we're gonna comput a <num> precis and we're go to averag them all to get a macroaverag precis .
in microaverag we instead collect all the decis for all the class into on singl conting tabl and we get , and then we evalu our precis on that .
let's look at an exampl .
here we have two class , class on and class two .
and here's all the thing that ar true yess and true noe for class on , and here's thing that ar realli about , realli in class two and realli not .
and here's what our classifi return .
so our macroaverag precis we're gonna comput precis separ for the two class .
so for class on , ten over ten plu ten , so that's . <num> .
for class two , <num> over <num> plu ten , or . <num> .
so our macroaverag precis is the averag of point five and point nine , or we get point seven .
for microaverag , on the other hand , we're gonna take the two conting tabl and just add them all togeth to get a singl microaverag conting tabl .
and now we're gonna comput precis directli from that .
so we'll get <num> over <num> plu twenti , or <num> . <num> .
so you can see that the microaverag score is domin by the score in the common class .
class two is much more common than class on , these number ar much bigger .
in microaverag , that class will domin these sum number in thi sum conting tabl .
in macroaverag , each class is gonna particip equal .
for text classif evalu we need more than just precis or recal .
as in mani machin learn base algorithm for natur languag process , we'll need a train set , a test set for measur perform , and someth call a develop test set , or dev set .
on the train set , we'll comput our paramet .
and what we'll do with the dev set is test our perform while we're develop our system .
and so whether we're look at precis recal f1 , or whether we're look at accuraci , we'll look at our score on the develop test to find bug in our algorithm , and develop new featur .
and onc we're done develop the algorithm , we can then test on a clean unseen test set .
and the reason why it's import to have thi clean unseen test set is that , otherwis , if we report number on our develop test set that we've been us all along , we're gonna end up overfit .
we're gonna report much higher accuraci probabl than ar reason becaus , we've tune our algorithm to thi develop test set .
a clean , unseen test set give us a more conserv estim of perform .
now we can get sampl error due to small dataset .
mayb our test set is small , or our train set is unrepres .
so it's common , we talk about earlier , about cross valid .
so we're go to tell multipl split of our data and cross valid .
for exampl , let's sai we set asid some portion of our data for a dev set .
we'll take the rest of it , and we'll train on thi train set , and then look at our perform on the dev set .
and then we'll take a differ split , train on thi part of the train set , and report on the dev set .
take thi part of the train set , and get our perform on the dev set .
and we're go to pool our result from each split and then comput a total pool dev set perform .
thi let us avoid have veri small test set or veri unrepres test set .
a lot of the data get us for both train and test , in differ split .
still at the end , we need to have our clean , unseen test set , so that we don't overfit to these dev set .
we've now given you a number of wai to evalu text classif .
we've introduc precis and recal and f score and talk about what to do in the multi class problem where we have more than two class .
we'll see the us of these idea , and also of microaverag and macroaverag throughout natur languag process .
final , let's talk about some practic issu in text classif .
now that we have seen the math of naiv bay we can turn to some real world question .
how in practic do we build our classifi in the real world .
what classifi you build and what you do depend a lot on what kind of data you have .
let's suppos you have no train data .
well in that case the right thing to do is to us manli written rule .
so here is a rule for decid if a document is a grain let's sai .
we might sai if the word wheat or the word grain is there and doesn't have the word whole or the word bread so it's not a recip then we sai thi is a grain document .
now manual written rule ar difficult .
thei need care craft .
thei have to be tune , and develop in data and it's veri time consum becaus thei take dai to write the rule for each class .
but if we have no train data thi mai be the right approach .
what if you have veri littl data ?
well , if you have veri littl data , then naiv bay is just the algorithm for you .
naiv bay is what's call a high bia algorithm in machin learn .
a high bia algorithm is on that doesn't tend to overfit train data too badli .
it sort of trade off , varianc or gener to a new , to a new test set .
so it doesn't overfit too much on a small amount of data .
so that's the advantag of naiv bay .
but it's also import to get more data .
and you can often find clever wai to get human to label data for you .
i mean , that's an import thing , if you don't have enough data , get more data .
there's also variou wai that we're not go to talk about so much in thi class , of , of semi supervis train .
find some wai to us a small amount of data to help train a larger amount of data , and that's call bootstrap .
anoth thing you might do if you have veri littl data .
if you have a reason amount of data , now you might try all the clever classifi we'll talk about later in the quarter , classifi like regular logist regress , or support vector machin .
in fact you could even us decis tree .
decis tree have advantag and disadvantag .
but a big advantag of decis tree is thei're user interpret .
and that's help becaus peopl like to be abl to modifi a rule or hack in a classifi and it's veri easi to modifi a decis , much easier to modifi your decis tree at a rule or chang a threshold by hand .
it's much harder to do that with an svm or logist aggress .
now if you have a huge amount of data , well now you can achiev high accuraci , although there is a cost .
mani classifi just take a long time , svm especi .
hm , k nearest neighbor can be veri slow to train a classifi .
logist ration can be somewhat better .
but realli , if you have a huge amount of data , then it mai just be effici to train naiv bay , which is quit fast .
actual if you have a veri huge amount of data , here is it mai turn out that the classifi mai not matter .
here's a result from brill and banko on spell correct , compar the perform of three , four differ machin run algorithm .
a memori base learner , wino , a percept and naiv bay with on a spell correct task with a million word , ten million word , <num> million and so on , and so on .
so , a log scale , and where we're measur how accur the classifi ar and you can see that as you , that the differ between the classifi is much smaller than the differ you get by just ad more data .
hm .
and in fact thing , depend on how much data you have , the classifi cross over in their perform curv .
so with enough data , it mai not matter what classifi you have .
so a real world system in gener will combin thi kind of automat classif , whether it is from rule or supervis machin learn .
with manual review of uncertain , or difficult , or new case .
there ar some import detail for the comput in naiv bay .
on is underflow prevent .
so it turn out that multipli lot of probabl can result in float point underflow .
we talk about thi for languag model .
so , sinc b , by the , the definit of logarithm , the log of xy is log of x plu log y .
in gener , we , we keep , store our probabl in the form of log , and we add them instead of multipli them .
so we still have the same , formula .
here's the , the naiv bay formula express now in term of log probabl , instead of probabl .
it's still max .
but now , instead of multipli a probabl , and a , and a product of likelihood , we're ad a log probabl with a sum of log likelihood .
so now the model is just maxim , choos a class that maxim over some sum of weight .
veri simpl model .
final , we're go to want to treat the perform .
and domain specif featur for your particular task , domain specif weight ar veri import in the perform of real system .
for exampl sometim we're go to want to collaps term .
let's sai we're deal with part .
number and some inventori task .
now we might want to class all the part number togeth into a part number class , part number kind of word or chemic formula .
we might want to have just on , name entiti call chemic formula .
but , other kind of collaps such as stem gener doesn't help .
so , you have to know about whether you need to collaps term or not .
it's also veri import to upweight .
upweight is count a word as if it occur twice .
and so often we upweight titl word or we might upweight the first sentenc of each paragraph .
or sentenc that have word that occur in the titl , you might upweight all the other word in that sentenc , and so on .
so small wai that can help in treat perform .
so we've seen a number of practic thing that we can do in build a , a real world test classif system .
what is sentiment analysi ?
we discuss in a previou lectur that decid if a movi review wa posit or neg is an import text classif task .
and sentiment is a kind of text classif .
so decid that unbeliev disappoint suggest that a movi wa bad .
or richli appli satir is probabl good .
screwbal comedi , good , pathet , bad .
thi kind of sentiment can also be appli to product .
for exampl , googl product search might wanna for a given product like sai , thi particular printer might want to automat extract fact about the printer and sentiment for these fact .
so , these ar call aspect or attribut .
we like to automat decid that act ; aspect or attribut of the printer like it eas of us , or costum servic ar import aspect of the printer and then automat determin the sentiment .
we might wanna sai , from sentenc like , thi wa veri easi to set up .
that most of the sentiment for thi product , for thi aspect , eas of us is posit .
perhap other kind , other aspect of thi product like custom servic , mayb we saw more neg sentiment in review and less posit .
so , the task of read all the review and from each review n notic phrase that suggest a sentiment and then aggreg thi sentiment over all review .
and the task of s , attribut .
for aspect detect and sentiment analysi .
and we can see thi for mani other product , not just googl , so bing ha the same thing for the same printer again .
we can extract aspect like perform and eas of us and print speed .
and summar from the variou .
and .
review the posit and neg sentiment .
sentiment can also be appli to thing that aren't product for exampl measur consum confid .
so thi love graph from look at sentiment from twitter tweet to try to predict consum confid over time .
and what thei've shown you in thi blue line here is a sentiment as express in twitter posit and neg sentiment and in the black line you've got gallot poll data .
and thei show a pretti good correl here between twitter sentiment and the gallot poll , suggest we can us twitter to measur public opinion .
in fact , thi kind of twitter sentiment ha been shown to directli predict the stock market .
it all and from twitter extract thing like calm or happi , variou kind of sentiment attribut thei could extract from twitter and then show that those attribut in particular .
thei show that calm predict the dow jone industri averag , three dai later .
so here's the dow jone averag and here's their measur , the score measur of calm , thei get from twitter and thei show that by shift calm a littl earlier that , that thei have high predict in these region , of the dow jone .
tweeter sentiment ha also been us simpli to protect , predict peopl sentiment toward brand in gener .
so for , here's the tweeter sentiment ap .
the project by alec goe and hi colleagu at stanford who show you can take queri like unti airlin look at all the tweet mention unit airlin and determin if thei're posit , here's posit down here , neg there's a coupl neg on , or neutral and then , and then summar all of these sentiment so peopl , at least when i ran thi queri had some pretti neg thing to sai about unit , more than thei had posit thing to sai .
so we can us twitter to see , in gener , how peopl feel about all sort of thing .
the task of sentiment analysi is often call by other name .
we call it opinion extract or opinion mine .
sentiment mine , subject analysi .
lot of differ name .
and we're go to us thi for all sort of task .
so we've seen a lot of these r movi , posit or neg ?
what do peopl think about product like some new iphon ?
in gener , can we measur public sentiment ?
how is consum confid ?
ar peopl in despair ?
what do peopl think about particular candid or particular issu in polit ?
and in gener , onc we can measur peopl ? s sentiment , can we us that to predict thing ?
market trend , elect outcom , anyth we might want to predict .
the psychologist klau scherer ha shown that there ar lot of differ kind of effect state .
when i talk about sentiment , it's realli just on of mani kind of effect state .
so he talk about emot , that's a brief kind of sentiment be angri or be sad , mood like be cheer or be depress .
and interperson stanc , how do i feel toward you .
i feel friendli , i feel warm , i feel distant .
sentiment is a subtyp of attitud .
attitud ar my belief or disposit toward somebodi so do i like someth , do i love it , do i desir it , do i valu it .
then we can talk about veri stabl thing like person trait .
am i an anxiou person ?
am i a hostil person ?
and so on .
so sentiment is an attitud , a kind of endur effect color belief or disposit toward an object or person .
and that definit suggest the task of sentiment analysi , we're detect attitud , again i've given you the definit there .
and we can talk about detect who ha thi attitud ?
the holder or sourc of the attitud ?
the target of the attitud or aspect .
what is it that we have thi attitud about ?
the type of attitud .
so you can imagin from list , measur like and love and hate and valu and desir .
or much more commonli , a simpl weight polar .
do we , ar we posit inclin toward thi target , neg inclin or neutral ?
and we might have some strength valu .
how posit and how neg ?
and we're gonna do thi from some text which ha the attitud and we might sometim consid an entir document and ask , what's thi whole document sentiment or we might , sometim , just look at individu sentenc and sai , what's the sentiment of thi sentenc .
so the simplest task is the attitud of some particular text , posit or neg .
slightli more complex task rate the attitud of thi text from on to five .
so give , not just a binari valu , but some kind of ordin or numer valu .
and more advanc sentiment task detect the target , detect thesauru , and mayb detect more complex attitud type .
let's give a base line algorithm for sentiment analysi .
the task we're go to us is sentiment classif of movi review and i have drawn the work of pam and lee and their collabor in thi lectur .
so , their task wa what's often call polar detect , simpl , posit , or neg .
no complic sentiment issu and thei're gonna appli thi to movi review from the imdb websit .
and thei've releas some data that's , that's often us in research call polar data <num> , which is a set of imdb movi review that have been text normal , and i have point you here at the url for that .
here ar some exampl of movi from their databas .
take a look and see if you can decid which on's posit and which on's neg .
hopefulli , you decid that the first on is posit and the second on is neg .
and the wai you would decid that is word like aggrav and unbeliev disappoint for neg , and cool for posit .
so these ar word that ar go to help us in the classif task .
in the baselin algorithm itself ha a number of step , we're gonna start by token the word , in the , review itself , then we're gonna extract featur and the featur we're gonna look at mostli ar word themselv and then we will take these featur and appli them in a classifi .
and we have talk about naiv bay , and so we're gonna us naiv bay in todai's lectur but in practic , we might , just as often or even more often us a maxent classifi , which we will talk about in the futur , or svm classifi , realli ani classifi , work fine .
sentiment token a lot of the same issu come up as ani kind token we've talk about earlier .
in sentiment you ar like to be deal with websit , so you're go have to deal with html and xml mark up .
you might be deal with twitter , so you'll have to deal with hash tag and twitter user name .
capit , which in mani other kind of text normal isn't so import .
often it's we get rid of capit we might in sentiment want to preserv at least some of it .
perhap word in all cap peopl ar often shout by us capit .
we're gonna want to normal phone number and date and we , it's veri import in sentiment token to , to recogn emoticon so i'll show you here a set of .
regular express for detect emoticon from chri pott .
so here we have on long regular express for recogn an option half follow by an ey , an option nose , an option mouth and so on so you can see each of that either in posit or revers orient .
and thi set of regular express come from a whole sentiment token that i point you at here , and there's other token like brendan o'connor twitter token you can also go look at .
so .
a number of issu come up in extract featur for sentiment classif .
on is negat .
it's veri import to detect , negat in a word like didn't , so we know that i didn't like thi movi .
we should be abl to detect that it's quit differ , than i realli like thi movi .
so we're gonna need to deal with negat .
and we also have to deal with the question of which word to us .
we might wanna us just adject .
we might wanna us all the word in the text .
it turn out that , at least on thi imdb data , and mayb in gener , that look at all word is better than look at just adject .
becaus , often verb or noun or other word give us a lot of inform about sentiment .
so how do we deal with negat ?
here is the simplest algorithm first propos by and and us veri frequent after that .
we simpli take the four letter , n o t under bar , and we , we prepen them to everi word between the negat word , and then the follow punctuat .
so we have a , a phrase like didn't like thi movi comma , but i .
and we turn that into didn't , not like , not thi , not movi .
so now we've essenti doubl our vocabulari size .
everi word could be itself , or the , the word with not under bar prepend , and we're gonna learn that these not under bar word we've creat word for neg sentiment , or for flip the sentiment .
let's remind ourselv about naiv bay .
the most like class accord to , is that class out of all class which maxim the product of two probabl , the prior .
the probabl of the class , and , the product over all posit in the document of the likelihood of the word in that document given the class .
so how like ar we to see a posit movi review time for everi posit in the document , how like is that word to have been express by a posit movi review and the same for neg , and we pick whichev on , posit or neg , ha highest probabl .
or if we're do three , we're do neutral as well , we can have three class .
and , in practic for sentiment analysi , and lot of other text classif task , we us , simpl laplac or add on smooth with , naiv bay .
so , the wai we're comput thi likelihood , probabl of a word , given class , it just by ad on to the count , and then the vocabulari size to the denomin .
for sentiment and other text classif task , we often us a slight variant of the base algorithm call binar or bullien multinomi i mean , the intuit of thi algorithm is that for sentiment and for other text classif task , we care more whether a word occur or not than exactli what it frequenc is .
so , for exampl , the occurr of the word fantast tell us mayb a lot that , that we have a posit review , but fantast occur three time or five time mai not tell us a lot more than just occur onc .
so with multinomi , we simpli click all the word count in each document .
had a count of on .
so instead of us the full term frequenc , we'll just us a count of on for each document .
so if we look at our origin learn algorithm for multinomi naiv bay .
here , rememb , we extract our vocabulari .
and now we're gonna calcul our prior , rememb the prior ?
by look for everi , how mani document , occur with a particular class over the total number of document .
so there's our prior .
and for the likelihood term , for each word , for each class , we roughli count how mani time thi word .
the count of thi word over the count of all word in a class .
that give us the likelihood of a word in a class and then we did some add on smooth .
now we're go to do the exact same thing .
with bullion with on extra step .
befor we do our concaten of all the document into on big document , and count all the word in it , we ar gonna remov duplic .
so for each document , for everi word type , we're just retain a singl instanc of that word .
so if a word occur five time , we'll keep onli on copi of that word .
and then we'll concaten all these document and then we'll do out count , and , and our add on smooth as we did befor for naiv bay .
so that's our train algorithm for naiv base in the bullion form .
the test when you're do bulli multi naiv bay we do the exact same thing .
we remov , from the test document all the duplic word .
so if a word occur , five time we , we keep onli on copi of it .
and then we us the same , naiv bays equat that we've been us befor , on thi , slightli reduc test document .
let's look at an exampl of bulli , multinomi naiv bays , and here we put up the littl document that we saw when we were talk about naiv bays origin .
so we have here , four train document , and on test document .
and so the word chines occur in class c , three document ar in class c .
so it occur four time .
on , two , three , four , oh five , sorri , five time .
so the count of chines is five .
and it occur three time in our test document .
chines equal three .
and so on , and so in our base equat we're go to be us thi count to comput the likelihood , the probabl of chines .
given document class c .
but in the format , we're simpli gonna p reprocess the document to remov all multipl copi of a word .
here's our version now .
so you'll notic that there's onli on copi of the word chines in document on , on copi in document two , on copi in document three .
so now our count in the c class in train for chines , the count of chines .
now the count of chines is go to be three instead of five , and in the test set here , the count of chines is onli on .
so it turn out that thi version of naiv base binar bullion featur multinomi naiv base work better than the full word count and i wanna note that , for those of you who know about , that's there's an altern version of naiv bay call multivari naiv bay .
us binar featur in multinomi naiv bay is not the same as multivari naiv bay .
in fact , multivari naiv bay doesn't seem to work as well for sentiment or other task .
so we gener us , word , rather than the full word count .
although , some research like remial have suggest that mayb someth in between the frequenc and just on word , like mayb look at the log of the frequenc which is smaller than the frequenc , but mayb differ than us just on mayb a us thing to try .
and if you're interest , you can read the literatur on thi whole question of which version of naiv base is most us .
as we introduc last time , pang and lee in our baselin classifi .
we're gonna us , cross valid .
so cross valid , rememb , we break up our data into ten fold .
i've shown onli five fold here .
and insid each fold , let's sai we might have the same number .
let's sai we have fift , half posit and half neg in our data .
so we have , let's sai , four .
posit and four neg in our test set .
then we're gonna have , also , let's sai we have , <num> posit , and <num> neg in our train .
so we're gonna , we're gonna make sure that our test set ha the same distribut of posit and neg as our train set .
and then we're gonna rotat our test set through our data .
and each time , we're gonna train a classifi .
so classifi on , classifi two will train on thi train data , test on thi test data , and comput an accuraci .
so we'll have accuraci on , accuraci two , accuraci three for each of these classifi and will , comput these perform of these classifi , so of five differ or in our exampl here , nine each time we're train on a , on a fold , and comput , our accuraci .
we'll take the averag of all those , and we'll import the averag of these ten run , each on train on nine fold , and test on on test fold .
so , and in gener , it's nice if we also add a final test , test .
it turn out that other kind of classifi max in and s b m often do better than naiv base it depend a lot on your data set and the size of the data .
but you'll want to take a look at all kind of classifi when you ar do naiv base .
now thi baselin algorithm ha a lot of problem .
on problem is that cinnamon is just a hard task in gener .
so here's some exampl from penn and lee .
take a look at them .
if you ? re read thi becaus it is your darl fragranc , pleas wear it at home exclus and tape the window shut .
so that's a neg review of a perfum , but veri hard to find just by us posit and neg word .
or the famou , by dorothea parker , on kathryn hepburn , she run the gamut of emot from a to b , again quit difficult to detect .
anoth problem that often occur in sentiment is call the thwart expect problem ha to do with order effect so .
here we read thi first review , thi film should be brilliant , sound like a great plot , actor ar first grade , lot of posit thing go on , but at the end , the review sai , can't hold up .
so , seem like a posit review , but it's not .
similarli , in thi sentenc , the veri talent lawrenc fishburn , not so good .
so here we're set up , i expect the movi to be good , and it wasn't good .
so thi kind of order effect is someth that we're gonna have to deal with in ani kind of more advanc sentiment algorithm .
so that's the basic baselin algorithm that we can see , for sentiment analysi .
descript , languag code en , dir ltr , name english , metadata , note , resourc_uri api<num> partner video zeljjoyqduxt languag en subtitl , site_url http www . amara . org video zeljjoyqduxt en <num> , sub_format json , subtitl end <num> , meta new_paragraph true , posit <num> , start <num> , text in our baselin algorithm , we us all the<br>word to do sentiment analysi .
but , end <num> , meta new_paragraph fals , posit <num> , start <num> , text there's lot of inform about which<br>word have which kind of sentiment that we , end <num> , meta new_paragraph fals , posit <num> , start <num> , text might want to make us of in addit . <br>these ar call sentiment lexicon .
on , end <num> , meta new_paragraph fals , posit <num> , start <num> , text sentiment lexicon is the gener inquir . <br>it's been around for quit a long time . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text sinc <num> .
and i point you here at the<br>homepag and the list of categori for , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the gener inquir and you can see the<br>spreadsheet that list all the word . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text let's go look at the gener inquir .
so , <br>here's the gener inquiri categori .
and , end <num> , meta new_paragraph fals , posit <num> , start <num> , text here for exampl , you can see thi a<br>littl larger .
here you can see posit , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and neg list of word .
let's go look<br>at on of them .
let's look at the posit , end <num> , meta new_paragraph fals , posit <num> , start <num> , text list .
so here's some list of word , <br>abil , abid , abl , abound , absolv , and , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so on .
so here's posit word from thi<br>intim list .
so , in addit to posit , end <num> , meta new_paragraph fals , posit <num> , start <num> , text and neg the gener also<br>ha lot of other list of kind of , end <num> , meta new_paragraph fals , posit <num> , start <num> , text class or word , strong versu weak , <br>overst versu underst , word about , end <num> , meta new_paragraph fals , posit <num> , start <num> , text pleasur , cognit orient , all sort<br>of other word .
so we can us it not just , end <num> , meta new_paragraph fals , posit <num> , start <num> , text for the simpl polar question that<br>we're talk about right now , but more , end <num> , meta new_paragraph fals , posit <num> , start <num> , text rich sentiment kind of question .
luke , <br>the linguist inquiri and word count , is , end <num> , meta new_paragraph fals , posit <num> , start <num> , text a , databas develop by , jame pennybak<br>and hi colleagu .
and here we have lot , end <num> , meta new_paragraph fals , posit <num> , start <num> , text of class , <num> class of word talk<br>about neg emot , posit emot . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word like bad or weird or hate .
cognit<br>process .
all the pronoun .
first person , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text second person pronoun .
word have to do<br>with negat , like no or never . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text quantifi and so on .
and luke ha a<br>small charg , but ha a larg list of , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word that peopl often us for sentient<br>analysi .
the mpqa subject , the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text qs lexicon , by wilson and weebe and their<br>colleagu .
give you again a list of , of , end <num> , meta new_paragraph fals , posit <num> , start <num> , text polar item , your posit word and<br>neg word .
and word in thi lexicon , end <num> , meta new_paragraph fals , posit <num> , start <num> , text ar also annot for intens , for how<br>strong or weak thei ar .
anoth polar , end <num> , meta new_paragraph fals , posit <num> , start <num> , text lexicon that's avail on the wed<br>web is , is bing leo's opinion lexicon and , end <num> , meta new_paragraph fals , posit <num> , start <num> , text i point you at that .
again , anoth list<br>of posit or neg word .
final , we , end <num> , meta new_paragraph fals , posit <num> , start <num> , text haven't talk about wordnet , we'll talk<br>about wordnet later .
but wordnet is an , end <num> , meta new_paragraph fals , posit <num> , start <num> , text onlin thesauru and the entri in<br>wordnet have also been label for their , end <num> , meta new_paragraph fals , posit <num> , start <num> , text polar .
so for exampl , the , word<br>estim , mai be comput , or estim , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text is label as , neither posit or<br>neg , but object , in the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text sentiwordnet , but the word , estim , <br>mean deserv of respect or high , end <num> , meta new_paragraph fals , posit <num> , start <num> , text regard , that's a mostli posit , although<br>sometim not , sometim object word . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so we can see that each word get a label<br>for how posit , neg , or object , end <num> , meta new_paragraph fals , posit <num> , start <num> , text it is .
and again you can go look at that<br>here , and you have to fill out a form to , end <num> , meta new_paragraph fals , posit <num> , start <num> , text download that .
and we'll talk about<br>wordnet itself a bit later .
so , end <num> , meta new_paragraph fals , posit <num> , start <num> , text each of these lexicon ha differ<br>thing in it but thei overlap in all , end <num> , meta new_paragraph fals , posit <num> , start <num> , text talk about polar .
and chri pott<br>ran a love correl between these , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text look at whether the polar of a word<br>is in multipl lexicon doe it have the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text same polar ?
and you can see that<br>sentiwordnet ha a whole lot of differ , end <num> , meta new_paragraph fals , posit <num> , start <num> , text thing go on in it , in gener .
if a<br>word is in multipl lexicon , the , the , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the disagr between the lexicon ar<br>veri small .
so most of the time , for the , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text opinion lexicon , the gener <br>lexicon .
for , for the mpqa .
in , end <num> , meta new_paragraph fals , posit <num> , start <num> , text gener , for all of these , you find pretti<br>similar sentiment if you're just do , end <num> , meta new_paragraph fals , posit <num> , start <num> , text polar .
obvious , if you're do more<br>complic thing , then thei're , thei're , end <num> , meta new_paragraph fals , posit <num> , start <num> , text gonna start to differ .
you might want to<br>look at each of word and , and talk about , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text what the polar is of word , like word<br>that we get from these lexicon , if we're , end <num> , meta new_paragraph fals , posit <num> , start <num> , text look at a databas like imdb .
and<br>there's some nice research by chri pott , end <num> , meta new_paragraph fals , posit <num> , start <num> , text look at thi , so he ask the question , <br>how like is each word to appear in each , end <num> , meta new_paragraph fals , posit <num> , start <num> , text sentiment class ?
and let's look at imdb<br>where there's ten star so , eh , a word , end <num> , meta new_paragraph fals , posit <num> , start <num> , text could like , bad , could appear in on star<br>review with a certain count , two star , end <num> , meta new_paragraph fals , posit <num> , start <num> , text review with a certain count , three star<br>and so on .
can we us that to understand , end <num> , meta new_paragraph fals , posit <num> , start <num> , text what's the sentiment of that word bad just<br>by count how often it occur in , end <num> , meta new_paragraph fals , posit <num> , start <num> , text differ review of differ star and<br>as he point out , you can't just us raw , end <num> , meta new_paragraph fals , posit <num> , start <num> , text count .
here , thi show you along the x<br>axi you look at the differ categori , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so on star , two star , three star , ten<br>star .
and here's the count of time that , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the word bad occur , the adject bad<br>occur .
and you can see it's mostli in on , end <num> , meta new_paragraph fals , posit <num> , start <num> , text star review .
but it actual occur more<br>often in ten star review than in two star , end <num> , meta new_paragraph fals , posit <num> , start <num> , text review .
which seem like a , a problem , <br>becaus bad should be a realli neg , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word .
but that's becaus .
there ar just a<br>lot more nine and ten star review or , end <num> , meta new_paragraph fals , posit <num> , start <num> , text eight and ten star review than there ar<br>two star review .
it turn out that , end <num> , meta new_paragraph fals , posit <num> , start <num> , text review tend to skew posit .
so we can't<br>us the raw count .
instead we're go to , end <num> , meta new_paragraph fals , posit <num> , start <num> , text us the likelihood .
we're gonna take the<br>frequenc of the word in the class and , end <num> , meta new_paragraph fals , posit <num> , start <num> , text then we're gonna divid it by the total<br>number of word in that class .
and that , end <num> , meta new_paragraph fals , posit <num> , start <num> , text will solv the problem of class like<br>nine and ten in gener be bigger . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text caus we're comput the likelihood<br>condit on the class size .
now an , end <num> , meta new_paragraph fals , posit <num> , start <num> , text addit thing we often wanna do is<br>compar these number for differ word . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text some word ar veri frequent .
some word<br>ar less frequent .
and , so to do these we , end <num> , meta new_paragraph fals , posit <num> , start <num> , text wanna do a scale of likelihood where we<br>take the likelihood of the word given the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text class and just divid by the probabl<br>of the word , so now if a word is , end <num> , meta new_paragraph fals , posit <num> , start <num> , text particularli frequent or particularli rare<br>we can still compar them with each other . , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so we can comput the scale of<br>likelihood .
and if we look at that , we , end <num> , meta new_paragraph fals , posit <num> , start <num> , text can see here's the word good or the word<br>amaz , we can see that as for amaz , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text as review number , as the star go up from<br>on star to ten star , the word is much , end <num> , meta new_paragraph fals , posit <num> , start <num> , text more frequent us as we get higher up . <br>for great similarli , though not as strong , end <num> , meta new_paragraph fals , posit <num> , start <num> , text as amaz .
for awesom , again we see a<br>veri sharp , rise in it , in it us , in , end <num> , meta new_paragraph fals , posit <num> , start <num> , text review .
for good it turn out<br>to be us mostli in the middl .
a good , end <num> , meta new_paragraph fals , posit <num> , start <num> , text movi is not great , and it's not terribl , <br>and that's what we might expect from the , end <num> , meta new_paragraph fals , posit <num> , start <num> , text word good .
and similarli for , bad review . <br>so , again , a word like depress , more , end <num> , meta new_paragraph fals , posit <num> , start <num> , text often can occur in these neg review , <br>the on , two , and three review .
a word , end <num> , meta new_paragraph fals , posit <num> , start <num> , text like bad , again , occur in these on , <br>two , and three star review .
terribl in , end <num> , meta new_paragraph fals , posit <num> , start <num> , text these on , two , and three star review .
so<br>we can look at the review , along the x , end <num> , meta new_paragraph fals , posit <num> , start <num> , text axi , the , the review qualiti , on through<br>ten .
and along the y axi , the scale , end <num> , meta new_paragraph fals , posit <num> , start <num> , text likelihood to see the polar of the<br>word .
so potter also ask , whether , , end <num> , meta new_paragraph fals , posit <num> , start <num> , text whether we can see sentiment in other<br>word , then , good or bad .
he look at , end <num> , meta new_paragraph fals , posit <num> , start <num> , text logic negat , word like no , not , <br>never and the nt as isn't and didn't and , end <num> , meta new_paragraph fals , posit <num> , start <num> , text so on .
ar those associ with neg<br>sentiment ?
now , what he did again , is he , end <num> , meta new_paragraph fals , posit <num> , start <num> , text count negat of onlin review and<br>look at them , how thei , how thei match , end <num> , meta new_paragraph fals , posit <num> , start <num> , text the review rate .
and , just like the word<br>bad .
and , or the word terribl .
you can , end <num> , meta new_paragraph fals , posit <num> , start <num> , text see that the word , these negat word , <br>like no , not , never , occur more frequent , end <num> , meta new_paragraph fals , posit <num> , start <num> , text in these low star review .
either in the<br>imbd databas , or other databas which , end <num> , meta new_paragraph fals , posit <num> , start <num> , text ar onli from on to five star .
so in<br>these lexicon , we can see lot of word , end <num> , meta new_paragraph fals , posit <num> , start <num> , text ar posit and neg .
well it's great that there ar sentiment lexicon avail .
for lot of other purpos we'd like to build our own sentiment lexicon .
and the wai we do thi is often by , by semi supervis learn and the idea of semi supervis learn is we have some small amount of inform .
mayb we have a few label exampl or mayb we have a few hand built pattern .
and , from that set of data , we'd like to bootstrap a complet lexicon .
and we might want to do learn of lexicon instead of take an onlin lexicon , if we're look at a particular domain that mayb , that doesn't match the domain of the lexicon that wa built , or we're try to do a particular task , or mayb we're just , think that our onlin lexicon might not have an , enough word that ar relev to the topic we're look at .
on of earliest wai of induc thi kind of sentiment lexicon wa propos in <num> by and i'm gonna show you thi becaus the intuit , although the paper is old , thi intuit is includ in almost all modern wai of , of do thi semi supervis learn of get a lexicon .
and their intuit , veri simpl , is that .
to adject if their conjoin by the word and thei probabl have the same polar and if their conjoin by the word but thei don't .
so if you see on a veri high frequenc of fair and legitim or corrupt and brutal probabl fair and legitim or both on the same polar .
and , and so we might suspect that fair and brutal ar just less like to occur on the web or in some corpu .
and so if we see a lot of someth occur with hand , mai be the two ar like to have the same sentenc .
but if we see two word link by but , thei probabl have differ sentiment .
so fair , but , brutal more like to occur than fair and brutal .
and here's how thei us thi intuit .
thei first label by hand a seed set of <num> adject and so thei had about a similar number of posit and neg adject .
so central or clever or famou or thrive and neg adject like ignor or , or list listserv unresolv .
and now from the seed set , thei expand that to ani adject that's conjoin with the word in their scene set .
so , for exampl , we can go to googl and we can type in wa nice and , and look at what word occur next , and what do we see after wa nice and .
what we see wa nice and help .
so that tell us that nice and help ar like to have the same sentiment .
and here we see nice and classi , so that tell us that nice and classi might have the same sentiment .
and we can do that for , for all sort of word that occur conjoin with our seed set .
and we take ani word that occur frequent enough in the right conjunct with our seed set word .
and now we can build from that , a classifi .
and the job of a classifi is just to assign .
for each pair of word how similar the two word ar .
and so we can give the classifi the count of the two word occur with an and in between them and a count of the word with a but in between them .
and it , the classifi can learn that nice and help ar somewhat similar .
nice and fair ar veri similar .
fair and corrupt ar veri dissimilar , mark them in red with a dot line .
becaus mayb but occur more often between these two and , and occur more often between these two and so on .
so we can get between ani pair of word a number that indic it's similar .
and now we can just cluster the graph and us ani kind of cluster algorithm and we get for the word that tend to be link togeth help and nice and fair and classi will get that these ar link togeth and brutal and corrupt and irrat link togeth then we can get our , our output .
polari lexicon and here i have shown you an output polari lexicon and of cours it'll have some mistak in it .
so , see if you can find the mistak in thi lexicon .
here ar some of the mistak .
so we have disturb as a posit word or strang as a posit word , or pleasant as a neg word .
so of cours there's gonna be error in ani of these kind of automat , semi supervis algorithm .
so while the and algorithm automat find the sentiment or the polar of individu word , it doesn't do well with phrase , and we'd like , often to get phrase as well as word .
so the algorithm is anoth wai to bootstrap in a semi supervis wave lexicon .
and what thi doe is extract a bunch of phrase from review , learn the polar of review , and take all the phrase that occur in a review , take their averag polar and then us that to rate the review .
so let's look at these stage in the turn algorithm .
first we're go to extract , everi two word phrase .
that ha a particular set of part of speech tag .
and we haven't talk about part of speech yet , but , so thi is ad to the tag for adject .
so if the first word is an adject and the second word thi mean noun and thi mean plural noun .
so if the first word is an adject , so an adject follow by a noun or plural noun , we'll extract that phrase , whatev the third word is .
or of the first word is an adverb , rb mean adverb , and the second word is an adject and the last word is not a noun , we'll extract that phrase .
so adverb , adject .
adject , adject .
noun , adject .
add the verb .
so these ar particular phrase .
we ar gonna run our part of speech tagger .
we're gonna talk about that in a few lectur and assign for each word a part of speech adject noun and so on and then we'll extract two word phrase that meet these criterion .
the first word is thi tag , second word is thi tag the third word ha some constraint but we don't extract it and now we look at all those phrase .
and our goal is for each of those phrase we have extract to measur it's polar .
so how posit or neg is a particular phrase and the intuit of the tournei algorithm is just like the hustl bustl of maculen algorithm .
to think about co occurr .
so we ask , well a phrase is posit if that co occur a lot , nearbi let sai in the web or some larg corpu with the word excel .
a neg phrase is like to co occur more often with a word like poor .
so how we're go to measur thi co occurr .
the standard wai to measur , thi kind of concurr is by , point wise mutual inform .
point wise mutual inform .
and that's a variant of a standard inform theoret measur call mutual inform .
the mutual inform between two random variabl is the sum over all the valu the two variabl can take of the .
joint probabl of the two , time the log of the joint over the individu probabl .
so point wise mutual inform take thi intuit from mutual inform and just ask a veri simpl comput a veri simpl ratio .
the probabl of two event x and y divid by the product of the individu probabl .
so what , point wise mutual inform is ask , it's a ratio is how much more the two event x and y co occur than if thei were independ .
if thei were independ thei would have these independ multipli probabl .
and , and how much doe the joint occur more often than we'd expect from independ .
it's the intuit that point wise mutual inform .
so look at point wise mutual inform , i just repeat the equat here , we can look at it between two word .
we sai how much more do these two word , word on and word two co occur than if thei were independ ?
by take the ratio , the probabl of the two word occur togeth divid by the probabl of each word separ and multipli and we take the log of that .
how do we estim thi ?
well , the wai tournei , did it origin is by us the altavista search engin .
and we're gonna estim the probabl of word just by how mani time , how mani hit we see for that word .
and we're gonna estim the probabl of two word by how often word on occur near word two .
so all of thi ha the near oper that , that let just check if a word is near anoth word .
and in each case we're gonna want to normal to get a real probabl from these count .
and by our definit from the previou slide , point wise mutual inform , it's the probabl of the joint , so that's hit of word on near word two over n squar .
divid by hit of word on over n over hit of word n two over n and these n's ar gonna cancel and we're gonna end up simpli with , with thi equat .
so how , how mani time word occur near each other , how mani time do thei occur individu , multipli it togeth .
so onc we have a measur of co occur of how , how much a phrase co occur with the word excel or anoth word good we can now determin it polar .
so the equat for polar in the algorithm is just how much is the mutual inform , the point mutual inform of the , of ani phrase with the word excel and we subtract it neutral inform with the phrase poor .
so how much more doe the phrase appear with excel than poor or vice a versa ?
and just do a littl algebra .
so that's the , the number of .
here's the definit of point wise mutual inform of phrase with excel .
how often it occur near the word excel divid by the individu number of hit of , of the phrase itself in excel , and then subtract the same thing for poor .
and of cours by the definit of log we can bring thing insid the log and turn the minu into a divid and then we can do some , some mess with term so we have hit of phrase here .
and we have hit of phrase there .
and so , in the end we can get our formula for decid the polar of a phrase .
it's just how mani time doe the phrase occur in your excel , time how mani time it occur with , the word poor occur , over how mani time the phrase occur in your poor , over how mani time excel occur .
these ar , these ar constant that we can get onc , and then for each phrase we can comput these chief quantiti .
and us the twenti algorithm , here's a posit review obvious of a bank and so here's a phrase like onlin servic .
adject noun phrase .
and here's the polar assign by it , <num> .
here's anoth on direct deposit , <num> .
and here's some neg phrase with neg polar .
so the phrase occur more often near the word poor than near the word excel .
so in commun locat ha a neg polar .
so on an averag though more , i have just shown you a sub set more of the posit phrase occur in thi review than the neg phase and the averag polar is posit .
so thi is a thumb up review .
a , thumb down review , you can see that phrase like virtual monopoli , neg polar , lesser evil , neg polar , other problem , neg polar , all occur more frequent , and on averag , have a more neg polar than the posit polar word that occur in thi review .
and we end up with a review that ha an averag neg polar .
so the turn algorithm wa evalu on variou kind of review site and doe a better job than just the major class base line , at predict the polar of a review .
so what's import about the turn algorithm is that it let us look at phrase , learn phrase rather than just word and what's true about in gener about these unsupervis algorithm , is that we can learn domain specif inform that might not be in some onlin dictionari .
so , for do , go back to the previou slide , if we're do .
bank , a word like direct deposit or virtual monopoli might , simpli might not be in an onlin polar dictionari or onlin web so we , we need to us some of these semi supervis algorithm for learn the polar of these kind of word .
final , i mention briefli a third algorithm for learn polar , and thi us wordnet , again that's the onlin thesauru which we'll talk about in detail later .
and the intuit of us these onlin thesaurus is , is similar to what we've seen in the first two semi supervis algorithm , we'll start with a posit seed set and a neg seed set , so when i have word like good in the posit seed set and terribl in the neg seed set , and now we us the thesauru just veri simpli to find synonym and antonym .
so we add synonym of posit word like well .
and antonym of neg word to the posit set .
and to the neg set , which we start out with word like terribl , now we add synonym of all those word .
mayb aw is a synonym of terribl .
and antonym of our posit word to the neg set and we just repeat and the set grow as we keep ad synonym and antonym to it .
and then each of the algorithm that make us of word net for learn clariti will have variou wai of filter our bad exampl or problem with word synthesi and thing like that .
so , in summari , we , learn lexicon can help us .
deal with domain specif issu .
in bank we might have a word like direct deposit .
it's just not gonna be in a standard onlin polar lexicon .
and we can more robust in gener , as new name ar , peopl start us new name , or a new compani name might not be in some train data , but , but we might be abl to learn it from the web or from the data we're look at .
and , again , the intuit of all these algorithm is realli the same .
we start with some seed set .
we find other word that have similar polar to that seed set in some semi automat wai and the wai you've seen ar us on of the ca , word ar conjoin with and or but us word that just occur near , by , toward , like poor or excel so that's a receipt set there .
or us coordin , synonym or antonym .
there ar other sentiment task than just posit and neg polar .
on import on , ha to do with find the sentiment of a , of an individu sentenc , which we're gonna wanna do for find aspect or attribut that we talk about , earlier .
so that's the , find the target of the sentiment .
what it is that we're , talk about the sentiment about .
so here's a sentenc .
the food wa great , but the servic wa aw .
thi whole sentenc doesn't realli have a sentiment .
it ha a posit sentiment about food , and a neg sentiment about servic .
so we'd like to be abl to deal with thi kind of micro sentiment , where we have a , we have a sentiment about on attribut , and a differ sentiment about a differ attribut or aspect .
so how ar we gonna find the aspect , or attribut , or sometim it's call target , of a sentiment ?
and i'll show you , an exampl , that a wai of do thing that wa develop both and by blair goldensohn at all .
and the idea is , we first look through all the review , and we find everi highli frequent phrase .
so let's sai that the word fish taco occur a lot , in .
in thi particular restaur review of thi restaur , and then we mark down , we have filter let's sai we prefer highli frequent phrase if thei occur right after a sentiment word .
so mayb we , veri often , see the phrase great follow by fish taco or terribl follow by fish taco .
that suggest that fish taco is the , is an aspect , a like aspect of a , let's sai , a tacoria .
so if we see great food .
a lot .
we see food occur veri often .
in , in a .
review and we see the word great , and terribl , and aw and befor and a lot , we know it's probabl an aspect that peopl ar appli sentiment for .
and thi can be us automat .
defin sentiment for lot of thing .
so , blair goldensohn us it , for exampl , for casino , review of casino , to find word like casino , buffet , pool , resort , and bed ar the kind of thing peopl comment about in review on casino .
for a barb r you might talk about the experi , or how good a job it wa , or fact about the haircut .
for a depart store , you might talk about sale , or , or differ depart , or the select of the store , thing like that .
so we can automat find the aspect .
in other case , though , the word that describ the aspect mai not be in the sentenc .
and so , for some sentiment topic , like restaur or hotel , it's pretti well understood what the aspect ar that peopl care about for restaur .
restaur , thei tend to care about food , decor , servic , valu .
and so , for case where we know in advanc what the frequent aspect ar .
here , we can take a small corpu of restaur review , and we can hand label it .
we'll just mark everi sentenc , doe thi sentenc talk about the food , doe it talk about the decor , or is it none of the abov ?
and then we can just build a classifi , which , given the sentenc .
assign an aspect to the sentenc .
is thi sentenc about food ?
is thi sentenc about decor ?
is thi sentenc about servic , and so on .
and it could be a sentenc , or we could assign that with , you know , with phrase , or mayb with claus , with piec of sentenc .
whichev we'd like to do .
so two wai of find aspect of a sentiment .
we can automat find frequent phrase .
so we can find phrase .
and then we can , build up a set of phrase that occur frequent , and the titl ar good phrase to , to re , to , that we'd like to know about for thi particular product .
or we can , decid the aspect in advanc , aspect ar , ar , come in advanc , like we know that thei ar the restaur .
and then our job is just to build a classifi to find them in the review so we can decid if what a person said about the food is posit or neg .
so put thi all togeth , thi is the blair goldensohn algorithm from googl , we have a set of review .
we're gonna extract a bunch of sentenc or phrase .
so here's our extract sentenc or phrase .
and now for each on of them , we're gonna run our sentiment c lassifi , is the phrase posit , neg or mayb it's neutral .
okai ?
and then for the on that ar , that have sentiment in them , the on that ar posit or neg , so like we set out , we have sentiment and phrase with sentiment , plu minu , minu plu , plu minu , and so on .
we're go to extract the aspect of these .
so is thi about food ?
is it about , is it about , decor ?
what's the sentenc about ?
so now we have plu food .
and now we have .
minu decor , minu servic , and so on for each sentenc or phrase , we have both a sentiment and an aspect .
now we aggreg these togeth , and produc a final summari .
and here's an exampl of what you might get as the final summari .
so mayb for a hotel , for room , we might get that thi sentenc , the room wa clean and everyth work fine .
posit sentiment .
anoth posit sentiment .
here's an exampl of neg sentiment .
so , we first figur out that worst is a neg sentiment .
and then we might ex , extract from somewher in here that there's some mention of room .
and similarli for servic , we might see , servic give new mean to slow , and that's a neg .
we made some simplif in the base line method we gave for sentiment classif .
we assum that the class , posit and neg , occur with equal frequenc .
and that's , of cours , not usual true in the real world .
and it turn out that when the class ar not balanc .
you can't us accuraci as an evalu .
and the f score , can deal well .
and when evalu class where there's mani more posit than neg , or mayb there's mani more neg than posit review .
it turn out that thi .
the , there's a sever imbal in class frequenc , that can actual degrad the classifi perform .
and there ar two standard thing we do to deal with that .
on is , we just re sampl befor we train .
so for exampl , if on class ha , a million review , and on class ha , <num> , <num> review , then , we might just danc down sampl and take onli ten to the fourth of these re view .
to match with these review .
instead of re sampl , we can us cost sensit learn .
cost sensit learn we take , we actual chang the classifi , and we tell the classifi , don't , even though thi on , you've seen a lot more of thi frequent thing penal it for mi classifi the realli rare thing , and so that'll forc it to focu a littl more on the rare thing , then on the veri frequent thing .
so two thing you can do to deal with the imbal frequenc problem that often occur in real class .
we also in our , in our base , baselin algorithm , made the simplifi assumpt that , sentiment wa a binari problem , posit or neg .
so how do we deal with five star , or seven star , or ten star ?
there ar two wai we can do thi .
we can map , onto a binari class , we can sai thing that ar more than <num> star ar posit , or less than <num> ar neg .
or we can take the averag of all the data .
we can z score .
we can do variou wai of , of draw a boundari between posit thing and neg thing .
or we can .
actual attack the seven , on to seven classif task directli , by us linear ordin regress or special model like metric label that's us by pang and lee .
so again , we can either down sampl and just do binari classif , or we can us a more advanc method of classif that let's us predict an ordin valu or , or a real valu number .
so in summari sentiment gener model as a classif task .
polar often with binari .
less often , with some kind of ordin or linear valu label .
and , negat , a veri import featur to us .
for lot of task , us all the word in naiv bai seem to work well .
for other task , us subset of word mai help .
and we can either us hand built polar lexicon .
or , for task for which the polar lexicon seem inappropri , we can induc lexicon us semi supervis learn from some hand built seed .
now rememb that .
sentiment is realli just on of mani kind of affect state .
it's a , it's a kind of attitud classif , but .
there's lot of other kind of effect state classif that come up .
and thei're comput problem that ar veri similar to sentiment analysi .
so to look at some of them for emot , we might want to detect annoi caller to some dialogu system .
so it's detect the emot of annoy .
or detect confus or frustrat student in onlin tutori , versu confid student .
for longer term mood , we might wanna find traumat or depress peopl like writer of some , blog or text .
in convers , we might wanna detect if someon's friendli , or unfriendli .
in veri long term person trait , we might want to detect extrovert or introvert for build dialog system that can commun better with extrovert or introvert .
and there's lot of research actual on all of these task we'll just show you on of them , which is a task we've work on here at stanford , which is detect of friendli .
so , friendli speaker us , it turn out , us collabor convers style .
so we built classifi that look at featur like how often someon laugh , or how often thei us neg emot word .
or how frequent thei us phrase like that's too bad that indic sympathi or agreement .
i think so , too .
so the more friendli someon is , the more like thei ar to us sympathet word or agreement , or the less like thei ar to us a hedg like kind of or sort of .
and we've shown that these classifi help in , we've look at the task of detect how friendli someon is or how flirtati someon is in speed date .
and we've found that these kind of , featur help detect friendli .
so , simpl lexic featur , like word and phrase , and where we have speech , speech featur like or pitch .
all of these can help in detect sentiment , simpl polar , and much more rich effect mean of all sort of kind .
