hi , i'm go to discuss the first lectur , a brief introduct to data mine .
we ar go to briefli , discuss thi becaus the concept , is well understood .
and , if you want to know more , pleas feel free to get into my chapter <num> of the textbook , and you can , you can read and get more detail .
the first question i'm go to ask you is , why data mine ?
you probabl sai , sinc i'm take the cours i know , why i'm go to do data mine .
it's a veri simpl common sens concept .
we ar live in a digit societi .
we ar surround with big data , or we sai , we ar encount the explos growth of the data .
the data ex , explos problem ha been here , sinc comput come to the stage , especi with the web appear with lot of data collect tool appear .
we're collect lot of data .
first , we actual got the web and the comput's rise to societi .
the new , digit media , social network , e commerc , and you know , bank , you know , everi corner of societi gener huge amount of data .
even in scienc and engin , we got a remot sens , bioinformat , scientif simul and mani other domain .
thei keep gener tremend amount of data .
so , peopl sai we ar drown in data but starv for knowledg .
why ?
just becaus a lot of data we've gener ar unstructur .
the knowledg is deepli , buri insid .
if we do not have power tool to mine such data , it is imposs we can get lot of benefit from such data .
so , data mine is a realli a necess .
so peopl sai , necess is a mother of invent .
data mine is a tool to automat , to do scalabl analysi of massiv am , amount of data .
so thi is a realli need technolog .
so , what is data mine ?
most peopl base on my understand refer data mine , as knowledg discoveri from data .
actual , the first kdd workshop start in <num> , wa call knowledg discoveri in databas .
but a few year later in <num> , when the first kdd confer come to the stage , thei chang the titl of the confer to knowledg discoveri and data mine .
so , data mine realli becom a popular term , you know , even at that stage .
okai ?
so , for data mine , most peopl think is extract of interest pattern of knowledg from massiv data .
what is interest pattern ?
usual the pattern could be non trivial , implicit , previou unknown , potenti us , or ev , even action .
okai .
but actual if you think .
sit back and think a littl .
data mine is actual , a misnom .
why ?
becaus we think gold mine .
the gold mine is not , we got lot of gold , or we mine someth out of gold .
we actual got lot of sand , or or .
we mine you know , sand , and or to get gold .
so to that extent , more precis , we should sai , we mine data , actual we want to get knowledg .
so essenti , it is knowledg mine from data .
but sinc data mine is a short term , peopl sai , we ar do the mine process .
we ar deal with data , so data mine becom a veri popular term , so peopl understand that we ar do knowledg mine from data , so there's no real ambigu if we call just it , as data mine .
but data mine , some peopl sai , data mine is essenti some , some other field , like machin learn , or pattern recognit , statist databas .
actual , data mine is a interdisciplinari field , integr mani method , and develop a lot of new method of it own .
so , we ar go to discuss thi relationship .
in the next sever slide .
okai .
so let's look at thi pictur .
thi pictur , peopl call thi on , is a knowledg discoveri process .
essenti is we got from data , actual data could be anywher , we drawn as a disk , actual could be on the web , could be a data stream , could be fire as long , as is digit .
we take the data from multipl sourc .
we do data integr , data clean , data normal , or featur select , dimens analysi reduct .
all of these we call data integr and data process .
okai , then we get integr and process data , which could be store in databas , data warehous or some or some other data repositori , or even in the form of data stream .
okai .
then with such huge amount of data in mani case , peopl onli interest in part of it .
so , we mai need to do data select by us queri , by us featur select , or ani other method .
then , we perform data mine .
data mine these function , we ar go to discuss in the next slide a littl more .
final , we get pattern but a pattern is not the final product , becaus the pattern could be redund , pattern mai not be quit meaning .
so we still need to do pattern evalu , pattern select , data pattern interpret , and data visual .
sometim we found the pattern is not what we need , that we can have a backward process to redo the data mine , or you call some other data mine function .
final , we develop a pattern , we think thei ar valuabl .
we present in a visual wai , or in a usabl wai that mean we realli gotta patent inform and knowledg .
rememb for these process , we onli sai , data mine is part of kdd process of cours , is most critic on .
actual , data pre process sometim is also veri critic , take a lot of effort it will depend on your applic by the data mine .
for thi cours is a core function we're go to studi .
for data mine , we can have a multi dimension view .
that mean , data mine can be view from multipl angl , for exampl to see , from the data view , from knowledg view , from methodolog view , and from applic point of view .
from data view , essenti , as we studi what kind of data we're go to mind ?
essenti , there ar two kind .
on , we call structur , and semi structur data , and other is unstructur data .
the structur and semi structur data could come from relat databas , object relat databas , data warehous , or even from transact , transact databas .
okai .
for unstructur data , data can come from anywher .
for exampl , from the web , from text , new .
okai .
or from spatial , spatiotempor devic' sensor .
or from camera , like from mul , you get multimedia data .
it could be from multipl sensor , you get data stream , or we get a biolog sequenc , tempor data , time seri data , and also , recent , there ar lot of network data .
for exampl , social network , inform network , web graph , need to be mine .
so , essenti , sinc the data ar of so differ kind .
it creat lot of new challeng , and problem for data mine .
anoth view for data mine is knowledg view .
knowledg view , you essenti studi , what kind of knowledg to be mine .
for exampl , we mai us differ method to do data summari , multidimension aggreg , const , get data cube .
put in the data warehous to do old lab , onlin and a littl bit process .
thi is on wai to mine data .
anoth wai is we mai look at it to find pattern .
like frequent pattern , correl , associ , and us such pattern in mani other task , or even implicitli .
okai .
the third major task is classif , and predict model .
thi mani is we mai get some train data .
it could be label by expert .
then we us thi train data we get model .
us thi model we can predict , or label the new data .
okai .
so thi predict , we call thi model construct and predict , as predict model , so classif and the predict model in mani case , thei could be synonym .
but if you got data , you do not realli have expert give you labor , that mean the data could be unsupervis .
then thi form a cluster process .
okai .
cluster essenti , is try to group data to from differ group , or differ categori , or even hierarchi .
the onlin analysi is anoth task .
essenti , it's to detect anomali , or rare event , and label them outlier to pai special attent out of it .
okai .
and if you get a time evolv data , like you get time , as on of the dimens , or on of the factor .
okai .
then we mai want to find the trend , or evolut .
so , trend and evolut analysi is anoth data mine function .
and data mine as we just mention , it come from mani differ disciplin , as a confluenc of these disciplin , it come a new domain or new field for data mine .
of cours , machin learn statist pattern recognit feed data mine a lot of us , interest algorithm and methodolog .
but data mine , itself , gener lot of new method .
and new data mine hardwar , and meth , and methodolog as well .
plu visual , where will greatli help data mine , or understand the mine pattern , or understand the data to be mine .
in the meantim , algorithm , databas technolog , distribut and cloud comput will all plai a veri import role in data mine develop .
final , applic is a drive forc for data mine .
so there ar veri divers applic .
you need to develop mani , mani differ kind of data mine method .
final , we look at the applic view .
we also know data mine ha a lot of divers applic , for exampl , mine textbook .
text data , mine the web , do web page classif , web page rank , weblog analysi , recommend system to recommend a page differ web page or differ product .
okai , so these ar on kind of applic .
but mine busi data like transact data , market analysi , market basket analysi , fraud detect .
all of these attribut , need lot of data mine measur .
data mine actual also benefit comput scienc quit a lot .
data mine can help softwar engin to find bug .
and help system engin to optim their system perform .
or can even you know , help with comput vision , okai .
and mine biolog and medic data will be veri us .
for exampl , we get a exponenti growth of the biolog data like gene , protein , microarrai data , biolog network .
and data mine mai plai a import role in it .
and also recent , there ar lot of social , and inform network come out .
okai .
how to find their commun , how to do inform diffus , how to see the inform interact within thi network , is veri interest and veri excit .
final , i want to mention invis data mine .
actual , data mine now plai a lot of import role , but you mai not even know data mine is in sight .
kai ?
for exampl , you do a web search .
'kai .
there ar web page rank , web page classif , web click string analysi , or recommend .
there ar mani , mani data mine method , actual embed in your click , okai ?
to that extent , data mine can plai a veri import role in our societi , you mai not even sens it .
'kai .
so to learn more about thi introduct , i recommend three book .
on is my own book on concept and techniqu of data mine .
the second on is a recent book publish eh , in cambridg univers press by zaki and meira .
it's a veri , veri nice written book .
pang ning tan , michael steinbach , and vipin kumar got a book , thei got a new edit on introduct to data mine , publish in <num> .
final , i want to sai , charu aggarw eh , and myself actual co edit a new book on frequent pattern mine , which is close relat to thi cours .
becaus we ar do pattern discov data mine .
there ar lot of known author , thei contribut to differ chapter to thi book .
reflect the current data of , of our pattern mine .
so , you mai realli want to read it .
thank you .
hi , in thi final session of thi lectur we're go to discuss mine close pattern .
as we alreadi know befor , close pattern is a compact form but it's a lot less compress of frequent pattern .
so mine thi close item set is veri interest and us .
and with pattern growth approach there is on interest method , would be better to call it closet .
let's look at thi closet , how to develop effici , direct mine of close item set .
so let's look at thi transact databas , it contain onli four transact and these ar the item in these transact .
suppos a minimum support is two , we would be abl to get these as frequent item set .
and base on thi we can work up the f list , like the follow , okai ?
now we look at , you know , an interest method of develop call itemset merg , okai ?
the philosophi can be repres us thi exampl , okai ?
let's look at these project databas .
for these project databas , we will have acf , ef , and acf base on thi .
you can see thi project databas will get a c d e f and acf , but the interest thing is acf happen in everi transact project in thi databas .
acf have the same support as d .
in that case , we can grab acf out from acfd project databas , which contain onli on item , e .
it is not frequent .
therefor , we will be abl to get acfd , support is two , it's the final result .
thi method itemset merg .
simpli said , sai , if y appear in everi occurr of x , then item in y is merg with x .
now the x is d , and the y is acf .
acf occur in everi occurr of x , which is d .
then we will merg acfd togeth to form a more , you know , compress form .
that mean you can mine that all these immedi .
okai , so thi is more effici .
actual there ar mani trick develop in closet .
for exampl , hybrid tree project .
we us bottom up physic tree project , top down pseudo tree project , there's a on technic , sub itemset prune , itemset skip , effici subset test .
but i'm not go to get into the detail .
for detail , you can read thi paper .
okai .
so final i'll summar the recommend read .
these ar all classic paper you know algorithm , you know , algorithm mine and the , you know , the further improv of algorithm mine .
then we have vertic method , we have fp growth method and we have closet method .
so final there's an interest survei articl call frequent pattern mine algorithm which contain mani more algorithm cover in thi lectur .
if you have interest in , go ahead and read thi chapter .
hi .
in thi lectur we're go to introduc some basic concept of pattern discoveri .
we ar go to discuss these three issu in three session .
the first session we ar go to discuss , what is pattern discoveri ?
why it is import .
to know why pattern discoveri is import , we need to know what ar the pattern , okai ?
pattern actual ar a set of item , subsequ or substructur that occur frequent togeth in data set .
we call these strongli correl .
pattern usual repres intrins and import properti of data .
that's why pattern discoveri is a process .
try to uncov , try to mine pattern from massiv data set .
it ha mani interest applic .
for exampl , in transact databas , you mai want to find what kind of product ar often purchas togeth .
then you can do some target market .
and for custom , you mai want to studi , if a person bui an ipad , what ar like other product he or she is go to bui in the futur ?
okai .
and even for softwar engin you mai want , base on the code segment analysi , try to find copi and past bug .
for text analysi , you mai want to see what ar like the keyword were form phrase .
find phrase mai need pattern mine as well .
so we can see pattern discoveri could be veri import becaus it , it is uncov inher regular in the data set .
it form the foundat for mani thing .
for exampl , associ correl causal analysi , mine sequenti structur pattern , pattern analysi in spatiotempor data , multimedia data and stream data .
even for classif , if we us discrimin pattern base analysi , the classif could be more accur .
and for cluster analysi , pattern base subspac cluster could be an import direct for cluster analysi .
so there's no wonder why we want to studi pattern discoveri .
let's first introduc some basic concept .
frequent pattern and associ rule .
we , first look at thi simpl transact exampl .
there ar five transact , <num> to <num> other transact id .
and these ar the set of item .
thei bought .
for exampl , transact id <num> contain beer , nut , and diaper , which form an itemset becaus thi is a , a set of item .
and for thi particular on , it is three itemset becaus it contain three .
item .
and for each item set , you mai have a concept of support .
support mean , in these transact data set , how mani time doe beer happen ?
like in thi particular case , there ar three occurr of beer in thi transact data .
so the support can't appear as three .
but you also can us rel support , that mean the fraction of the transact .
for exampl , there ar a total of five transact .
three of them contain beer , so the rel support is <num> over <num> , or you can sai <num> .
so , we mai see whether in itemset x is frequent or not .
if x , the support of x , pass a minimum support threshold .
for exampl , if we said the minimum support threshold is <num> .
then , we can see the frequent <num> itemset , in thi data set , you will find there ar <num> , like , beer , you can see there ar , <num> case , the absolut support is <num> , the rel support is <num> over <num> is <num> .
okai .
but for frequent <num> itemset , you mai check it .
there , there's onli on , okai ?
becaus there's beer and diaper , thei happen togeth <num> time out of <num> .
that's why we get thi .
but none of the other , you know , itemset , thei pass thi .
<num> threshold .
so there's onli <num> .
and from the frequent itemset we can introduc an interest rule , associ rule .
that impli for exampl x impli y simpli sai , if peopl bui x , what is the support and confid peopl will bui the itemset y , okai .
then s is the support which is the probabl x and y contain togeth in thi rule set , okai .
then c is the confid which is a condit probabl .
that mean , if the transact contain x , what is the probabl it also contain y ?
so , for thi probabl comput , you can us support x union y , divid by support x .
that mean , take the whole rule support , divid by the left hand side .
you mai see thi notion x unit y .
thi is a set unit .
but if you look at the venn diagram .
actual , connect contain x could be thi part .
connect contain diaper not beer is thi part .
diaper is thi part .
contain both actual is their intersect .
okai , the intersect of the event , okai .
but for from itemset point of view .
echo the transact contain both x and y .
both beer and diaper , you will sai thi on work on .
that's why we us not x intersect y , but x union y .
okai , if you think that x is beer , y is diaper .
x intersect y will be empti .
x union y mean it contain both .
then , for associ rule mine , is actual try to find all such rule which ha minimum support and confid threshold .
we alreadi know if we set minimum support at <num> , we'll find these ar the frequent <num> itemset , these ar the frequent <num> itemset .
from here , if we set minimum confid as <num> , we ar go to deriv two associ rule , becaus these two rule if you us thi comput .
beer and diaper and get togeth is <num> .
in here , okai .
then , it's <num> becaus <num> over <num> .
and if everi time beer occur , you can see diaper also occur , that's why the confid is <num> .
but for diaper impli beer , you probabl can see the diaper support is <num> , beer support is <num> .
so , that mean there's onli <num> of probabl the custom bui a diaper , like to bui beer .
ar their more rule in thi on ?
actual if you check thi , becaus thi is the onli frequent two item set .
these two ar the onli associ rule that can be gener from thi transact data .
now , we're go to introduc anoth set of import concept , close pattern and max pattern .
actual , in associ rule mine , or frequent pattern mine .
there's on challeng , is in mani case we mai gener too mani frequent pattern .
usual a long pattern will contain a combinatori number of sub pattern .
just give you on simpl exampl , suppos we get a transact databas , it contain onli two transact .
t sub <num> and t sub <num> .
for t sub <num> , it contain onli <num> item , a sub <num> to a sub <num> .
for transact in t2 , it contain <num> itemset , a sub <num> up to a sub <num> .
suppos we set the minimum support , the absolut minimum support is onli <num> , that mean everyth is frequent .
then how mani frequent item it contain ?
let's have a try .
okai .
so , for frequent <num> itemset , we have a1 occur twice , a2 occur twice , up to a50 occur twice .
then a51 occur onli onc , up to a100 occur onli onc .
then , what about <num> itemset ?
actual , <num> itemset is all the possibl combin of a1 to a2 , a1 to a3 , up to a99 to a100 .
you probabl , can see some ar support two , some support onli on .
then , we can go on and on , up to <num> itemset , we get a1 to a sub <num> , the support is on .
it can chop on by on .
so , final , you get a2 , a3 to a sub <num> support is <num> .
final , we'll get a <num> itemset , where support is onli <num> .
if we add all of these togeth , that mean <num> choos <num> , <num> choos <num> , up to <num> choos <num> we'll get a huge number <num> to the power of <num> minu <num> .
that mani sub pattern .
thi is a huge set for ani comput , it's too huge to comput or store .
then how can we handl such a challeng ?
why interest propos is introduc the concept of close pattern ?
what is close pattern ?
we sai , x is close if x is frequent , and there exist no super pattern y with the same support as x .
just give you a simpl exampl .
we look at the same transact databas , like t1 and t2 .
we still sai is minimum support is <num> .
so , how mani close pattern doe thi transact contain ?
so , instead , of <num> to power <num> minu <num> , we actual find onli <num> .
a1 , to a50 support is <num> .
a<num> to a<num>00 support is <num> .
you mai sai , wait , you mai lose someth .
let's look at it , do you know a1 to a49 ?
actual , it's contain here , it must be support of <num> .
do you know a1 to a51 ?
it contain here , it support must be <num> .
so you can see , you do not lose anyth becaus your guard the concret pattern , which ar the maximum close to <num> .
in the meantim , you do not lose the support inform for ani sub pattern .
so to that extent , we can sai close pattern in a lossless compress of frequent pattern .
it doe reduc the number of pattern but doe not lose the support inform .
for exampl , you will still be abl to sai , a sub <num> to sub <num> .
the support is <num> , a5 , a51 , thi item said support is <num> .
then let's look at anoth possibl compress call max pattern .
max pattern , the definit is veri similar , except it doe not carri the support anymor .
that mean x is a max pattern , if x is frequent and there exist no frequent super pattern y , which is a super pattern of x , and we don't care to support anymor .
so the differ from the close pattern is we do not care the real support of the sub pattern of a max pattern .
let's look at the transact databas like thi .
okai .
we still sai , minimum support is <num> , then how mani max pattern doe thi transact databas contain ?
okai .
so , we will find onli <num> .
the reason is , even a sub <num> to a sub <num> the support is <num> .
but , rememb we do not care support ani more .
for thi a<num> to <num> sub <num> , it doe exist a super pattern like thi .
okai .
which is also frequent to that extent is the further compress .
so , you onli get a on pattern , but it is a lossi compress becaus we onli know a sub <num> to a sub <num> is frequent .
but we will not know the real support of a sub <num> to a sub <num> and mani more .
so , you do lose a support inform of mani frequent itemset .
okai .
to that extent in mani applic , mine close pattern is more desir than mine max pattern .
we ar go to see thi in our futur discuss .
okai .
so for thi lectur , we mainli , give you four recommend read .
the first on actual , is a first time introduc frequent pattern and associ rule .
the second on , is the first time introduc max pattern .
the third on , is the first time introduc the close pattern .
and the fourth on , actual is an overview , give you overal , you know , thi field , how the frequent pattern mine came , and how it evolv up to thi stage ?
thank you .
hi , in thi lectur we're go to introduc some effici pattern mine method .
sinc we alreadi know frequent pattern is a veri import concept and mine frequent pattern in larg dataset could be challeng .
so then , we ar go to studi some interest principl and algorithm for mine frequent pattern .
but first , we ar go to introduc you a veri properti of frequent pattern , which is call the downward closur properti of frequent pattern .
let's look at thi simpl transact databas , tdb1 .
it contain onli two transact , t sub <num> and t sub <num> .
suppos we get a frequent itemset a1 to a50 .
then we actual can clearli see all it subset like a1 , a2 or a1 a2 is itemset that ar all frequent .
then you mai wonder , there mai , must be some interest hidden relationship among differ frequent itemset .
actual , there is on call downward closur properti of frequent pattern , which is also call the apriori properti , okai ?
now , let's look at thi .
suppos we know beer , diaper , nut , thi itemset is frequent .
obvious beer , diaper should be frequent as well , becaus ani transact which contain beer , diaper , nut must also contain beer , diaper as an itemset .
that's why the beer , diaper as a itemset should be at least as frequent as beer , diaper , nut .
so we can easili deriv thi properti .
said ani subset of a frequent itemset must be frequent if we keep the minim support ratio as the same .
so in that context , we can deriv an effici mine methodolog .
the gener philosophi is thi .
if you find an itemset s , ani of it subset is infrequ , then there's no chanc for s to becom frequent becaus base on thi apriori properti , then we do not even have to consid to mine s .
thi actual turn out to be a sharp knife for prune .
so , thi apriori prune and , base on thi , it gener quit a lot of scalabl pattern mine method .
so the first apriori prune principl wa discov by rakesh agraw and srikant in vldb <num> .
heikki mannila in kdd <num> workshop also gener a similar methodolog .
the methodolog gener sai if there's ani subset , ani itemset which is infrequ , then it's superset should not even be consid or not even be gener .
base on thi , there ar three major approach develop in subsequ studi .
on essenti is apriori .
the first repres work wa publish in vldb <num> call level wise , join base approach .
anoth method wa develop by zaki et al .
and what thei got , call eclat , is base on vertic data format .
then the third approach essenti is pattern base .
it's frequent pattern project and growth .
pattern growth approach or fpgrowth , develop by us in year <num> .
hi , let's introduc the veri famou apriori algorithm .
thi algorithm is that the first candid gener and test approach for frequent pattern mine .
okai .
it is a level wise candid gener test approach .
initi , the first time you just scan the databas onc to get frequent <num> itemset .
then take thi frequent <num> itemset your go to gener length <num> candid itemset .
the case iter , you ar go to take a length k frequent itemset to gener length k plu <num> candid itemset .
then you go against the databas to test these candid gener and to find the real frequent k plu <num> itemset .
okai .
everi iter you set k to k plu <num> , so you can go until no frequent itemset can be gener or no candid itemset can be gener .
okai .
then after you exit from loop , you just return all the frequent itemset that's deriv , that's the algorithm .
let's look at the pseudocod .
we set c sub k to be the candid itemset of size k .
f sub k to be the frequent itemset of size k .
initi , we just get a frequent <num> itemset , then we get into the loop .
suppos the frequent k item set is not empti , then we get in .
we us k's frequent item set to gener k plu <num>'s candid itemset .
then we go against the databas us the menu support to see , which k plu <num> candid itemset ar frequent .
okai .
we deriv the frequent k plu <num> itemset .
okai .
then we reset k to k plu <num> until we get out of thi loop , we just return all the frequent k itemset for all the k's deriv .
okai ?
let's look at a concret exampl , okai ?
you'll look at thi transact databas , it contain onli four transact .
okai .
the first scan , you just try to find their support for everi singl itemset .
okai .
then we find d , the support is onli <num> , so it's not frequent .
so then we just remov it , we get a frequent <num> itemset and their support .
okai .
then we us thi to deriv the candid <num> itemset c sub <num> , then we scan the again , we find the real support then we find these two blue marker on ar infrequ , we deriv frequent <num> itemset .
then us frequent <num> itemset , we deriv the frequent three candid itemset .
rememb , thi why you probabl can't see a big cut .
why ?
becaus you probabl see a , c , b , c ar frequent .
you mai think a , b , c could be candid itemset , but a , b is not frequent here .
so , a , b , c will not be deriv .
so we onli deriv b , c , e .
okai .
with anoth scan , we find it support two , so it's frequent as well .
then we find all the frequent , on , two , three itemset .
the concret implement actual involv self join and prune .
self join goe like thi .
okai .
we've got abc , abd .
the first two ar the same , the third on is differ .
so we gener thi self join gener thi candid set .
okai .
the similar thing , we've got acd , ac , the first two ar just the same .
the third on , we get them togeth .
we get thi candid set .
but okai , we mai need a prune process .
the prune is pretti simpl .
befor you count against the databas , you proce for abcd .
thi bcd is there , so thi is the candid on .
but for acd , cde it is not in the frequent three itemset .
so acd cannot be the candid , so it's prune .
so that's the simpl wai of self join and the prune , we can solv thi problem .
okai .
the final we get onli on candid set , abcd .
let look at the sql implement of thi candid set gener and test .
so we proce the candid gener is essenti thi .
okai .
you perceiv the self join , how to do self join ?
you will see the first pai minu <num> , thei ar ident .
then the last on , k minu <num> item , <num> is smaller than the other , we get it on candid set .
then for thi candid set , we still need prune .
the prune , essenti to check whether these subset , the k minu <num> subset , contain thi on .
if s is not on f k minu <num> , then we just delet thi candid from the candid set , c sub k .
so thi is a candid gener , the kei step of sql implement of app arrai .
sinc the propos of the famou apriori algorithm there have been mani interest extens or improv to thi method .
let's just examin some of them , 'kai .
so the first line of work is how to reduc the number of pass of transact databas scan .
we know that databas scan involv disc access , which could be quit expens , but if you want to find size ten frequent itemset , like you have to scan thi databas ten time .
that's quit expens to reduc a number of scan , so there ar sever public .
on is call partit method , we're go to introduc , anoth on call dynam itemset count method .
you probabl notic the first author search brand who wa a pg skill in of stanford univers .
the second year , <num> , he occur to propos on of the most famou algorithm page rank and set up a googl compani .
so the , anoth line of the work is how to shrink the number of candid .
you gener veri larg number of candid set in mani case .
how to shrink the number of candid set to be gener could be a big save .
hash method is interest on for mine max pattern .
bayardo pro , also propos prune by support lower bound .
anoth interest method is toivonen , propos sampl method .
the surlin work is to explor some special data structur , like some tree project method us h3 to do h miner , and , hypercub decomposit is anoth interest method .
we're go to introduc a new tree structur call fp tree .
let's look the first on , partit method , partit method guarante you onli need to scan data twice , you know , befor you gener all the frequent itemset , no matter for how mani case it is , so thei observ a veri interest properti core .
ani itemset potenti frequent in the transact databas must be frequent in at least on of it partit , that mean suppos you get a global tr , big databas , big transact databas , you partit them into k partit .
if an itemset x is not frequent ani on of these k partit , it cannot be frequent in thi global transact databas .
why ?
the frequenc .
if you got an itemset x , which is not frequent in the first on , not frequent in the second on , not even , not frequent in the case on , so you add them togeth .
you add all these support togeth you get a global support .
you add all these databas togeth you get a global databas .
you pretti can see if everyon is smaller than sigma time it size , then , the final , the global on will also smaller than six sigma time the size of global databas .
that's why at least on of these databas contain must be frequent .
'kai , so with thi observ , 'kai you can see thi partit method we need onli two scan .
the first scan is you partit databas into k partit databas .
how do you partit them ?
each partit you want them to fit into the main memori .
why ?
becaus no matter how you scan them , you ar not involv in ani databas io .
so that's the reason you can scan onc , you put them in the databas , you can work on it , deriv k frequent item that , for no matter how mani case you can get , okai .
that mean you can deriv all the local frequent itemset , okai .
then base on thi properti , you can do the second scan like thi , okai .
you can sai , sinc ani frequent itemset in on of these patron action can be potenti frequent so the global candid set , is , which is frequent ani on of them , it will be a global candid itemset .
then the second scan just can't against each databas combo count of the global candid set , okai .
then you will get their count , you add them togeth , you will be abl to either deriv global frequent pattern , that's why thi method guarante to scan databas onli twice .
so thi is pretti interest .
anoth method or i'm go to introduc you call di , directli hash and prune .
thi method is try to reduc a number of candid set base on hash and prune techniqu .
the gener philosophi can be like thi , okai .
the observ is if the k itemset is frequent , then in thi hash bucket , it contain sever potenti itemset and must be frequent .
so if the hash count is not frequent , then ani on of them cannot be frequent , the gener philosophi like thi , okai .
why you ar go to deriv the frequent on item set ?
in the meantim , you set of a hash entri for the potenti two item set , but why you said hash entri rather than count each on becaus there could be mani , mani distinct item .
so the frequent two itemset befor you get the frequent on , the candid two could be realli , realli big , but if you set a hansh , hash entri to get sever item , you share on hash entri , the whole hash tabl will not be too big .
then the interest thing is why do you deriv frequent on itemset ?
you also can cut mani two itemset , thei can not be frequent .
for exampl suppos our minimum support can be <num> , then thi on ha onli <num> , thi on ha onli <num> , that impli those item , none of those item can be frequent , becaus thei share thi entri .
their support come thi here , is still below the threshold .
so , onli the second on , thi bd , be , de , the support is quit big .
pass the support threshold these could be thi mai contain the real candid set , okai .
that's a read you can reduc the number of candid substanti us thi hash and prune method .
now , we ar go to look at the anoth interest pattern mine method , is mine frequent pattern by explor vertic data format .
thi is method is call eclat , or equival class transform method .
the gener philosophi look like thi , okai , in origin transact databas is horizont data format in the sens that in everi row , you could connect the id and a set of item in thi itemset entri .
then you can transform thi horizont data format to vertic data format like thi .
for everi item a , you will see which connect id it , a social with thi as item a .
that mean a bought you which transact .
what's the benefit of thi ?
the first thing is you transform the itemset into tid list .
the total size is approxim same .
if everi entri , or everi id thei have the similar number of five .
but the wai to comput thi would be differ with thi tid list .
for exampl , if you sai what is tid list of e , you get <num> , <num> , and <num> .
what is tid list of a , you get a <num> and <num> .
then how did thei arriv ae ?
you just intersect them togeth .
you can see , you intersect thi set with thi set , intersect two tid list .
you deriv thi , the tid list of these ae , thi itemset .
if thi on contain suffici number of transact , that mean it's frequent , like two then ae were be frequent .
if thi on's infrequ then you don't need ae to go further , that's a similar thing as princip .
then the properti of tid list basic sai if these two tid list is equival that mean thei have the same set of transact .
if thi on is the subset of the other on , that simpli sai the transact contain x must alwai contain y becaus of thi on's , thi on's subset .
so you probabl can see , if we try to deriv frequent pattern base on these vertic intersect , we just need to see the size of thi connect list .
but there's on interest method call diffset to acceler the mine .
the reason is that when you get a veri larg number of connect , thi , each item mai be associ with a veri long tid list .
then their intersect , like thi e and the ce , their intersect could be small , but their differ could be small .
so , you , you look at the , the intersect's larg , the differ could be small .
for exampl , you intersect these two intersect , <num> and <num> .
but the differ is onli <num> .
so you onli keep thi , you don't have to keep tce that mai save a lot of space , okai .
thi is the gener idea .
us diffset you can further improv thi effici .
hi , i'm go to introduc you anoth interest pattern mine approach call pattern growth approach .
thi approach is repres by an interest algorithm call fpgrowth .
let's look at how thi algorithm work .
the gener idea is , first , we find the frequent singl item .
and then we partit the de , databas base on each such item .
then we recurs grow frequent pattern by do the abov .
each or recurs for each partit databas , also call condit databas .
to facilit effici process , we us a new data structur call fp tree .
the whole mine can be summar as follow .
we recurs construct and mine condit fp tree .
until the result fp tree is empti , or until it contain onli on path , thi singl path will gener all the combin of it sub path , each of which is a frequent pattern .
let's look at a simpl exampl .
for thi connect databas it ha onli five connect each contain a set of item .
so we can scan the databas onc , find the singl item frequent pattern like the follow .
suppos the minimum support is <num> we will be abl to find .
the fall frequent singl item .
then we can sort the frequent item base on it support frequenc in descend order like thi .
base on thi , we can construct a tree .
but first construct the header follow thi order .
then scan the databas .
we can construct thi tree as follow .
for exampl , okai if you look at the , the first transact , it's f c a b m p , so we construct f c a b m p with onli support as <num> .
then we get f c a b m , we construct thi f c a b m .
and thi power support becom <num> thi need to be <num> .
it , you know , on by on we reconstruct the fall tree .
then to mine thi tree , we can us divid and conquer and do it recurs .
we can do it like thi .
okai , for each such tree , we can construct thi pattern base , or we call p's condit pattern base , m's condit pattern base .
so let's look at how p's condit pattern base is construct , okai ?
so you probabl can see , p .
is the leaf of the note .
you want to look it up .
you get f , c , a , m .
the support is <num> becaus p occur togeth with thi branch onli twice .
that's why you get f c a m <num> .
then for the same reason .
you look at the other p , it cb happen onli onc togeth with it p .
that's why it get cb <num> .
okai .
then if you want to look at m's condit databas , the philosophi you ar think the pattern have m but not .
have p , becaus p all readi been take care by the p's condit pattern base .
so you look at it , m's condit databas .
actual , you also look up .
you get fca .
so for , for thi branch , it'll occur twice .
that's why you get fca <num> .
for the same reason you look at thi m , you get fcab <num> , so you get fcab <num> .
you can do thi on and on .
then with thi you can get it to transform the prefix pass of p .
okai , then the remain task is just mine thi condit pattern base .
for thi condit pattern base , we mine singl item pattern , we construct thi fp tree and mine it recurs .
for exampl , for p's condit pattern base , you will get onli c <num> .
the reason is f , a , m , and b ; thei never occur more than three time , so thei ar out .
thei ar not frequent , you onli get three .
therefor , m's condit pattern base , you can see fca realli occur three time .
the b onli occur onc , you , then you get a fca <num> .
then b's condit pattern base , you probabl can none of them actual pass a support threshold of <num> , so it's empti .
so let's just look at , of cours , you'll , you will get a and a c's pattern base like the , like f's <num> and fc <num> .
actual , for the singl branch , you can dump all the possibl combin there , so for all three .
but if we just , look at the , in the recurs wai , how we can mine thi pattern base , for thi pattern base , you plug in c , you can sai for a , .
condit pattern base you'll get thi .
you'll get fc <num> .
and for c , you'll get f3 .
you can see that's the same thing .
then for thi particular a m condit fp tree you might , you just take a c , you sai c's that mean a m c or c a m .
their condit pattern base is onli f <num> .
you get thi .
then you can , you can dump all the pattern , you know , like thi .
for singl branch , essenti that's the same thing , you'll dump all the pattern .
like all the possibl combin , thei ar all three .
okai .
you probabl can see , if you get a singl prefix pass , you actual can partit thi into two part .
like thi pass , you can mine and thi part you can mine it too .
then you can just concaten the pattern result .
what about thi tree ?
can that fit in the main memori ?
if it cannot fit in the memori , we can us databas project .
that mean we project the databas base on the pattern .
base on singl item set , okai ?
so , then we can construct and mine thi tree for each project databas .
so we can have parallel project , or partit project , two differ method .
the parallel project mean , for thi on , you will get a .
f4 project databas and f3 project databas .
for exampl , for thi first string you'll see g h suppos it's not frequent onli f , f's ar frequent .
you'll get f4's project databas , you get f2 , f3 .
okai , then f3's project base you get f2 , that's a on and everi on of these project databas is independ of the other so you can mine them in parallel , but then you also can have partit project .
project the gener philosophi is for each string , you onli put into on place .
for exampl , thi on contain f <num> , you onli put f <num> or f <num> but you don't put thi f <num> to f <num> project databas .
when you finish mine thi f <num> .
when you do thi project , then you think thi on is relat to f <num> you will put on into f <num> .
okai .
so that's a , just a differ choic .
you know , for differ partit .
or parallel project .
i , it's just differ wai of implement it .
in thi lectur , we're go to discuss how to evalu pattern .
we're go to introduc interesting measur in pattern mine .
especi , we're go to discuss lift and high squar .
that's two popularli us interesting measur .
especi , we're go to introduc null invari measur and compar all those interesting measur .
the first thing we want to discuss is the limit of the support confid framework .
as we know , pattern mine mai gener a larg number of rule , but not all of the pattern and rule gener ar interest .
in gener , we can classifi the interest measur into two class object versu subject .
for object measur , like support , confid , correl ar defin by mathemat formula not chang from person to person , but the subject measur mai chang from person to person , becaus on man's trash could be anoth man's treasur .
so , the first thing is , we realli want the user to sai what do you like to see .
so , it's queri base .
anoth thing is we mai base on user' knowledg base .
try to mine someth unexpect , fresh , or recent , or we can map pattern and row into two dimens space , let user interact pick some interest thing .
so we know we have support and confid as two interest as measur in associ rule .
so we mai be care about thi becaus not all the strong support and confid rule ar interest .
for exampl , think of the follow tabl , it's a two wai conting tabl , to look at thi to see how to interpret thing we found .
for exampl , in thi tabl , <num> out of <num> , <num> student plai basketbal and eat cereal .
but <num> student , thei plai basketbal , but thei mai not eat cereal .
okai .
in thi on , you mai deriv associ rule like thi .
plai basketbal impli eat cereal .
you probabl will get a <num> support , becaus it's <num> over <num> , <num> .
you'll get two third of the confid , becaus you get <num> over <num> , so thi is pretti high support and confid .
and thi rule interest .
let me try anoth rule we gener .
actual , you will see , if we sai not plai basketbal , eat cereal , you will see not plai basketbal is <num> .
but the confid even higher becaus you know , thei have <num> eat cereal out of <num> peopl , so thi on is even higher .
so if you recommend these two rule to the cereal compani , thei will get confus .
thei sai , oh , the first rule said i better give them free basketbal .
becaus if thei plai basketbal you know , eat cereal .
the second rule sai i better take their basketbal awai , thei actual eat cereal even more .
which wai is right ?
let's examin it .
now , let's come down to the veri last session of thi lectur , mine coloss pattern .
anoth name for coloss pattern could be long pattern , or larg pattern .
howev , larg item set .
those kind of idea , or term have been us in earli dai of mine frequent pattern research , so to avoid such confus , we give a new term call coloss pattern .
actual , mine coloss pattern is veri much need in bioinformat , social network analysi , softwar engin , becaus quit often , thei ar go to find pre larg set of gene sequenc , subnetwork , or execut sequenc .
howev , the method we just introduc befor mine onli short pattern , such as length around ten .
'kai .
what's a major challeng for mine long patter ?
actual , the major challeng come from the downward closur properti of frequent patter .
becaus ani subpattern of a frequent pattern is frequent .
if we want to mine a veri larg pattern , suppos a sub1 through a sub100 is frequent , then ani of it subpattern will be frequent .
that mean , we ar go to mind to the power of <num> , such frequent pattern .
thi is a huge number , it's just cannot be done by ani comput no matter time , and the space .
actual , if we want my veri larg pattern , no matter we us breadth first search like apriori , or depth first search like fpgrowth .
if we just go step by step grow from smaller to larger we'll never be abl to even reach veri long , veri larg pattern .
now , let's look at on simpl exampl .
suppos , we have <num> through <num> differ element .
if we copi thi on <num> time , and then remov on by on , like the first transact we remov <num> , the second we remov <num> , the fortieth we remov <num> .
then we form set of transact of the element <num> .
if we add anoth <num> element , suppos , from <num> to <num> element , but we copi it <num> time .
if we sai the minimum support is <num> , actual the number of close or max pattern of size <num> is about <num> <num> .
okai , thi is a huge number .
but there's onli on pattern , realli big , close to size of <num> , which actual is thi <num> through <num> , like thi , but the problem becom , if we want to find thi larg pattern , but we have exponenti number of size <num> pattern .
then no matter we us the fastest pattern mine algorithm like fpclose or lcm , becaus thei have to go through these mani pattern befor thei reach size <num> .
so thei're will never be abl to complet .
howev , we develop a algorithm call pattern fusion .
it will output thi coloss pattern within second .
the trick is pattern fusion .
so , what is pattern fusion ?
the gener philosophi is , we know when we mine pattern , the size <num> pattern , usual small number .
onc you get into size <num> size <num> , a number of pattern will becom more , and more .
that mean you were realli have swamp of mid size pattern befor you can reach a small number of coloss pattern .
if we onli want to mine these small number of coloss pattern , how can we jump out of thi swamp of mid size pattern ?
then the major trick is , suppos we give up the complet set of answer becaus if you sai , your algorithm did not miss a singl on , like you have to finish mine of thi big swamp , but it thi is forbidden .
then , if we have a short cut , or some kind of leap and bound , we can jump out of thi swamp .
quickli we can reach thi small number of coloss pattern , so there is a good chanc we can get it .
actual , we have an interest observ .
it's the , the larger of the pattern , or the more distinct of the pattern , essenti , for the coloss pattern , the greater chanc it will gener mani more small on .
then , if we sai , we can get a collect of small size pattern .
thei mai give a good hint on where ar those larg pattern .
so the pattern fusion philosophi is not crawl but jump .
that mean we can fuse small pattern togeth in on step to gener new pattern candid of veri larg size .
let's look at some exampl you probabl will becom eas .
okai .
suppos , we have a dataset , d , it contain onli <num> coloss pattern like thi plu there could be mani , mani small pattern .
okai .
suppos we get , thi pattern will get support at <num> , thi pattern support at <num> , thi pattern support is <num> , thi pattern's support is <num> .
okai .
suppos , we have these pattern there we want to mine them .
the interest thing is if you just examin the pattern pool of size <num> .
what kind of pattern you mai get , and what could be their support ?
veri like you mai get for exampl , the size a sub2 , a sub4 , and a sub45 , you'll get someth around <num> .
becaus you can see there's a <num> here , but none of them of these pattern will never be abl to gener anyth like thi .
similarli , a sub3 , a sub34 , a sub39 .
now , you will get around <num> , okai .
then similarli , you mai get a sub5 , a sub15 , a sub85 .
you gener support is around <num> , and thi on is also <num> .
okai , if you just watch thi pattern , you will see if you merg those pattern with similar support like around <num> , you'll merg them togeth .
actual , you will be abl to obtain the candid of much bigger size , but thei like indic where ar the two pattern , okai .
it simpli sai , you will be abl to merg multipl small size pattern of similar support to gener candid for veri larg pattern .
such kind of pattern ar the core pattern of a coloss pattern alpha .
that mean thei ar a set of subpattern of alpha .
thei cluster around alpha by share a similar support .
becaus those ar all share similar support , those cluster , we can merg them togeth .
actual , a coloss pattern should have far more core pattern than a small size pattern .
then if we randomli , draw from a complet set of small pattern , like thi pattern size of <num> .
then we would be more like to pick up a core pattern , like these other core pattern .
then the coloss pattern can be gener by merg a set of core pattern .
if we merg those set of core pattern , like your were hint where ar the true pattern ?
now , we introduc some concept a littl formerli .
we can us a pattern of core ratio to denot , such core pattern .
like if you get a frequent pattern alpha , you get subpattern beta .
there's support that mean alpha's support thi data and beta's support thi dataset .
thei should be compar .
that mean if thei have exactli the same support their ration is should be <num> .
otherwis , beta usual is a littl bigger than alpha .
so thi tau repres the core ratio , should be within <num> and <num> , and if thei ar realli cluster togeth , like it is veri close to <num> .
then we can defin d tau robust .
we said a pattern alpha is d tau robust .
if d is the maximum number of item that can be remov from alpha .
'kai , you can remov a lot of thing from alpha , but the result pattern mai still be a tau core pattern of alpha .
that mean thei mai still share a veri similar support .
then , we can prove actual for d , tau robust pattern , it should have , you know , in the order of two to the power of d core pattern .
just think about thi .
if thi alpha is size of <num> , okai , then suppos thi d is <num> .
okai , so that mean , you can remov <num> .
you get a onli size of <num> , mai still have the similar support , then you press c that mean thi it will be <num> to the power <num> , that will be a huge number .
simpli sai , you can see coloss pattern , were have mani , mani core pattern .
okai , that mean we realli have thi properti , thi robust of coloss pattern .
that mean a coloss pattern tend to have much more core pattern than the small pattern .
then such pattern can be cluster togeth to form dens bar .
base on such a distanc .
that mean these two , the number of transact thei share actual , would be veri close , the unit togeth would were also be , you know , veri close , so thi distanc would be veri small .
simpli sai , if we randomli draw from the pattern space , you'll like hit somewher in the ball .
then we come down to the algorithm .
actual , the algorithm base on observ .
the algorithm can be natur deriv , like follow .
initi , we creat initi pool , as we can us exist pattern mine algorithm to mine all pattern up to a small size .
sai , <num> that mean you us frequent pattern mine algorithm i would studi befor combin small size pattern .
then we can get into iter .
that mean at everi iter , we pick up k seed pattern from the current pattern pool .
then for these k seed pattern , we actual would be abl to find all the pattern within the bound ball center at the seed pattern .
base on the seed pattern , we will be abl to find a bound ball .
thei have mani , mani such similar support pattern .
then , these pattern can be fuse togeth to gener a set of super pattern so we can test in the data to see these other real pattern .
and these super pattern gener can form a new pool for the next iter becaus if thei ar real , thei can form new pattern with even larger size .
so when the current pool contain no more then k pattern at the begin of a transact , then you will know you almost finish everyth thei can determin .
so let's look at the experi to see whether thi is the real thing .
actual tri mani dataset .
here is on dataset from leukemia .
there ar two kind of gene , on call all , on call aml .
'kai , and each of them actual , with a veri wide column .
okai .
then , when the minimum support is set realli high , like <num> , the pattern fusion , everyth , get all the largest pattern with size greater then <num> .
you probabl , can see those experi .
that's a real pattern size , and these ar the , what pattern fusion can discov ?
you probabl , can see for size <num> , and over , thei actual , the pattern fusion discov everi rear sect .
'kai .
for the smaller on , thei mai miss someth but thei do not miss much .
so we ar now strive for find veri complet set of pattern .
but we , for the coloss pattern we find most of them .
actual for thi y if you look at execut time on anoth dataset you probabl , can see we compar pattern fusion with lcm , and top k measur , these ar quit effici algorithm in the mine gener pattern .
so you can see , coloss pattern mine method like pattern fusion , even you reduc the minimum support gener more and more , larger and larger pattern .
it cost remain low .
but , if you look at the other two method , onc the minimum support structur goe down .
then you will gener larger and larger pattern .
their execut time go up veri quickli , so that is a solid proof pattern fusion is highli scalabl , especi for mine veri larg pattern .
now , we summar what we have studi in thi lectur .
we have examin a mani differ kind of pattern , and we discuss the effici mine algorithm , especi , we studi mine multi level associ , mine multi dimension associ , mine quantit associ , mine neg correl , mine compress and redund awar pattern .
final , we also discuss how to mine coloss pattern , effici .
so , there ar lot of paper around those divers pattern mine .
here i just introduc sever initi paper , you mai like to go deeper , if you ar interest in thi topic .
thank you .
hi , in thi lectur we're go to introduc you anoth interest direct call constraint base patter mine .
so , what is constraint base pattern mine ?
you see centuri , we do pattern mine but we want to push variou kind of constraint into the mine process .
why we're go to studi constraint base mine ?
becaus , we can see differ kind of constraint were lead to differ prune strategi .
in thi lectur , we ar go to discuss a few typic constraint , includ constraint base mine with pattern anti monoton .
constrain base mine with pattern monoton or constraint base mine with data anti monoton , and also we ar go to studi constraint base mine with succinct constraint , with convert constraint .
final , we're go to discuss how to handl multipl constraint .
let's look at the first issu .
why constraint base mine ?
in datamin , you mai sai , oh , you just need minimum support , minimum confid , or minimum co relat . i go into my data set , find all the pattern .
find all the pattern in the data set .
automat , thi sound to be attract , but is not good .
why it is not good ?
becaus you will get too mani pattern , but the major of those pattern mai not even be interest to you .
okai .
so , we suggest that pattern mine should be an interact process .
that mean , user just to sai , what if you want a mine , us a data mine queri languag , or us graphic user interfac .
then the system will do constraint base mine .
that work give user maxim flexibl , just to sai , what kind of constraint you want to have ?
then , the system will behav like an optim , if we explor such constraint for effici mine , that mean we want to push constraint as deepli , as possibl .
thi is a similar to databas queri process you want to push queri constant , deepli into the queri process process .
but now we want to push constraint into the mine process .
then we look at it , in gener , what kind of constraint we mai have ?
suppos we want to design a data mine queri languag , what we should have ?
okai .
on is knowledg type constraint .
that mean .
what kind of knowledg you want to mine ?
you want to mine classif , or associ , or cluster , or outlier ?
anoth thing is , you want to specifi , what kind of data you want to mine ?
that mean , in a veri larg databas , you mai sai , i onli want to mine the product all togeth in new york store thi year .
so , you , you have , the particular locat , particular time , particular thing you want to studi .
then , you mai also specifi dimens and level , as constraint .
for exampl , you can sai , i want to find those differ pattern in relev to region , price , brand , custom categori essenti is almost you came to multidimension multilevel mine .
then anoth import thing is you want to specifi pattern constraint , or rule constraint .
for exampl , you want to sai , i want to find small sale .
for exampl , those item on sale with a price less than <num> .
but if we want to see , how thei can trigger bigger sale ?
that mean , the total sum of your shop basket will be greater than <num> .
then we also can specifi some interesting constraint , like your set of constraint for minimum support , minimum confid , minimum correl .
see whether those thing can be push deep into your mine process .
so in gener , you can specifi a meta rule like thi .
you can sai , i have two predic .
i want to find , what ar the two predic that mai trigger x to bui ipad ?
then the result mine rule can be if x's ag is within <num> to <num> , if x is a student , like x is go to bui ipad .
so that could be an interest on .
so in gener , you can specifi meta rule .
on the left hand side you mai have l predic , on the right hand side you have q predic .
so , our r predic .
so you want to find method to mine such meta rule .
and , what interest on could be , you first find frequent l plu r predic base on minimum support .
then you can push constraint , if there's ani deepli into the mine process .
you can even push minimum confid , minimum correl , other interest is measur , as deepli , as earli , as possibl .
so thi is what we're go to sta , studi , how to push constraint into the mine process ?
thank you .
for constraint base pattern mine , the first import thing we should know is differ kind of constraint will lead to differ prune strategi .
in particular , we can partit constraint into two categori .
on , we call pattern space prune constraint .
anoth record , data space prune constraint .
for pattern space prune constraint , we're go to studi anti monoton constraint , monoton constraint , succinct constraint and convert constraint .
what is anti monoton constraint ?
that mean if the constraint c is violat , thi further process can be termin .
that mean it's pattern will not need to be examin again .
then for monoton constraint , if these constraint ar satisfi , there is no need to check c again in the subsequ mine process .
what is succinct constraint ?
that is the constraint can be directli enforc manipul the data .
what is convert constraint ?
that mean the constraint can be convert to monoton or anti monoton , if we process the item in certain order .
then for data space constraint , we ar go to di , discuss data succinct constraint and data anti monoton constraint .
data succinct is constraint is data space can be prune in the initi pattern mine process .
and data anti monoton constraint mean , if the transact of the current process that thi transact cannot satisfi , thi constraint t .
then thi transact can be prune to reduc the data process effort .
in the subsequ discuss , we're go to discuss all these constrain , on by on .
now we studi the first pattern space prune constraint call pattern anti monoton .
so the first thing we want to know is what kind of constraint is anti monoton ?
okai .
the definit is if item set s violat thi constraint .
if the situat is you add anyth to thi s , it still violat thi constraint , then thi constraint is anti monoton .
that impli , we will mine s .
you find s violat thi constraint .
the subsequ mine on item set s can be termin , becaus further mine is useless .
let's look at thi first exampl .
the first exampl said , sum of the price of item at s is less than or equal to v is anti monoton .
let's assum v is <num> .
your current item set , your shop basket for thi s , the price ad togeth is alreadi over <num> .
what you can do , the first import thing is you can , for item that mine , you keep ad new thing .
but clearli you add more item into thi s , thi sum of the prize will never be abl to , you know , lower down to below <num> .
that's why thi mine on s , you keep ad new thing is hopeless .
thi should be drop .
then we look at anoth constraint .
suppos we sai , s . profit , we look at their rang is less than <num> .
thi on is still anti monoton .
why ?
becaus suppos your profit rang , it could be plu or minu .
like ab , you know ?
that ab rang suppos thi <num> , then you can see <num> is greater than <num> alreadi .
so , it's alreadi violat thi constraint .
if you add more thing in , no matter posit or neg , can you make thi rang , you know , lower down ?
obvious , even at c , actual the rang will be even bigger , like becom <num> .
so no matter what thing you add in , you can never satisfi thi constraint .
further mine on ab , you try to add more thing in should be termin .
then we look at anoth on .
with a sum of s , the price is greater than or equal to v .
that on is not anti monoton .
for exampl , it still sai , v is <num> .
okai .
then thi item sai , suppos now you add the corn shuck basket is violat thi constraint is less than <num> .
what ar you go to do ?
add more thing in .
and eventu , mayb it will becom over <num> .
that's a reason , even the kind of wise not as violat the constraint .
you still can't add more thing in is possibl you can satisfi these constraint .
so thi on is not anti monoton .
then we look at anoth veri interest on .
thi on sai , suppos a current support count , thi s is less than sigma .
is thi on anti monoton ?
it is anti monoton .
becaus if it is less than sigma , you add more item in .
rememb , you add more item in , you start from two item set , you want to get it three item set , for four item set .
the support will onli be lower , becaus it will be less frequent .
so , if it's current lower than minimum support , you can prune them .
actual , thi on is apriori prune .
becaus apriori prune sai that ani time you , you , you're current item that is below the minimum support .
so you can prune it out , you don't have to mine on it .
so that impli apriori prune principl , essenti is prune with an anti monoton constraint .
let me introduc anoth pattern space constraint call pattern monoton .
you mai first want to know , what is pattern monoton constraint ?
we sai a constraint c is pattern monoton , in a sens if a pattern , item set s , satisfi these constraint .
then ani of a superset also satisfi the same constraint .
then thi constraint is pattern monoton .
how to us these constraint ?
that mean we will not need to check the c as long as thi pattern s satisfi thi constraint , then we're not need to check it again in subsequ mine .
so let's look at the first on .
the first constraint is monoton .
why ?
becaus thi item set is price , sum up togeth , suppos it's greater than the valu at <num> .
then you add more item into thi item at s , the price will onli increas .
so it will alwai satisfi the <num> , so you don't need to check it anymor .
so thi is the first exampl .
the second exampl is , thi item in s , it's minimum price is less than <num> .
then you add more item into it .
the minimum price we're still less than <num> , so it is still monoton .
let's look at third exampl .
suppos thi item set , it's profit rang is greater than <num> , you add more item in .
it's profit rang will still be greater than <num> .
for exampl , ab , their profit rang is <num> .
thei satisfi thi c sub <num> , then you add ani item in the price rang would go , profit rang will go even higher , so you will still satisfi thi on .
so we mai find also mani item set .
thi pattern , mani constraint could be monoton as well .
now let me introduc anoth class of constraint call data anti monoton .
what is data anti monoton ?
okai .
let's look at thi .
the data anti monoton mean you can us thi on to prune the data space , while you do the mine .
in the mine process , if we find a data entri t , that data entri cannot satisfi the pattern p , under these constraint c , then we know t cannot satisfi p's superset either .
thi kind of constraint , c , is data anti monoton constraint .
how do we us it ?
in thi context , we just prune data into t .
then , that whole data space is reduc .
that's why we call thi on data space prune .
let's look at exampl .
we first sai c sub <num> in thi constraint is data anti monoton .
why ?
suppos transact id <num> , we get b , c , d , f , g .
suppos now , we ar in the mine process , we got b , then we look at c , d , f , g .
suppos the c , d , f , g get togeth , okai ?
the price of these b plu c , d , f , g togeth , thi sum of price is still not greater or equal than v , suppos v is <num> .
thi , is still get togeth is less than <num> .
then , thi transact t , will not be abl to contribut to the pattern which satisfi thi constraint .
that's why thi transact should be remov .
so if you remov more transact , the mine will becom more effici .
similarli we can sai thi c2 is , which is , look at the item set , the minimum price is less or equal than v .
suppos thi transact , the minimum price is alreadi violat thi .
mean there's no item which actual the minimum price is , is less than b .
in that sens thi transact would not be abl to contribut to your mine .
the transact can't be prune .
so , that's similarli we can examin like thi s .
suppos thi whole transact , the profit okai , rang is , it cannot be greater than <num> .
then thi transact can be prune as well .
but the subtl thing is sometim the whole transact will be abl to satisfi .
but dure the mine process , the trend actual will not be abl to satisfi that gee .
okai .
let's look at thi process .
suppos we still take thi we want a mine .
suppos we first find a frequent item that is b .
we look in the b's project databas .
in thi case you per can see , to an action <num> doe not contain b .
so it's , it's not in the project databas .
here we have a , c , d , f , h , then c , d , f , g , h , and c , d , f , g .
okai .
the interest thing is , if you look at thi transact databas , you immedi can chop a off becaus a is not frequent anymor if the minimum support is two .
onc you chop a off , actual c , d , f , h , togeth with b , that rang get togeth , actual is , cannot satisfi thi constraint anymor .
okai , then thi whole transact can be remov .
if thi on is remov , h is not frequent in the remain part .
we can remov h .
and then immedi we will get singl branch c , d , f , g two .
okai , that b's fp tree can be prune rather quickli .
and of cours , we can check b , c , d , f , g .
that on , if you look at it , c , d , f , g , you will see that rang , that profit rang actual is greater than <num> , so the whole thing will becom a pattern , you know , return .
so you probabl can see dure the mine process , you can us thi on to chop up mani thing , to make your mine more effici .
so that's data space prune .
now , we introduc anoth kind of constraint call succinct constraint .
so , what is succinct constraint ?
we sai , if a constraint c can be enforc by directli manipul the data , then we sai c is succinct .
succinct can be us to prune both data and pattern space .
let's look at the first exampl .
to find the pattern without item i , on possibl wai to do it is remov i from the databas , then mine it .
that on , essenti is prune those item set or those pattern that contain i .
so thi is pattern space prune .
then , then second exampl , to find those pattern contain item i .
thi on we can mine by onli do the i project databas .
that essenti is if ani transact that doe not contain item i , it can be remov right awai .
thi is data space prune .
the third exampl c3 , thi transact is succinct .
in the context is we can start with onli those item whose price is less than or equal to v .
in the mean time , we also can remov transact with high price item onli .
the first half , we can think is a pattern space prune .
the second part , we can think is data space prune .
rememb , for the same transact c3 we alreadi discuss .
we can sai , thi on is a monoton constraint or data anti monoton constraint .
so the constraint can plai multipl role , it realli depend on how do you us it .
so for exampl four , constraint c4 is not succinct in the sens , becaus these s . price keep increas by keep ad new item .
so there's no wai you can directli manipul data to satisfi thi constraint , so reinforc thi constraint .
so , that's why c4 is not succinct .
now we introduc anoth new kind of constraint call convert constraint .
what is convert constraint ?
some constraint could be tough .
for exampl , thi profit averag greater than <num> thi constraint .
thei're not monoton , not anti monoton .
why ?
becaus , for thi item set s , suppos the current on satisfi these constraint .
you add a veri low profit item insid that would violat the constraint .
but , suppos the current on violat the constraint , you add a high profit item insid , and mai final averag mai bring to higher thei mai satisfi these constraint .
so , that simpli sai sinc we do not know the next item , the profit , valu .
so thi on we just cannot us ani knive to cut thing .
okai , can we still take advantag of some kind of prune ?
on interest wai we can do , is we can sort item in valu descend order .
and we mine the frequent item by us thi valu descend order as well .
for exampl , we know a is the highest profit , then g , then f , so we can get a , g , f in thi valu descend order for the profit .
what would happen in thi case ?
okai .
for exampl if ab , we know thi itemset alreadi violat the constraint c1 , just becaus the averag ab is <num> .
it's not a greater than <num> .
then ab star , simpli sai ab's project databas , will never be abl to satisfi thi constraint , becaus thei , all the remain valu will be smaller .
to that extent , thi c1 is convert into anti monoton constraint .
and so we us the pattern grow in the right order .
that mean what's around thi on is in descend order , we'll be abl to us it , a anti monoton constraint .
what about we sort it in ascend order .
that mean the c first , a the last .
okai .
if that's the case , thi constraint can be chang into monoton constraint .
but monoton constraint , you know , the onli benefit is you don't have to check it anymor onc it is satisfi .
it doe not give you lot of prune power .
that's why we still like to do in thi order in the valu descend order , so it becom anti monoton .
no on can us thi on as a knife to prune a lot of part of search .
but then , anoth question is , can we do item reorder and still us apriori algorithm .
unfortun , the answer is no .
why ?
let's look at thi .
for apriori algorithm , if you want to get agf as three item set , you want all the subset .
that mean everi two item set actual will be abl to frequent and pass the constraint .
then we'll sai agf will , will be possibl be a candid .
but now you probabl can see , agf's averag , the profit averag , actual is greater than <num> .
but gf's averag profit is less than <num> .
that simpli sai even when you get a frequent two item set violat the constraint , the frequent three mai still satisfi the constraint .
so we cannot us apriori prune in thi case .
so , we need to be care how to us differ algorithm to accommod differ constraint push method .
now we studi the veri last session in thi lectur , handl multipl constraint .
sometim a user mai have multipl constraint .
he mai want to enforc those constraint in the mine process .
in mani case , if we have more constraint , it's benefici to us those multipl constraint at the same time in pattern mine .
but the problem could be , sometim differ constraint mai requir potenti differ item order method .
so if there exist an order r , that make both c1 and c2 , these two constraint , convert .
then , which order we should try to sort ?
usual , we try to sort order that benefit the prune most .
for exampl , we can sort order to turn them into anti monoton constraint becaus the anti monoton constraint have more prune power than monoton constraint .
but if there exist conflict order between c1 and c2 , the problem becom we need to sort the data in , in first , on constraint first , then enforc the other constraint when mine the project databas .
'kai .
for exampl , suppos we have two constraint , c1 and a c2 .
c1 sai the averag of the profit of the item set is greater than <num> .
c2 sai the constraint is averag of the price of the item is , is less than <num> .
so , suppos we know c1 mai have more print power , that mean unlik , that everi profit will be greater than <num> .
but c2 ha less print power becaus unlik , the averag price will be greater than <num> .
okai .
so , in that case , we mai , you know , try to us the , we can sort the profit descend order then us constraint c1 first .
then in the project databas , we can sort transact in price ascend order .
why ascend ?
becaus ascend , we return thi on into anti monoton constraint .
similar thing is while we saw profit descend order , is we can turn c1 into anti monoton constraint .
in that sens , we prune the databas in differ project databas .
so these two order to the height stand is not a conflict to ani .
so in summari , in thi lectur we discuss differ kind of constraint mai lead to differ prune strategi .
we discuss pattern space prune with pattern anti monoton constraint and pattern monoton constraint .
we discuss data space prune with data anti monoton constraint .
we discuss how to us succinct constrain in prune .
we also discuss how to handl convert constraint and a multipl constraint , so we can see constraint push and dai mine in pattern discoveri is pretti interest and is sophist .
here ar a set of research paper you mai like to read to deepen your understand on what we have discuss so far .
thank you .
we have learn support and confid , these two measur ar not suffici to describ associ rule .
so the problem becom , what ar addit interest measur good enough to describ the relationship ?
so that's the reason we want to examin it a littl more like lift and chi squar , whether thei ar good enough to describ us addit interest measur .
so lift ha been popularli us in statist as well .
we look at the same tabl .
okai ?
for the same tabl we can think b mean plai basketbal , c mean eat cereal , so we have the exact same distribut .
then for thi condit tabl , we can us lift to comput it .
the lift is defin as thi , b and c ar two item set .
and for rule , b impli c , that confid if it's divid by c support , we get lift .
or we can sai if bc , these rule support divid by b support time c support .
okai ?
so for thi lift , the gener rule is if the lift is <num> , then these two item ar independ .
if it is greater than <num> , thei ar posit correl .
if it is less than <num> , thei ar neg correl .
for our exampl data set , we were calcul lift of b and c , and b and not c , we deriv <num> . <num> and <num> . <num> .
okai ?
then from thi data set and the rule , we probabl can see , b and c should be neg correl becaus the lift is less than <num> .
b and nought c ar posit correl becaus the lift is greater than <num> .
thi actual fix our problem , becaus we know , b and c should be neg correl , b and nought c should be posit correl .
so thi look so veri nice .
let's look at anoth measur , popularli us in statist as , as well , call chi squar .
in chi squar , the definit , we need to calcul the expect valu .
okai , how to calcul the expect valu ?
if we can see , thi <num> is a real valu , it observ valu .
but expect valu is just base on the distribut .
for exampl , c and not c the distribut is <num> over <num> .
thi is <num> to <num> .
among <num> stu , student , <num> to <num> , you will get <num> versu <num> .
in that case we probabl can see , we still can us the popular , the rule like if chi squar is <num> thei ar independ .
if it is greater than <num> , thei ar correl , either posit or neg .
so we need addit test to , to see whether thei ar posit or neg correl .
now for our exampl we can easili calcul chi squar should be almost <num> .
so , b and c should be correl .
further we'll consid thei ar neg correl becaus the expect valu is <num> , the observ valu is onli <num> , it's a less .
so these thing can solv the problem as well .
but the problem becom whether lift and the chi squar ar good in all the case .
let's examin some interest case .
in thi case , you probabl can see thi not b , not c actual is quit big .
there ar <num> , <num> , okai ?
these actual call null transact becaus thei're transact contain neither b nor c .
and if we just look at b and c relationship , we probabl see b and c should be neg correl , becaus it's not easi to get b and c togeth .
b and not c is far bigger , c and not b is also far bigger .
but , if we us lift , we comput the lift of b and c , we will get thi <num> . <num> which is far bigger than <num> .
that's just b and c should be strongli posit correl .
thi seem not right .
even we try to us thi same conting tabl , we add the expect valu , we do the comput , we will find chi squar is bigger than <num> .
in the meantim , the observ valu is far bigger than expect valu .
so , we also should sai b and c ar strongli posit correl .
thi seem to be wrong .
what's the problem ?
actual there ar too mani null transact .
that mai make thing distort .
we need to fix it .
we alreadi seen lift and a chi squar mai not be veri good measur examin the transact data that contain lot of null transact , so what we mai like to see is what ar good measur ?
thei do not influenc much by number of null transact .
let's look at those differ measur .
some measur , thei have the properti call null invari .
that mean their valu mai not chang with the number of null transact , okai .
let's see what measur ar null invari , what measur ar not null invari .
we alreadi know chi squar and the lift , thei ar not null invari .
their valu chang with number of null transact but peopl have found that the fold five measur , if you check their formula , their definit , thei ar actual null invari measur .
so you pretti know , jaccard coeffici and cosin measur quit well .
these two measur ar properli us , thei ar null invari but all confid which actual take the smaller valu amount a and b as the denomin .
the numer just the , the transact support , the support of the rule , so the max confid is try to find the , the maximum on of them .
these two , actual , were propos in the studi of measur associ rule .
kulczynski measur wa propos around <num> by us , by our group .
we , or we can call these as balanc measur but , later , the review actual point out that thi measur wa actual propos by a polish mesh , mention karl kulczynski in <num> so we chang the name of thi measur to kulczynski measur .
let's first look at null invari , why thei ar veri import .
that mean why in analysi massiv transact data the null invari is so critic becaus in mani , mani transact the transact set contain particular set of item the chanc actual is veri rare like walmart transact , thei mai contain neither milk nor coffe .
we will try to analyz milk and coffe us the fall continu tabl , so thi mc mean the number of transact that contain both milk and coffe .
thi not m not c mean the number of transact that contain neither milk , nor coffe , so thi not m not c is the number of null transact , then we see lift and chi squar thei're not null invari , so thei're not good in invari data that contain too mani or too few null transact .
for exampl , we just look at thi , okai , for thi data set , d1 , mc mean number of transact that contain both milk and coffe .
not mc mean the number of transact that contain no milk but coffe .
m not c mean the number of transact that contain milk but not coffe .
not m not c mean the number of null transact that contain neither milk nor coffe , 'kai , so you'll probabl go to , like , a , a walmart , thi kind of shop market .
you'll probabl see , that could be the case is you get ten solvent transact that contain milk and coffe , but on solvent contain not milk , but coffe .
on solvent can have milk but not coffe .
in that case you probabl sai actual like if peopl bui milk , thei will bui coffe as well , becaus there ar <num> , <num> such case but onli on of them is onli on solvent , but if you have a lot of null transact , thi valu could be quit posit .
if you have veri few null transact it turn out thei ar independ .
you could look at the valu thei ar not independ .
on the other hand , if you see there's onli <num> case that got milk and coffe togeth there ar mani more case thei bui it alon , but onc you have mani null transact , it turn out to be veri posit .
the number is quit big , so no matter you get a veri mani null transact or veri few null transact , someth mai go wrong , so we do need to analyz such data us some null invari measur .
and we'll examin thi in more detail .
now we come down to compar those null invari measur .
so which on is better ?
we know , we probabl can sens that not all null invari measur ar creat equal .
so , we want to see which on's better in all the case ?
let's examin the two variabl conting tabl the transact contain milk and a coffe .
let's look at a case in thi data set .
so the first , you look at d1 and d2 .
d1 and d2 you probabl can see the differ ar onli on the number of null transact , but you also can see veri like milk and coffe should get togeth , thei should be posit .
in that sens , you probabl see , all those <num> null invari measur , thei give equal valu in the sens no matter how mani transact on the null part , okai , thei do not chang their valu .
and also , thei ar veri close to on in the sens these ar possibl get togeth .
then you look at d3 .
d3 mean , mc get togeth is quit rare becaus thei get along , actual , it's more frequent .
in that sens , all the other valu ar veri close to zero .
and then you look at d4 .
d4 sai , mc get togeth or mc alon , thei ar all like <num> , <num> , <num> , <num> , and <num> , <num> case .
no matter how mani null transact .
thei actual doe thing right in the middl , thi <num> , <num> , onli jaccard , <num> . <num> , actual thi , in jaccard thi on mean it's balanc is right in the middl .
then we look at three case , it could be d5 and d6 .
d5 , if you see , thi is <num> , <num> case .
so , what you probabl can see is from coffe point of view , like a coffe gui mai sai , oh , mc ar like get togeth becaus thei get along , it <num> case bui coffe but not milk .
<num> , <num> case , thei bui both coffe and milk .
but , for a milk gui , thei probabl sai oh , thei ar veri unlik get togeth becaus i've got <num> , <num> case bui milk but not coffe , but onli <num> , <num> case bui milk and coffe .
so , in that case you look at differ measur .
it's interest , you see allconf and jaccard , thei also sai it's closer to zero .
yeah , unlik get togeth .
but maxconf sai , it's close to on , thei ar veri like get togeth .
then we look at kulczynski said i'm right in the middl becaus the tug of the war each side is ten to on .
and then cosin said i'm a littl prone to unlik get togeth .
now , we chang thi on even more .
thi is <num> , <num> to <num> or <num> , <num> to <num> , <num> .
the coffe gui said thei ar veri , veri like to get togeth .
but , milk gui said that thei ar , veri unlik get togeth .
now , in thi case you probabl can see , allconf and jaccard drop down to <num> . <num> , and even cosin drop down to <num> but in maxconf sai i'm veri confid thei ar veri close to on becaus thei ar veri like get togeth but a kulczynski said i'm still in the neutral becaus thi is <num> to on the other is is on to <num> thei have the equal ratio .
so which on do you like ?
so , probabl we can see d4 to d6 ar the real case that differenti the <num> null invari measur .
but we probabl can see kulczynski measur hold firm when in thi veri imbalanc case , but the ratio is balanc on both side and it hold firm at <num> .
that look interest .
but on the other hand , we also know those case some ar veri imbalanc .
we mai want to introduc anoth measur for imbal ratio .
the imbal ratio is introduc in the sens , the support of item at a and support of item at b .
their differ plai import role in thi imbal ratio comput .
then you proce for the same case in the last of three , the kulczynski measur hold firm , at <num> .
but the imbal ratio , okai , the , the , d of <num> case is <num> becaus thei ar realli balanc .
and d of <num> case becom <num> . <num> , thei ar rather imbalanc .
and , d of <num> case , it is a veri imbalanc .
so , imbalanc ratio realli can show you , you know , how balanc the two side ar .
so , we feel kulczynski plu imbalanc ratio , these two thing get togeth will present a clear pictur for all these three data set , d4 through d6 , becaus d4 is neutral imbalanc .
d5 is neutral but inbalanc and d6 is neutral but veri imbalanc .
final we ar go to show you some real data set , like the dblp data set , we will want to look at coauthor relationship .
so let's look at thi tabl .
thi tabl we got around the year <num> .
we studi the recent databas confer we look at those author .
thei publish paper and thei coauthor paper in databas confer .
but you probabl can see the interest thing is for exampl , you look at han peter kriegel and martin pfeifl .
martin pfieffl got <num> paper , but all of them ar with han peter kriegel .
han peter kriegel got <num> paper , <num> wa with martin pfieffl .
what can , you can see in that case is kulczynski show pretti strong valu .
that simpli sai thei're , these two author ar close ti togeth in some wai .
but thei ar imbalanc , as well .
you probabl can see the imbalanc ratio .
us the calcul , imbal ratio is reason high .
so in that case , you probabl can easili judg han peter kriegel like to be the advisor of martin pfeifl .
so us kulczynski and imbalanc ratio , we can easili find advisor advise relationship and the close collabor .
in on research paper on find advisor advise we ar realli us those measur to find them with realli high accuraci .
so , final , we will show you a bunch of paper .
these ar the paper quit a repres on how to judg the correl relationship .
the differ measur includ the interest measur on the null invari on and the measur we discuss on the kulczynski .
thank you .
we have studi effici method when mine frequent pattern and also studi how to evalu pattern and rule nice .
now we ar go to discuss how to extend our scope to mine variou kind of pattern and rule .
in thi lectur , we're go to discuss how do my multi level associ rule , multi dimension associ rule , quantit associ , neg correl , compress and redund awar pattern and long pattern .
we first discuss how to mine multi level associ rule .
multi level associ rule come down to a veri natur set .
for exampl , to your boss , if you sai , now i find milk and bread sold togeth , probabl everybodi think thi is common sens .
but if you find <num> percent with the brand dairyland , solv togeth with wonder wheat bread , probabl , it becom someth more interest .
but if you see thi dairyland <num> milk is actual sit the hierarchi the top level , it will be milk , then go down to <num> milk , then go down to dairyland <num> milk .
so it is interest to my multi level associ rule pattern .
then , the interest thing becom how to set a minimum support threshold .
on wai is we set a uniform minimum support threshold across all the level .
but , there's on problem .
if you set it veri high becaus thei natur have lower support , the low level pattern will not show up .
but if you set veri low , the high level will get too mani interest pattern , becaus everyth mai show up .
so a reason wai is set level reduc , min support .
that mean , item at higher level us higher level min support , like five percent .
when you go down to the lower level , you mai adopt lower level min support , like on percent .
to that extent , the skim milk will show up at higher level , you know , peanut or some other thing might show up if thei ar not interest , thei ar not frequent at all .
so then , the problem is , if we set a multi level minimum support threshold associ with differ level , then how can we us on scan and on shot , we mine all the differ level ?
the interest thing could be we can us share market level mine .
we can us the lowest minimum support to let the high level pass down to the low level .
but in the meantim , when we analyz rule , when we analyz pattern , we can fill out the high level rule us higher level support threshold .
so , anoth problem for mine market level associ rule is the redund becaus the rule mai have some hidden relationship .
for exampl , suppos <num> milk sold is about <num> <num> of total milk sold in gallon .
then , if you see these two rule , <num> and <num> , the rule on sai milk impli with bread with the support <num> , confid <num> .
but , rule two actual drop down a littl down to from milk to <num> milk .
in the meantim , the support also drop down correspondingli , for exampl from <num> to <num> .
in that case you probabl can sai and rule two is essenti redund becaus we can deriv such thing from rule on , okai .
that mean , if the rule can be deriv from the higher level rule , that lower level rule ar redund we should remov them .
anoth interest thing is differ kind of item inher , mainli differ support threshold .
for exampl , you go to walmart .
you mai see diamond watch or some expens thing that ar more valuabl but sold mayb less frequent , but milk and bread probabl is so veri frequent .
so if we said , minimum support for all kind of item us the same minimum support threshold , then the valuabl item mai be even if you're out .
so , to that extent , it is necessari to have custom min support set for differ kind of item .
okai .
instead of take each item try to decid on a minim support , we can us group base , individu minim support .
for exampl , we can group diamond watch or some expens thing set a low minim support , take milk and bread those frequent set a higher minim support structur .
then , the question becom how to mine such pattern effici .
actual , if we take our previou studi , the scalabl pattern mine method we can easili extend them by ad differ minimum support threshold .
i will now discuss the detail but i think it could be a good exercis .
now we come down to anoth interest issu is mine multi dimension associ rule .
sometim the item or the thing we want to mine actual sit on multi dimens .
the singl dimens rule we studi , for exampl , x bui milk then x bui bread .
milk and bread , those ar from the same kind of dimens call product .
but sometim we mai get a multi dimension associ rule .
for exampl , for thi inter dimension associ rule like , if x ag is <num> to <num> , <num> and if x occup is a student .
then like x go to bui a coke .
that on , actual , ag , occup and item to bui ar three differ dimens .
that kind of rule we call inter dimension associ rule .
then , even we look at thi , sometim we see the hybrid case .
like if x ag is <num> to <num> , if x bui popcorn , like x is go to bui a coke .
that kind of rule , you have h is on dimens , but these two bui ar sit on the same dimens .
okai , so if you see thi kind of case thei like these three differ case mai slightli need differ kind of algorithm .
anoth thing is that the attribut can be categor , can be numer .
for exampl the categor case could be product like endeavor , and you mai have profess like a student to professor .
so for thi for thi categor attribut we can for multi dimens and inter dimens associ rule .
we can insert a data cube to mine such thing or just us data cube structur to mine such associ effici .
then anoth case could be quantit attribut .
so or we can sai those attribut ar numer data , for exampl ag , salari or some other thing .
for these case , quit often we mai us discret measur , cluster measur some kind of gradient approach .
we're go to discuss thi part in the quantit associ rule mine .
thank you .
now , we come down to studi anoth interest issu call mine quantit associ .
what is quantit associ ?
quantit associ mean some attribut , actual numer data like ag and salari .
so how to mine such rule ?
there's on wai , is we can do static discret .
the reason we need to do static discret is , if you do not discret them , you try to parallel everi possibl ag and salari , you will not be abl to find ani interest or sensit rule with suffici support .
but if we try to , sai we partit ag everi <num> year or partit incom everi <num> , <num> us some predefin concept of hierarchi , we're go to be abl to construct a data cube and we're go to be abl to gener some interest associ .
but these fix , predefin concept of hierarchi mai not fit your data distribut .
for exampl , in the univers , like you mai want to partit the ag .
for student you mai sai it's <num> to <num> , <num> to <num> , or someth like that .
but for incom , you mai sai <num> , <num> is on partit , or low and high .
but if you go to hospit , there ag distribut you mai like to sai , you know , middl ag , old , or young .
so , anoth wai is we do cluster base on data .
that mean we take everi dimens , we studi their distribut of the ag and incom , we perform certain cluster algorithm , gener a few cluster .
and then we find the parallel , you know , a frequent pattern of each such cluster pair .
then final there's also popular wai is to do deviat analysi .
that mean instead of do fix interv , we mai do , you know , base on certain condit like gender is femal , we mai find their mean or , you know , like a median or someth , some statist measur .
you will find if the wage mean is substanti deviant from the overal mean , then thi could be an interest rule .
let's go a littl further to see how to find some such deviat .
we also call extraordinari or interest phenomena .
usual for thi , when we mai sai the left hand side is the subset of the popul , and the right hand side is some kind of extraordinari behavior , express us some statist measur which could deviat from the overal .
then , the rule , whether it's true , or is just a , you know , veri except case , we need to do some statist test like a z test to confirm whether such kind of rule is of high confid .
further , in mani case you mai even want to go deeper to get a subset of the popul .
for exampl , not onli look at the gender as femal , but look at the locat as south .
you mai get a , the wage mai further deviat from the overal mean , or even from the bigger or , like a gender is femal .
so that sub rule could becom a extraordinari sub rule associ with it super rule .
for exampl , sometim you do not have the left hand side as a subset of popul , but you can , base on numer data , you can group them into certain interv or cluster .
for exampl , the left hand side could be if the educ ha pretti mani year like <num> to <num> year , and you will find a mean wage actual is , is substanti higher than the mean .
so that mai form anoth interest quantit rule .
effici method actual have been develop to mine such extraordinari rule .
for exampl , on research paper publish by aumann and lindel kdd'<num> is a veri interest case studi of a veri interest algorithm .
now , we studi anoth interest issu call mine neg correl .
so we first need to distinguish rare pattern and neg pattern .
what is a rare pattern ?
rare pattern usual mean there ar some rare occur item .
thei have veri low support but thei ar interest .
you want to catch such pattern .
for exampl , bui rolex watch , how to mine such pattern ?
we previous alreadi discuss thi .
it's for differ item , it's like for those rare item we should be abl to set some individu , group base min support threshold .
that mean for rare pattern , for just those item , we should set a rather low minimum support threshold , then we will be abl to captur such pattern , but neg pattern could be anoth veri differ on .
neg pattern mean , those pattern , thei ar neg correl , that mean thei ar unlik happen togeth .
so for exampl , if you find some custom , the same custom , who bui ford expedit , which is a suv car , and also a ford fusion , a hybrid car togeth , so thei ar unlik to happen togeth , so we call these pattern neg correl pattern .
the problem becom how to defin such pattern .
we mai have on support base definit , like thi .
we sai , if the itemset a and b get togeth , their support is far less than support of a time support of b .
that mean a chanc to get togeth is far less than random , 'kai ?
then we can sai a and b ar neg correl , is thi a good definit ?
actual thi definit mai remind us the definit of lift , then we mai see whether thei work well for larg transact dataset .
let's look at on exampl , suppos a store sold two needl packag , a and b , <num> time each , but onli on transact contain both a and b , 'kai ?
then we will see these two needl packag a and b ar like neg correl but when there ar in total onli <num> transact in your dataset , you mai see support a and b get togeth becaus thei've got onli on time , so <num> over <num> , you get thi number .
thi is a pretti small number , 'kai ?
but then you look at support of a which is <num> over <num> transact , so it's <num> , same as support of b so their product should be <num> . <num> .
so thi number is far bigger than thi .
that mean support of a and b get togeth is far less than support of a time support of b so we can easili sai a and b ar neg correl .
thei ar neg correl pattern , okai , but when thi store sold in total <num> to the power of <num> , that mean <num> , <num> transact .
then suppos all the other doe not contain packag of a nor b , then the situat could be complet differ becaus suppos a and b togeth is <num> over <num> to the power of <num> , but support a now is <num> over <num> , <num> .
so you get a <num> over <num> , <num> .
support b is also <num> over <num> , <num> .
onc thei time togeth , you get a <num> over <num> to the power of <num> .
thi number is even smaller than a and b get togeth .
though you mai sai , oh , a and b get togeth is veri frequent or it's particl relat , actual it's not .
what's the problem ?
the problem actual is null transact becaus there ar so mani transact that contain neither a nor b , thei ar null transact , okai , so we probabl can see a good definit on neg correl should take care of the null invari problem .
that mean when two itemset a and b ar neg correl , thei should not be influenc , okai , whether thei ar neg correl or not , thei should not be influenc by the number of null transact .
okai , now we give you anoth interest definit , which is , kulczynski measur base definit .
that mean , if we want to sai a and b whether thei ar neg correl , what we need to see is a and b ar frequent but the condit , the probabl of a under condit of b , and the probabl of b under condit of a , their averag should be less than epsilon , where epsilon is a small , neg pattern support threshold .
then we probabl can see a and b neg correl can be justifi for our needl packag problem .
we can see , no matter thei ar in total <num> transact or <num> , <num> transact , if we sai , if sum is <num> . <num> , what we can see , thi kulczynski measur base judgment , we can easili see the averag of the condit probabl should be less than epsilon , so thei ar neg correl .
so thi seem to be a veri interest and a good definit , and how to mine them .
actual these ar the method similar to our previous discuss packag mine method .
we'll not discuss it further .
now , we ar go to discuss anoth interest problem , mine compress pattern .
we know frequent pattern mine mai often gener mani , mani pattern but in mani of such pattern mai share some similar , mayb veri scatter but , you know , not so meaning if you gener all those pattern .
let's look at such an exampl , suppos we final get five pattern id , so you perceiv these five pattern , ar like p1 and p2 , thei ar veri similar and their support ar also veri similar , but p2 and p3 , thei ar similar in item set , but their support ar rather differ , can we compress them ?
when we first examin close pattern , actual for close pattern , there's noth that you can compress .
the reason is close pattern requir the support ar ident then you can compress them .
but , none of them have ident support count , so noth can be compress ?
what about max pattern ?
of cours we can us max pattern like p3 to repres all the pattern , but on the other hand , you'll probabl see p3 , thi support is rather differ from all the other so with p3 , you mai lose a lot of inform on the support of other pattern , so the desir output actual is p2 , p3 and p4 .
the reason peopl can see , is p1 and p2 , thei share a similar item set , and also thei share veri similar support count , so we mai need a good measur to see what thing can be compress .
the good measur could be pattern distanc measur .
we us thi definit to defin the pattern busi .
you probabl can look at p1 and p2 in the , when we look at thi .
p1 and p2 , their transact , intersect of their transact id .
what should be the count ?
the number of transact you can intersect , actual a smaller on here becaus everi transact contain p2 must also contain p1 , so their support count intersect should be thi number , 'kai , what about the unit ?
the unit actual sai all those transact , becaus we know all the transact p1 , must also contain the transact of p2 , so their unit number should be thi number , okai ?
in that case , we proce these two number veri close to <num> , then the distanc , <num> minu thi veri close to on number you'll get someth veri close to <num> .
that mean their pattern distanc is rather small .
base on thi we probabl can see we mai be abl to defin a delta cluster or delta cover .
that mean if we can see the pattern , p , the delta cover of pattern p is find those pattern .
all the pattern can be express by thi .
the distanc is within delta .
actual , you pretti see p2 is a good on , becaus p2 essenti can cover p1 becaus p1 just to have a littl less item set , but their support is so close to that extent we mai sai thei ar within thi delta cover .
'kai , so for delta cluster , we'll be abl to cluster p1 , p2 togeth , us p2 to repres the pattern , so that mean if we do thi delta cluster , then all the pattern in the cluster can be repres by on pattern p .
so the problem becom whether we should mine all the pattern , then compress them or we should directli mine these compress pattern .
actual , there's a effici method which can directli mine those compress pattern .
i'm not go to get into the detail , but you mai refer to thi interest paper , okai .
then anoth interest think is redund awar top k pattern , okai , that mean , we want to get a desir pattern , which is similar to the compress on is we want to get high signific and low redund .
these kind of set up pattern .
okai , let's look at thi a , b , c , d , four differ kind of compress .
actual a is a set of origin pattern .
their cluster show their pattern distanc and the , the color the , high color show it's more signific .
the lighter show it's less signific , 'kai .
in that case you probabl can see in thi bigger cluster there ar three pattern .
thei ar quit signific .
if you just do the top k pattern mine that mean you , you take the support count or other signific measur , you will onli find these three pattern .
suppos you want onli find top three , then all the remain pattern like here in the other cluster is complet miss but if you sai , i just do the summar , try to find , you know , cluster , and within each cluster , try to find their center , then you'll veri well find those less signific pattern , so thi mai not be a good balanc .
actual , better balanc is you take care of both signific and the redund simpli sai , you look at thi , thi on .
there is someth veri signific , and thei ar also in the cluster center , you mai want to show these pattern , in the mean time suppos you can onli show three , you mai show these ar signific and less redund .
thi on is signific and also it repres thi cluster , so the problem becom how to develop effici and effect method , find such redund awar top k pattern .
there's an interest studi which us maxim margin signific to measur the combin signific of pattern and develop effici method to mine such pattern .
we ar not go to get into detail of thi method , interest in reader mai read the paper we point out .
thank you .
now we come down to a new lectur call sequenti pattern mine .
so , to studi sequenti pattern mine we're go to cover the follow topic in differ section .
first , we ar go to discuss sequenti pattern and sequenti pattern mine , the concept .
so , the first thing is , we should sai sequenti pattern mine is veri us , ha veri broad applic .
on applic could be in custom shop sequenc .
for exampl , you get a loyalti card for your , you know , shop .
you mai want to see , mayb on custom like go to first bui a laptop , then a digit camera , then a smart phone , within six month .
if thi form a pattern you mai be abl to , to try to do some kind of advertis to other similar custom , or you know serv some new incent for thi custom .
like a medic treatment , form sequenc , natur disast like earthquak happen .
it mai have some sequenc of natur and also human phenomenon .
scienc engin , a lot of thing ar process .
thei evolv along with time .
similarli , stock , market , thei , thei have some kind of durat sequenc .
weblog click stream , call pattern , for telephon , and other thing form sequenc .
even for softwar engin , the program execut from sequenti pattern .
the biolog sequenc , veri , veri us and help analysi like dna sequenc , protein sequenc .
so we'll see , try to get sequenti pattern out of those veri big , vast applic could be veri us and import .
actual , we can distinguish transact databas usual mai not be import to look their time effect .
sequenc databas , thei have time stamp attach with it .
time share databas usual the time .
thing happen actual along the even or equival time interv .
sometim it's veri consecut .
then for sequenti pattern , actual there ar two kind .
on is a gap , anoth is non gap .
the gap pattern mean you do allow to have gap within those pattern .
the non gap pattern mean you will not allow these pattern .
the sequenc everyth is import .
the concept of these ar import if you gap then you have to treat them veri serious .
for exampl , for shop transact , probabl , you don't care , custom in the middl of bui some other thing , so it's not import to studi the gap .
click stream , sometim you mai sai , you know , some click stream you mai care about gap .
some probabl do not care about gap that much .
for biolog sequenc in mani case you do carri gap .
so the protein sequenc or dna sequenc , if you insert mani thing in the middl of the two dna set , sometim you mai complet chang the function .
so , let's look at the custom's shop sequenc as a major exampl to studi how to do sequenti pattern mine .
sequenti pattern mine essenti is , you give me a set of sequenc .
the algorithm is try to find the complet set of frequent subsequ satisfi the certain minimum support threshold .
let's look at thi exampl .
we have a sequenc databas contain like a four custom shop sequenc .
okai , what's the mean of thi ?
we will look at thi particular sequenc .
thi sequenc the parenthes mean thi on is within the same shop basket .
then , after that you get anoth on , ab , that mean thi ab , you know , follow ef , but ab is get togeth at the same time .
similarli , the f get togeth , but follow ab then c , then b , okai ?
that mean each on of these you can think is an element .
it mai contain a set of item , or you call event .
then these on event mai follow anoth on .
the item within the event , the order is not import becaus thei ar in the same shop basket , but for our conveni we can solv them alphabet .
then what is subsequ ?
actual , ani substr within thi on , you probabl can see , here the subsequ , you mai have a gap .
for exampl you sai , you can have a , you have bc , bc actual you would drop thi a .
you can drop complet ac , then you get d .
you can chop on in half , you can get a c .
so , thi on is a subsequ of thi longer sequenc .
then , sequenti pattern mine .
the sequenti pattern essenti is , you , if you set up support .
like , a minimum support is <num> .
that mean at least the two sequenc contain these subsequ .
you find those subsequ .
thi is a sequenti pattern .
for exampl , ab get togeth then c .
in these sequenc thei base thei base thi is a pattern of support , too .
so sequenti pattern mine algorithm is you try to develop algorithm which ar effici , scalabl .
and these algorithm should find complet set of frequent subsequ , we call sequenti pattern .
and also should be abl to incorpor variou kind of user defin constraint .
for sequenti pattern mine , actual apriori properti , the properti we have us in frequent pattern mine still hold .
for exampl , if we sai a subsequ f sub <num> is infrequ , then ani of thi super sequenc can now be frequent .
so that's almost the same idea as apriori .
so base on thi idea we actual can develop lot of algorithm .
on repres the algorithm call gsp , gener sequenti pattern mine develop in <num> .
anoth on is a vertic format base mine or spade , develop in year <num> .
the third on we're go to introduc is pattern growth method call prefixspan , develop in year <num> .
and then we ar go to studi mine close sequenti pattern , call clospan .
final , we're go to discuss constraint base sequenti pattern mine .
so , we have discuss effici algorithm for mine graph substructur .
for exampl , apriori base fsg or pattern growth base gspan .
now , i'm go to introduc a more interest wai to mine graph pattern call mine close graph pattern .
close graph is on , is on repres algorithm .
why we should mine close graph pattern ?
actual thi is , the idea is veri similar to why you want to mine close item set or close sequenti pattern .
an n edg frequent graph mai contain <num> to the power n subgraph base on the simpl apriori principl .
for exampl , if you realli can find a frequent subgraph which the , contain <num> edg , essenti there will be <num> to the power <num> frequent set graph in thi dataset .
okai so , to that extent , it is imposs for ani comput or storag space to handl thi .
to avoid thi combinatori explos problem , the best wai is we onli mine the close frequent subgraph , okai .
a , the definit is in the veri singular spirit as close item set , or close sequenti pattern .
that simpli sai , if a graph g is close , if and onli if there exist no supergraph of g that carri the same support as g .
so conceptu , you can think about thi .
suppos in your chemic compound databas , you find thi rather larg chemic structur .
suppos you get certain support s .
you , you mai find there ar , you know , some subgraph structur in thi databas , which is smaller than thi but is a subset of thi , thei carri exactli the same support .
that impli , actual ani time it carri a littl less structur , actual is a subgraph of thi .
thi on , the largest on , would realli make sens for chemist , for scientist to examin thi compound .
you better just show the largest on becaus thei , thei have the exact same number of occurr of , it's a slightli smaller subgraph .
that mean if you onli show the close graph , it is lossless compress becaus you don't miss anyth .
it's , those non close subgraph .
actual it's hidden as a substructur in thi graph , but thei carri the same support .
there is no point to mine of .
so thi method that we mine the close graph pattern directli instead of first mine all the pattern and just final compress them into thi close up graph .
we call these , is a effici algorithm , the typic on call closegraph , okai .
then , how to directli mine thi on .
we extend the gspan to work out thi closegraph algorithm .
the gener philosophi is pretti straightforward , okai .
suppos we get thi k edg graph , subgraph g .
we want to grow thi g into a bigger graph like k plu <num> edg graph , g sub <num> , g sub <num> , g sub n , okai .
suppos at thi mine , we mai want to check redund .
we don't want to grow ani redund on .
we know we're grow certain order , that's gspan .
but , if both g and g1 ar frequent , but g1 is a supergraph of g .
then if , in the databas , you'll find these two , on is the other on's subgraph , but thi g1 carri exactli same support as g .
that mean , anytim the graph show your dataset show g , actual show g1 the supergraph as well .
then , there's no point .
you grow g in some differ wai to grow it , becaus i , in principl , thi trojan of g , try to grow it and essenti will be cover by grow g1 in , in , you know , instead of grow g in some other wai .
you can grow g1 in some other wai , so that mean you can , you need a lot of redund grow .
actual , be care .
there's some veri special subtl case you still need to grow thi .
those special case , you can set it effici measur to check it , and you onli go those veri special case .
but for the detail special case , i'm not go to lectur here .
you can check the origin close graph paper in kdd <num> .
but the gener philosophi you probabl can see is , except in veri special case , you actual can omit all the remain draw .
so , that's effici algorithm .
now , we want to show you some real test on the real dataset .
we actual took on dataset .
is from nci nih databas is aid antiviru screen compound databas , okai .
you probabl know , to control the aid viru , medic scientist alreadi develop a lot of drug .
thei have lot of complic chemic structur , okai .
so thi dataset actual contain more than <num> , <num> chemic compound , okai .
we want to studi if we want to mine thi what thing we can gener to the common chemic substructur which could be interest to hint thi structur mai be effect .
you , some chemic scientist , again , us thi to group , to develop further some interest drug , okai .
so if you said support , a minimum support is <num> we actual found thi kind of structur .
but if you lower down the support , you will find a bigger and bigger structur like you said to support onli <num> , you will find rather big chemic compound structur , okai .
of cours , we ar interest in set the support rather low we find a bigger chemic compound structur .
if thei ar common , thei ar close graph .
thei probabl will make a lot of sens for , for scientist to studi them .
and then we test thi .
we first , to see how mani pattern we're go to gener .
if you us frequent graph pattern mine , you will gener , like if you said <num> as minimum support , you will gener million of frequent subgraph .
but if you onli look at a close up graph , you will see their order is in the order of thousand .
and thei essenti carri the same inform , thi is lastli is compress .
if you give <num> , <num> chemic structur to scientist , thei probabl will be interest in examin them , even thei contain the same inform if it give them a million .
probabl nobodi will have time to examin them in detail , so thi lossless compress is realli veri interest and effect .
anoth thing is , even you look at run time , okai .
if you mine onli close graph , the run time will be us close graph method .
it would be substanti smaller , okai .
for exampl , if you look at thi , fsg were take multipl high order of a thousand second to get thi .
if gspan were get take about a thousand second to get thi .
but a close graph will take about a hundr second , you can finish mine .
so , that impli not onli will you find a veri effici or , effect set but you also , your run time is substanti reduc as well .
that's why , for mine larg substructur , veri import thing is we should think about mine close graph instead of mine all the frequent graph .
we have just introduc the graph pattern mine method .
so , peopl mai wonder , what's the us of mine such sub pattern , okai ?
of cours , on usag is you mine thi , you can find some interest substructur you can studi .
anoth interest applic , actual , thi mai help you search .
that mean we're go to discuss on interest method call gindex , which is us frequent sub graph to index the graph databas .
we know in mani case the graph , the larg databas mai , which mai contain mani interest , us graph , for exampl , you go to a chemic compound databas , there ar hundr of thousand of differ chemic compound .
if you want to search base on the structur , you mai want to build a structur base index , like a graph index , okai .
if you do a graph queri in such databas , if you have index structur , then the correspond graph , you can fetch them veri effici , and also you , it will help you to do a lot of studi , 'kai , thi is like googl search , right .
googl search is search document , and the graph search is search subgraph .
so you can look at thi .
suppos we have a graph queri , 'kai ?
then you have a big databas , a graph databas to store mani mani chemic compound , of cours you mai not want to index everi possibl structur .
but if thi graph queri contain some structur which is index , then thi index will be abl to grab onli those graph which contain thi index structur out for you to do further match , to see whether thi graph realli contain thi queri graph , you can return it .
thi will help similar search a lot as well .
so index is a veri power tool .
the problem becom what kind of index we should have , of cours , a simpl wai to do index is pass index .
that mean , becaus a graph ha so mani possibl structur , it is quit big number , it's like it explod , exponenti number , so you want to have limit number of index entri .
you mai sai instead of index everyth in the graph , i onli index the path .
for exampl , for thi queri graph , we mai think we mai index thi queri graph us four carbon , like a carbon , carbon , carbon carbon , in thi path , then we can do the search .
that mean the path index mayb you can index on carbon .
you can index two link togeth .
you can index three or four , but if you index in thi wai , even you us the largest graph index like four carbon link like a chain , thi path index , suppos we have a , b , c as three chemic compound , you us thi path , you actual everyon will contain thi path .
you grab everybodi out , but actual , so onli c will match thi graph correctli .
you probabl can see if you index a graph , it doe not help you much .
you would try to grab everybodi out .
actual , it's not effici , not effect as well .
so the best solut is we directli index substructur , that mean on graph instead of index just on the path .
then the problem becom there ar so mani potenti subgraph .
we better onli index frequent substructur instead of index ani possibl substructur becaus you give me a veri larg substructur , like you have <num> ad substructur , you will have <num> to the power of <num> possibl substructur .
it will be too mani substructur to index .
you don't want to do that .
okai , then we mai onli want to index frequent substructur .
why index the frequent on ?
becaus the frequent on is much smaller , and that you index them , you actual will have the overal search , so the interest thing is , if on index them , what is the min support threshold ?
we actual decid us what we call size increas support threshold .
that mean the smaller size , we actual us veri low minimum support threshold .
when the size increas we will us higher and higher minimum support threshold .
why we do thi ?
the reason is , rememb , you get a larg graph .
actual , the if you onli think of the singl vertex or singl edg , thei mai not have that mani , but if you look at a , all the possibl mid size substructur , the , you'll realli reach the communitori stretcher , the statu .
but actual , the larg structur ar like to be cover by smaller substructur .
that's the reason if we want to maintain limit size structur to achiev the same effect , we will want to us thi size increas minimum support threshold to index the graph .
but on the other hand , not onli we want an index of frequent substructur , but we also want to index discrimin substructur .
the discrimin substructur mean if you alreadi have some substructur in your index structur , 'kai , then you get some new subgraph , even thei ar frequent , but if thei ar veri similar to your current on , or cover by your current on , you probabl don't want to pick thi as an index structur .
thi will reduc the index size by anoth order of magnitud .
then , how to select discrimin substructur ?
suppos you have a substructur like f sub <num> to f sub n alreadi select as your index structur , okai , then you get a new structur x .
should x be select into thi on as a new substructur , new index , or we should not select thi on ?
okai .
the interest thing is you see whether your exist structur , base on your exist structur , whether thei can cover your x in the high probabl .
that mean if your x , you have a sever substructur , thei can easili cover by the exist index structur compon , like f sub <num> to f sub n .
then probabl index x will be least benefici , you probabl don't want to pick enough , but if thi structur , these substructur base on their combin to , to predict thi x , to campaign thi x , actual , the probabl is small .
then that mean x is quit distinct , quit discrimin , then thi discrimin structur should be select as a new index entri .
thi is similar , like , suppos we have sever student , thei have differ job skill or program skill , if you find you have those skill in your research group , then you get a new student .
if thi student hi or her skill be well cover by the group of student you alreadi includ , okai , you probabl sai thi student will be less in demand rather than you get a student who ha some except skill .
you mai want , realli want to pick him or her up .
so thi is the similar idea pick up these index structur .
the research paper for gindex , the gindex structur construct us thi frequent and discrimin substructur idea , it is veri compact .
okai , but our experi also show not onli it is small but it is also veri effect .
okai , that mean most the structur you can find them in a veri effici , effect wai and thei ar stabl as well , stabl in the sens , if you add some new chemic compound , new graph into it actual do not need to chang thi gindex structur much , 'kai .
so that's a veri nice , effici structur to us for index graph databas or ani other structur databas .
now in thi last session of thi lectur , i'm go to introduc you anoth interest direct is mine structur pattern in a singl larg network .
i'm go to introduc you , spidermin , thi interest algorithm .
spidermin is try to mine top k larg structur pattern in a massiv network , and we know there ar lot of big network , like in social network in a web , or in a biolog network .
so in mani case you want to mine larg pattern becaus the larg pattern mai impli there is a subcommun .
there ar some interest biolog structur , so it is a great wai to character a larg network .
similar to pattern fusion , if you want mine larg structur pattern in a singl larg network .
so we ar not aim for complet , find everi possibl larg pattern , but we mai have a effici algorithm to find repres , major larg graph , okai .
so spidermin is on of such interest algorithm develop by feida zhu , and the group publish in <num> , call mine top k largest frequent substructur pattern .
and these structur the theorem sai , you , if you take the structur , the diamet is bound by d sub max .
then you ar go to find all such , all of such larg k frequenc substructur with a probabl of at least <num> minu epsilon , and epsilon is a small threshold .
the gener idea from the spider to mine such larg pattern , is veri similar to pattern fusion algorithm we introduc befor .
so , what you can see is , we want to mine larg pattern , but those larg pattern can be grown from the smaller compon , like spider .
that mean if we first mine spider , we try to merg those spider , connect those spider , in mani round of pattern growth .
we will be abl to find rather larg structur in a effici wai .
so what is spider ?
the spider actual is a small structur which , with certain featur .
eh , essenti , we sai r spider is a frequent graph pattern , p , and that pattern , actual there , all , you alwai will be abl to find a vertex u .
from u , you reach all the other vertic within distanc of r .
that mean start from thi , thi u , like , like a center , okai , then you would jump over , r hop .
you ar go to find these frequent pattern , which cover the vertex , vertic of p within the distanc of r , and we consid thi on is a spider .
then thi spider mine algorithm goe like thi .
it's veri similar to pattern fusion you mai think in that wai .
it's initi you us effici graph pattern mine algorithm similar to gspan , you can mine all the r spider of certain size .
that mean r spider could be <num> , you know it could be <num> , you mine these spider .
onc you find these spider , thi big pool of frequent small on , frequent r spider , you just randomli draw m , these r spider .
then , you grow thi m , spider is in iter wai , you iter t iter .
essenti t is half of the d sub max .
that mean you grow thi , you're go to reach to d max , and while grow , you merg those two pattern when possibl .
that mean that thei becom connect and also thei ar frequent .
those pattern which can not be merg , you just discard them .
then keep grow to the maximum size , that mean you keep grow those remain on to the maximum size within thi pool , you will find a return , the top k largest on .
thei ar frequent , and thei ar veri larg .
you just return top k such on .
so then you find the , the top k largest sub structur and you almost find all of them with the probabl <num> minu epsilon .
there's a detail algorithm to prove the correct of the algorithm .
i'm not go get into detail , but the gener philosophi is why is spidermin like to retain larg pattern and prune small on ?
thi is veri similar to pattern fusion .
you start from the small on , you can think these ar the core pattern .
when you grow them , you find new core pattern .
the small pattern ar much less like to be hit in the random draw , becaus thei do not have that mani core pattern .
then , even if a small pattern's hit , it is like to be hit multipl time becaus even thi time , it hit , you get it , you , you , you grow .
you mai find you can not grow anymor , so the next round you grab those small pattern , their descend is like to be hit multipl time .
the larger the pattern , the greater chanc it is hit and save .
that's the reason if , even you think you ar us random draw , you actual were guarante you have high probabl you will find those top k pattern .
i'm not get to the veri detail of thi algorithm , but it , it is an effici algorithm .
you guarante the complet to certain extent , so it's veri interest .
now i'm go to show you some experiment result you will see even for the real network like dblp network .
you will be abl to find some interest pattern .
we actual did some experi with take <num> top confer , nine major comput scienc area and also we focu our studi on <num> southern author in the databas and data mine area .
then we label these author slightli differ us differ label .
if the peopl publish over <num> paper in these area , like databas and data mine area , we sai thei ar prolif .
if thei publish <num> to <num> , we sai thei ar senior .
publish <num> to <num> , thei ar junior .
thei publish onli five to nine , we sai thei ar beginn , okai ?
of cours some , you know , mai not be just the beginn .
but anywai , we label in thi wai base on number of paper publish in the area .
now , we actual do thi mine , the smaller pattern but it's also interest on we found is thi , okai ?
you get four prolif author link togeth like a ring , okai ?
but thei also co author with some senior author .
we actual found like a on is h ctor garc a molina link with je , jennif widom , jefferi ullman and yehoshua sagiv , as a prolif author , thei ar also link to some well known senior author .
and you , we also find like ramakrishnan ha on interest in link ring and also togeth with some some senior author , okai ?
though we find that there ar pattern like thi , you have prolif author , thei link togeth , and thei also link to some beginn , link to junior author , and senior author .
and senior author also link togeth , okai ?
so , we find such pattern like thi it's on interest pattern , instanc .
we also find the larger structur , like a start from on prolif author , link to quit a bunch of senior author .
thei link togeth and thei link to junior author and begin author as well , okai ?
so thi pattern like a , on interest case .
there ar mani such pattern .
on is han peter kriegel link to a lot of author in thi manner .
okai , so you probabl can see from dbrp , the real comput scienc research network , we can find some interest collabor pattern , okai .
so you probabl can see , we cover some interest concept graph pattern and the graph pattern mine .
we introduc apriori base graph pattern mine method like fsg , we introduc a pattern growth base method like gspan , we also introduc method for mine close graph pattern , closegraph .
we introduc an interest applic , build a graph index , a gindex base on graph pattern mine result .
and final , we introduc a , an interest algorithm call spidermin , which mine top k larg structur pattern in a larg network .
there ar mani research paper work on graph mine , graph pattern mine , and we onli list a bunch of them .
these ar the initi paper in the field , and also we introduc them in thi lectur .
if you want to go detail on the techniqu i introduc in thi lectur you better read those paper in some more detail .
thank you .
in thi new lectur , we ar go to introduc on import applic of pattern mine , call pattern base classif .
we know classif is a veri us concept .
so we ar go to introduc to you the basic concept of classif for someon who mai not know much about classif , and then we ar go to introduc what is concept of pattern base classif ?
in particular , we ar go to discuss three kind of method associ with classif , discrimin pattern base classif , and also the direct mine of discrimin pattern .
so , we first discuss the basic concept of classif .
actual , classif , some peopl also call thi supervis learn .
the gener idea is , given a set of train instanc , the train instanc mea , mean some particular dataset that the expert will give you the class label , for exampl sai , thi on is posit , thi on is neg thi is typic for like a , for medic diagnosi , you find certain tissu , then you mai check it .
the expert sai thi tissu is cancer , thi on is benign , okai .
if we got these train instanc , we mai try to wake up certain classif method , okai .
those procedur thei , thei do the machin learn process , thei construct model , essenti go through thi learn process we ar go to get a set of predict model , and onc thi model is valid we can us it for the real classif .
that mean we get a real test instanc or someth you want to classifi , okai .
then you go through thi predict model .
the , these test instanc would be given particular label , like posit or neg .
it's more like check whether a tissu is cancer or not , 'kai ?
so , you probabl can see thi is veri us machin learn and data mine method is you take the train instanc , you construct the predict model .
onc model is valid , it can veri us to predict the new case , 'kai , so there ar lot of classif method introduc in machineri , statist , and data mine research .
for exampl , a popularli us method call support vector machin is try to find the maximum margin so you can , you can partit the differ case in a , in a clear wai so you can get a high qualiti , you know , classifi construct , okai ?
anoth method is decis tree , essenti you try to find differ condit which , you know , can go through to differ branch , certain branch you can clearli predict the class , class label .
the other you mai get addit test final try to get to confirm what is the right you know , class thi particular instanc should belong to .
anoth interest method call network approach , thi is a simul of the human brain idea , you see , we got the input layer , output layer , and some hidden layer .
then , go through thi network you'll adjust the differ weight base on the train data .
so final you train thi network can be us for good qualiti classif , 'kai .
bayesian network is anoth veri popularli us method .
you , base on the bayesian , induct bayesian infer , you will be abl to find the formula , which can be us as classifi when the new case come , you ar go to go through thi network , and final give the right class .
there ar mani , mani more method , we ar not go to introduc here , but we will onli focu on on method , thi pattern base classif .
actual classif ha a lot of interest applic .
for exampl , for text classif , you can us a set of text as train data and peopl can give you some pretti fine class labor , for exampl whether the text belong to literatur and art , or it belong to educ , so you , base on thi , you will be abl to construct us machin learn method , construct a text classifi , or we would sai , text categor system .
then you get some new text by webpag or document .
you go through thi classif system , you ar go to give the right class label .
you can put those class into right categori .
for drug design it is veri us .
you take some train exampl to see whether thi drug is effect for treat certain diseas or it's not , and onc you construct good classif model , then you get a new drug .
you mai be abl to test whether thi drug , like will be effect or ineffect for treat certain diseas .
for face recognit in imag , comput vision and pattern recognit , you mai see you mai get certain input of the face , and it goe through certain pattern recognit system , you will be abl to construct a qualiti face recognit system .
and onc you get the new face , new imag , you will be abl to recogn a person to a certain extent .
anoth popular applic could be spam detect .
it mean you'll get email , the spam detector built in some web server , base on the model construct your , your email come through thei will read the email , and with certain good accuraci to sai thi is not a spam , it should be put in the mailbox .
or thi is definit a spam , you should put it in the the junk mail , or someth not quit sure , you mai put some warn sign , or ask expert to do certain , further evalu , okai ?
from here , you probabl can see , there ar so mani applic of classif , so the import thing is whether we can build up veri power classif method on both effici and effect and scalabl .
so , we're go to studi on classif method , which is pattern base classif in thi lectur .
now , we introduc the concept of pattern base classif .
pattern base classif mean we us the frequent and discrimin pattern to do effect classif .
wh , what is pattern base classif ?
if we can see it's pattern base mine , like a pattern discoveri , thi is on mess knowledg in data mine .
classif is anoth mess knowledg in data mine , mission learn and the statist .
if we effect mine interest pattern , we us thi pattern to serv as good classifi , or help classif process .
that on we call pattern base classif .
philosoph , thi is an integr of both scheme , pattern discoveri and classif , then we work on a pattern base classif .
but why we want to do pattern base classif ?
on interest thing is pattern actual usual is a littl longer , like you know , frequent on itemset , two itemset , to k itemset .
thi is a featur construct process .
we can get , you know , start from singl featur .
we can construct multipl featur togeth , you get higher order , but thei could be discrimin , thei could be compact as well .
just give you a simpl exampl .
suppos in mani case you just give a singl word , ask peopl what it is .
sometim it's veri hard to sai .
for exampl , appl .
i actual do not know whether you mean for appl , whether appl is a fruit or whether appl is a compani , or someth els , 'kai ?
but if you give me a phrase , that mean you combin multipl word togeth , take that on as a featur .
for exampl , you give me appl pie , or you give me anoth on , sai appl ipad , okai .
immedi , i have no ambigu to interpret what do you mean thi appl becaus appl pie , thi appl is a fruit .
but appl ipad , thi appl is a compani .
that's a product , okai .
so in that sens , we sai featur construct construct a higher order featur .
if thei ar veri discrimin , veri tell , it is worthwhil , okai .
anoth import thing for pattern base classif is we will be abl to us complex featur , for exampl , graph .
if you sai what , the graph is rather complic , whether it should be a featur , whether thi nitrogen is a featur , oxygen is a featur , or whether their structur is featur , there ar so mani possibl .
so it's pretti hard to captur the most import featur .
the similar thing happen in sequenc , in semi structur unstructur data like imag or text .
we mai , see , give you on exampl of the graph .
suppos we have three graph , thei ar label , on label as inact , the other on sai activ , the third on sai inact again .
for exampl , for cer , for design certain drug , you will want to find activ compon or activ , activ ingredi , okai ?
then , if you think thi structur is rather divers , it's pretti hard to analyz , what about we do a littl pattern mine ?
suppos we said minimu , minimu support is two ?
okai , then we mai be abl to find two pattern .
on is mark as blue , the other wa mark as red , okai .
with thi structur , we mai be abl to , to sai thi red on is su , su , substructur is g sub <num> .
thi blue substructur is g sub <num> .
then we mai be abl to transform these graph into a small tabl .
the tabl actual you can see , we have two featur , g sub <num> , g sub <num> .
we have on cla , class label as attribut .
we have <num> mean inact , <num> mean activ .
okai , if you chang thi set of graph into a small tabl , which contain just featur and a class label , for classif , thi becom much , much easier task .
that's the reason , if we do pattern base classif , we transform some complic thing into a set of clear discrimin pattern we will be abl to perform more effect classif .
then we're go to di , discuss a littl more on these pattern base classif or some peopl call thi as associ classif .
the philosophi is if we get a set of data , it could be transact data , micro data , sequenc of graph , mani thing , text , if we can find pattern or associ rule we will be abl to construct good classif model .
i'm go to introduc you a few piec of interest work .
for exampl cba , or we sai classif base associ , wa done by bing liu et .
al .
in kdd <num> , their philosophi is to us high confid , high support class associ rule to build classifi .
anoth interest studi , is by guozhu dong and li eh , publish in kdd <num> , call emerg pattern .
thei us those pattern to support chang significantli between the two class , then to do effect classif .
anoth interest studi wa by us , back in <num> at icdm , call cmar .
thi classif base on multipl rule , okai .
the , that mean we , instead of us on rule to make a final decis we us multipl rule .
cpar wa done by us in <num> wa us beam search on multipl predict rule .
anoth interest studi wa done by cong at sigmod <num> , construct a classifi idea call rcbt is to build a classifi base on mine top k cover rule group with row enum , enumer .
thi is for high dimension data like for some bioinformat data .
it show amaz accuraci on classif .
there's also a studi on how to construct a lazi classifi .
that mean for a particular test t , we can predict the train data d on thi t , then mine rule from thi d sub t to predict on the best rule .
final we're go to introduc anoth interest studi is us discrimin pattern base classif to do effect classif , okai .
so thi on is done hong cheng in icd <num> .
those mark in green ar the sever typic exampl we will studi in more detail .
now we ar go to introduc the first pattern base classif method call associ classif .
in particular , we ar go to discuss two method , on call cba , anoth call cmar .
cba , also call classif base on associ , it wa develop in <num> by bing liu's group .
the gener philosophi is , we first mine high confid , high support class associ rule .
what is class associ rule ?
that mean thi associ rule , the right hand side must be a class label equal to a certain class .
that mean for thi attribut , there's a class label .
for exampl , whether it's cancer the diagnosi .
and then , the , thi is a constant , mean , whether it's cancer , or benign , okai ?
then the left hand side is a set of condit and is predic p sub <num> to p sub l .
okai ?
then it , is associ with confid measur and the support measur .
then with thi set of class associ rule , we ar go to first sort those rule , or you can sai rank the rule , in descend order of first confid , then support .
then how to do classif onc these set of rule ar mine , it can be us as a classifi .
then when you get a test case , you get a tupl count , you want to test whether thi on should be posit or neg , you want to get a class label .
you first appli the first rule that match the test case , okai .
if the first rule , becaus the high confid and high support , you will sai , if thi on sai class a , then it should belong to class a .
otherwis you can appli the default rule .
and interestingli thei test mani exampl in some typic data repositori .
thei found thi veri simpl class associ rule base on classifi , can be more accur than some tradit classif method .
such as c4 . <num> , which is a rather sophist interest classif base on tree , you know , it's a decis tree method , okai ?
then you mai wonder why ?
why these high confid associ among these attribut can be to decis tree classifi ?
the , actual , what you probabl can see is , in mani case , the high confid class associ , that mean if thi on is veri high , usual the left hand side will contain multipl associ , multipl condit .
or you can think , you ar explor multipl attribut at the same time .
but it , rememb , mani decis tree algorithm , you ar onli test on attribut at a time to decid which branch you want to construct or to go down .
so thi on attribut at a time , versu multipl attribut you want to examin base on their high confid and high support you probabl can see thi simpl rule mai have it's edg .
now we're go to studi anoth interest classif method call classif base on multipl associ rule , is repres by cmar .
cmar actual us two improv in compar with cba .
on is rule prune .
we actual us some kind of classif base happi tree method .
but whether the rule should be insert into the tree , we first do some test .
the test is given , two rule , r1 and r2 , if the an , anteced of r sub <num> is more gener than that r sub <num> , and also thi r sub <num>'s confid is no weaker than r <num>'s confid , simpli sai r1 is more gener and more power in classif than r2 .
so there's no us of r2 anymor , we just prune thi r2 from the tree .
anoth thing we can prune the rule could be if the rule's an , anteced and if the class labor ar not posit correl , base on chi squar test of it statist signific .
that mean if thi anteced and the class label ar not posit correl , then we better remov those rule becaus it's mislead , it , it cannot deriv good result .
then anoth import thing is why we do classif .
we ar not so simpl just base on sort confid and support .
we mai be base on multipl rule , okai .
just give you a simpl rule , rule for the cmar that's if there's onli on rule that satisfi the tupl x , that mean your current test tupl x , onli on rule can match it .
then we just assign thi rule's class label , that simpl .
but in some case where you test thi x , you mai find multipl rule , or we sai a rule set , actual match thi condit , okai .
in that sens , we look at thi rule set , when not simpl just base on high confid to make a a final readout , we will divid thi s into group accord to it class label .
that mean if it posit , it will go to posit class , if neg , it go to neg on .
then we us weight chi squar measur to find the strongest group of the rule , base on the statist correl of the rule within a group .
that mean , we see whi , amount these two rule set we partit , which on is the strongest on ?
then we just assign , thi x , the class label of the strongest group .
so thi on is a littl more care at decis make , so we find it's quit effect .
base on our test , cmar actual improv the model construct effici becaus we us tree , we us root prune .
and also , it improv classif accuraci .
we compar with cba and c4 . <num> , we show thi cmar ha some multipl perform improv on classif accuraci .
for more you mai like to read those paper .
now we , we're introduc anoth interest pattern base classif method call discrimin pattern base classif .
what is discrimin pattern base classif ?
you probabl see rule base classif , like work out class associ rule , like what cbac market ar quit effect .
but it , in mani case , peopl still like their own classif method .
for exampl , you mai like c4 . <num> as a typic decis tree classif method .
or you mai like support vector machin like svm method .
can pattern base classif help those method ?
actual the answer is ye .
actual we just need to take a discrimin pattern as featur we can feed into ani classifi to improv it classif perform , okai .
the gener principl is we first mine discrimin frequent pattern as high qualiti featur .
then those featur can be appli to ani classifi , okai .
thi framework , call patclass framework is , goe as follow .
okai .
the first is we , to featur construct by frequent itemset mine .
and then base on these construct featur , we do featur select .
for exampl , we mai us maxim margin relev , mmr method , which i'm not go to get into detail .
which is in the paper , and also in mani other refer .
the gener philosophi , we mai us these kind of measur to select discrimin featur .
that mean those featur that ar relev , but minim similar to the previou select on , okai .
that mean , suppos you alreadi have a set of select featur .
if those featur can cover the current featur , then thi current new featur will now becom discrimin , becaus thei do not have to make new contribut .
that mean you want to find minim similar to the previous select on , you will count those featur as new discrimin featur to includ them , okai .
then you will remov those redund or too close correl featur .
that mean you onli find high qualiti , discrimin featur .
then us thi featur , we can do model learn .
that mean , we can us thi featur as a major featur to feed into typic classifi like support vector machin , c4 . <num> , to build classif model .
then you mai wonder why these discrimin pattern mai have certain power .
let's look at typic exampl .
we do a littl detail studi on on typic measur peopl us to core inform gain .
and we will see the pattern length , that mean when we mine frequent k pattern , these pattern length is k , it's not <num> .
we will see why the pattern length is longer , to certain extent , it will have have high inform gain .
actual we us featur score , in some other measur we got a veri similar distribut as well .
now you look at thi , okai .
thi red mark , red line , actual is the singl featur .
and those dot ar the inform gain of the singl featur , okai .
we have three dataset .
actual , we tri mani machin learn we possibl the , the dataset is in those depositori .
like austral , cleve and sonar .
you can see , for singl featur , their inform gain is somehow scatter , okai .
but , if you make the pattern length a littl longer like <num> , <num> , <num> .
immedi , you can see those longer pattern actual carri a lot of inform gain , okai .
even you make the pattern longer to such as , somewhat close to less than <num> , okai , thei still have reason qualiti to contribut to inform gain .
that simpli sai , on those real , real data set , we comput their inform gain , we found the k itemset for k greater than <num> , but often less or equal than <num> .
least discrimin power inform gain , is even higher than that of the singl featur .
that impli if we just us like decis tree or someth , you look at on featur at a time , you mai lose a lot of the valuabl thing .
but those thing can be captur by longer frequent pattern mine , okai ?
so that's on thing we studi , the discrimin pa , power .
then anoth thing is we studi the frequenc power .
the power of pattern frequenc , okai .
we also watch the inform gain distribut , okai .
we still take some data set in machin learn repositori , like austral , breast , and sonar .
okai .
the typic inform gain comput formula appear as follow .
the inform gain for , under condit x , you look at thi inform gain on c .
then thi is uncondit , you get thi inform gain .
and thi inform thi is the entropi , okai .
thi is the entropi of the given data , thi is entropi base on the condit of x .
we call condit entropi base on x .
the inform gain actual is a given data , you comput their entropi , then you comput a condit entropi , and there the condit of x .
their differ is the inform gain .
then you probabl can see , if you stick with veri low support , the inform gain could be low as well .
but you increas the inform , in , increas the pattern frequenc , actual , their gain increas as well .
you probabl can see thi , okai .
that mean we need certain frequenc to captur more inform gain .
of cours , we do not need too frequent pattern , you probabl can see , if the pattern frequenc's too high , thei do not have too much valu .
but on the other hand , thei ar more frequent than the veri low on , thei do contain lot of power .
that simpli sai , we want to captur good inform gain .
we also need to explor frequent pattern as well , okai .
actual the further comput , if you take the deriv on thi inform gain , you do the further comput , you will find inform gain upper bound .
thi monoton increas with pattern frequenc up to certain extent , okai .
so with thi result we ar convinc , both discrimin and frequent pattern actual pretti power in captur the inform gain , or captur the discrimin power , okai .
with thi , we construct the pattern class .
that mean that we first grab , find frequent and discrimin pattern , then we feed into those typic classifi like svm and c4 . <num> .
then what do we get ?
you probabl see , thi is a perform , base on a set of data includ in machin learn repositori in uc irvin , okai .
these ar the set mani , mani peopl in machin learn and data mine us as exampl , okai .
then , we look at svm .
if we just us singl featur , no matter we us differ kind .
like , thi is all thing , these is the frequent set , thi , us rbf theater , okai .
then , you see , thei get a good , like us svm , you get good classif accuraci .
but , on the other hand , if we us frequent pattern .
thi is us all the pattern , thi is us more , select pattern , simpli sai .
if you us frequent and discrimin pattern select base on the process , you get the veri best perform among all the differ possibl classifi .
that mean , for the same svm , if you were us good featur , us frequent discrimin pattern featur , you realli get a better game .
okai .
the same thing happen for c4 . <num> .
of cours , if you look at overal classif accuraci , c4 . <num> mai not be as competit as svm .
howev , if you us frequent discrimin pattern , you still see non trivial gain of the classif accuraci on c4 . <num> .
that mean , frequent and the discrimin pattern could be pretti power to get a better classif accuraci on almost ani kind of classifi .
then we look at scalabl test on discrimin pattern .
we know , mine veri , veri low support will take a long time to mine them .
then what about if we mine reason frequent ?
that mean higher support pattern .
we'll get less pattern , but whether we have much less power in classif .
so you probabl can see , of cours , you see minimum support is <num> , everyth is frequent .
you get too mani pattern , you just cannot finish , okai .
but if you rais the minimum support on thi chess data to <num> , <num> , we still gener <num> , <num> pattern .
it will take <num> second to mine such pattern .
then , you get a pretti high classif accuraci on the chess data for both svm and c4 . <num> .
but on the other hand , okai , even you increas minimum support from <num> , <num> to <num> , <num> , at thi time your number of pattern decreas quit a lot , from <num> , <num> to onli <num> .
and the mine time also reduc quit a lot becaus you find much less pattern .
but even you us <num> pattern , you can sort your classifi by svm and c4 . <num> .
you probabl see , their classif accuraci is still pretti high .
that simpli sai , we don't need that mani pattern , but we do need frequent , and the discrimin high qualiti pattern .
for almost a similar studi on waveform data , you probabl can see , you increas minimum support from <num> to <num> .
the number pattern down from <num> , <num> to <num> , <num> .
the run time for these pattern mine down from <num> second to <num> second , but still the classif accuraci onli decreas a tini littl .
but simpli sai , if you want mine fast , scalabl , actual thi frequent discrimin pattern is also an interest method .
now i'm go to introduc you the veri last session of thi lectur call ddp mine , direct mine of discrimin pattern .
we see the previou mine eh , frequent discrimin pattern look like thi .
you take the data , you first do frequent pattern mine .
you get a pretti larg frequent pattern .
then you do filter process , that mean you us certain good criteria to select the discrimin frequent pattern , then you get a set .
you can us them for classif or feed into other good classifi , 'kai , then on interest observ is the , thi middl sect , frequent pattern , you mine , it could be realli big , whether we can have a method which we just a , transform data into some kind of form like fp tree , then we do , directli mine , directli mine , not a frequent pattern but a discrimin frequent pattern .
that mean we jump over thi big pattern we short circuit thi , thi process , okai , then like we will get more effici mine in mine frequent discrimin pattern .
thi is the idea of ddpmine .
it wa develop by a home trend group in publish in icd <num> .
and let's look at the gener philosophi .
we still take a set of train instanc d and a set of featur f , 'kai .
then we will iter perform featur select process base on the philosophi of sequenti coverag .
look like thi , 'kai .
that mean , iter , it mean everi time we select the , the best featur , the highest discrimin power featur , f sub i , then take thi featur , we will remov the instanc from the dataset .
those instanc ar cover by these , by thi select featur f sub i .
then , in the , anoth iter from the remain dataset you find the highest discrimin power featur .
anoth on , then you remov those dataset to cover the other featur .
you do thi iter , okai ?
so how to implement thi , we can us branch and bound search .
but in particular to make it effici , we can integr thi branch and bound search with fp growth mine togeth .
then also we can iter elimin those train instanc and progress shrink those at the fp tree .
let's look at how thi on is perform .
first , we alreadi know the discrimin power or we sai , inform gain of a low frequenc pattern is upper bound by a small valu , becaus we ar right know the upper bound of the inform gain is , you know , monoton associ with the high frequenc .
that mean , you get more frequent pattern , like you get a more chanc to get higher inform , 'kai ?
'kai .
then if you get a veri low frequenc pattern , unlik those inform gain will be higher .
so it's upper bond by a small valu .
then in the fpgrowth mine we know while we try to mine more and more the prone the condit databas becom smaller and smaller .
when smaller to certain case the inform gain of thi remain part is upper bound by smaller valu .
if it's not avail , you can prune it , okai ?
so that's the philosophi , let's see .
dure thi fpgrowth mine process , we will record the most discrimin itemset discov so far and it inform gain , valu g best .
that mean , when you first see so far , what's the best inform gain , we alreadi discov .
and , what is the associ discrimin item set , discrimin pattern ?
then , we ar go to keep mine befor we construct the fp tree to , to mine thi on , we first check the condit or databas .
that mean under the chronic condit of thi databas , we just look at thi databas to see what is the pop , best possibl inform gain , that mean we can estim the upper bound of inform gain base on the formula we deriv befor .
if thi upper bound valu is smaller than g sub best it simpli sai thi upper bound , the best thing you possibl can get is still smaller than kind of on you alreadi got .
then you better skip thi condit fp tree becaus , oh , you know , there's no point to construct it .
it , it's not avail , but it subsequ tree , if you further do mine on these fp tree , the data , condit of databas will becom even smaller , so their frequent , their support is smaller , frequenc smaller , their gain will be lower , okai .
it's upper bound .
to that extent , we find a knife , we actual can cut thi tree and it subsequ condit databas , okai .
so , base on thi philosophi , we can prune , for exampl in thi pattern gross becaus thi fp tree is upper bound onc you get a certain part of condit or databas like b's condit databas , okai ?
if you find these b's condit databas is even lower , that mean b's upper bound inform gain is lower than the inform gain of a , of it parent , then there is no point to further mine b .
you can prune the b .
all these condit databas out .
that's the reason you can remov part of the mine process .
you don't have to mine the whole condit fp tree , okai .
so , with ddpmine , it is also , it is a featur base approach .
that mean you will mine onli the most di , discrimin pattern .
after thi gener principl , we work out the dppmine algorithm .
now the major thing we claim is effici , we want to compar the run time effici with other compar algorithm .
essenti , we ar go to compar three algorithm altogeth .
on is discrimin pattern base classif by hong cheng we just introduc .
that time we did not give a concret name , and in the later paper ddpmine we give the previou algorithm as patclass .
anoth algorithm of jianyong wang and georg karypi thei publish in <num> call harmoni .
and thi ddpmine is directli discrimin pattern mine , 'kai .
all these three method mine discrimin frequent pattern for effect classif , but the wai thei mine it ar differ , 'kai ?
let's look at the effici .
you probabl can see ddpmine .
the run time is much much lower , even you reduc the support threshold , compar to harmoni , and a patclass , just becaus ddpmine will not mine frequent pattern , but directli get frequent discrimin pattern , so it is veri effici algorithm .
not onli it is veri effici , but it's also veri effect .
it ha high classif accuraci .
we took a bunch of dataset in machineri repositori .
we check the , their respect classif accuraci .
we found that ddpmine almost all the case , is the best accuraci holder compar to patclass and harmoni , 'kai .
to that extent , we realli like thi ddpmine becaus it's effici , and it's effect .
the further extens of thi method actual ha been develop by david lo and hong cheng as well .
thei us it for softwar bug analysi .
the paper wa publish in kdd <num> .
you mai like to read it to get more detail .
so now we summar the whole lectur .
in thi lectur we introduc the concept of classif and pattern base classif .
we introduc two associ classif method , cba and cmar .
we introduc concept of discrimin pattern base classif and ddpmine .
especi , ddpmine is directli mine of discrimin pattern .
it show both effici and accuraci in mine and in classif .
so in summari , i'm go to list a bunch of paper , which we take in thi lectur , re , refer them , and lectur quit a few of them .
but , we did not get into veri detail , if you want to learn more , you should spend time to read and studi those paper .
thank you .
now i'm go to introduc an interest algorithm call gsp .
it's apriori base sequenti pattern mine method .
thi method is pretti simpl , okai .
you start from a sequenc databas , okai .
then you first try to get the singleton sequenc like the first on appear .
that mean you scan databas onc , you count everi singl item it occur in everi sequenc .
then you probabl see a occur <num> time , b occur <num> time .
so if you said minimum support is <num> , like g and h will be gone becaus their support is onli <num> , 'kai .
then you find frequent length <num> subsequ a , b , c , d , e , f .
with thi , you can combin them as candid subsequ .
you mai have aa , ab , ac , ba , bb , bc .
rememb , aa is still import , it mean you first bui a , and bui anoth a .
then , for the , for the shop basket , you mai get ab togeth .
that's why you mai have ab as on event , on element , ac is anoth on element .
you mai get these kind of subsequ .
in total , you will get <num> by <num> .
there you get know , <num> by <num> divid by <num> , you'll get thi number of candid size <num> sequenc .
but without apriori , without thi prune you will get much more , so even thi minim prune still can reduc search space substanti .
so in gener , we can work out the algorithm like thi .
you put thi on into a loop .
st the begin , you can scan to find length k , which is lengh <num> frequent sequenc .
then base on the apriori , you can gener a length <num> or length k plu <num> candid sequenc .
then you can go back to check and find the real frequent sequenc , then you go back to do the apriori base candid gener .
so gener test , you can go into the loop until no frequent sequenc or no candid can be found .
so if you look at the execut sequenc for thi particular data set , you'll find at the veri begin you get a , b , c , d , e , f and g , h will be gone .
thei ar paint in blue .
so then you can , from here you can go up , you get a frequent length <num> subsequ .
then you can gener length <num> , length <num> , len , length <num> , and here you cannot go along anymor , okai .
so thi is the gsp algorithm .
now i am go to introduc you anoth algorithm call spade , which is sequenti pattern mine base on vertic data format .
you probabl still rememb the vertic data format base frequent pattern mine algorithm call eclad .
here for the same set of author , thei actual develop an interest algorithm for sequenti pattern mine .
okai .
the idea is pretti simpl .
if you take the sequenc , you do a littl detail studi , you get a sequenc id , element id , and set of item .
so , what you can see is , for the first sequenc id , <num> .
element id <num> , you find item a .
element id <num> , you find a , b , c so on , okai .
then we can transform thi into vertic format .
that mean we just look at the , where a occur and where the b occur .
so the a occur , you probabl can see , it happen in sequenc <num> , elementid <num> .
in sequenc <num> , element <num> , and also sequenc <num> , element <num> .
so you get thi on the same .
you can get sequenc id <num> , <num> , <num> .
similarli for b , you can find where the b come , is sequenc <num> , element <num> .
so you get <num> , <num> .
then how we can combin them into frequent sequenc , like a then b , or b then a .
if you sai a then b , you , you will be requir a is in front of b , or a's element id is in front b's element id .
that mean for the same sequenc id <num> , if eid <num> is smaller than eid <num> of b , then you get <num> sequenc e ab .
similarli , for b then a , what you get is these element id of b should be smaller than element id of a .
for the same reason you can get all of them for the line <num> , okai .
so for line <num> what you need is , you just get line <num> frequent 1s .
then you do draw .
how do you do draw ?
you probabl can see these element id sequenc id should be the same .
the element id , thi b share with thi b , thei ar both <num> .
so you can join them togeth , you get <num> , <num> , <num> here .
and you join the other togeth , you can get a <num> , <num> , <num> becaus it , element <num> , you get a , you get a b , you can get a again .
so , the two line and you actual can find all of them , okai .
so that's a reason you can us apriori base principl to find all the frequent subsequ .
thi algorithm wa develop by zaki in <num> call spade .
it's sequenti pattern discoveri us equival class .
we have introduc base , sequenti pattern mine algorithm , gsp .
we have introduc vertic format base sequenti pattern mine now we come down to see pattern growth space algorithm , call prefixspan .
prefixspan is a slow space mine algorithm , okai ?
to examin the sequenti pattern in more detail , we need to introduc the concept of prefix and a suffix .
the prefix mean anyth in the front , if it's frequent you want to captur them , as a frequent prefix .
like a , a , a , ab .
then their project becom a suffix .
rememb if you get a a , a , what you see is you got a posit holder for the next on is b .
so that's why we us underscor as a posit holder .
okai .
the similar thing , you get an ab , the posit holder will shift to c .
so that mean , given a sequenc , you will find a set of prefix and a set of suffix , or you can sai prefix base project , okai .
then for thi predict , what we will find is first find length <num> sequenti pattern .
if thei ar frequent , we call them length <num> sequenti pattern .
then we can do divid and conquer , that mean we divid the search space to mine each project databas .
we have a project databas , b project databas , c project databas , up to f project databas .
thi mine method , methodolog , call prefixspan , or prefix project sequenti pattern mine .
so let's examin a littl detail .
for thi sequenc databas , if we find length <num>'s sequenti pattern like thi , then we can actual get length <num> sequenti pattern by first do project databas .
then find length <num> sequenti pattern .
that mean , if thei ar frequent in thi project databas , thei will form length <num> sequenti pattern .
and then we can do length <num> sequenti pattern base project from aa project databas , af project databas .
okai ?
and we can keep thi on ongo .
okai ?
the major strength , or advantag .
there's no candid subsequ to be gener .
and the project databas keep shrink .
so , let's look at some implement trick .
so if you do the project , like you realli take the sequenc to do a , ab .
you will get larg redund subsequ , or you see postfix but thei ar larg redund with the origin string .
howev , we do not need to do the real physic predict .
what do we need is we call pseudo project .
that mean you just sai , what is a's predict base ?
it's the same sequenc , but the posit is number two .
what is ab's project databas is the same sequenc as s , but the posit is number four .
so if you regist the next posit to be scan , it will essenti regist the project sequenc , or suffix .
okai ?
so if the databas can be held in main memori , thi pseudo project is veri effect .
becaus there's no physic copi or suffix .
you onli need pointer to the sequenc .
you just get offset .
you get suffix .
but if it doe not fit in the memori , you us pointer .
you mai involv lot of disk access .
so you mai realli want to do physic project becaus onc you project it quickli thi set can be fit in the main memori .
that mean we can integr physic and pseudo project ani time when the data fit in the main memori , you can us pseudo project .
now we studi anoth interest issu , call mine close sequenti pattern .
the algorithm mine thi is call clospan .
so what is close sequenti pattern ?
similar to close frequent itemset .
the close sequenti pattern s mean if there exist no superpattern s prime and thi s prime and s have the same support , then s is a close sequenti pattern .
in anoth word , close pattern mean for the same support , you will find the longest on .
that's the close pattern .
the , the other on probabl , doe not realli matter , okai ?
for exampl , let's look at thi exampl .
suppos we find three sequenc , a b c with support <num> , abcd with support <num> , and support <num> , and abcd with support of <num> .
which on ar close ?
abcd is close in the sens , for support <num> , abcd is the longest on .
okai .
then abcd is also close becaus the support is <num> , there's no longer <num> .
and <num> , which , you know , is a super sequenc of abcd .
so there ar two wai to mine close sequenti pattern .
on wai is you first mine all of the sequenti pattern .
then you find out which on is close .
like thi , abc , you can knock it down , then you get set of close sequenti pattern .
but thi is not veri effici .
what we want is directli mine close sequenti pattern .
thi will reduc the number of redund pattern to be gener in the middl .
but it will attain the same express power becaus it's a lossless compress .
so there's a veri interest properti .
like thi .
if s is a superset of s1 , s is close if and onli if two project db have the same size .
let's look at thi .
for exampl , for these sequenc db , if you find f , the project databas .
and anoth sequenc is af .
thi is the project databas .
if these project databas have exactli same size , then thi actual mean af is close .
essenti , you onli need to mine on , that mean you can merg them , okai ?
so you will be abl to develop two kind of prune , on call backward subpattern prune , okai ?
thi is a subpattern , you can , you can prune it .
anoth backward superpattern prune , so probabl see .
there you can us superpattern to chop up the , the subpattern you will get into thi , okai .
so with thi spirit we can develop a effici algorithm call clospan , it greatli enhanc the process effici .
i'm not i'm go to discuss veri detail .
for detail , you can read thi paper , it give you all the detail on the clospan .
now we come to studi constraint base sequenti pattern mine .
what is constraint base sequenti pattern mine , which is veri similar to constraint base frequent pattern mine .
but now , the onli differ is now we check the sequenc .
sequenti pattern see , instead of , you know , no other item set pattern .
if , it share mani similar as item that mine .
for exampl , we mai figur out the constraint could be anti monoton .
monoton succinct behavior or properti .
so the first thing is some constraint could be anti monoton .
what doe thi mean ?
simpli sai , if thi sequenc violat a constraint c , then it's super sequenc also violat the constrain c , then we sai , thi constraint is anti monoton constraint .
for exampl , okai .
we look at the first on .
if the constraint sai , some of the price of the sequenc must be less than <num> .
obvious , if your current sequenc , the sum of the price is alreadi over <num> , you further mine it , you add more thing into the sequenc , you'll never be abl to bring the sum of the price down to <num> .
that's why thi on is anti monoton and thi pattern , if it violat the constrain can be toss awai .
similarli for the minimum while we ar not go to discuss it , you just check it .
okai ?
then we look at monoton constraint .
monoton simpli sai , if thi sequenc satisfi thi constraint , it super sequenc also satisfi thi constraint .
for exampl , we look at the first on .
the first on sai , the element count of s is greater than <num> , then you probabl can see , if the count s , you know , satisfi these constraint , you add more thing into s .
the element count can onli grow , so you will still satisfi thi greater than <num> .
that's why thi monoton , onc satisfi these constraint will not need to be check again .
then we look at data anti monoton .
data anti monoton is a knife us to prong some part of the data .
that here is prong the remain of the sequenc data .
now suppos we look at sequenc s sub <num> with respect to your current mine sequenti pattern , s .
then if thi on violat the constrain c3 and thi sequenc can be prone , what thi mean ?
you look at thi , suppos you have a sequenc , your current pattern ad the remain part of the sequenc .
the sum of thi price , violat thi constraint is greater than v .
simpli sai , even you add the remain part of the sequenc into thi pattern , the sum of price still cannot reach v <num> .
then these sequenc will not be abl to contribut to thi s and it further gross .
to that extent , you can prune thi sequenc , s1 , c3 without ani .
so then we look at succinct constraint .
succinct constraint mean thi constraint can be enforc at directli by explicitli manipul the data .
for exampl , if you see thi s , constraint sai , must contain iphon and macair .
okai .
if that's the on , what you can do is you check the constraint as long as it contain , iphon and mac error , these item you'll us thi sequenc .
if thei do not contain it , you can prone those sequenc right awai , becaus you ar not interest .
convert constraint in thi context is tricki most .
the reason is in the past , we studi convert constraint .
these element , these item can be swap .
for exampl , for valu averag s , suppos thi on is itemset is less than <num> , you can start from the most valuabl on .
then get the second highest valuabl on , mine in thi order by , you know , swap the order in thi itemset .
but now in sequenc , you simpli cannot swap the order of the element in the sequenc , becaus there ar in strictli sequenti order .
but what you can do ?
actual , thi on is a littl trickier , you need to revis your proof expand algorithm a littl .
okai .
what you need is you start from the most valuabl element .
but when you mine it , when you do project .
you're not onli go to project the suffix , but you're also go to predict someth in front of it .
that mean you still preserv the order .
you know thi current sequenc , thi current element is here .
but you find the second most on , you steal it for the pointer there .
then you can mine in it valu order , but you still keep the sequenc order .
so that's the reason you need to modifi the algorithm to make the convert constraint c work for in thi case .
so now we said the constraint in sequenti pattern , veri much like constraint in item set pattern .
but actual , there ar some constraint that explicitli involv time .
okai ?
for exampl , order constraint .
that mean some item must happen befor the other .
for exampl , you take algebra and geometri , you see these two cours must be in front of calculu .
okai .
that impli the to order .
these constraint is anti monoton in a sens .
if you find somebodi violat thi order , then you further mine it to get more sequenc .
is here violat thi order ?
so thi sequenti pattern should be thrown awai , becaus it will alwai violat thi constraint .
then we look at a minimum gap and maximum gap constraint .
suppos we have minimum gap is <num> , maximum gap is <num> .
it confin two element in a pattern , then thi constraint is succinct in the context .
you can enforc these constraint directli dure the pattern growth , that's veri easi .
when you find ani element , you just check the next on , the minim gap must be <num> , but the maximum gap is <num> .
you will not check ani further pattern if you cannot find frequent just drop it .
okai ?
the maximum span constraint , basic you're sai , the first element and the last element , their maximum time allow is maximum span , suppos is <num> dai , then thi pattern is succinct .
when you find the first element , you will measur the time is <num> dai .
then beyond <num> dai , those dai you can throw them awai , becaus you can never satisfi thi pattern .
the window size constraint simpli sai previous we said that event in the element must happen in the same time .
now , i allow that the window size could be <num> .
simpli sai , even you happen the next time slot , i still consid you happen in the same element , then i can do mine .
so you have variou wai to merg your pattern into element .
final , we're go to introduc episod and episod pattern mine .
thi is a littl differ from sequenti pattern mine .
in the context , you mai give episod in serial wai and in parallel wai .
for exampl , you can sai a must be in front of b .
that's the serial episod .
or you can sai , a can be in partial order of b .
thei two can be in parallel , these ar the parallel episod .
and you also can give a complic regular express .
you can sai , a and b ar in parallel , then you can see start time and see mean zero two more time , then you get it d , then e .
so you will have someth constrain to the parallel wai , someth in the zero wai .
so thi is a regular express base .
then for thi , we can extend the gsp or apriori like algorithm .
we can solv thi problem or we can us project base pattern growth algorithm .
you probabl mai work out detail for thi kind of method .
and also , you probabl want to see what is the major differ between sai , mine episod and constrain base pattern mine .
i will leav thi on as an interest question for you to answer .
so final , i'm go to summar the lectur we have discuss so far .
we've introduc the concept of sequenti pattern , the concept of sequenti pattern mine .
then we also introduc a set of sequenti pattern mine algorithm , includ gsp , gener sequenti pattern .
we includ spade , vertic format base mine and prefixspan , which is pattern growth method .
veri importantli , we discuss the close sequenti pattern mine and introduc close span algorithm .
final , we discuss the concept of constraint base sequenti pattern and how to do effici mine us constraint .
so we have a bunch of paper .
these ar the initi paper peopl studi the differ sequenti pattern mine method and you know , workout quit interest research paper .
if you ar interest in dig deeper , you mai like to read those paper .
thank you .
hi , in thi lectur , i'm go to introduc you anoth interest topic call graph pattern mine .
so we're go to discuss a few topic , includ the concept of graph pattern and graph pattern mine .
we're go to introduc a few graph pattern mine method , includ apriori base graph pattern mine method , pattern growth graph pattern mine method call gspan .
then we will introduc how to mine close graph pattern , like closegraph .
we will introduc on interest applic of graph pattern mine , graph index .
final , we ar go to discuss how to mine larg structur pattern in a singl massiv network .
we will first introduc graph pattern and graph pattern mine concept .
what is graph pattern ?
actual , we can assum you have a larg set of graph .
for exampl , you have graph g sub <num> to g sub n in the data set , you , you can think these ar mani , mani chemic compound .
the support of graph data set g is defin in thi wai .
thi g is a small subgraph , so how mani such graph contain thi subgraph g ?
and thi g , everi such g bigger graph actual is in your dataset .
that mean , within thi dataset , how mani graph contain thi graph g , then the support of g essenti is defin by the graph contain the subgraph g , how mani of them divid by the whole number of graph in your dataset .
that's a support .
veri similar to how mani transact contain these item .
if we sai a subgraph g is frequent if g's support is greater than minimum threshold min support , okai .
that's also veri , veri similar to item set definit and frequent to pattern , sequenti pattern definit as well .
let's look at some exampl .
suppos we get a bunch of chemic structur .
i just show you there three chemic compound structur .
then you mai find some frequent subgraph pattern .
for exampl , you mai find thi nitrogen and oxygen base on their type of link .
you will find some kind of interest chemic compound structur .
like you find in two set structur on is in b , y is in c .
so you find thi .
you will sai thi is a subgraph whose support is <num> .
actual , you can find even a bigger on .
for exampl , you can find the two nitrogen from a ring structur , then link to oxygen .
so you can see these two nitrogen form a ring structur link to oxygen , you'll find two as well .
so what you can see is , thi two is actual a super graph of on .
but the support you find two out of three , that's why you can claim the support is <num> .
besid mani , mani small subgraph , you try to find their common subgraph structur .
and we mai sometim we mai encount big network , like a social network or , or a web .
you get a singl larg graph or network .
you mai want find frequent subgraph structur insid of thi network .
so thi anoth problem we ar go to discuss in thi lectur as well .
then graph pattern mine ha lot of applic .
for exampl , in bioinformat , we see gene network , protein network , metabol pathwai .
there ar lot of such thing you mai find their frequent substructur .
in chemic informat , you mai find chemic compound structur .
in social network , web commun , tweet , you mai find sub network as a sub commun as well .
even in cell phone network , comput network , you mai find lot of subgraph .
and web , you have web graph , xml structur , semant network , inform network .
you mai find lot of subgraph structur as well .
even in softwar engin , you mai think of the program execut flow , or even you look at the static program , you look at their program structur .
you mai find there ar lot of sub graph as well .
plu , the graph classif , cluster , graph compress , graph comparison , or graph correl analysi , the graph pattern mine we'll form build block for those more complic graph mine task .
final , i should mention , even for applic , for exampl , graph search , graph in lab research , or graph search us index .
we mai us graph pattern mine to build effici index structur , and perform more effect and effici graph similar search .
graph pattern mine ha been studi in data mine commun for quit a while .
okai .
there ar lot of algorithm develop .
if we look at those algorithm , we can classifi the algorithm in differ wai .
for exampl , whether thi method is apriori base or pattern growth base method .
for exampl , fsg is a typic apriori base method , and gspan is an interest pattern growth method .
then we can look at search order .
we want to do mine by breadth first search versu depth first search .
and we can see how the method elimin subgraph .
you can do passiv elimin , that mean you first find someth then you try to get rid of them .
or you can do activ elimin , that mean befor thei gener , you try to elimin them .
and how to do support calcul .
for exampl , whether you want to store those embed , like , store some subgraph or tree or someth , store some embed .
or you want to do onlin dynam comput .
and even for order of the pattern discoveri , whether you want first mine , frequent path , then mine frequent subtre , then you mine graph .
whether thei ar more effici , that's also in consider in the algorithm's develop .
now , i'm go to introduc to you some interest graph pattern mine method .
the first on i'm go to discuss is apriori base approach .
apriori base approach mean we base on the apriori principl , or we call anti monoton properti , of the support .
that mean size k subgraph , if you sai it is frequent , then if and onli if all of it subgraph ar frequent , okai .
in that sens , if we sai we want to grow size k plu <num> edg or vertex subgraph .
what do we need is , just us candid gener , is we just , sai , start from size k ani two size k graph , like a k edg or k vertic .
these kind of subgraph , we just grow base on these two subgraph .
thei share some common thing .
for exampl , if you get a size k or k edg subgraph , thei ar frequent .
you'll get a g and g prime .
then if g and g prime , thei share k minu <num> common edg .
okai , that mean thei , thei have onli two edg not in common .
then you can merg them togeth .
you gener k plu <num> edg subgraph .
for thi k plu <num> subgraph , k plu <num> edg subgraph like g sub on .
what you'll still need to do is you can do candid elimin by prune if ani of these g sub <num> k edg subgraph is not in thi frequent set .
okai , then you can clone it becaus base on the apriori , it's imposs thi on can't be frequent .
so , you can think thi is iter mine process .
you took candid gener by merg those graph if thei share k minu <num> common edg .
okai , then you do candid prune becaus someth you know is not frequent if ani other of the c <num> stai at subgraph is not frequent , you can prune it .
then you can do support count .
that mean you count thi g sub <num> , how frequent it realli is in your graph data set , okai .
then if it's not frequent , we can elimin them .
then you can go back to the loop again .
so , thi is a typic , we call , apriori base .
you grow on vertex , grow on edg in differ wai .
you can gener bigger and bigger graph pattern .
so you can see , the interest thing is onc you see these subgraph , if you merg the post you get two subgraph .
thi part is in common .
then you have d and e , two vertex .
okai , of cours , you also can son , consid edg when you grow it .
suppos we had get it , two d and e , two vertic .
what you can do is you can grow in mani differ wai .
for exampl , you can constructor d and e separ but thei all link with thi edg .
okai .
or you can think becaus thei ar all c , we can link in the other on .
like link anoth c , or link in diagon c , you can link thi , thi c .
so you have multipl wai , even you look at thi structur , you have sever wai , you can go into subgraph structur .
so , it is definit more complic than the item set mine , okai , or the complex would be higher , okai .
so , the first such algorithm develop call agm develop by a set of japanes research led by inokuchi , publish in year <num> .
call agm , or you can think is apriori base graph min , mine , okai .
so , thi on is everi time you grow on vertex at a time .
but you also can grow on more edg at a time .
thi algorithm develop by kuramochi and karypi in the univers of minnesota .
publish in <num> call fsg , or you can think thi frequent subgraph .
so , the perform studi done by thi fsg paper show that edg grow is more effici than vertex grow .
you probabl also can imagin becaus the , thi grow the edg is a littl bigger compon compar to the vertex .
after thi , there ar a bunch of algorithm .
for exampl , whether you want to store those subchur , substructur , the frequent substructur befor you , you , you try to grow them , mayb the substructur can be share by other process , okai .
there ar more paper where go to introduc you , but you can just read the , the book and some survei paper on it .
now , i'm go to introduc you the pattern growth approach for graph pattern mine .
especi , i'm go to introduc you first of such hour in core gspan , or i think it's a graph span .
rememb we have prefix span .
now , we come down to gspan .
the gspan is a pattern growth approach .
the idea is , instead of do wen research like apriori base , you have to gener all the k edg subgraph befor you go to k plu <num> .
now , we don't do that .
we just take the k edg you gener anyon , k edg graph .
you just look at it , you know , for thi kind of k edg graph , which addit edg ar still , you know , frequent .
and you take that on , add on more edg at a k plu <num> edg .
and then i , see in the data set , which address associ with k k sub <num> is still effici to a grow that edg wh , i get k plu <num> edg .
okai , that mean you realli can do depth first search instead of breadth first search .
but the major challeng of thi method is , sinc on graph , you can go from small to thi subgraph in mani differ path , mani differ wai .
so , if you do not have a control , you will genet , gener a tremend number of duplic on graph .
that mean , you actual have to check if you have mani differ wai , you want to check each wai befor you , becaus you don't want to miss anyth .
then you will gener tremend duplic , okai .
so the wai , actual , try to control it us pattern growth approach is , if we can defin an order to gener such graph , that mean we forc them to gener onli in on order , rather than gener in mani differ order .
that wai we will reduc of lot of duplicatoin , okai , then we can gener an effici algorithm , okai .
that wai we do is , we were think about how to gener the depth first search span tree .
that mean , whether we can flatten the graph into a sequenc us some depth first search method .
that mean we onli allow you to grow in that wai .
third method creat wa by xifeng yan and me , publish in icdm <num> , call gspan .
with gspan algorithm , the major philosophi is try to control the order of growth .
suppos we got a graph like thi on .
thi on we do not know to grow and there ar mani wai to grow , okai ?
what about we first defin an order .
suppos the differ node like vertex , you sai , thi is <num> , <num> , <num> , <num> , <num> , okai .
if you defin thi , then what we do is , we sai we grow in order we call right most path .
what is right most path ?
it's everi time you start from the lowest on , like <num> .
you start to grow .
but when you grow everi time you have multipl wai to grow , you will alwai choos the smartest index at everi step .
for exampl , <num> , you can grow to <num> .
you can grow to <num> .
you can grow to <num> .
which wa , smallest on is <num> .
so we grow from <num> to <num> .
of cours , from <num> to <num> you have onli on wai to go .
you , you go from <num> to <num> .
but at the <num> , you still have two differ wai to go .
you can go to <num> , you can go to <num> .
if we sai you have to go to the smallest on first , then you get a <num> , you got to <num> .
you can see thi <num> goe to <num> .
okai , then you go from <num> , go to <num> .
and from <num> , you have onli on wai to go , you go to <num> .
then thi , you don't have other graph except to <num> to <num> .
that's why you go from <num> to <num> .
if you grow thi wai , you almost think your graph grow is like a sequenti thing .
from the graph , you think thei ar essenti a sequenc .
then to that extent , you reduc the gener of ani duplic graph , but you still get a complet travers .
essenti , the enumer of the graph us thi right most path is complet .
then , the interest thing is you can flatten the graph into a sequenc us the depth first search .
you'll get a df code .
then you can essenti mine your graph like a mine set up sequenc , so you can gener veri effici algorithm , okai .
i'm not go to introduc you veri detail algorithm , but you know thi is the gener spirit .
after you follow thi spirit , you can work out gspan even by yourself , or you can read the paper .
so it is an effici algorithm .
in thi new lectur we ar go to explor pattern mine applic .
instead of discuss all kind of applic , becaus there ar so mani , we will focu onli on wang in thi lectur .
there's frequent pattern mine for text data , especi we're go to focu on phrase mine and topic mine measur .
we will discuss three strategi .
on strategi call simultan infer phrase and topic .
that mean , while we ar do topic model we actual do simultan work on the phrase as well .
strategi <num> sai , post topic model phrase construct .
that mean we first do topic model , think the document is bag of singl word .
then , after thi for each topic model we try to construct phrase .
then , for third strategi , is we first do phrase mine then we do topic model .
'kai .
so , for these strategi , we're go to discuss in some depth .
but for other applic , we put some applic to lectur <num> .
some oth , other applic like index , or some other thing we alreadi discuss in other lectur .
now we look at the first session , eh , about frequent pattern mine for text data as a gener introduct .
why we want to do fre , frequent pattern mine is for text data , essenti , is we want to do phrase mine and topic model .
what is phrase mine ?
what is topic model ?
okai .
topic model essenti is you look at mani , mani differ document for thi corpu .
you try to work out how mani topic or you can sai , focu the discuss on certain thing .
there , there could be multipl topic in thi set of document , 'kai , for these document , for these topic , each topic you get a distribut of word .
if thei ar singl word , thi consid as unigram .
if thei ar mixtur of a unigram and marketgram , usual thei call these as n gram , or you can sai phrase .
okai ?
then , if you just read suppos that get machin learn as on topic , but you'll read thi , thi distribut of singl word , sometim you mai get it confus .
for exampl , machin could be anyth you know , it could be hardwar machin , softwar machin , mechan machin the vector could even belong to mathemat like linear algebra .
and , there ar mani , mani other thing can be , you know , consid almost for each singl word .
but if we gener the phrase , for exampl , learn , support vector machin , reinforc learn , most peopl were easi to recogn thi topic actual is realli machin learn , and those phrase ar veri , veri repres term from machin learn .
so that give us , you know , less ambigu , is more meaning concept for machin learn .
okai .
there ar mani strategi for phrase base topic model .
the first strategi is we try to gener word and token in topic model at the same time .
okai , thi kind of measur includ bigram topic model , topic and gram model , and face discov topic model .
okai , then anoth strategi second to on , call post bag of word model , model infer , visual topic with n gram .
that mean you first do model construct , topic model , us bag of word model .
it simpli sai you treat everi singl word as an individu on .
okai ?
then you do topic model .
after that , you try to combin those word in the same topic .
you try to visual those topic with phrase , with n gram .
these measur includ label topic , turbotop , and kert , those ar measur .
the third strategi is , we first do , we call prior bag of word model infer .
that mean we first try took , to phrase mine , construct phrase , okai .
then we try to impos those phrase constraint to bag of word model , essenti we do phrase base topic model .
thi on is done in , it's go to be publish in to some <num> topmin algorithm .
we ar go to discuss and compar those method .
in thi session , we're go to discuss anoth import issu .
nbsp ; pattern mine and societi .
especi , we will be focus on the privaci issu in data mine .
we know , we ar live in a data rich or big data societi .
thi of cours creat , you know , creat opportun for a success data mine .
but on the other hand , peopl do have concern whether pattern mine mai violat peopl's privaci or secur .
okai .
actual , data mine mai natur have potenti advers side effect is compromis the privaci .
the privaci and accuraci ar typic contradictori in natur .
that mean , if you want your pattern mine veri accur .
in mani case , the privaci mai be compromis .
but if you want to protect your privaci in a veri tight manner , then the accuraci of the pattern mine mai suffer .
okai .
so the problem is we need a new technolog to privaci preserv data mine .
we ensur the data mine will achiev good result .
but in the mean time , we're go to protect peopl's privaci .
there ar lot of research work on thi issu .
we call privaci preserv data mine .
okai .
the studi gener focus on three categori on privaci .
on , we call input privaci or you can think about data privaci .
essenti , how do we hide data ?
we can distort data , we can hide data to prevent data miner from reliabl extract confidenti or privat inform .
anoth categori of privaci is output privaci or we sai , knowledg privaci , knowledg hide problem .
that mean , where the input data mai be okai , but onc we find pattern , certain pattern or knowledg could be sensit .
you mai not want to disclos it .
so how we can guarante ?
we do the mine , we can ensur the output privaci .
okai .
thi is anoth interest issu .
the third issu peopl studi call owner privaci .
that mean the peopl , ani parti mai host certain data .
for exampl , differ hospit mai hold differ kind of patient inform .
you mai want to studi , for exampl , aid or heart problem or lung cancer or those problem .
okai .
but you mai not want to see where the data come from .
that mean , you actual want to hide the data owner or the hospit hide the sourc of the data , then you can do more , you know , studi .
thi we call owner privaci problem .
so , in thi short session , we ar go to focu on how to ensur input privaci or you can sai , data privaci problem .
okai .
of cours , on simpl approach could be you trust servic provid .
thei ar go to anonym their data especi the us of privat inform .
thei're go to anonym them and thei ar go to ship those anonym data to data miner .
thi model we call busi to busi environ or servic provid to data miner environ .
that mean you have to trust both parti .
but the problem , do you realli trust them ?
and most peopl mai sai , no .
okai .
so that's the on , of cours , thi could be quit popular in , in the regular practic , but you want more .
okai .
so the second approach we call data anonym or perturb .
that mean , we ensur data privaci and owner cite data sourc site .
usual , thi anonym even you mai not trust the servic provid .
you actual , will try to ask a third parti vendor to anonym the data befor thei can releas to anybodi .
okai .
thi on we call busi to custom environ .
and the typic method develop , like a data perturb , data transform , data hide .
that mean , we know that hide some sensit valu or sensit attribut .
let's look at thi tabl , thi is a , suppos we have a hospit , it contain certain problem of , like the data is patient id , you get zip code , you get ag , you get the diseas .
peopl mai not be comfort on thi kind of data , becaus even you anonym patient name , but you mai got to , some peopl mai know the patient's ag , and zip code .
thei can easili sai , oh , thi person got a heart problem .
okai .
so then the messag could be , what about we , we make it a littl more gener .
we hide the last two digit , digit of zip code , then we hide the last digit of their ag .
that mean you onli know is add a zero , on , eight , someth , zip code , and in the 40s , got a heart problem .
but then you see for the same , you know zip code and same 40s , there ar lot of other diseas .
so you will have no wai to tell thi person whether thei got a heart problem , cancer , flu or anyth els .
so in that sens , you mai feel more comfort .
if you got three record , you mai still not feel veri comfort , but what about you get <num> ?
you probabl sai that's okai .
so thi actual record k anonym problem .
that mean everi equival class , that mean thei have the same zip code and h , thei at least have case record .
if thi k is realli larg , you'll feel probabl much more comfort .
becaus within thi <num> differ diseas , nobodi can guess what you have .
okai .
so , but thi mai not still be suffici .
for exampl , what about all these ag , the , the zip code and in the thirti , everybodi got a diabet .
then , you know , then you agre to us it , then you can see you do get diabet .
okai .
so that mean , thi record even thi k is not suffici , you want ensur thi diseas is sensit attribut got at least the same valu , so we call thi l divers .
but even thi l divers , what about the major of peopl who have diabet you still feel not the , not comfort .
okai .
so there ar other studi , further studi for exampl t close , differenti privaci .
so there ar lot of research i'm do to see how to insur peopl have no wai to guess what you have .
of cours even for error divers , if the major suppos is hiv neg , probabl thi , thi major will not realli have concern , becaus , you know , hiv neg is not a concern .
okai .
so that's the thing the approach can see there ar , there ar mani issu to studi to ensur privaci in thi wai .
okai .
so , i'll not get into detail , but there ar mani good research paper i list at the end some typic paper , you mai like to go deeper by read them .
now , i'm go to discuss a littl data perturb method for privaci preserv pattern mine .
data peturb , perturb usual peopl us statist approach .
that mean us some random algorithm to distort the data .
on wai you can do is we call independ attribut perturb .
that mean , you take each attribut .
you perturb the , the valu independ of the other attribut .
okai .
you don't have to think about their other attribut exist .
like you perturb the zip code or you perturb the h .
so , it doesn't matter .
okai .
but for pattern mine , you will find the correl .
so sometim , you mai like to see depend attribut perturb , becaus , becaus the purer independ you do perturb these correl mai be lost .
that mean , if you want to take care of the correl across attribut , your mai concern to develop some depend attribut perturb method .
okai .
and for thi data perturb , there ar some interest studi .
on method call mask .
the , that method wa if you think your shop transact contain lot of item .
okai .
usual , on transact mai onli , that mean on shop basket mai onli contain ten or fifteen item .
but if you go to walmart , there ar so mani item , you can think all these item is a veri long vector .
okai .
in your own shop basket , onli small number of these factor turn to on .
okai .
but you want to hide what exactli you , you , you ar bui .
okai .
so , in that sens , you mai turn the other item from zero to on and some of your on item will turn from on to zero with certain probabl .
okai .
with thi , if you're carefulli tune thi p , you mai achiev both good averag privaci , becaus you insert a lot of randomli , particular item .
thei were will not figur out what you bought , but you still will get a good accuraci .
the reason is in gener those pattern , onc you random a lot of differ item thei , with thi random and like , thei will becom frequent .
okai .
so the pattern mai real frequent pattern mix you up .
but thi method actual mai turn , becaus of the major of thing ar zero .
you mai turn mani thing into on with certain probabl , so that mai increas your number of item in each transact quit a lot , make your databas realli big .
okai .
either you can find pattern it could be substanti slow .
there ar stu , studi now how to , you know , ensur both , you know , effici and accuraci .
okai .
anoth method studi call cut and past oper .
thi essenti is us uniform random .
the method is you get a real transact , you don't increas item in thi real transact .
but you take the exist item , you have a probabl to randomli replac some exist item with a new item .
okai .
not present your origin transact .
so as long as you do not make your probabl too big to replac them , you still get a good pattern show up .
so but sometim , if you replac too much , you go quit worst case .
you know , on the accuraci or you will pai is too littl , you mai get worst case privaci .
so their master studi how to balanc both .
how to select item in an organ wai .
okai .
to improv the worst case privaci , but still keep good accuraci .
actual , the experi show , if you want to mine , you know , short transact , short pattern , short itemset .
okai .
so these cut and past random the databas you mine it .
you mai correctli identifi <num> to <num> short pattern , like len , less or equal to three .
but if you want to mine long pattern , becaus such perturb , a lot of the long pattern mai be destroi .
okai .
so how to effect mine such long pattern , still remain good privaci .
it an open problem .
okai .
so privaci preserv data mine is still a veri activ research field .
there ar lot of research paper publish .
for exampl , r agraw , thei got the first paper publish in year <num> in sigmod and there ar book .
and thei , i , also intention put a privaci , differenti privaci like t close , l divers , those paper here , becaus i did not realli lectur thi to ani detail .
okai .
so interest reader , you mai like to read those if you want , you ar interest in , you know , further push forward the frontier of privaci preserv and data mine .
thank you .
now we come down to the veri last session of thi lectur , which is also the veri last session of thi short cours .
look forward , we have studi a lot of pattern discoveri master , some advanc concept , some algorithm , and a lot of applic .
we probabl see there ar lot of interest issu to be studi from the first paper in thi field , like <num> , up to now , year <num> .
we introduc variou kind of research discoveri and method even last and here todai .
'kai .
so , thi is still interest research and applic domain in data mine .
so what we want to emphas is how we further build up a new applic us invis pattern mine approach .
what is invis pattern mine ?
it's you build thi pat , pattern mine method into variou kind of function .
okai .
like a search function , rank function , recombin , or other mat , mine function .
sometim when peopl search the web , thei mai not know the implicitli ar us pattern mine result .
for exampl , we haven't seen , we ar studi classif .
there ar so mani classif method , but definit frequent and discrimin pattern mine could be interest contribut to the differ repositori of the method on classif .
then we also studi the graph index and a similar search .
when you do index you do search , you do variou kind of retriev , you mai us index construct by pattern analysi .
then even for pattern mine , we mai first mine frequent pattern and do topic model , then we can get a lot of , you know , interest primit and featur for effect text mine .
when peopl do softwar analysi , you mai us pattern mine method to discuss lot of bug or softwar specif , regular and pattern which mai help us to find bug in softwar .
for spatio , spatiotempor , trajectori , or moment pattern mine , you mai see , the frequent pattern analysi can be an interest measur ad to , you know , to thi set of weapon .
then for imag mine , multimedia data mine like a video , audio analysi , the pattern analysi could be an implicit approach , you know , method pure insid .
even for a biolog and chemic data analysi , like analyz dna sequenc , or biolog graph , lot of pattern mine , mai be us as primit .
even in cluster , for exampl , we ar go to studi cluster in data mine in thi cours , i'm go to introduc you subspac cluster , which actual frequent pattern mine will becom anoth interest techniqu for sub , subspac cluster .
even peopl search the web , or make recommend of your product , we'll studi click stream to enhanc your search .
the pattern mine might plai an import role in thi .
so you can see , pattern mine can be appli in mani , mani differ domain .
so , thi invis pattern mine could be veri import , take thi as a pre process as some compon in your whole analysi packag .
so we ar face veri big data , we're face lot of challeng .
but the pattern mine sure is a tool .
so i hope by learn thi cours , you will becom part of research and develop forc to push pattern mine into new era on most research applic .
hopefulli in the futur , in your work , you mai find , us the materi you learn from thi cours , you ar go to develop some interest function and push the technolog into applic into the new frontier .
thank you .
now we examin the first strategi , simultan infer phrase and topic .
we know the phrase integr wa topic , mai gener some meaning topic and a more tell phrase .
so , we will see how we can do thi in on shot .
that mean we can do simultan infer phrase and topic .
the gener philosophi access shown , in thi bigram model , propos in <num> .
the gener philosophi is , you base on the previou word , and the current topic .
you look at , you , you know , what ar the probabl , the next word will be gener , us thi probabilist gener model .
then you mai form bigram , base on thi method .
on gener of thi method , call topic n gram or tng .
thi on , wa done , on year later , <num> .
the , it is still a probabilist gener model , that gener word in textual order .
instead of just think about bigram , thi on actual creat n gram by concaten success bigram .
so thi on is a gener of bigram topic model .
the third on is call phrase discov lda , or pdlda , done by lindsei in <num> .
thi is view , everi sentenc .
there's a time seri of word .
then for thi phrase discov lda , it essenti , is try to find gener paramet , or topic , it chang period .
us everi word , access drawn base on previou m word , or base on the previou context , and the current phrase topic .
thi kei word base on surround , you know , word .
and the topic , try to see whether the , the phrase or the new word will be drawn , will be , you know , natur from a nice phrase .
so all these three masser have some high model complex , becaus your due model to draw everi subsequ word .
just a look at the previou m word , and the topic and try to call in thi gener model .
it is realli made expens .
it ha high infer cost and is slow .
but also it mai be over fit .
let's look some exampl .
thi slide basic show the tng , thei experi on the research paper will show a few topic it gener .
on topic call reinforc learn , and for thi if you compar the three row .
the first row done by lda , the second row wa gener by n gram on when n is two or more .
the third on is in gram on is gener onli the unigram , the onli on singl word .
if you look at thi reinforc learn , optim polici , dynam program , look thi is much more meaning and tell .
then the unigram , then the lda output .
similarli , you look at human recept system .
you will also clearli see , the n gram , especi when n is two or more .
it's better than the unigram , than the lda output .
similarli , we can see like for the topic , speech recognit and support reckon machin , you look at these n gram , when n is two or more , it is much more tell and meaning , than the uniqu gram or rda output .
okai , the support of up back machin , you'll for peopl from miner wast , the field proquenc it is , much more meaning than you just to look at the unigram and lda .
okai ?
so , to that extent , we sai it is good when we do topic model , we promot think about phrase format , phrase gener .
do topic model , you'll gener more inform interest topic .
so , thi is the first strategi we ar learn .
then we will proce to the next two strategi .
thank you .
now we discuss strategi <num> post topic model phrase construct .
what is post topic model phrase construct ?
which in our philosophi is , we first do topic model base on back off word in document .
the typic topic model method could be latent dirichlet alloc , okai .
after that we do phrase construct , we construct phrase base on those word with the same topic label , 'kai .
that mean eh , base on thi turbotop , done by blei and lafferti in <num> , it work like thi .
we first do latent dirichlet alloc on corpu to assign each token a topic label , that mean we first do topic model to get topic label .
then we merg the adjac unigram with the same topic label by some kind of test , usual is a distribut free permut test on arbitrari length back off model .
we're not go to get in to detail into thi part .
then we will end recurs merg when all signific adjac unigram have been merg , mean when we can not see we should merg further we're finish , 'kai .
thi is on method .
anoth method call kert done by danilevski eh , and other , that's all group work in <num> .
eh , we , we also do latent dirichlet alloc first , and then do phrase construct afterward .
but the differ is we perform frequent pattern mine on each topic , then perform phrase rank base on four differ criteria .
let's look at the turbo topic .
the turbo topic method essenti is we first do lda on corpu to assign everi token a topic label .
for exampl , phase and the transit both assign topic label <num> , but game and theori game mai assign the topic label <num> , theori mai assign <num> .
eh , in thi time , when we do merg for the adjac unigram with the same topic label , and phase and transit , mayb merg togeth , but a game and theori were no chanc to be merg togeth , becaus thei belong to differ topic , 'kai ?
pleas note there is is like a quantum phase transit .
thi , actual , if you look at a quantum , is also topic label <num> .
but , base on the test , you mai not see quantum should be merg with phase transit .
then phase transit is a phrase .
but a quantum phase transit or quantum phase mai not be a good phrase , 'kai .
thi is turbo topic .
then we look at anoth wise by danilevski and it call kert which essenti is keyphras extract and rank .
the keyphras extract is still base on topic model .
we first do topic model like lda .
then you can see these differ color mean actual differ topic label .
to that extent , knowledg and discoveri , thei belong to differ topic , then there's no chanc you can gener a phrase for knowledg discoveri .
okai , but you do have chanc to gener support vector machin , but , of cours , we have chanc to gener support vector machin classifi .
but we ar go to see how we should rank them and which on would have better chanc , 'kai .
actual after the rank for machin learn we get the set of phrase .
some could be unigram .
some can be trigram , bigram , okai .
so thi is kei phrase extract and rank .
let's look at littl detail .
the framework .
first is we run bag of word model , the topic model by treat everi word independ , we run bag of word model infer , then we assign topic label to each token , 'kai , then we will do frequent pattern mine , extract the candid kei phrase within each topic .
for those candid kei phrase we ar go to rank , those candid kei phrase in each topic .
what ar the criteria we ar go to rank them ?
okai , we have four criteria .
on call popular .
for exampl if you see inform retriev versu cross languag inform retriev , you'll find that inform retriev is much more frequent than cross languag inform retriev .
that inform retriev would be rank higher becaus it is more popular .
then the second criteria we call discrimin .
what is discrimin ?
is if thi kei phrase is onli frequent in document about a topic t , but not frequent in the confer about , sai t prime or other , you know , topic .
then thi keyphras is more discrimin than the other keyphras , so that why it can be rank higher .
the third right here call concord , okai .
simpli sai whether the phrase get togeth , thei realli , quit frequent , get togeth .
for exampl , activ learn could be a nice phrase but learn classif , even occasion , thei get togeth , but thei ar not so natur get togeth .
you actual were , mayb you will see activ learn classif scheme .
so , to that extent activ learn mai be more concord than learn classif .
the fourth on is call complet simpli sai , if vector machin versu support vector machin , you in the , you see support vector machin happen much more frequent than vector machin alon .
that mean everi time you see vector machin , almost you alwai see support vector machin .
then support vector machin is more complet than vector machin , okai .
base on these criteria , we actual can directli compar phrase of mix length , that mean unigram , diagram , triagram , or other engram .
we can compar them togeth base on these four criteria .
then we can rank them in a nice wai .
for exampl , thi is on , you know , run output base on mine paper titl in dblp .
dblp is a comput scienc bibliograph databas .
it contain about <num> million paper .
onli mind these paper titl .
okai , those keyword eh , get togeth we want to us first , the first us topic model , then we rank them .
if you can see , base on these learn readout thi on , two , three , four , ar the four differ those on occur , not on differ criteria .
for exampl thi kprel you , you'll see what it gener essenti is unigram , 'kai , then that , that mean the , that it mai not consid , you know , so mani phrase .
but if , for , if you knock down the popular for kert criteria you will gener these kind of phrase .
but actual mani of them ar unigram as well .
but if you knock down discrimin , you will find that thi is the output , okai .
if you knock down the concord for the full criteria , you get thi output .
but you take all the full criteria , what you final get is thi right most column .
if you look at thi , some ar unigram , some ar trigram , bigram .
but the interest thing is these top rank to phrase ar veri good phrase for machin learn .
rememb thi softwar know noth about comput scienc or even english , but if thei can just look at the paper titl in dblp for machin learn topic , it can get thi phrase rank , it is not easi , 'kai .
to that extent , thi is pretti interest algorithm .
gener veri good result .
now we studi strategi three first phrase mine then topic model .
in the previou studi of phrase mine and topic model .
eh , it's almost like we first do topic model , then do phrase mine .
or do topic model and phrase mine togeth like the strategi <num> .
then thi strategi <num> is we first perform phrase mine , then perform topic model .
thi wa done .
in my group by ahm el kishki , he got a paper in vldb <num> , call topmin .
thi is topic phrase mine , that's why i write it thi wai , okai ?
the philosophi is we first do , phrase construct , then do topic model .
'kai .
thi is differ from kert , which first do topic model , then do phrase mine , okai ?
so let's look at topmin framework .
the first step is we do frequent contigu pattern mine to extract candid phrase and their count .
the differ you perceiv with mark in red is , contigu pattern .
kert , actual doe not requir contigu pattern .
that's why , if you want to find a phrase like a frequent pattern .
then the phrase , like a , frequent close pattern or a frequen , frequent topic pattern , all these were a contribut to the support of frequent pattern , so which is nice .
but on the other hand , if the word separ too far awai , you put them togeth , you will gener some pattern or some phrase which ar not you know , the real phrase .
so that's why we've found us frequent contigu pattern ha some edg .
so after thi , we were to agglom merg of adjac unigram to form bag of phrase , and how to do thi , is guid by signific school test .
we ar go to get into thi soon .
okai .
after we get a bag of phrase , then we pass thi bag of phrase into phraselda , which is the extens of lda .
it will constrain all the word in a phrase to each share of the same latent topic .
then we gener more meaning phrase base topic model .
let's look at , why we want first do , phrase mine , then topic model .
in strategi <num> you probabl , can't see if we first do topic model .
suppos knowledg discoveri ar in differ topic , and support vector ar in differ topic , then you ar not go to be abl to gener knowledg discoveri , discoveri as on topic , and support vector machin as anoth phrase , anoth topic .
so there's a constraint , that if the topic model happen to put nice phrase apart , then such phrase will not be abl to be gener .
so our solut is we want to switch the order of phrase mine , and topic model infer .
that mean we first do phrase mine , we mai be abl to recogn knowledg discoveri is on phrase , least squar is anoth , support vector machin is anoth , classifi as anoth .
if we can do thi in a high qualiti , then we pass to topic model you know , procedur .
where we'll be abl to identifi knowledg discoveri as on topic , mark as red .
least squar as anoth support vector machin , as a sur , as a support topic , and classifi it as a support topic as well .
so , to that extent , you mai gener the better qualiti phrase and topic .
thi method is wai to phrase mine , and document segment like thi .
then we do topic model infer with phrase constraint .
let's look at littl detail .
how to do frequent pattern mine , and statist analysi to gener qualiti phrase , okai ?
so , the interest thing is when the phrase , for exampl p1 , p2 , could be two word , or two phrase , on word on phrase .
but anywai , these p1 , p2 , whether thei should get togeth , togeth , we do the signific score test .
okai , we calcul the signific score , alpha , 'kai ?
how do we calcul alpha ?
actual thi is the p1 p2 in the corpu we first see how frequent thei ar .
okai , then we'll receiv p1 p2 natur , what is the chanc thei should get togeth .
that mean if you just look at p1 and p2 , how frequent thei ar in thi corpu without look at concret instanc .
you ar go to get some kind of a new , or as you can see , the averag case , okai ?
then the kind of re observ minu the expect case , you will see how the real frequenc deviat , or thei might be substanti frequent togeth than the expect case .
then we will normal it , 'kai ?
that's the reason we can calcul thi signific score .
thi is the idea you can think about thi normal distribut base on statist we know if thi on is normal distribut we get a mu as a mean .
if your distribut is three standard deviat awai from mu .
okai .
then what you got here like , is not by chanc .
it's realli , thei should get togeth .
that's a reason when thi number deviat wai is high , the phrase you gener should be in high qualiti .
okai ?
so let's look at thi case for exampl , if you calcul the support vector , you find their alpha valu is <num> .
then you should sai , support vector should be a phrase , becaus thei ar so close togeth .
okai ?
but after that , if you calcul your complex you find that support vector machin get togeth the alpha valu is <num> .
that simpli sai , not onli support vector should be a phrase .
and support vector machin get togeth .
it should be a good phrase .
that's why you will take thi on as a phrase .
okai .
then if you look at the remain on .
like featur that actual you find them get togeth .
the alpha valu is six , you gener featur that ar i've just won .
you'll find a markov blanket at get togeth is <num> .
you'll put anoth markov blanket as anoth phrase .
and final , you'll gener markov blanket featur select , as anoth phrase .
then support vector machin for the third phrase , 'kai .
by do thi signific test you mai get anoth discoveri is on phrase .
least squar is anoth phrase , support vector machin is the third on , and classifi the fourth on .
so that mean , instead of you just to look at the raw of frequenc , you see the support vector machin as a phrase .
there ar two frequenc <num> , you put them as a phrase .
after you put them as a phrase , the vector machin .
actual if you check them thei absorb by the support vector machin , then their true frequenc will drop down veri quickli .
and but the support vector , you'll find even you consid thi true frequenc , support vector itself , mai still have <num> as a true frequenc .
base on thi , you ar go to sai final , support vector machin is a realli good phrase , but like support vector could be anoth independ phrase .
so , thi kind of test we essenti call thi the colloc mine .
the collooc mine mean , a sequenc of word that occur more frequent than expect .
thi a colloc .
for exampl , if you look at the strong tea , you'll see these get togeth is realli colloc , that mean much more frequent then expect .
you will not see peopl sai , power tea , okai , or weak tea , but you see the strong tea .
in the meantim , you mai see power comput , but you do not see a strong comput .
so that mean , those word that ar get togeth is not by chanc .
it's much more frequent than expect .
so we can us mani differ measur to check such colloc .
for exampl , like the exampl we show , just show is a signific score .
you also can us pmi , us chi squar , you also t test , z test , or likelihood ratio .
you'll probabl get veri similar result .
okai ?
let me us mani of such measur can be us to guid the agglom phrase segment algorithm .
so it's not just becaus we us signific score we get some good result .
so after thi , we actual feed those phrase we gener in extend lda call phrase lda .
thi essenti , is do constrain topic model .
essenti , the spirit is the same , the same gener model for phrase lda , as the origin lda .
but the major differ is for the phraselda , we us bag of phrase as input to do these topic model .
that mean we actual we're see for these plate , you'll probabl see .
there's a chain graph show those word in the same phrase should be constrain or should be chain togeth on the same topic valu .
base on these constraint , we will be abl gener high qualiti phrase and also better qualiti topic becaus you put phrase togeth like will gener better topic than the unigram .
we will have a littl look to see whether it is true we can gener good phrase .
you probabl can see .
thi is on exampl we took ddrb , we find thi on topic , you can see thi on , is on from the inform retriev web search , thi is on topic .
the other topic is more like machin learn , featur like featur select is other topic .
then you will probabl see , for freez discoveri pdlda , pdla .
not onli it is a quit complic model that take much longer time .
but also the final result gener is reason mix .
for exampl , the topic <num> .
you're go to see there ar some search thing , some machin learn thing , some other thing .
but if you look at topic <num> , you will see there ar lot of inform retriev thing , but also there ar some machin learn , support vector machin mix in thi topic .
so , the topic qualiti mai not be gener as high as topmin .
let's look at some output for experi .
okai , so we conduct the experi on the dblp abstract .
you proce thi topmin gener it for dblp abstract .
of cours , you can see those unigram , you probabl see thei're , thei ar not bad qualiti but n gram actual is much more tell .
for exampl you you look at thi n gram , the first on genet algorithm , optim , optim solut .
thi is veri much like optim problem .
the second on , veri much like natur languag on .
the third on is more like machin learn .
the fourth on , more like a program languag , and softwar .
the , the fifth on is veri much like a data mine , or a data set .
'kai .
so you probabl , can see thi is a pretti high qualiti result .
and it's pretti tell for those phrase .
similarli , we conduct experi on associ press new , ap new , in <num> .
what you can see is not onli the topic model , thi unigram is pretti interest and a good .
but also the n gram is much more tell .
you proce , you get energi depart , the environment , these energi environment thing it's here .
and as a second topic is veri much like cathol and church .
the third topic is about middl east .
the , the fourth topic is presid bush , and white hous is becaus thi is <num> .
the fifth on is more like health care , 'kai .
peopl can see the qualiti of the , for the new is veri good .
even for yelp thi on is a littl like peopl just write review on yelp .
you mai find that the languag mai not be so formal .
grammarli you know , it can mai contain mistak .
but you can see , even that we still gener a high qualiti topic like the first topic , you will see ic cream , ic tea , french toast .
thi is like fast food , western food .
and the second on , like , you will see thi is more like , chines food or thai food .
and , the third on , thi is veri much like a hotel .
the fifth on is like a groceri store , shop center .
the , the , the last on is like mexican food .
so , the qualiti over all , even for yelp , you do thi topic model gener , and nice phrase , and the topic modern qualiti is pretti good .
so that's the topmin .
in thi last session , we're go to compar the three strategi we just did discuss .
we will do a littl quantit analysi .
the first interest thing is run effici .
we probabl see , if you look at these sever method , we still summar into three differ strategi .
then becaus pdlda , turbo topic , and tng will do more sophist analysi , so their comput , you know , time mai take longer , that on you probabl can see .
then lda actual is veri fast and effici compar to the other .
but lda onli consid singl word , unigram .
so kert and top mine , the major differ is , kert first do lda , then combin them into phrase .
but top mine , first gener the phrase base on frequent pattern mine and signific test , then do phrase lda .
you're go to see , for run time wise , the last of three is fast but compar , the top mine is still gener veri effici .
thu , you mai wonder top mine actual take phrase lda , the sub part , the second part of process .
why top mine can in mani case , can be even slightli faster than lda ?
just becaus top mine want you first gener phrase , with the number of phrase usual is less than number of word , so you have less number of unit to be consid .
that's why mai run a littl faster even than lda .
then we look at the result , the qualiti of the result , the first thing we want to see is the coher of topic .
that mean which topic that gener the , the phrase more coher .
then actual top mine go to the highest coher z score .
the turbo topic actual gener also not bad and the kert is not bad .
but the top mine gener the highest on .
then pdlda and tng , the coher of the topic mai not be as good .
then we look at phrase intrus .
what is phrase intrus ?
phrase intrus mean you mai take phrase in on topic and put it into anoth topic .
then you see whether peopl can recogn thi on as intrud .
for exampl , suppos we have two topic , on's topic a , anoth's topic b .
you take some phrase in topic a , you throw them into topic b , then you ask peopl to judg .
which on like , could be the intrud ?
if you correctli recogn those intrud , simpli sai , these topic is veri distinct , a human can distinguish them .
it , that mean the topic you gener actual is pretti high qualiti .
so you probabl can see , we us a human to do the test .
the top mine have the highest valu averag number of correct answer .
the turbo topic is not bad .
thi is base on the acl proceed on <num> confer , these two copra .
so what you can see here is the tng and pdlda , their averag number of correct answer mai not be as high .
then we look at the phrase qualiti , that mean whether thei gener the phrase , as in better qualiti .
then we also measur the mean z score .
then you see top mine , the phrase qualiti is the highest then turbo topic .
then tng , but the kert and the pdlda , their phrase qualiti mai not be as high .
so , in summari , we , we studi the three strategi on topic phrase mine .
the first on is we do sophist analysi , try to do the bag of word , infer in the meantim , try to gener the sequenc of token .
it is integr , but it is a complex model .
the phrase qualiti and topic infer realli reli on each other .
that mean whether you gener better qualiti phrase , you mai gener better qualiti infer or vice versa .
so it is slow and it tend to be overfit .
the strategi <num> is first do topic model infer .
then try to combin three , them into n gram with each topic .
so the phrase qualiti realli reli on the topic label for unigram .
that mean if those two unigram , even thei look like a phrase , if thei ar , even thei carri differ topic label .
it's imposs to merg them to gener phrase .
it can be fast , gener it mai gener high qualiti topic and phrase , but it mai not be so stabl .
the strategi <num> is first , we mine phrase us frequent pattern , us signific test .
then we impos the phrase on the rda so we do , that mean we do the prior bag of word , topic infer .
we gener phrase first .
okai ?
then measur the topic infer reli on the correct segment of the document .
but thei ar not sensit becaus us frequent pattern mine and segment task gener phrase in gener , ar in high qualiti .
okai ?
so , it can be fast .
you , you can gener good qualiti topic and phrase .
okai ?
then we list a bunch of paper we actual lectur or referenc in thi lectur .
if you want to go deeper , you mai like to dig in into those paper .
thi is a pretti dynam research frontier .
hi now we come down to the veri last lectur of thi cours , advanc topic on pattern discoveri .
there have been lot of research paper publish pattern discoveri in data mine and also lot of applic develop in mani field .
it is imposs in thi short cours to cover everi corner of thi field .
but here in thi veri last lectur , i'm go to introduc a few interest topic includ frequent pattern mine in data stream spatiotempor and trajectori data mine .
and also , pattern discoveri for softwar bug mine pattern discoveri for imag analysi , and pattern mine .
in thi societi , the privaci issu .
we also look forward to see , you know , how thi pattern discoveri will be appli , and how the research will develop in the futur .
so let's first discuss frequent pattern mine in data stream .
we know in current big data euro .
besid , we've got a huge amount of data store in databas system , in fire system , on the web , but also we have internet of thing , or internet of sensor .
so , in those kind of scenario , there ar lot of stream data .
the stream data , the featur usual is thei come in and go continu .
thei ar order in a sequenc , but thei ar chang dynam .
thei come in and go veri fast but in veri huge volum , okai .
thi is veri differ from tradit financ persist data set store in the fire system , in databas manag system or on the web .
so now let's look at the major characterist of data stream .
first , data stream usual repres huge volum of continu data .
thei could be potenti infinit .
like , the onlin censor .
thei have no end at thi point .
so , thei , thei ar fast chang .
thei mai also requir faster real time respons like anomali or emerg handl , okai ?
and then , for data stream usual , it captur nice off our data process need of todai .
becaus even in databas system the data could be so huge , you mai onli want to scan them in a sequenti manner like data stream .
or you can want to keep a repeat , get them back again .
so to that extent even in the larg data store you mai also process them .
in the data stream manner , not to sai you realli have to enter larg amount of sensor data .
so in gener we sai for process data stream , random access is expens becaus of the data come and go .
you don't want to fetch back or you can now fetch forward as well .
so usual we call these algorithm develop data stream call singl scan algorithm .
that mean , for ani particular data set , particular page , you can onli have on look .
so , anoth interest thing is the data stream , in mani case you mai like to store some kind of sketch to store a summari of the data seen so far .
instead of store the detail data becaus you don't have such volum or such process power .
then anoth import challeng is most data stream , actual ar come at the veri low level like the particular sensor and thei also come in multi dimension featur for exampl , even for the data .
thei mai come , some part could be the temperatur , some part could be video , some part could be audio , some could be the text date , okai ?
so it's multi dimension in natur .
but for the inform process need , usual you will want to summar or process the data and pattern , in a multi level and multi dimension wai .
so we can see there ar lot of research challeng data stream process .
if we look at the architectur of data stream process system , usual we call thi stream data manag system , what we have , is we have a stream queri processor , you get multipl data stream stream into thi processor at the same time in a continu wai .
but even for user applic program , we usual post continu queri rather than adhoc queri .
it's like a watch dog .
thei won't see anyth abnorm .
thei won't summar the data in a multi dimension space .
and then , for the , thi stream queri processor .
usual you have some scratch space , it could be a big main memori or even plu some disk .
these multipl stream come down here where we summar the process , especi answer the continu queri .
the result will be stream back to the user applic program .
so thi is a typic wai to process stream data onlin in the real time , okai ?
the problem becom for frequent pattern mine .
how can we find a frequent pattern in thi data stream ?
actual it's pretti challeng .
then we look at the , the major differ between stream mine versu stream queri .
the first thing is the stream mine and the stream queri , thei share mani common difficulti .
for exampl you can onli do singl scan , you need a fast respons , you have to handl dynam and noisi data .
but for stream mine usual you want to see the global pictur , you want to see pattern , you want to find the cluster .
it often requir less precis than the stream queri .
becaus stream queri on the pin point to a particular point , you mai need to perform join group , sort which act in stream join and the sort , those algorithm ar actual pretti difficult becaus you cannot see the past data , you cannot , you know , predict futur of incom data as well .
but pattern ar hidden , thei ar more gener than queri , so their process is differ .
actual , for stream data mine , there ar lot of research and develop activ alreadi , okai .
on branch actual studi pattern mine data stream .
anoth is do the multi dimension , multi level onlin summari of data stream .
and also data stream can be cluster dynam .
can do dynam classif .
can find outlier and , and anomali .
so there ar lot of research in data mine .
all these frontier .
so what we ar studi in thi lectur .
we will onli touch pattern mine in data stream .
so on import thing is for mine frequent pattern .
in stream data it is unrealist to try to find precis frequent pattern simpli sai the pass .
precis thing mai alreadi been gone .
it's pretti hard to captur them .
the futur on mai not be come yet , so it's pretti hard to predict them .
and even for the on you can hold , just becaus thei ar so big , you can not store them even in the compress form like fptree .
so , what we expect is try to find approxim answer .
but in mani case , approxim answer mai be suffici for our analysi purpos .
for exampl , you mai find a router , could be interest in find the flow .
those flow in the network , you mai find whose frequenc is at least <num> of the entir traffic stream seen so far .
if you identifi those pattern , those frequent occur flow , you could probabl be pretti happi .
on the other hand , you mai sai , if you for thi sigma , if you give me the count , it's probabl a littl less like over <num> over <num> or you would sai the error rate is <num> .
you probabl feel it's veri comfort .
you're think you're , you're do good thing alreadi .
in that case , we mai develop some veri effici algorithm to mine such pattern with good approxim like thi .
then in thi lectur , i'm go to onli introduc on algorithm on thi , which is call lossi count algorithm , develop by manku and motwani in <num> .
okai ?
the major idea is not to keep all the item , especi not to keep the item with veri low support count .
that mean , if thei ar veri low , thei unlik will reach the frequenc's threshold .
you do not even keep them .
okai ?
the advantag is you can guarante some error bound , that mean i activ find all the frequent item .
okai ?
but of , of cours you need still to keep realli larg set of trace .
your buffer size should be rather big .
let's look at lossi count algorithm , but we onli introduc frequent singl item .
okai .
thei didn't studi multipl item set for the simpl explan of the idea .
we first look at the frequent singl item count .
okai ?
suppos you get a veri , veri huge , you know , data stream .
you mai di , divid the stream into bucket , but the bucket size could be on over epsilon , becaus your epsilon is <num> error bound .
then the bucket size could be <num> over epsilon , mean <num> over <num> is <num> , mean each bucket should have <num> item , okai ?
then , thi on's on the item we assum that the , the main memori , the size , is big enough .
you can easili hold thi on bucket .
then at the veri begin , when the first bucket come , the , the main memori , at the veri begin , you can see for thi part is empti .
okai , thi summari is empti .
but then , you come , you get the first bucket , you get <num> item , okai ?
then you start count them .
for exampl , you mai count that thei ar four red on there ar two yellow on , it's on green on , on blue on , on black , and so on , okai ?
so , at the end of thi bucket boundari , we will decreas all the counter by on .
you probabl can see , there ar mani , if thei onli appear onc , like the green on , a blue on , the black on .
thei all gone becaus on you decreas a counter by <num> , the counter is zero , thei're all gone .
even for red on , you get four , now you becom , you get , the counter becom three .
you get two yellow on , the counter actual is onli on .
okai ?
so that mean you final , your counter actual get a less than the real on .
with the next bucket of the stream come , okai , you'll probabl see you'll actual empti a lot of space becaus the counter wa not exist there too , there ar too few item in thi color , okai ?
then you mai get addit red on and yellow on or , or even black on .
okai ?
so you can see at the end of thi bucket , okai , then you will do the same , you decreas all the counter by on .
so those , not frequent the , thei , thei ar gone again , so the the yellow on you can see , origin you get a <num> , you get a on more but you decreas by <num> and still keep on .
the black on is on , the red on you get a littl more becaus the , the red on is so frequent .
after , you know , you , you're thi , think bucket by bucket you get mani , mani bucket if you , you , you , you still keep thi counter , you know , <num> counter okai , at , becaus you get <num> bucket and most of you have <num> counter .
you onli get a number , so the space , actual , is quit limit .
okai .
then , we final , we want to output the frequent pattern .
the interest thing is , whether we will be abl to output all the frequent pattern if thei ar frequent in thi data stream .
but mayb we have some reduc count , okai ?
so actual , suppos the support threshold is sigma , the error threshold is epsilon , and the stream length is n , so far .
okai , then the output of those item with frequenc count exceed sigma minu epsilon time n , simpli sai if for ani item you final found it counter with thi number of bigger , your output as a frequent item , okai .
so the kei is becaus at , at end of each bucket , we decreas the counter by <num> , okai .
so we , we , we're undercount someth , so the question becom how much do we undercount ?
so if the stream length seen so far is n , the bucket size is <num> over epsilon .
we can easili , you know , determin the frequenc count error should be no more than number of bucket .
why ?
becaus for each item , for everi bucket , you decreas a count by on .
okai .
if some item come even later , at the veri begin thei did not come , you did not d , decreas them .
so , to that extent , you decreas less than number of bucket .
so if thei come even at the veri begin , the first bucket thei stai along the wai , you will decreas the count by number of bucket .
but how , what is the number of bucket you will see ?
becaus you have seen n element , okai .
and the bucket size is <num> over epsilon , so n element over bucket size is number of bucket , which is n over <num> over epsilon .
so you get epsilon time n .
that mean the , the frequenc count error at most is epsilon time n .
we know each on is pretti small , so the error con arrow is not that big .
okai .
then we look at the last account , thei have some interest properti we call approxim guarante .
the first thing is there's no forc neg .
simpli it sai , if the item is frequent , it will be captur and output .
why ?
as we can see is if your item is frequent , that mean your total support threshold should be sigma time n or more .
then becaus we output the frequent item exceed sigma minu epsilon time n .
we know we alreadi under count , count thi part at most .
that mean after you've output all these , you were output everi item who's a poor count is no less than sig , sigma amount n .
so there's no fals not neg .
okai .
but there could be fals posit .
where the fals posit come , just becaus probabl sometim thei come later like you mai have on color .
okai .
suppos orang becom at a veri late stage .
okai .
so thei actual the previou decrement by on actual did not touch them .
so now , thei realli have thi suppos , their support is sigma minu epsilon time n ?
you , you actual output thi .
sai , thi on could be a frequent item .
okai .
but thi orang on , actual you the real support is thi .
okai .
thei do not suffer the decrement , so or onli suffer on .
so then , your support count .
thi wai act principl is not frequent in rear , but you think of frequent just output as a , as a frequent item .
okai .
so the fals posit ha a true frequenc at least of thi , so that mean that the orang on ha the true frequenc of thi on , you still count them as a frequent item .
so you do have some fals posit .
but frequenc count underestim by at most and sometim n , becaus we alreadi have thi .
so you have some nice guarante for , you know , final , you have , just a , thi bucket size as your main memori .
you actual can return you know , all the frequent item and it mai contain some fals posit .
mai have some underestim of your frequent count , frequenc count .
but still it is a nice eleg algorithm with veri limit resourc that can handl data stream .
of cours , after thi , there ar lot of studi by improv , for exampl , improv lossi count algorithm .
on interest studi is by metwal in <num> .
thei do space save comput of frequent and top k element .
the gener philosophi is thei mai try to us more memori space .
if you realli have bigger memori , you don't have to restrict it down to <num> over epsilon as your bucket size .
but we ar not go to discuss thi in more detail .
anoth interest thing is in thi lectur , we onli discuss frequent on itemset .
that mean for frequent singer item we discuss how to mine the approxim on .
actual in manku and metwal paper , thei also discuss frequent k itemset .
how to maintain counter .
how to do you know , lossi count .
but due to limit time , we were not introduc to thi algorithm .
you mai like to read the paper by you know , yourself .
then there ar also some subsequ studi of how to mine sequenti pattern in data stream .
it is a pretti challeng task , as well .
okai .
so , i'm go to onli introduc you two paper .
on is manku and motwani's paper , anoth is metwal's paper .
you mai want to see you know , in , how to mine the stream in the pattern in stream in a veri interest wai .
thank you .
in thi session , we ar go to introduc an interest seam in the advanc topic on pattern mine , which is spatiotempor and trajectori pattern mine .
and we have lot of data now associ with spatiotempor trajectori .
for exampl , sensor , smartphon eh , there ar lot of gp devic .
okai ?
so we first want to see how to mine spatial pattern and their associ rule .
we probabl can think about spatial pattern and associ rule in thi form .
a and b ar frequent pattern .
you mai want to see the rule , like a impli b with certain support and confid .
and a and b could be spatial or non spatial predic .
for exampl , thei could be topolog relat like , intersect , overlap , and disjoint .
thei could be , spatial orient like , left , right , west , east , or under , abov , or remot , or someth .
then you also can have distanc inform like close_to , within certain distanc , you know , those relationship .
okai , and support of confid or correl there , lot of interest measur we can associ them with your spatial rule or pattern .
for exampl , like you mai find certain thing like thi .
if x is a larg_town , and x intersect with highwai , than like x is adjac to water .
okai .
so thi you mai have certain support and certain confid of thi rule .
anoth import thing for spatial pattern mine is we shall explor spatial autocorrel .
what is spatial autocorrel ?
basic , we sai the spatial data tend to be highli self correl .
that mean the nearbi thing ar more relat than the distanc on .
peopl often sai everyth is relat .
but the close by thing ar more relat than the far awai thing .
okai , thi is like common sens , but it's veri us .
for exampl , you can us neighborhood inform even when you studi temperatur .
mayb the closer by on will be closer in temperatur as well , okai ?
so there ar lot of properti we can explor to increas the power and effici of mine .
so we first introduc a gener principl call progress refin in mine spatial associ .
usual even for spatial predic or relationship , there is someth more gener .
for exampl , gener close_to that mean as now thei ar somewhat close by .
you will sai thei ar , thei ar geograph close to , okai .
but you also mai studi a littl more detail , whether thei ar nearbi , or thei realli touch , or thei intersect , or on contain the other .
okai .
but you mai sai , i first mine the veri rough on .
that mean we first search rough pattern .
then we find thei ar pattern .
then we mai refin to see what exactli pattern thei ar , like exactli relationship .
you mai first get a gener geograph close to .
then you see whether thei ar nearbi or thei realli touch each other or thei intersect or on contain the other .
so that act to refin will save a lot of process power to focu on those unrel thing to do veri expens spatial relationship comput , okai .
that mean we can do two step mine of spatial associ .
on is we mai do rough spatial comput .
so as a filter that mean there's more like close by , gener close by .
you can first do veri rough comput .
how do you to comput it ?
for exampl , you mai us some interest data structur like a minimum bound rectangl or r tree to do rough comput to find the rough pattern .
that assum that you will have less comput , you know expens becaus you just us some inher or given data structur you can comput it .
then after you find those rough pattern , you mai want to refin it .
find a more refin pattern like whether thei intersect or whether on contain the other or thei touch each other .
so in that sens , you mai appli those object pass the rough pattern .
then you , you mai , you mai do more detail comput .
for exampl , for thi graph , what we can see is we , you can assum these rectangl ar shop center .
and these circular on ar ga station , and those line ar the highwai .
okai , then you mai first to find close by mayb you will find pattern sai ga station in gener is veri close geograph close by shop center .
okai , then after you find those , you mai refin it .
you sai how close thei ar by whether thei ar realli , realli just nearbi or thei touch each other or on contain insid the other .
so you mai , you mai give more detail refin the rule .
okai ?
so that's eh , actual we'll save the cost .
you try to find all kind of pattern in all the differ place .
those comput could be rather expens .
okai ?
anoth interest thing we mai want to studi is relat to the trajectori we call rel movement pattern .
that mean you mai find some object dure the time of movement , thei keep close togeth .
for exampl suppos thi is a timestamp .
you get timestamp t1 , t2 , t3 , and t4 .
okai .
you mai find a group object in the space like these ar the , the map okai .
so you find that thei ar geograph close by dure the moment .
that mean in t1 thei ar close by , you get t2 thei mai also be close by .
you mai want to find thi .
on definit could be flock .
the flock is , you find these m entiti like m object , okai , anim or , you know , peopl .
thei ar within the circular region of radiu dure the movement .
that mean thei move in the same direct everi time you check them , you see thei ar all within certain radiu like r .
'kai .
you , then you sai thei ar flock .
'kai .
for exampl , you mai find deer or sheep .
those moment you mai find thei realli found flock .
'kai .
and anoth interest pattern you want to find could be convoi .
the convoi mean thei're not necessari within a rigor circl like radiu , but overal thei form cluster .
if you us densiti base cluster , at everi timestamp you ar go to find thei form cluster .
for exampl , you mai find better shape .
you mai find those better shape thei could be sometim mayb have a pretti big radiu .
you just look at their rel distanc .
but thei form cluster so the , you find densiti base cluster .
these ar the convoi .
okai ?
but flock and convoi in their definit , thei requir at least a k consecut time stamp .
thei alwai be togeth .
but in realiti , if you look at thi case , like thi particular graph , you mai see sometim thei're get togeth , like from if you look at time t1 to t4 , thei ar get togeth .
but if you look at t2 and t3 , sometim thei ar a littl far apart .
so we mai defin anoth interest movement pattern call swarm .
swarm is like a bee .
thei , the , the big swarm , thei mai move in the same direct .
but on the other hand , you , if you look at everi particular second , okai , thei mai be far apart in certain time stamp .
actual , even you think about it , sheep or deer , you mai find a certain time thei graze in a rather wide field .
but sometim when thei start move , thei form flock , again .
okai .
so , sinc thi swarm , ha less constraint on consecut time stamp , so there ar potenti pattern try to find swarm and more .
but the comput is also more expens .
so there ar studi on how to us effici pattern mine algorithm to uncov such swarm pattern .
the detail can read the paper .
but you can see those differ definit lead to differ process effici in the design of the algorithm .
anoth interest pattern in spatiotempor trajectori pattern is period pattern .
look at , let's look at a on interest exampl .
'kai ?
thi on actual is a three year bird migrat data .
but thei ar veri spars , in the sens .
when you put a sensor on the bird , okai , thi sensor will send you veri spars signal becaus if you send dens signal , the batteri will drain out veri quickli .
you assum for thi tini batteri , thei mai last for a year .
then the onli wai you can have the batteri last for a year is you do veri spars sampl and send out veri spars signal .
for exampl , you mai onli do <num> hour on sampl .
and everi seven dai or ten dai , you mai shift to the satellit onli onc .
in that sens , <num> hour of bird can fly veri far awai so you get a veri spars pattern .
okai .
for such spars pattern , how could you find their migrat pattern everi year to year ?
so the interest thing is you , if you try to find their period and check their pattern , you had to us differ method becaus unlik you find the two sampl point thei'd realli overlai .
it's imposs almost .
so , that mean if you try to find the period pattern without , you know , further think , you just directli us fourier transform and auto correl .
it's almost imposs you can find good pattern .
but , if you think about it , some sampl , thei form cluster .
year after year you mai find cluster .
and for these cluster if you treat , you know , the big amount of those cluster as dens point , as a refer point , a singl point , then you mai see when thei ar go to get in , when thei ar go to get out of thi cluster .
and then you look at their time , and year after year you will be abl to clearli identifi their period pattern .
okai ?
that's the reason if you chang your algorithm find dens cluster and treat dens cluster as singl point , then you studi their period us a fourier transform and auto correl .
you ar go to detect , nice detect those period .
after detect period , base on those period , you can summar their trajectori pattern , you mai find it nice pattern even the data is veri spars .
for exampl in thi studi , you can see you were base on differ cluster point .
you mai find there pattern everi time from januari to march , april to june , what is the thing that can be overlaid .
you'll find nice pattern .
okai , thi is why interest studi how to mine period pattern with veri spars data .
okai ?
anoth line of interest studi is find semant trajectori in spatiotempor domain .
okai ?
actual , there ar lot of peopl , for exampl , you , you trace peopl's you know , movement pattern .
you'll find lot of peopl , thei start in the morn from home .
thei go to starbuck .
then thei go to offic , then thei go to restaur , then thei go to cafe , then go to restaur for dinner , then go to bar .
suppos you find these , you won't find such pattern .
the interest thing is , there ar so mani peopl that go to differ starbuck , thei go to differ offic , thei go to differ restaur .
how could you find such a pattern in the spatiotempor domain ?
okai .
the interest thing is , we mai try to find the seq , meaning sequenti pattern by observ certain constraint , like a semant constr , consist , spatial compact , and tempor continu .
we can also adopt two step approach , like a progress refin , thi kind of philosophi .
that mean first , you try to mine coars pattern that satisfi the semant and tempor constraint .
rememb , thi time is differ from spatial where you just find a rough spatial on .
actual what you want to find is is suppos you first even ignor the spatial domain .
you just look at the semant and tempor domain .
you mai find the peopl semant try to creat like thi .
that mean you first mine semant meaning pattern .
then , you try to detect dens and compact cluster in the high dimens space .
and then you mai split the coars pattern into fine grain on to meet certain spatial constraint .
for exampl , you mai be abl find , first find it peopl actual from the offic at certain point thei want to go to a gym .
that's a frequent pattern .
then you mai want to studi which offic or which kind of peopl want to go to which gym .
so these ar the you know , first find semant meaning trajectori .
then refin them , us more spatiotempor inform find their sub pattern which mai get a fine pattern mine .
thi , on such studi call splitter actual work out some effici algorithm to mine such pattern .
overal , spatiotempor and the trajectori pattern mine is a pretti rich domain .
there ar lot of studi on it .
and here we just introduc a few interest algorithm .
and we also go to just introduc some survei paper and some book you mai like to read , to do in depth studi on them as well .
thank you .
now we examin anoth interest line of pattern discoveri applic .
there's pattern discoveri for softwar bug mine .
we us softwar all the time , but we also know softwar mai contain bug .
but , softwar usual could be quit big , quit long , and it's run data could be even larger and more complex .
so , automat debug .
find softwar bug is a veri challeng issu , becaus often there's no clear specif or properti we can reli on , we have to invest a substanti human effort first to analyz data , analyz the sourc code , and analyz bug .
so if we can have certain ultim tool to do softwar reliabl analysi to find bug , it will save a lot of peopl's effort .
okai .
so , for bug detect , there's on categori call static bug detect .
that mean you check the sourc code , try to find bug .
anoth call dynam bug detect or test is you run the code , you try to detect to where could be the bug .
the debug usual is you symptom or failur .
i try to pinpoint the bug locat in the code .
if we can't do good pattern mine , becaus the code ar run sequenc that mai contain some hidden pattern .
if we can do pattern mine , we will be abl to find some common pattern and some anomali .
usual , the com , common pattern like ar specif or properti , but the violat ar anomali compar to the common pattern , like bug .
okai .
so if we can do pattern mine , we mine the pattern , it mai narrow down the scope of inspect .
for exampl , usual the code locat or predic that happen more in the the fail run .
that mean that everi time it fail , it like see somewher you can see it happen more for certain predic or happen more for code locat .
thi kind of pattern , but it happen less in the pass run .
the like those code locat or predic ar suspici bug locat .
that mai narrow down the scope for the bug inspect .
so the typic softwar bug detect method contain there mani on , could be mine rule from sourc code .
we mai find deviant behavior .
those ar bug by statist analysi , you mai find some sourc code that substanti deviat from the other .
or you mai find mine some program rule .
for exampl you first do variabl valu assign , then you do the condit test .
so if you us frequent item set mine , you mai be abl to identifi those typic program practic or rule .
or you also can mine function preced protocol .
simpli sai , if you find some function alwai preced the oth , some other function .
so you can us frequent subsequ mine to find such pattern .
and also , you mai reveal neglect condit .
you base on differ condit whether it would be test or were never touch .
you us frequent item subgraph mine .
you'll be abl to find such pattern .
anoth interest practic is mine rule from revis histori .
you look at sequenc of the program code revis .
you us frequent item set pat , mine .
you mai find certain gener practic or certain anomali .
actual , we ar go to focu on on interest bug that we do , we studi , try to find a copi and past the bug .
so we try to mine , copi past pattern from the sourc code , then we try to find the copi past bug .
'kai ?
thi why actual wa on interest piec of work publish in <num> call cp miner by yy zhou's group .
and we ar go to just a show thi is interest idea how to extend pattern mine to find a copi and past bug .
so copi and past bug is veri common .
for exampl , some statist show there ar <num> <num> of the code in linux file system ar copi and past code .
and <num> of the code in x window system ar also copi and past .
but a copi and past mai introduc error .
for exampl , the typic forget to chang bug .
that mean , someth like thi code .
okai .
if you see the first part , the upper part , the flow loop , okai .
thi part and the down part anoth full loop , these two actual you , if you see mani other thing like just the same but there is someth chang so you like , you know , thi part of the second part wa copi and past from the first part and then do some chang .
but the problem is , when you do global chang , sometim the programm mai forget to chang someth like here , okai ?
that mean all the total in the front is chang to taken but the last total at the second occurr in the statement forget to chang , to taken .
so thi is a veri common practic becaus the careless of some programm , okai .
but how can we find thi kind of bug ?
actual thi bug ha realli happen in linux <num> . <num> , okai .
then we will see how to automat find such bug .
the , the gener sequenc , the gener wai to solv thi problem , we will be detail later , as we mai build a sequenc databas from the sourc code , mine such sequenti pattern .
then we mai find some mismatch id name then the bug as well .
okai .
thi on wa done by yy zhou's group .
at that time , she wa at ucsd .
so that's is veri interest studi us pattern mine .
so , we first to see how to build sequenc databas from sourc code .
okai .
so the gener philosophi is , for each statement , we mai match the statement into a number .
but how we can match the number for exampl , thi two or <num> , the other two ar also <num> .
how we can find such similar sequenc , okai .
so the methodolog is we token each compon .
like a , just give a exampl , like old is assign to <num> and the variabl new is also assign to <num> .
how we can , we us , we basic , for differ oper , constant , or kei word , we mai map into differ token .
but for same type of id we map to the same token .
for exampl , old and the new ar both same type of id .
these ar both old and new .
we do not care thei ar differ id , we just map them into the same token <num> .
but it's assign to these two , a map of <num> , then the valu like <num> will map into anoth valu , like anoth token like <num> .
then these two statement , if we take the same sequenc , we give them a hash valu .
these two sequenc , two statement should have the same hash valu .
simpli sai these ar veri similar statement , okai .
so , if we do thi , you probabl can either see thi total and thi total i dot address and the other on is still i dot byte which is essenti a map to the same thing , thei've got the same sequenc .
okai .
so , then a program is map to a long sequenc .
we can further cut the long sequenc by block .
for exampl , we can find these fragment of the program is match to sequenc in the same block .
you probabl can see these as a , map into se , sequenc databas .
then we can do sequenti pattern mine to detect the forget to chang bug .
how to do sequenti pattern mine ?
rememb the previou sequenti pattern mine allow arbitrari number of gap in the middl .
thi is for , deriv from the custom shop sequenc .
but it , for program , for find sequenc in the copi and past , past box , you want to have exactli on , becaus you allow peopl to insert some program state , program statement , right in the middl , you count these as the same kind of sequenc .
but you don't want them to insert too mani state , statement in the middl becaus thei like , thei could be complet differ thing .
so we can constrain the maxim gap , like a small number of statement in the middl .
then we will still sai these two sequenc of the code ar the same sequenti pattern .
okai ?
then , and we can compos larger copi and past segment .
becaus thei have mani thing in the common .
we can combin the neighbor copi and past segment , repeatedli .
then we mai try to find conflict .
essenti , we try to identifi those name that cannot be map to the correspond on .
for exampl , if you see here , you mai see the concret identifi f map to f1 .
thi f also map to f1 .
but the other on is f , f is map to f2 so there's a conflict .
so we try to calcul the ratio .
suppos if we find <num> out of <num> total is unchang .
so the unchang ratio is onli <num> . <num> .
if you set up unchang ratio , if it's , it's greater than <num> , it mean someth did chang .
but , unchang ratio is less than , specifi threshold .
then we can sai thi like could be a copi and past but , okai .
so thi on actual wa done by yy zhou's group .
thei work out the cp miner .
thei mine that common sourc code that like linux , apach you know , postgr .
so thei actual got to find mani copi and past bug out of million of line of code .
and you probabl can see thi sequenti man , pattern mine is veri effici .
that mean that within the short time , you can prows through million line of code find such copi and past bug .
and base on their human identif , these ar the real bug .
so it's a veri interest to see thi pattern mine can help find softwar bug .
in thi session , i'm go to introduc you anoth interest applic of pattern discoveri , call pattern discoveri for imag analysi .
we know imag analysi or comput vision , there have been lot of techniqu develop in thi field .
then for , there's on interest recent line of work is us visual pattern discoveri to do imag analysi .
we can take thi pictur as a exampl .
for thi imag we mai find there ar mani interest point .
these interest point we call visual primit .
everi visual primit can be describ by a set of visual featur .
or you can think it's a , a high dimens visual featur vector .
and then each imag is a collect of visual primit .
then we'll studi how to turn those visual primit into pattern , and how to do pattern mine you know , on larg number of imag .
so that is , we'll understudi visual pattern discoveri problem .
for a visual pattern discoveri we can think about from these imag we can us featur extract .
we can extract lot of high dimens featur awai from visual primit .
those visual primit can be cluster base on their space .
thei mai cluster into visual item , okai ?
those visual item , if thei ar similar then the belong to the same item .
you can map them into differ to a item id , for exampl , you can map thi featur into w , and mayb anoth pictur i have of a similar on , thei actual also call w , okai .
then each visual primit thei can have base on the primit's neighbor , thei can form small cluster and those we usual call them a transact .
'kai , for exampl , you mai find these three eh , visual primit can form a transact , like cba as a transact .
then a imag mai have mani transact , 'kai , in , in everi imag .
then if there ar mani , mani imag you mai base on their transact , you do the frequent pattern mine , you will be abl to find some visual pattern .
for exampl in these pictur you mai find a ba and ca .
actual ar quit frequent togeth in those transact .
so you can sai b and ca or cba ar frequent pattern .
ar the frequent visual pattern in those imag .
so the , the simpl exampl could be thi .
suppos we have four pictur , each pictur you mai find know certain primit , visual primit for item .
those item , the , base on the nearest neighbor , you'll , thei form transact .
then for thi car , you mai find the two transact here , g12 and g13 , okai .
then thei contain a set of you know , item you edabc , could be the transact contain those item .
each item actual is a set of visual primit from the item , okai .
but if you do frequent pattern mine , you mai find among those imag .
you can get ab as frequent item set .
and actual thi ab repres someth relat to the wheel of the car .
so that , that's a reason those frequent pattern mine can help identifi certain uniqu featur in differ imag .
so then the interest thing could be , becaus imag ar spatial data then spatial configur doe matter .
then becaus sometim those transact spatial ar veri close , thei mai overlap each other .
then thei mai have over count problem .
for exampl , thi ab actual in thi imag , there's onli on such ab pattern but sinc the , base on your , your featur imag thei ar cluster togeth and you get a two transact , you actual count ab twice in thi pictur , okai .
so there , there's a over count problem .
anoth problem encount thi visual pattern discoveri , is uncertainti problem .
becaus you mai have eh , some like those wheel can be partial eh , occlud by other object or mix with other object , you mai have lot of nois , okai .
some featur you mai not be abl to detect .
anoth is visual synonym and polysemi problem that essenti is sometim the , the , the same featur after it made wrap in differ object .
or on object can be wrap in differ kind of featur set .
so , we need to revis our frequent pattern discoveri algorithm to handl thi over count problem , uncertainti problem , synonym and polysemi problem .
actual there ar interest studi in compar vision commun .
thei develop some interest visual pattern discoveri matter for imag and video data .
so if you ar interest in thi , you , i would recommend you studi those research paper to gain your knowledg to a certain depth .
hi , welcom to lectur on , cluster analysi an introduct .
in thi first lectur , we're go to introduc the basic concept of cluster analysi .
we ar go to discuss what is cluster analysi ?
the applic of cluster analysi , the requir and challeng of cluster analysi and we will pre , then clu , cluster analysi from a multi dimension point of view , essenti it's a multi dimension categor .
where we'll provid an overview on typic cluster methodolog and overview on cluster differ type of data , an overview on user insight and cluster .
and final , we'll provid a summari of thi lectur .
so the first session is on what is cluster analysi ?
to understand what is cluster analysi , we should know first , what is a cluster ?
the cluster is actual a collect of data object , those object ar similar within the same cluster .
that mean , the object ar similar to on anoth within the same group and thei ar rather differ or thei ar dissimilar or unrel to the object in other group or in other cluster .
okai .
then cluster analysi , which is also call cluster or data segment .
essenti is given a set of data point .
the cluster analysi is to partit them into a set of cluster or sai , group .
thei ar sim , as similar as possibl within the same group and as far apart , as possibl amount differ group .
cluster analysi is unsupervis learn in the sens there's no predefin class .
thi is veri differ from classif , which need supervis learn or need to given the class label , then you can construct the classif model .
there ar mani wai to us or appli cluster analysi .
essenti , cluster analysi can either provid as a stand alon tool to get insid into your data distribut , like a summari .
okai .
or you can serv , you can us it to serv as a pre process step or intermedi step for other algorithm , like classif or predict or , you know , like other mani other task includ data mine and other applic .
thank you .
in thi session , we ar go to discuss proxim measur for symmetr versu asymmetr binari variabl .
for binari variabl , we usual refer their occurr us conting tabl .
okai .
suppos we have two object , i and j .
the number of time thei both appear could be q , thei both miss could be t place , i appear and j doe not , but there ar case and i doe not appear and j appear s time .
then for symmetr binari variabl , that mean the chanc thei appear or not appear actual have equival chanc or proxim same chanc .
we call these symmetr binari variabl .
in that case , their distanc like r and s .
these two case , thei ar differ .
so their distanc measur is r plu s divid by all the case .
okai .
for asymmetr binari , usual we assum , the , thei both appear the case is much rarer than thei both not appear .
okai .
so for these asymmetr variabl , thei ar differ , thi r plu s .
but thei , thei both , you know , not appear .
in that case , actual it's t is , is not so import .
the reason is for exampl , a medic test , onli the posit on will attract attent .
okai .
then for the distanc measur we , we look at r plu s divid by q plu r plu s .
that mean all the case with t case remov .
okai .
then for their similar measur , that mean how mani time thei ar the same , actual is a q case .
and for the q case , we probabl can see that's the same denomin .
okai .
actual , jaccard coeffici wa somehow rediscov in the pattern discoveri field .
thei call thi on coher .
actual , thi coher definit , if you realli map on into thi conting tabl , thei have the same demi , definit as jaccard coeffici .
so then we look at the rare case .
suppos we have some medic test .
then we have jack , mari and jim three case , three peopl .
their test , actual is repres in , in , thi tabl .
we can map them into conting tabl .
okai .
sinc gender , the chanc to be male and femal ar roughli equival .
so thi is a symmetr case .
so for us , we ar onli interest in asymmetr case .
that mean , the remain attribut becom more import , then we try to examin how thei ar differ .
suppos we sai y and p is posit case and a valu n and no other case would be zero .
okai .
in that case , we look at our distanc measur for asymmetr attribut .
we can work out these conting tabl approach for jack and mari we proce .
jack and mari thei ar the same part of the case like both have fever .
those the test on act as posit .
there ar two such case .
okai .
thei have both neg , like a cough test two and test four .
actual , thei , there ar three case , thei ar both neg .
but thei do have on case , thei ar differ .
okai .
so the similar we can workout conting tabl for jack and jim , jim and mari .
in thi case , we can calcul their differ , but we can easili see jack and mari actual ar most similar .
jim and mari ar most differ .
we mai conclud that jack and mari mai have similar diseas in thi case .
okai .
now we examin distanc between categor attribut , ordin attribut , and mix type .
what ar categor attribut ?
categor attribut ar also call nomin attribut becaus thei ar a valu referenc by name .
take color as the exampl .
we mai have a yellow , red , orang , blue , green .
if forget their posit in the color spectrum in physic , thei ar not order .
in that sens , either the two valu ar the same or thei ar differ .
the same is profess and the mani other thing .
then how to calcul their distanc .
we can us simpl match method .
for exampl , suppos there ar total p variabl and there ar m match .
then the number of mismatch is p minu m .
so their distanc between i and j will be the number of mismatch versu the total number of variabl .
anoth method is we can map them into binari variabl .
that mean each valu , like a red , if thei ar exist as a red we write , as <num> .
if thei ar not red we write them as <num> .
then we can chang the categor attribut in , into a set of binari variabl .
then we can us the previou binari attribut valid function to evalu them .
anoth kind of variabl call ordin variabl .
ordin variabl mean thei do have order .
thei , thei can be discreet like a rank and like a militari rank or even the rank for a undergrad student in the univers .
or thei could be continu , like time .
then order is becom import .
for exampl , in the univers the freshman is a first year , the senior is a fourth year student .
in that sens we can replac , ordinari variabl valu by it rank .
then we can map ani particular variabl us thi formula , map them into either <num> or <num> .
for exampl , we can map a freshman into <num> , becaus their posit is <num> , <num> minu <num> is <num> .
and the same for senior is four , so by the total rang is four that's why thei map into on .
then the distanc for exampl between freshman and senior would be <num> minu <num> , their distanc is <num> .
but between junior and senior , their distanc is onli on third becaus with thi formula .
then we can comput their dissimilar us the interv scale variabl .
what about we get a dataset mai contain all attribut type .
nomin , symmetr binari , asymmetr binari , numer numer or ordin .
then we can us a weight formula to combin their effect .
then , if there ar numer data , we can us normal the distanc , like if thei ar binari or nomin data we can us thi formula , as we just discuss .
if thei ar ordin data , we can comput their distanc us thi formula .
then we can combin all their effect to comput their overal distanc .
in thi session , we're go to introduc cosin similar as approxim measur between two vector , how we look at the cosin similar between two vector , how thei ar defin .
so we can take a text document as exampl .
a text document can be repres by a bag of word or more precis a bag of term .
each document can be repres as a long vector , each attribut record the frequenc of a particular term .
the term can be a word or it can be a phrase .
<num> mean the term team , actual occur in document <num> , five time , okai .
so , we want to compar the similar between document <num> and document <num> .
so how similar thei ar ?
we could us cosin similar to do that .
other vector object like gene featur in micro arrai can be repres in the similar wai as a long vector , 'kai .
for inform retriev , biolog taxonomi , gene featur map , like a micro arrai analysi , these ar good applic to compar similar between two vector .
the cosin measur is defin as follow .
for exampl , we can consid the term frequenc vector to look at their similar .
thei ar defin by dot product of these two vector divid by the product of their length .
so we look at the , the cosin similar definit , and take as an exampl .
suppos d sub <num> and d sub <num> ar the vector of these two document , okai , then we can calcul their vector dot product as follow , 'kai .
then we can calcul the length of each on of d sub <num>'s length , calcul us thi formula , okai .
d sub <num>'s two length eh , can be calcul also us the squar root of sum of their product .
then the cosin similar can be calcul us the formula given abov .
we can see their cosin similar is <num> . <num> , simpli sai these two document ar quit similar .
in thi session , we're go to discuss correl measur between two variabl .
especi we will reintroduc covari and correl coeffici .
befor we introduc covari between two variabl .
we will first examin or review the varianc for a singl variabl .
what is varianc ?
i think you all know what is averag , for exampl the averag salari of employe in a compani .
okai .
the averag also call mean mathemat .
it repres as mu .
so mu actual is the expect valu of x .
howev , if we just us mean , we mai not be suffici repres the trend or the , the spread of the valu in variabl x .
for exampl , we mai not onli like to know the averag salari of in a compani , but we also like to know how these valu spread .
that mean , whether thei ar veri close to the middl or the mean , or thei're widespread .
thei have a mani veri high salari and mani veri low sal , salari .
in that case , we introduc the concept of varianc which actual is to measur how much the valu of x deviat from the mean or the expect valu of x .
essenti , we us sigma squar to graph in the varianc x .
sigma is call standard deviat .
varianc of x actual is expect valu of x deviat from the mean .
if we take the squar , simpli sai , no matter if it's posit or neg , we alwai chang them into posit .
okai ?
then , if x is discret variabl , then the formula is written like thi .
it simpli sai we us some of thi function .
okai ?
thi function is , actual is , x deviat from the mu , from the mean valu , take the squar time x densiti function , okai ?
if x is a continu variabl , so we will take integr where the rang is from minu infin to infin .
to that extent , we sai varianc is the expect valu of the squar deviat from the mean .
for thi formula , we can also do a littl transform .
for exampl can transform for the expect valu of x , you know , squar deviat from the mean .
we can write down it expect valu of x squar minu , mean , mean squar .
thi transform actual ha been introduc in mani textbook .
it's quit simpl when i continu to introduc here .
in mani case , if we write in thi form , it mai lead to more effici comput , especi when you want to do increment comput .
if we take a sampl then the sampl varianc is actual the averag squar deviat of the data valu xi from the sampl mu hat .
so the sampl varianc is read as sigma hat squar .
so we often us thi formula or us a similar transform formula to comput it .
now we introduc a covari for two variabl .
onc we get two variabl , x <num> and x <num> , we want to see how these two variabl , thei chang togeth .
whether thei ar go up togeth , go down togeth .
which could be the posit covari .
let's look at the definit .
the definit actual origin definit is x <num> minu mu <num> squar .
now we see actual these two variabl we s , want to see x <num> .
the differ from it mean valu of x1 .
x2 , the differ from x2's mean valu or expect valu .
and we look at their expect .
okai .
mathemat we also can transform thi into thi form .
if we get a sampl covari , we look at the sampl covari between x1 and x2 .
so the , the sampl covari is calcul by their differ from the sampl mean , okai ?
so that's also popularli us .
actual the sampl covari can be consid as a gener of sampl varianc .
for exampl , a reason we will want to look at the two variabl x1 and x2 .
thei're a covari .
but if we think these to x <num> and x <num> , we replac it by x <num> , that mean we just look at the two variabl x <num> x <num> , what is the covari ?
then we can repres thi <num> by <num> .
in that case the sampl covari the formula .
we look at thi formula we chang the variabl from i2 to i1 and mu <num> to mu <num>'s hat .
though then we deriv thi formula .
and thi formula essenti is sigma <num>'s hat and it squar .
okai , so , we probabl can easili see the sampl varianc is just the special case of sampl covari , when the two variabl ar just the same .
when thi , the covari valu is greater than <num> , we sai it is a posit covari .
if it , it valu is less than <num> , it is neg covari .
if these two variabl ar independ , then their covari is <num> .
howev , the convers is not true .
that mean now , when the covari is <num> , it doe not mean x1 and x2 ar alwai independ .
onli under certain addit assumpt , for exampl if the data follow multivari normal distribut .
in that case , the , the covari of <num> impli independ .
now we look at a , a concret exampl .
suppos we have two stock x1 and x2 .
thei have the follow valu in on week , like these five valu , five pair of valu .
then the question is , if the , the stock affect , whether the stock , okai , affect by the same industri trend .
that mean whether their price will rise or fall togeth .
okai , then we care for their covari .
we will be abl to know whether thei ar posit correl or neg , okai ?
so if we look at the covari formula especi we us a more simplifi comput formula .
then we can calcul the expect of x1 which is a mean valu of x1 .
the expect valu of x2 which is a mean valu of x2 .
then we look at their covari actual as we us thi formula we look at their product , their dot product , the sum of the , and divid by sum of them divid by the number of variabl pair .
then we minu thi is expect valu of xy and expect valu of x2 .
then we get the final valu is <num> .
okai .
that mean , the covari is greater than <num> .
that mean x1 and x2 thei rise or fall togeth .
then if we want to normal them we will introduc correl coeffici .
that mean for two numer variabl we want to studi their correl , which essenti is a standard covari .
that mean we want to normal the covari valu with the standard deviat of each variabl .
so it is defin as correl coeffici is the covari divid by the product of their standard deviat .
or you can sai the covari is divid by the product , the varianc , get their squar root .
so if we look at sampl correl for two attribut x1 and x2 , then essenti we get , we get a row on two's hat is equal to the , the sigma <num> two's hat .
thi is essenti their sampl .
covari divid by their sampl standard deviat .
in a concret formula we can write in thi wai .
okai .
then if thi co , correl coeffici is greater than <num> , that mean a and b ar posit correl .
that mean , x <num>'s valu increas as x <num>'s .
the higher valu greater than <num> , the stronger correl , okai ?
if rho12 equal <num> , that impli thei ar independ under the same assumpt as discuss in the co varianc .
if thei ar less than <num> , thei ar neg correl .
then we can look at a , a set of variabl .
we can see for exampl for two variabl when thei ar perfectli neg correl , thei line up like thi .
their correl coeffici is minu <num> .
okai .
then if thei gradual becom not so perfectli neg correl , you will see their trend .
when thi valu is <num> , you could not see anyth like a posit correl or a neg correl .
but when you gradual grow these correl coeffici you'll see their valu will becom more and more correl .
when thei ar perfect correl then their correl coeffici is <num> .
that simpli said the correl coeffici valu rang is from minu <num> to <num> .
'kai .
then , if we draw thi in the scatter plot , we will see the set of point , their correl , coeffici chang from minu <num> to plu <num> in thi shape .
in mani case , we mai want to write , for two variabl , we want , we mai want to write their varianc and the correl inform into the two by two covari matrix form .
okai .
for exampl , you mai sai the variabl on is self correl there , essenti there ar varianc is thi on .
and for their covari between <num> and <num> is defin here between <num> and y is defin here .
and then that's , that's variabl two's varianc .
so thi is a typic you know , <num> x <num> covari matrix .
in gener , if we have t , d numer attribut .
that mean , suppos we , you , you , we find a data set .
it ha n co , row , and d column .
that mean we realli have d numer attribut .
then their covari matrix essenti is written in thi form .
you probabl can see that thi is the varianc of variabl on .
thi is the variabl of the second .
thi is the variabl of the d dimens .
okai ?
and their covari for each on will be line up like thi .
so , in summari , in thi section , in thi lectur , we discuss sever import concept .
we introduc thi similar measur between object , okai .
especi for numer data we introduc minkowski distanc , for symmetr and asymmetr binari variabl .
we studi their proxim measur , especi jaccard coeffici .
we introduc the distanc measur between categor attribut , ordin attribut and mix type .
we also introduc cosin similar as proxim measur between two vector .
we introduc covari and correl coeffici as correl measur between two variabl .
and we give a few interest addit read .
these ar the sever book that contain interest chapter discuss the differ measur .
thank you .
hi , welcom to lectur <num> , partit base cluster method .
in thi lectur , we're go to introduc concept and most popular method of partit base cluster .
the first is we want to introduc what ar the partit algorithm .
then we will introduc on of the most popular cluster method call k mean cluster .
then we will discuss how to initi k mean cluster .
as altern of k mean measur , we ar go to introduc k medoid cluster method , k median , and k mode cluster method .
final , we ar go to introduc the kernel k mean cluster method .
so first we will introduc basic concept of partit algorithm .
the partit method , essenti to discov the group in the data .
that mean you get a , like k group , if you want to partit them into k group by optim a specif object function figur per sum of the squar distanc .
and then we iter improv the qualiti of such partit .
so , the k partit method is , with partit , dataset d of n object into a set of k cluster .
then we definit can iter improv it so that an object function is optim .
for exampl , the object function could be the sum of the squar distanc is minim , where as c sub k is the centroid or medoid of cluster c sub k .
so a typic object function is sum of squar error is often written as sse .
so a cluster , okai , for the sse is sum of k cluster .
k is from on to k .
then , for each such cluster , if an object i is in thi cluster , then the squar error .
thi simpli sai the distanc of sum of the squar .
these such distanc , the whole function , is a k cluster and each cluster get it , point to it center the sum of such squar , distanc of squar error .
we want to minim thi object function .
then in gener what we can think conceptu we can think of thi .
if you get a veri nice k cluster , then all the object to the center is somehow shorter , okai ?
if it is shorter , then the sum of the squar distanc or squar error , actual will be pretti small .
that's why we can get within the cluster center the sum of the squar error is pretti small .
then if we get a k such seen , we add them togeth , the whole thing will be smaller .
for the k partit method , essenti the problem is given a k , the number of cluster , we want to find a partit of these k cluster that optim the chosen partit criteria .
for exampl , the sum of squar distanc .
howev , to find that global optim , actual we need to exhaust enumer all partit .
becaus the potenti number of partit actual is exponenti .
so more realist we should get heurist measur , that mean that greedi algorithm .
in thi session we're go to introduc the most popular cluster method , the k mean cluster method .
so you mai wonder , who first propos thi k mean cluster method ?
some peopl refer to macqueen becaus he publish a paper on the k mean cluster algorithm in <num> .
but , some other peopl said , lloyd actual intern , in a compani , publish the , the on in the intern report in <num> .
sinc the public wa intern publish mo , most peopl mai not even know it .
so , he republish thi on in a journal in <num> .
that's the reason some peopl sai lloyd first got thi algorithm .
but no matter what , actual , the gener common thing for thi algorithm is , thei consid everi center is repres by the center of the cluster , or we can sai , centroid .
the k mean algorithm essenti can be outlin as below .
it mean , given k , the number of cluster , first we can select the k point as initi centroid .
of cours , you can randomli select it , that's what our origin k mean , that , or you mai have some small wai to select it we'll introduc later .
then , we can put thi on on repeat and here , look , there is onc we select the k centroid , we can form k cluster by assign each point to it's closest centroid .
onc assign thi , we mai need to recomput the centroid becaus the initi randomli select centroid mai not be a veri good on .
so we recomput the centroid that mean the mean point of each cluster .
onc you recomput the centroid , some object initi assign to sai centroid a now becaus of all the centroid chang thi point mai be even closer to centroid b .
okai ?
that mean we need to go back to form k cluster by assign or re assign each point with it closest centroid now .
okai ?
then we need to recomput the new centroid .
that's why we need to get into thi repeat until loop , until some converg criterion is met .
there ar differ kind of measur that can be us for calcul the distanc and assign the , to it closest centroid .
we can us manhattan distanc , l1 norm , or euclidean distanc , l2 norm , but often , our initi algorithm wa us thi euclidean distanc .
we can even us cosin similar .
now let's look exampl .
how we can execut thi k mean cluster algorithm .
suppos we got the into <num> d space .
initi there were black point on the initi data point .
then we can base on the first line of the algorithm , we can select the k point as initi centroid .
now , k is two .
we can select the two initi centroid .
onc we select thi initi centroid we can form k cluster by assign each point to it's closest centroid .
now you probabl can see we can assign these , these black point to either red centroid or to the blue centroid , you can see that's a current assign .
onc you assign these to form two cluster , okai , then we need to recomput the centroid , okai , where you see , you can recomput the centroid onc you recomput thi red centroid mode down here , okai ?
then thi , blue on actual is also move , okai .
onc we recomput the centroid , we mai need to go back to reassign these point to it closest centroid .
in thi case you probabl can see , the closest centroid , like thi , blue point assign to the red on , and these red , thi red on assign to the blue side of the cluster .
okai ?
then we can calcul the centroid again .
okai ?
so thi repeat the loop here until it becom stabl or until the , the differ the differ assign becom realli , realli small .
now the first import thing we should know is , thi actual is a pretti effici algorithm for cluster .
becaus if we're told we'll have n object , we will want to find a k cluster .
suppos we get a t iter it becom stabl .
you probabl can see the comput complex is , everi time everi object that you need to see which cluster is it belong to , so thi time , you're assign to the correspond k centroid , so that's why it's n time k .
in total , you have t iter , that's why the comput complex is big o of t time k time n .
usual k and t , number of cluster and number of iter , is far smaller than number of object .
that's why if you give me n , suppos quit big number of object , thi complex essenti is a linear to the size of n .
the number of object , that's effici algorithm .
howev , k mean cluster often mai termin at a local optim .
then , the initi becom import if you want to find a high qualiti cluster .
we need to have a smart initi or we need to randomli initi thi mani time , try to find a good cluster green dot .
anoth import thing is how to specifi k , the number of cluster .
we need to specifi thi in a , advanc for the k mean algorithm .
there ar mani wai to automat determin the best k .
there ar some research paper , there ar lot of effort contribut to thi .
in practic you also can run a rang of valu as a k then select which on is the best .
then for k mean messr it's quit sensit to the nois data and outlier .
so there ar variat like k median , or k medoid algorithm we try to overcom thi outlier nois data problem .
anoth thing is k mean actual work onli to object in the continu .
that mean numer data in the , in the , in dimension space .
how to handl cluster on the categor data ?
actual , peopl invent someth call k mode algorithm .
anoth import thing we should know is , becaus we us the sum of the squar distanc , it then , it is not a good idea to try to find a cluster with non convex shape or non circular shape .
okai .
what you find is someth closer to a ball .
so we can us densiti base cluster or kernel k mean algorithm to do if we get into a non convex shape .
we're go to introduc thi algorithm later .
so , becaus k mean came quit earli and veri popular us , there ar lot of studi to work out the differ variat of the k mean method .
for exampl , how to choos initi centroid .
there ar method call k mean , intellig k mean , or genet k mean .
we ar go to introduc k mean in the next discuss .
anoth aspect is how to choos differ repres prototyp for the cluster .
that mean we mai not us the mean point , then we mai us k medoid or k median or k mode .
we're go to introduc thi in the subsequ discuss .
anoth thing is how to appli featur transform techniqu .
so , we mai be abl to cluster thing even if thei ar not in convex shape .
for exampl , thei ar massiv develop card , weight k mean or kernel k mean .
as we just discuss , the qualiti of k mean cluster , is quit sensit to the initi statu .
then we need to see how to do good in the initi , so we find a good qualiti cluster , us k mean method .
i just give you a simpl exampl you probabl can see .
differ initi mai gener rather differ cluster result .
okai .
some mai not be optim at all .
just give an exampl , suppos we have onli four point we want to find two cluster .
if we initi in these solid circl that mean these two point get to on cluster .
like if we get to the center like thi two thei will attract these two and these two , in on cluster .
then actual it becom stabil becaus you calcul the center .
the center is still here , it will not move .
so then you find a pretti ugli cluster becaus you preced these .
if you vertic group thing into two cluster , the sum of the squar distanc , or sse actual becom rather small .
from thi point of view , we know the qualiti of cluster could be veri sensit to the initi .
origin , macqueen , in <num> , said we should select the k seed randomli .
then we can , we need to run the algorithm multipl time , us differ seed .
in some softwar packag , thei mai even sai run these algorithm like <num> time .
final , you find out which on you deriv the best k mean cluster simpli sai you want to find a sse , the sum of a squar error , is minim .
there ar mani other method also propos for better initi .
for exampl , there is on call k mean , propos in <num> .
essenti thi initi is as follow .
the first centroid , you can select it randomli .
but then , the next centroid to be select , you try to find the farthest point from the current select point .
okai , that mean a select then you base on weight probabl score .
you base on differ probabl , you select which is the best for the next on .
then thi select continu until the k centroid ar obtain .
that's initi , it's done .
the we can run the k mean algorithm .
now i'll give you a simpl exampl you probabl can see , poor initi mai lead to some poor cluster .
for exampl , we take the same data set , those black point as we've shown befor .
now we give you two centroid , initi as the red on here , the blue on here .
okai , with thi initi , we can get no like cluster , we can assign these object into two differ cluster in differ color .
okai , on the upper part , those point assign to the red cluster , the lower part assign to the blue cluster .
then we can recalc the center again .
you probabl can see the center actual move .
and with thi recalcul we can reassign those point .
you can run thi a few round , you probabl can see .
final , thei stabl like thi , but actual thi is not a veri good cluster at all .
okai ?
so that mean that thi run of the k mean gener some poor qualiti cluster .
but simpli give us a simpl shell , we do need some smart initi .
multipl random initi , then we select the best on .
okai ?
so thi is the initi of k mean cluster .
now i'm go to introduc you anoth interest k partit cluster method call the k medoid cluster method .
why we need to studi k medoid cluster method ?
just becaus the k mean algorithm is sensit to outlier .
becaus the mean is sensit to the outlier itself .
just give you a simpl exampl .
if you look at a compani salari , if you add in anoth veri high salari , the averag salari of the whole compani , it can be shift quit a lot .
so , let's look at a k medoid , what is k medoid ?
that mean that instead of take the mean valu of object in a cluster as our centroid , we actual can us the most central locat object in a cluster , or we call medoid .
that mean the k medoid cluster algorithm can go in a similar wai as we first select the k point as initi repres object , that mean initi k medoid .
the differ between k mean is k mean can select the k virtual centroid , but thi on should be the k repres of real object .
then we put thi on into a repeat loop , we can assign similar we assign each point of the , to the cluster with the closest medoid .
then we can select a , randomli select a non repres object , suppos o sub i .
we're see whether we's , us o sub i to replac on medoid m .
whether it will improv the qualiti of the class rank .
that mean the total cost of swap is neg .
simpli it sai , if we're swap , we can reduc some of the squar error .
then we ar go to swap m with object o i to form the new set of medoid .
then we need to do , redo the assign , and here , thi process goe until , the converg criterion is satisfi .
now we'll see a small exampl how a typic k medoid algorithm is execut .
you look at the pam as an exampl .
suppos we ar given ten small number of point in thi small graph , okai , in thi 2d space , we want to find the two cluster .
at the veri begin we arbitrarili choos k object here , we choos two object as initi medoid .
then we will find the cluster of these medoid as follow , okai ?
then we will see whether we can randomli choos anoth , object like o random .
sai thi non medoid object , we want to see whether it could becom a medoid if it were reduc , the total cost , or we sai , we get a better sse .
in thi case , suppos we choos on here , but we found it doe not realli reduc ani you know , the total sse .
then we actual can get anoth on , like we get thi orang on .
then we look at the cluster we can form , we know thi on , will reduc the total sse , that simpli said the qualiti of the cluster is improv .
then we will do the swap .
so thi , essenti is , is list here is , we initi , we select initi k medoid randomli , then we will do object reassign , then we try to swap medoid m with the random non mediod object o sub i , if it improv the cluster qualiti .
then we'll do it again and again until the converg criterion is satisfi .
so thi is just a simpl execut to illustr the idea of thi k medoid , how it is execut .
notic , see , these k medoid cluster essenti is try to find the k repres object , or medoid , in the cluster .
and the typic algorithm pam , call partit around medoid , wa develop in <num> by kaufmann and rousseuw .
start from initi set of medoid , then we iter replac on of the medoid by on of the non medoid , if such a swap improv the total sum of the squar error , that mean the total qualiti of the cluster .
thi method work effect for small data set becaus it , we can keep try differ swap , but it cannot scale well becaus the comput complex is quit high .
if we look at , into detail actual , thi comput complex for everi swap , actual it's , it's to the squar of the number of point .
thi is quit expens .
so how to improv it effici ?
there's on propos by same author in <num> call clara .
essenti , is pam on sampl .
that mean instead of us the whole point we choos a sampl s , s is a sampl size , then the comput complex , the squar actual come down to the size of the sampl .
howev , if the sampl , initi sampl select is no good , the final cluster qualiti could be poor .
then in <num> , there's anoth algorithm , call claran , propos is , everi iter , we do random re sampl , that mean we do not keep exactli the same sampl , we do random re sampl , that will ensur the effici and the qualiti of cluster .
in thi session , i'm go to introduc the k median , and the k mode cluster method , as two interest altern to the k mean cluster method .
why we want to do k median ?
becaus the median ar better than mean when we encount outlier .
median usual less sensit to outlier compar to mean .
for exampl , in a larg firm , okai , there could be mani employe .
we want to find the median salari , even when you're ad , when you add a few top execut , the mean salari mai chang quit a lot .
the median could still be veri stabl .
so that's the reason , instead of comput the mean valu of the object in the cluster , so we take the median as the centroid , and we us l1 norm , that mean the manhattan distanc is our distanc measur .
the , the criterion function for the k median algorithm is written as thi .
essenti sum , the total sum should be k from <num> to the number of cluster k .
and for each cluster , the object in the class where you just look at the differ , thi , the differ , that take the absolut valu of their distanc to the median .
the k me , k median cluster algorithm essenti is written as follow .
the first , at the veri begin , we select the k point as the initi repres object .
that mean as initi k median .
then we get into the slope , we assign everi point to it nearest median .
then we recomput the median , us the median of each individu featur .
then thi process repeat until the converg criterion is satisfi .
then we look at a k mode as anoth interest altern to k mean .
k mode essenti is to handl categor data , becaus k mean can not handl non numer categor data .
it , we , of cours we can map categor valu to <num> <num> .
howev , thi map can not gener qualiti cluster for high dimension data .
then peopl propos k mode measur , which is an , an extens to k mean , by replac the mean of the cluster with mode .
the dissimilar measur between object x and the center of a cluster z , is written as follow .
okai .
thi is the di , the dissimilar or we can , we can ay distanc measur , distanc function .
okai .
for the j sphere of object x , that mean x of j .
and look at the j sphere of the cluster center z .
okai .
what's the distanc ?
essenti is , if thi object is not equal to the center on the same attribut , essenti sai thei ar differ .
then we just sai the distanc is <num> becaus it is veri dissimilar , or the distanc is largest .
howev , when thi valu is equal to thi attribut center here , then we give thi formula .
the thi formula actual mean , if there ar mani , mani object occur the same in thi cluster , okai , then thi valu actual will be veri small .
for exampl , if thi , if the , the attribut cluster , there is onli on variabl , it is veri frequent , and thi gui have to equal to thi valu , then thi divis will be to <num> , then the distanc is the smallest .
in most case is the more frequent for thi j valu , and thi on will be closer to <num> , so the distanc is smaller .
and if thi on is veri rare equal to the veri rare valu , and the number of distinct valu is quit big , so thi , your distanc is closer to <num> .
okai .
so thi is quit reason definit .
that mean thi , these dissimilar measur distanc function essenti is frequenc base .
the more frequent , or the , the closer , the less frequent , a littl far awai , even ar equal to thi valu .
the whole algorithm , we will not list here , but is still base on the iter on .
we first do a object cluster assign .
then do a centroid updat , then we take thi on into a loop until the criteria ar reach .
then there's anoth altern method call fuzzi k mode method .
the fuzzi k mode method essenti is , calcul the fuzzi cluster membership valu for each object , to it cluster .
simpli sai , you know , you give a fuzzi cluster valu .
okai .
if it's veri close to thi cluster , the fuzzi valu is closer to <num> .
okai .
it's far awai from thi cluster , and the fuzzi valu is somewhat closer to <num> .
okai .
and if we have the data , some attribut ar categor attribut , some attribut ar numer attribut , we can us a k prototyp method .
essenti , for numer on we can us a method , the function similar to the k mean .
and the categor on , we can us similar to k mode .
and final , we integr thi , we can get a k prototyp method .
hi , in thi session , we're go to discuss applic of cluster analysi .
cluster analysi ha lot of applic .
for exampl , it ha been properli us as pre process step or intermedi step for other data mine task .
for exampl , you can gener complex summari of data for classif , pattern discoveri , hypothesi gener and test and mani other .
and it also ha been popularli us for outlier detect , becaus outlier can be consid those point that ar far awai from ani cluster .
cluster analysi also ha been us for data summar , compress and reduct .
for exampl , in im , imag process , vector quantiz ha been us cluster analysi quit a lot .
cluster analysi also can be us for collabor filter , recommend system or custom segment , becaus cluster can be us to find like mind user or similar product .
cluster analysi also ha been us for trend detect , for dynam data .
for exampl , we can cluster stream data or detect trend and pattern in dynam data string .
and cluster analysi also ha been us for multimedia data analysi , biolog data analysi and social network analysi .
for exampl , we can us cluster cluster method to cluster imag or video or audio clip or we can us cluster analysi on gene and protein sequenc and mani other interest task .
thank you .
in thi session we're go to studi the requir and challeng of cluster analysi .
there ar mani thing to be consid for cluster analysi .
for exampl the first on we , peopl ar often to consid is partit criteria .
that mean whether we want to get a singl level or multipl level hierarch partit .
in mani case multi level hierarch partit ar what we call hierarch cluster could be quit desir .
the second thing we want to consid whether , how the cluster can separ the object , it can be hard and exclus or soft and non exclus .
exclus mean on object or on custom can onli belong to on region .
nonexclus for exampl , on document mai belong to more than on cluster , then for similar measur there could be mani differ kind of similar .
for exampl distanc base , us euclidean space or road network , or a vector space , or we can us connect base .
for exampl base on densiti or contigu .
the fourth on we ar go to discuss is cluster space .
that mean we , whether we want to do full space cluster or we want to do subspac cluster .
full space mean , especi in the low dimens for exampl in 2d , in two dimension space , in the area or the region .
whether we want to find the cluster in those region , in the full 2d space , or onli we want to project on the 1d subspac .
howev , in mani high dimension cluster , it is hard to find meaning cluster in a veri high dimension space , but it's possibl to find interest cluster in the sub space .
thing ar in the lower , correspond lower dimension space so , that sub space cluster also becom veri import .
then , for a cluster analysi , there ar mani requir and challeng , the first most import thing is the qualiti of cluster .
whether we ar go to deal with differ kind of attribut , numer on , categor data , text , multimedia data , network , or a mixtur of multipl type .
whether we can discov cluster with arbitrari shape , like an oil spill , or whether we will be abl to deal with noisi data .
then the second import issu peopl consid is scalabl .
that mean whether we will be abl in the veri big amount of data environ .
whether we can cluster all the data , or we must draw some sampl and cluster onli on the sampl data .
whether we can handl high dimension data , or we can onli project them in the low dimension space .
and whether we can do time increment cluster , especi for data stream , whether we can do stream cluster .
especi whether the cluster result mai not be sensit to the input order of the data .
anoth import issu we need to handl is constraint base cluster .
that mean user usual have their own think on the prefer or constraint of the cluster thei want to deriv .
thei mai have domain knowledg , thei mai know certain subspac could be more interest , thei mai even rais queri .
so can we handl cluster , given user prefer or constraint ?
the final issu we ar go to discuss is interpret and usabl .
simpli sai the cluster method , whether thei can deriv meaning cluster which can be understood by peopl can be us by mani applic can interpret data nice , okai ?
all these ar veri import issu we ar go to studi in thi lectur .
in thi session , we're go to provid a multi dimension categor of cluster analysi .
cluster analysi can be view in mani differ wai .
we will provid a multi dimension categor on all the differ method .
the first categor is base on techniqu , base on differ techniqu emploi in cluster analysi .
we can consid these ar densiti base method or thei ar distanc base method or grid base method or probabilist gener model base method .
or we ar essenti us dimension reduct measur to reduc the dimens befor we do effect cluster or we can directli perform high dimension cluster or whether the measur ar scalabl for larg data analysi .
and the second categor is base on data type center .
so that mean differ kind of data type like were us , rather differ cluster method .
for exampl , how to cluster numer data , categori data , text data , multimedia data , time seri data , sequenc data , stream data , network data or uncertain data .
the third categor is whether we can provid addit insight for cluster .
for exampl , we can provid a visual insight or we can incorpor some kind of semi slightli supervis or we can us ensembl base measur or we can us valid base measur .
all these , we're go to get into more detail in the next session .
thank you .
hi .
in thi session we're go to provid a brief overview of typic cluster methodolog .
there ar mani cluster method .
thei can be roughli categor into a few cluster methodolog .
the first method ar call distanc base method , the , essenti , there ar two kind of method , on call partit algorithm , the other we call hierarch algorithm .
partit algorithm essenti is partit the , the data in the high dimens space into multipl cluster .
the typic method includ k mean , k medoid , k median .
those method we ar go to introduc .
for hierarch algorithm , essenti we can either do agglom or we call bottom up .
merg mani , mani point into cluster .
men , merg small cluster into bigger on then from hierarch cluster .
or we can us divid measur .
you start with a singl big all the data is encas in on cluster .
then we try to split them , divid them us the top down split into smaller and smaller cluster .
then the second methodolog call densiti base method , the third on call grid base method , of cours thei have some linkag as well .
densit base measur essenti is , we can think that data space mai be at a higher level granular , mai think about a , a with certain grade or certain structur or certain neighbor , we mai find certain densiti .
for the dens small region we can merg them into bigger region .
in thi wai we will be abl to find the cluster , those ar dens region with arbitrari shape .
the grid base method , essenti is partit the data space into grid like structur .
each grid ha got a summari of the characterist of the data in the lower grid or lower cell .
then the third method is call probabilist and gener model .
thi mean we can model data from a gener process .
we can assum underneath there's certain distribut , for exampl , a mixtur of gaussian , okai .
then with the data we can think the current point ar gener from these underli gener model , underli , you know , gener mechan .
then we can model paramet , us expect maxim algorithm , we ar go to introduc .
base on the avail data set , we mai try to find the maximum likelihood fit .
base on thi fit we will be abl to estim the gener probabl of the underli data point .
and base on thi , we mai find the model .
then , in mani case data mai be sit in a high dimension space .
we mai need to studi high dimension cluster method .
on influenti method call subspac cluster .
you essenti try to find the cluster of variou subspac .
so that method can be categor into bottom up method , top down high dimension cluster method or correl base method .
we will also introduc some interest method like vertic cluster .
then , for high dimension cluster , there ar mani method develop for dimension reduct .
essenti , we can think the , the high dimens is in a vertic form thei ar contain in lot of column .
and for those column when we perform cluster , essenti , the column ar cluster , the row or column can be cluster togeth , we call co cluster .
there ar sever typic measur .
typic measur .
on is , probabilist latent semant index .
later , peopl develop anoth measur call latent divis , alloc .
so , plsi , or lda ar typic topic model method for text data .
essenti we can think the text can be cluster into multipl topic .
each topic is a cluster .
and each topic is associ with a set of word .
or you can think of their dimens and also the set of document you can think thei ar row simultan .
then the second popularli studi in method call non neg matrix factor , nmf .
thi is a kind of co cluster method you can think is , you get a non neg matrix becaus the word , like a word frequenc document ar non neg .
okai , thei ar zero or , or more , but thei ar non neg , you know , valu in the matrix .
for thi non neg matrix , we can approxim factor it into two non neg lower rank matric like u and v that will reduc the number of dimens .
anoth veri interest method we're go to studi is call spectral cluster .
that mean we us a spectrum of the similar matrix of the data to perform dimension reduct , the higher dimens reduc into the lower , fewer dimens .
then we can perform cluster in fewer dimens .
that's spectral cluster method .
in thi cours , we're go to studi mani , low dimens and high dimens cluster method and studi their differ variat and applic as well .
hi .
in thi session , we ar go to give a brief overview on cluster differ type of data we encount variou kind of type of data .
for exampl , the earli cluster algorithm most time we , the design wa on numer data and the second type of data is categori data .
includ binari data , that most peopl consid as also can be handl in categor data categori .
for exampl gender , race , zip code , or market basket data you will consid as discret , there's no natur outer .
for certain kind of data , text data , thi is veri popular in web in social media and the web or social network .
usual if we consid a word as on dimens then you're , we ar handl veri high dimension data , but thei ar veri spars .
their valu usual correspond to word frequenc .
the method to handl such data includ a combin of k mean and agg , agglom method , topic mod , model method , or co cluster method .
we ar go to discuss thi extens later multimedia data is anoth kind of data that need cluster .
usual imag , audio , video , those on flickr and , youtub ar actual multimedia data .
thei ar multi modal in the sens thei have imag , audio , video , mani case thei have text as caption as well .
we can consid thei ar contextu data , thei contain both behavior and a contextu attribut .
for exampl for imag , we can consid the posit of a pixel repres it context , the valu repres it behavior .
for video and music data , we can consid tempor order of the record refer it mean .
time seri data is anoth popular encount data , like in sensor stock market , tempor track or forecast .
those task usual handl time seri time seri we usual consid data tempor depend .
thei ar consecut , usual the interv is somehow equal .
like , we can consid time as contextu attribut , data valu as behavior attribut .
usual such analysi includ correl base onlin base analysi like onlin cluster of stock to find stock ticker , or we us shape base offlin analysi .
for exampl , cluster ecg base on overal shape .
sequenc data is anoth kind of data .
usual in weblog analysi , or in biolog sequenc analysi , or analyz the sequenc , the system , command , thi we can think the as placement rather than time , the contextu attribut , that mean where thei ar , okai ?
then the similar function includ ham distanc , ad distanc , the longest common subsequ .
the cluster method we call sequenc cluster , mai us suffix tree , mai us gener model like hidden markov model .
stream data is anoth kind of data , stream data can be consid as a real time .
the data like a water flow in and out , it mai evolv along time .
it mai have concept shift becaus stream come and it go .
so we need a singl pass algorithm rather than you can see it again , again .
and the typic method for stream cluster could be micro cluster , that is we need to creat a effici intermedi represent by some kind of a micro cluster method .
graph and homogen network , or is anoth kind of data .
thi kind of data usual , so call homogen network , that mean , we consid network as graph , but node and edg ar of on kind , on type .
actual , ani kind of data can be consid , can be model as a graph .
with the node as differ attribut of entri and edg similar valu .
the typic method for handl graph cluster could be gener model , combinatori algorithm , like graph cut , spectral cluster method , non neg matrix factor method .
then a littl beyond homogen network ar heterogen network .
thi kind of network consist of multipl type of node and edg .
like bibliograph data , hospit , you know , handl diseas patient , doctor , and treatment to cluster differ kind of node and link togeth .
there's some interest algorithm , like a netclu algorithm .
uncertain data mean the data mai contain nois , mai have approxim valu or multipl possibl valu .
usual , we need to incorpor some probabilist inform like distribut or approxim valu , where that will improv the qualiti of cluster .
final , recent , there ar lot of discuss on big data .
that mean we can model system that mai store and process veri big data like weblog analysi .
usual like googl's mapreduc framework you have map function to to distribut the comput across mani machin .
then have reduc function , to aggreg the result obtain from the map stat .
so , we can see that the data is veri rich .
that's why the cluster is rather sophist methodolog .
you know try to handl differ kind of data .
hi , in thi session , i'm go to give you a brief overview of user insight and cluster .
usual , user mai have some interest insight or view .
user mai like to interact with the cluster process to influenc the final result of cluster .
on interest on is visual insight , becaus a pictur is worth a thousand word .
human ey ar actual power , high speed processor becaus thei ar also link with rich knowledg base .
a human can provid a lot of intuit insight on whether thi cluster result is good or someth thei desir .
it's not achiev base on the current cluster result .
okai .
there's usual user good on two , on or two dimension .
at most 3d thei mai , thei mai be abl to find cluster .
there , you know , algorithm for exampl ar hd ey .
how to visual high dimension cluster .
the second on is semi supervis insight .
that mean user mai have certain insight or intent on the cluster process .
thei mai like to pass their insight through the system to influenc the result of the cluster .
for exampl , user mai provid some seed .
that mean you can provid a number of label exampl , or some approxim represent of the categori of interest .
then you mai like the cluster process to take your insight to do some desir cluster .
the third on is the multi view and ensembl base insight .
so call multi view cluster is differ cluster mai gener differ perspect .
the multipl cluster , you know , you can ensembl them togeth to provid a more robust solut .
final , a veri import on is valid base insight .
that mean you mai want to evalu the qualiti of the cluster gener base on certain valid measur .
for exampl , you mai us case studi or you mai provid specif measur , or you mai provid some pre exist label .
all of these we ar go to discuss in detail in some chapter .
final , i will briefli summar what we have discuss in thi lectur .
we introduc the concept of cluster and a cluster analysi .
and we also introduc some applic of cluster analysi .
we discuss the requir and the challeng of cluster analysi .
we provid a multi dimension categor of cluster analysi .
especi we provid an overview on typic cluster methodolog , overview on cluster differ type of data , and overview on user insight and cluster .
final , i provid a few refer book and especi the chapter by charu aggarw .
an introduct to cluster analysi is a veri nice introduct of the gener idea of cluster analysi .
thank you .
all right , welcom to lectur two , similar measur for cluster analysi .
in thi lectur , we're go to introduc the basic concept of similar measur , especi we were focus on measur similar between object .
were we introduc minkowski distanc for measur numer data .
introduc proxim measur for symmetr versu asymmetr binari variabl .
we will discuss the distanc measur between categor attribut , ordin attribut and mix type .
we will introduc proxim measur between two vector , cosin similar .
first , we ar go to introduc correl measur between two vector call covari and correl coeffici .
so we first get into session on .
basic concept mean how to measur similar between object .
sinc cluster is an unsupervis learn process .
so we do not have predefin labor to judg whether it's right or wrong .
so how can we judg a cluster process is good ?
that mean whether it produc high qualiti cluster .
usual we need two method , on call high intra class similar and the other call low inter class similar .
what is high intra class similar ?
that mean you want to see the object within the same cluster , thei ar rather similar , thei ar cohes .
you also want to see inter class similar is low , in the sens you want to see the object in differ cluster thei ar rather de similar or thei ar distinct .
so , to measur thi we need some separ qualiti function to measur how good a cluster is .
howev , it is hard to defin what is similar enough or good enough but the answer usual could be quit subject .
thu the reason we want to look at differ similar measur or the similar function for differ applic but thei ar critic for cluster analysi .
for terminolog , peopl actual want to distinguish quit interest similar thing call similar measur , dissimilar measur or proxim measur .
what is similar measur or similar function ?
usual similar function is a real valu function to quantifi the similar between two object .
usual the higher valu it mean more similar , so the valu rang is <num> to <num> .
<num> mean there is no similar , <num> mean thei ar complet similar or thei ar ident .
then similar to similar , mani peopl want to us dissimilar or distanc measur .
distanc measur or distanc function is a numer measur to measur how differ these two object ar .
to some extent , the dissimilar is invers of similar , that mean the lower or the shorter the distanc , the more alik .
the minimum dissimilar is zero mean thei ar complet similar or ident .
the valu rang could be either <num> to <num> or <num> to infin base on the definit .
anoth term call proxim , thi is usual refer to as either similar or dissimilar .
now we examin session two , distanc on numer data minkowski distanc .
so we first introduc data matrix and dissimilar matrix or distanc matrix .
data matrix is rep in the typic matrix form is we have n data point , we us n row .
we have i dimens , we us i column to repres thi data set .
the distanc matrix or dissimilar matrix usual is rep in triangular matrix becaus you have n data point , we regist onli the distanc between like object .
<num> versu <num> or <num> versu <num> to look at their distanc , 'kai ?
but the distanc function usual could be quit differ with differ kind of variabl like real , boolean , categor , ordin , ratio and vector variabl .
their distanc definit could be differ .
moreov , sometim thei mai have weight associ with those differ variabl base on the applic and data semant , we're go to introduc thi more later .
then we look at an exampl , suppos we have four point , four object in two dimension space .
then the data matrix is repres in thi typic form , 'kai ?
then for dissimilar matrix or distanc matrix or euclidean distanc you can see the matrix is in thi wai .
that x1 , x1 , thei ar ident their distanc is zero .
x2 , x1 their comput is base on the distanc .
thi part is <num> , thi distanc is <num> , you take sum of the squar , you take squar root you get thi valu .
then in gener , we defin the minkowski distanc as thi formula that mean if we have l dimens for object i and object j .
then their distanc is defin by take everi dimens to like , look their absolut valu of their distanc then to the power of p .
then you sum them up , get the root of p , then we get the minkowski distanc .
such distanc the p is the order , we usual also call thi distanc as l p norm .
the minkowski distanc in gener thei have these properti .
the first properti is call posit , it mean the distanc must be <num> when thei ar ident , otherwis thei ar greater than <num> .
the second properti calll symmetri , mean the distanc between i and j , distanc between j and i should be ident .
then , the third on call triangl inequ mean for the distanc between i and j .
if you go through k , that mean you go first to k then from k to j that distanc should be no less than directli go from i to j .
should be you know , greater than or equal to directli go to i to j .
the distanc measur that satisfi these three properti call metric distanc , notic not all the distanc ar metric .
for exampl , set distanc is not metric , of cours in thi lectur , we mainli discuss metric distanc .
then look , we look at some special case of minkowski distanc .
if p equal to <num> , we call l<num> norm , thei also call manhattan or citi block distanc defin thi formula .
in particular , if we ar deal with binari vector , we call these as a ham distanc it's the number of bit that ar differ .
then , if p equal <num> is defin also call l2 norm is euclidean distanc defin in the typic wai like thi .
thi is veri familiar on eh , i think for all the peopl , 'kai ?
then if p goe to infin mean we're deal with l infin norm or l max norm .
thi is also call supremum distanc is defin is a limit for p goe to infin .
in that case , actual the distanc is realli the maximum differ between ani attribut of the vector .
for exampl , you can see for f from <num> to l , the maximum such absolut valu of the distanc is the distanc of l infin norm or supremum distanc .
let's look at some exampl , for the same data set like we get four point , we get two dimens .
then we look at the manhattan distanc is just base on delet there's a citi block distanc , for exampl from x2 to xy .
you will go three block down then two block left so the manhattan distanc is <num> plu <num> , you get <num> , okai ?
of cours x1 to x1 itself is <num> .
and euclidean distanc as we discuss alreadi that's the measur for supremum distanc l infin norm , you can see x2 to x1 .
so probabl you can see the differ is you first go down three block from manhattan distanc you can see you get a <num> plu <num> .
but for supermum distanc , you just see which attribut get a maximum distanc or maximum differ .
so you probabl can see the maximum is <num> instead of <num> that's why x<num> to x1 is <num> .
so it's pretti easi to comput .
thank you .
hi , welcom to lectur <num> .
hierarch cluster method .
in thi lectur , we ar go to discuss the basic concept and method for hierarch cluster .
first , we'll discuss basic concept of hierarch algorithm includ agglom and divis cluster algorithm .
then we happen to discuss the extens to hierarch cluster .
especi we were introduc birch , cure , and chameleon .
and then we were introduc probabilist hierarch cluster .
in the first section we'll introduc basic concept of hierarch cluster algorithm .
so what is hierarch cluster ?
the hierarch cluster mean we can start from either singleton cluster iter merg them into higher level cluster .
okai .
or we can start from on big macro cluster , iter split , the bigger cluster into smaller on .
okai .
then in thi wai we will gener a cluster hierarchi .
draw as a dendogram like thi , there is no requir to specifi k the number of cluster .
and it is more determinist , there , there's no iter refin process .
hierarch cluster method contain two categori of algorithm .
in gener , the hierarch cluster method have two categori of algorithm .
on call agglom .
it start with singleton cluster , continu merg two cluster at a time to build bottom up hierarchi of cluster .
the second categori is call divis method .
essenti it start with a huge a huge macro cluster then split them continu into two group .
it will gener a top down hierarchi of cluster .
in thi session , we ar go to introduc optic order point to identifi cluster structur .
actual , thi method is an extens of dbscan by almost the same research group still led by han peter kriegel .
publish in six march <num> .
thi method produc a special cluster order of data point , with respect to densiti base cluster structur .
that mean you try to find a cluster order the veri dens cluster , actual thei ar nest within the last dens cluster structur .
so that mean contain inform equival to densiti base cluster , but it's correspond to a broad rang of paramet set , that mai overcom the sensit to the paramet set problem of dbscan .
it is good for both automat and interact cluster analysi , becaus it find intrins cluster structur .
the gener idea is , we give a fix minimum number of point , then the densiti base cluster , with respect to a higher densiti , that mean a lower epsilon ar complet contain in cluster with respect to a lower densiti .
simpli it sai , the higher densiti if thei pass a threshold , thei definit , pass a lower densiti's threshold .
so you will find a massiv structur , becaus a veri dens on nest within the less dens cluster .
that mean the higher densiti point should be process first .
you can find the high densiti cluster first .
then you can store such a cluster order us two piec of inform , on call core distanc , anoth call reachabl distanc .
so we'll see how thi is defin .
the first , how the core distanc is defin .
okai , the core distanc of a point p is the the small epsilon , such that , that p's epsilon threshold at least minimum number of point object .
suppos we sai the minimum number of point is five .
in thi case , if you look at the p , p will becom a core , if you set the epsilon prime to be three millimet .
then it will reach five point becaus you can see p and it neighborhood , total number of point ad togeth is five alreadi .
so three millimet , it is a good core distanc .
that mean , if we want to find a core distanc with respect to epsilon and minimum number of point of p , then , we will sai , if p's epsilon neighborhood cardin is smaller than minimum number of point , then p is undefin .
otherwis , p is the minimum point distanc .
that mean whenev it reach the minimum number of point , that radiu actual is the core distanc .
then we can defin reachabl distanc of object p from object q , is the minimum radiu valu that make p densiti reachabl from q .
simpli sai , if you want to reach q you can see how what is the minimum radiu .
and we can make thi number of point as five , we can keep roll , we can reach the q .
then p and q reachabl distanc is is thi valu , okai ?
if we want to defin reachabl distanc us ipsu and minimum number of point to reach p , if q is not a core object , that mean that you actual you cannot reach no matter what ipsu and minimum number of point , then we sai thi is undefin .
otherwis , you just look at the core distanc and the , the real distanc between q and p .
pick their maximum valu .
then , if we us index , suppos k is number of dimens , n is number of point , then the comput complex is o n logn it is reason effici .
so , thi method will gener a interest cluster structur , which can be describ in thi curv structur .
essenti is , if , if thei ar densiti reachabl , their reachabl distanc actual is pretti low .
but onc you try to go out of thi cluster , then the reachabl distanc will becom higher .
so if it's veri far awai you try to get it , it is undefin .
so you basic , you write the point like thi .
so the vallei correspond to the cluster and becaus you have a nest structur , so that you basic can consid thi whole thing as on cluster , but these ar the nest cluster , thei can be hierarch nest .
becaus thi on cluster and if you were nest , is three sub cluster .
so you can probabl see you have cluster within cluster , onc you do the map you still find the , the cluster instructor with differ paramet set .
actual thei ar not so sensit to the number of cluster , becaus you still see on , two , three , four .
you mai see on , two , three , four as a major cluster , but thei have some minor cluster you still will be abl to find .
here , like thi is actual a tenni court , the tenni match if we can see these other field or tree , and thi is the tenni court , these ar the player , but with thi if you tri to us the densiti base cluster , you will be abl to find sever differ thing .
you will find a veri nice hierarch cluster structur us the optic method .
we first introduc sting , the first grid base maser which is statist inform in the grid structur .
thi maser call statist inform grid approach .
essenti you can think the grid structur look like thi .
you got a multipl layer .
that mean the spatial area divid into rectangular cell at the differ level of resolut .
then these cell actual form a tree structur .
that mean each cell at a higher layer , each cell actual store a summari of the lower level set of cell .
that mean a cell at the higher level contain a number of smaller cell of the next lower level .
to that extent , it form a hierarch tree structur .
what the cell will store .
the statist inform of each cell actual calcul and store beforehand , and then can be us to answer queri .
that mean you can calcul the statist inform and it correspond set of sale and the lower layer .
then the paramet of the higher level cell , the calcul contain a bunch of statist inform includ number of point in the cell which is a count .
okai .
then their mean , their standard deviat , the minimum valu , the maximum valu , and also the type of distribut .
for exampl , whether these point form a normal distribut , or a uniform distribut , or other kind of distribut .
so if we store these cell in the hierarch wai , to process a region queri , includ find cluster , we can start at the root of thi grid structur , and proce to the next lower level us thi sting index structur .
that mean we can calcul the likelihood a cell is relev to the queri at some confid level us the statist inform store in the cell .
onc we ar confid , we need to go down onli children at the like relev cell ar recurs explor .
so it is quit local , it could be quit effici .
we can repeat thi process until the bottom layer is reach .
the advantag , thi structur is queri independ and the process can be easili paralyz and we can do increment updat as well .
also , the effici is veri good .
that mean the complex is quit low becaus if the k is number of grid cell at the lowest level .
then the complex is big o linear to k , while k is much less than n , the number of data point .
howev , it's disadvantag , becaus you form a great structur you mai lose a lot of detail .
the probabilist natur of the higher labor mai impli a loss of accuraci in the lower level for queri process .
in thi section , we ar go to introduc cliqu .
a grid base subspac cluster algorithm .
thi algorithm cliqu , actual is abbrevi of cluster in quest .
quest is ibm data mine system .
it wa develop by a group of research at ibm .
it wa publish in sigmod <num> confer .
cliqu is a densiti base , grid base subspac cluster algorithm .
why it is grid base ?
becaus it discret the data space through a grid structur and estim the densiti by count the number of point in the grid cell .
why it is a densiti base ?
becaus a cluster actual is a maximum set of connect dens unit in the subspac .
that mean a unit is dens if the fraction of the total data point contain in the unit exce certain paramet , then you try to connect it those dens unit into a structur , into a cluster that it is densiti base .
why it is subspac cluster ?
becaus it start from a low dimens , like a sinker damag .
for thi particular subspac , you try to find neighborhood dens cell in arbitrari subspac and that you can grow into a to 2d , 3d and find the maximum number of dimens .
in the subspac , it contain cluster .
it also disco , discov some minimum descript of the cluster .
that mean for thi particular algorithm , it automat identifi the subspac of a high dimens data space that allow a better cluster than the origin space us the apriori principl .
let's look at an exampl .
okai .
suppos we want to find the cluster base on salari , ag and number of vacat week .
and we can first , start on dimens to find out where ar the dens part .
for exampl if you just look at salari , you mai find the salari , the dens point is 20k to 50k .
that part is pretti dens , especi around 40k .
you'll find that the ag , it is pretti dens between <num> to <num> , okai ?
and is veri few .
probabl , the lower on .
then base on number of vacat , you probabl can see the vacat is cluster around the two to four week .
that mean we find dens region in each subspac , then gener their minimum descript .
then we us thi dens region to find a promis candid in <num> d space base on the apriori princip .
simpli sai , you find it's dens and <num> d on salari and a <num> d on ag .
then you probabl can find that thei ar ca , combin on .
that mean you try to find cluster within thi candid space .
similarli , you can find cluster within the candid space in salari and , and the vacat .
okai .
then we can repeat thi process in the level wise manner in high dimens subspac .
us the apriori principl , simpli sai if you find a dens in thi <num> d part and thi <num> d part , then you might find the candid in the <num> d region base on their intersect .
so the major step of cliqu algorithm is first , try to identifi subspac that contain cluster .
that mean we can partit the databas space into the grid structur , then find the number of point that lie insid each cell of the partit .
then try to identifi the subspac that contain the cluster us the apriori principl .
then that mean we try to identifi cluster as a second step .
determin the dens unit in all subspac of the interest , then we determin the connect dens unit in all the subspac of interest .
then we'll regener the minimum descript of the cluster .
that mean determin the maxim region that cover the cluster of the connect dens unit then determin the minim cover of each cluster .
so thi method , the interest point is it automat find the subspac of the highest dimension as long as the high densiti cluster exist in those subspac .
becaus you find a <num> d , you try to find their intersect .
if the <num> d form a cluster , the intact intersect at a <num> d space , like you mai be abl to find cluster and it is insensit to the order of record .
we input and also let's not assum a particular data distribut .
and it's scale linearli with the size of the input and ha good scalabl as the number of dimens in the data when the data increas .
well , thi is quit effici algorithm .
the weak of the method is essenti , becaus us , you , we us the grid base cluster approach .
so the qualiti of the result would depend on how to choos the number and width of the partit and the grid cell .
nevertheless , it's a veri interest subspac cluster method .
then for thi lectur , we do a simpl summari as we introduc densiti base cluster and a grid base cluster .
we introduc the basic concept of the densiti base cluster .
we introduc dbscan and optic method and we also introduc the concept of grid base cluster .
we introduc the sting and the cliqu that's two interest grid base algorithm .
final , i should sai , all the origin paper on these densiti base and grid base cluster ar list .
also , for charu aggarw and reddi's book there ar two chapter .
on is call densiti base cluster by martin ester .
anoth is call grid base cluster by cheng , wang and batista .
thei have a veri good summari and also introduc mani more method on densiti base cluster and the grid base cluster method .
hi , welcom to lectur <num> , probabilist model base cluster method .
in thi lectur , we ar go to introduc basic concept of probabilist model base cluster .
especi , we will discuss mixtur model for cluster analysi .
we will focu on gaussian mixtur model .
we will discuss the expect maxim algorithm for both univari and a multivari case .
and then we will analyz the mixtur of model , model method .
but for the first session we were introduc the basic concept of probabilist model base cluster .
what is probabilist model ?
that is , the model is try to model the data from a gener process , that mean we assum the data ar gener by a mixtur of underli probabilist distribut .
then the method is try to optim the fit between the observ data and some mathemat model us a probabilist approach .
then for probabilist model base cluster as we , everi cluster , each cluster can be repres mathemat by a parametr probabilist distribut .
for exampl , could be a gaussian distribut , could be poisson distribut .
then each cluster actual is a set of data point or object that most like belong to the same distribut .
then the cluster process essenti is paramet estim so that thei will have a maximum likelihood fit to the model by a mixtur of k compon , and these k compon distribut essenti ar k cluster .
it ha lot of applic , for exampl for imag segment , document cluster , topic model , recent in machin learn , in statist , and in data mine topic model and the em algorithm ar veri popular method .
we were look at the typic probabilist base cluster method .
first , we will examin mixtur model that mean we assum observ to be cluster ar drawn from on of sever compon , and we'll infer the paramet of these compon , and assign data point to specif compon of the mixtur .
and each compon we can think is a cluster .
then the expect maxim algorithm is a gener techniqu to find the maximum likelihood estim in mixtur model .
we were discuss the em , the em algorithm for gaussian mixtur model .
actual , there's anoth veri popular method , method call probabilist topic model .
thi is veri popularli us for text cluster and analysi .
howev , in thi cours , we ar not go to cover thi part .
thi part would be cover in the text mine cours .
essenti , probabilist topic model includ probabilist latent semant analysi , call plsa , and latent dirichlet alloc , call lda .
in thi session we're go to introduc gaussian mixtur model .
that mean we'll assum all the cluster form base on the gaussian distribut with differ paramet .
we'll first look at a gaussian distribut for univari , that mean for singl variabl and for multivari , that mean for multipl variabl .
we first draw some plot and contour for gaussian distribut .
for exampl , we can see for thi plot thi is a gaussian distribut for univari , for the singl variabl the paramet like thi on is mu , the mean is <num> sigma the standard deviat is <num> .
thi is a typic standard bell curv .
then for mu equal <num> and sigma equal <num> we can draw a curv like thi .
then for <num> d gaussian distribut , we give mu and a sigma we draw the contour for for the on set of paramet mu and sigma here .
for anoth set of mu and sigma covari matrix we'll draw like thi .
so thi is a typic wai to have a visual of differ paramet for gaussian distribut .
now we studi gaussian mixtur model .
we assum everi cluster c sub i , is character by a multivari normal distribut , where f sub i of x is a probabl distribut of x attribut to cluster c sub i .
that mean we can repres thi densiti function of x under the cluster c sub i .
thi paramet is mu sub i and sigma sub i .
mu is a mean and sigma sub i is a covari matrix for the cluster i .
and we can write in the typic gaussian distribut form for multivari .
then we can assum the probabl densiti function of x is given by all the k cluster normal from a gaussian mixtur model .
that mean we will sai the densiti function f of x essenti defin by summat of all the k cluster .
so each k cluster , the densiti function thi f sub i of x .
the probabl of thi cluster is p , probabl of cluster c sub i .
then we can repres in the paramet form is mu sub i and sigma sub i we can sai for all the k cluster .
each cluster is a paramet is under the condit of mu sub i , and sigma sub i .
then the prior probabl , probabl p of c sub i .
if for all the k cluster thei ad togeth their probabl should be <num> .
so we call thi is mix your paramet .
then our task is try to get maximum likelihood estim of you know the model paramet .
essenti is if we want to sai given the data set d what's the probabl under the model paramet of theta , we can gener thi whole set of d .
sinc we assum all these number , these n point ar independ .
we can think their probabl is their probabl densiti function of x sub j .
thei all multipli togeth for all the end point , we get thi .
if we write thi in log form , so we can take the log of thi function and these multipl by take log , it becom summat , so the comput could be more effici .
and thi f of x sub j , we can us a previou on to thi function here .
then maximum likelihood estim is to choos the paramet set theta .
so to make thi probabl to be maxim .
or we can sai we can think to maxim the log likelihood .
that mean we want to find a set of paramet theta , so the log of these probabl gener , the theta data set d under the paramet theta is maxim .
howev , to directli maxim thi log , log likelihood over theta is hard .
it's veri expens .
so we can us em approach to find the maximum likelihood estim for the paramet theta .
the em approach essenti consist of two step iter on is expect anoth is maxim .
the expect is given the current estim of theta , we try to comput the cluster posterior probabl .
that mean , we want to com , comput what's the probabl of the cluster c sub i under the con , condit of x sub j .
and we can us the bay theorem to write in the bay theorem form that mean we want to find the probabl of c sub i under the condit of xj .
it is actual is the function essenti the numer is actual the densiti function of x sub j for cluster sub i time the probabl of cluster sub i .
then the denomin is all the k cluster for each cluster that gener a j .
then we were think about the probabl to gener the cluster c sub a .
then for all these k cluster , we add all these togeth .
that's our denomin .
so for maxim step we can , essenti is us the comput cluster posterior probabl to re estim the paramet's theta .
that mean we want to re estim mu sub i , sigma sub i , and the probabl of c sub i , for each cluster , c sub i .
in thi session , we ar go to examin the expect , maxim , algorithm for multivari case .
and we probabl can look at initi , then we get into expect and the maxim step .
each initi is veri much like the singl variabl case , is we randomli initi mu .
that mean for mu sub <num> to mu sub k , these k cluster , we just randomli initi them .
then for covari , we for all the k cluster we give them ident matrix assign to them .
and for the probabl of c sub i we give them equal probabl for all the k cluster .
then we put thi into repeat and hear loop and hear the sum of the chang of the mean across two iter is no greater than a pre specifi threshold epsilon .
then for the expect step , we just assign object to cluster accord to the current paramet of the probabl cluster .
that mean , for all the , cluster and for all the object j , from <num> to n .
we will calcul the posterior probabl for c sub i under the condit of x sub j .
and these calcul is veri much the same , you us you , you , the numer part you will get to the densiti function of the , of under the condit of mu and sigma , and then , time the probabl of c sub i .
then , for the denomin , we will calcul the same wai for everi cluster , and just sum them up .
then for maxim step , thi we try to find a new cluster or paramet that minim sse or the expect likelihood .
then , for for cluster i from <num> to k , we will recalcul in thi wai .
re estim the mean .
re estim the covari matrix and re estim prior .
and we will repeat thi until the chang becom realli , realli small .
now we look at the , the demonstr of these two , of two dimension data set to see how these exam , these em exam algorithm is execut .
when we start , thi is the origin data point .
then we look at the initi you probabl can see .
even you get a two cluster , center , but thei ar not a distinguish .
thei , thei , you can hardli see , the what thing ar go on .
but in the second iter you will see slightli chang .
but when you get into certain semi iter you clearli see two center .
then thei form nice into cluster .
so thi is in the , is a demonstr .
on the , everi step while thing ar chang and here to the thing becom re realli , realli stabl .
thi is the last session of thi lectur .
so we will studi the analysi of the mixtur model method .
first , we want to discuss the relationship between k mean and em .
k mean , actual can be consid as a special case of em .
in the sens , k mean is a hard em .
that mean if you look at these two condit probabl , actual the probabl of x sub j , thi object belong to the cluster c sub i .
it's either on or zero .
that mean , if thi object is , is closest is closest to thi center , then it belong to thi cluster .
okai .
so the assign is crisp , is hard is on .
otherwis , it doe not belong to thi cluster , the probabl is zero .
similarli for the probabl of cluster c sub i under the condit of x sub j , if x sub j is in thi cluster , then the probabl is on .
that mean , if thi x of j is closest to thi center , that is <num> belong to thi cluster .
otherwis , it's zero .
to that extent , we sai , k mean can be view as a hard em .
in the e step , in the expect step , we take the local minimum instead of a distribut .
that mean , we assign thi cluster thi object to <num> to on cluster , as long as it's closest to the center of thi cluster .
and the gaussian mixtur model is the soft version of k mean , becaus we calcul the distribut instead of the most like on in the e step and then us weight sum to comput the new center in the m step .
okai .
to that extent , gaussian mixtur model introduc the varianc to learn , but the cluster in the k mean have the same varianc .
now we look at the other issu , the initi and speed up of expect maxim .
and we alreadi know hard and soft cluster assign .
k mean is hard assign cluster and probabilist cluster is soft assign point to cluster .
then if we compar with k mean , the em algorithm for gaussian mixtur model will take mani more iter to reach converg .
becaus everi time you soft assign , thi assign is pretti gradual is pretti progress , but take mani more iter .
then if we want to speed up the converg of gaussian mix model , we can get a good initi .
the typic method could be we can first , run the k mean algorithm , becaus it's fast .
okai .
then we can choos the mean and the covari of the cluster base on the result of k mean .
then we will get the fraction of data point assign to their respect cluster to initi the mean , the covari matrix and the probabl , the prior probabl .
sometim , a gaussian compon .
a gaussian cluster mai collaps to a particular data point .
to that extent , it would caus a singular problem .
then when we detect a gaussian compon to thi cluster is collaps , we can reset it mean and covari , then continu with the optim .
now we summar the mixtur model , their strength and weak .
the strength is a mixtur model ar more gener than the partit method or fuzzi cluster method .
the cluster can be character by a small number of paramet .
the result mai satisfi the statist assumpt of the gener model .
howev , the weak of the mixtur model is it mai converg to local optim .
thi is similar to k mean .
to overcom thi problem similar to k mean , we can run the mixer model multipl time with random initi .
also , for mixer model , like em for gaussian mixer model , though computation it is quit expens .
if the number distribut is larg or the dataset contain veri few observ data point .
that also mean it , it take a larger dataset to run it .
and the finer problem is it is hard to estim the number of cluster .
thi is similar to k mean .
actual all these studi , there ar some follow up research .
thei tri to propos differ measur to overcom such difficulti .
in summari , in thi lectur we studi probabilist model base cluster method .
we introduc the basic concept of probabilist model base cluster .
we introduc a mix model for cluster analysi .
especi we introduc the gaussian mixtur model .
then we discuss the expect maxim algorithm for univari and multivari .
final , we analyz and discuss differ method for mixtur model method .
so , final i will give a list of refer if you want to go deeper you mai read those articl or book .
thei contain some rich discuss of gaussian mixtur model and em algorithm .
thank you .
hi , in thi and subsequ session , we ar go to discuss sever extens to hierarch cluster .
hierarch cluster method look simpl , but on the other hand , there ar some weak of hierarch cluster method .
the first weak is the master can never undo what wa done previous .
for divid method , you first try to figur out how to divid on cluster into two subclust .
but onc you divid them into two subclust , you work on each subclust and try to do further split .
you will never be abl to merg them back again and try to adjust to some particular element .
even for agglom cluster , the philosophi is similar in a sens .
onc you try to merg two cluster into on , the subsequ you know , analysi will be treat thi on as on unit .
you will never split it again , and you try to see whether other subclust obtain so far can be further merg .
thi , you know , requir you for everi split or merg must be final .
that's too high requir to gener high qualiti cluster .
the second problem is the master mai not scale well just becaus everi time when you try to merg , you try to check all the possibl pair .
so the complex is at least n squar , when you want to split , you try mani differ possibl choic , to try to find the best to split but complex is also high .
there ar some develop other hierarch cluster algorithm .
in thi lectur , we ar go to introduc three of them .
on is a birch , develop in <num> , you us micro cluster , macro cluster idea , so us cluster featur tree and increment adjust the qualiti of subclust .
the second on we ar go to introduc is call cure , develop in <num> .
that method essenti is to , repres the cluster us a set of well scatter repres point .
the third on , chameleon , it wa develop in <num> , it us graph partit method on k nearest neighbor graph of the data .
we will introduc all these three , on by on .
in thi session we ar go to introduc you an interest extens of hierarch cluster method , call birch a micro cluster base approach .
birch is an abbrevi of balanc iter reduc and cluster us hierarchi .
it wa develop by a group of research in univers of wisconsin in <num> .
the gener philosophi is , it increment construct a cf tree , or call a cluster featur tree , which is a hierarch data structur for multiphas cluster .
for phase <num> , essenti , it scan the databas to construct an initi in memori cf tree .
which is a multi level compress of the data that tri to preserv the inher cluster structur of the data .
then at phase <num> , it us an arbitrari cluster algorithm to cluster the leaf node of the cf tree .
the kei idea is multilevel cluster .
the low level it doe micro cluster , therefor reduc the complex , and increas scalabl .
at the high level , it doe macro cluster .
it leav enough flexibl for high level cluster us differ cluster methodolog .
the birch method scale linearli .
that mean you , it find a good cluster with a singl scan , and then improv the qualiti with a few addit scan .
the cluster featur we first introduc is the cf vector in birch .
the birch cluster featur essenti is suppos you get these five point into on cluster .
okai ?
then suppos these ar the five point , their posit , okai ?
then the cf vector contain three compon .
on is the number of data point .
the second is a linear sum of the point in the cluster .
the third on is squar sum of the n point .
okai .
so you probabl can see the , the first on , <num> mean there ar <num> point .
the second on act as a linear sum of each dimens .
okai .
the third on actual is the squar sum of each dimens .
so the cluster featur essenti is the summari of the statist of a given sub cluster , which you can consid the number is the zeroth , the first on is the linear , the second on is the second moment of the sub cluster from the statist point of view .
that mean it will regist the crucial measur of the , for comput cluster and util storag quit effici .
so we can look at the , the gener concept of centroid , radiu , and diamet .
okai .
the centroid essenti is the center of the cluster , okai ?
then , suppos we have a vector of n dimens , x sub i .
okai .
then , the centroid is essenti comput by the sum of all the point in thi cluster divid by the number of point in the cluster , so that what we get is a centroid of the cluster .
okai .
then the radiu actual is the averag distanc from the member object to the centroid .
that essenti is everi on you get a differ with the centroid , then we us the sum of their squar distanc .
divid by the number of point in the cluster .
take their squar root .
essenti it's the squar root of the averag distanc from ani point of the cluster to it centroid .
what is diamet ?
diamet essenti is averag pairwis distanc within the cluster .
that mean if x i and x j is within the same cluster , so essenti what we want , we will find is there ar total n time n minu <num> pair and we sum up all these pairwis distanc then you get the squar root .
that's the diamet .
then we look at cf tree structur in birch .
the cf tree structur essenti is veri much like a , b tree .
we can do increment insert of the new point .
that mean when the new point come , okai , we can find the closest leaf entri .
start from the root .
okai , then we try , travers we find the closest entri , we can add point to the leaf entri and updat the cluster featur , cf .
if thi entri diamet is greater than the maximum diamet , then we'll split the leaf and if it's possibl we even will be abl to split parent base on the b tree algorithm .
a cf tree ha two paramet , on call branch factor .
that mean the maximum number of children .
anoth is maximum diamet of sub cluster store at the leaf node .
then a cf tree essenti is height balanc tree that store the cluster featur .
the non leaf node store the sum of cluster featur of their children .
so we can see birch is an interest algorithm , becaus it , it is an integr of agglom cluster with other flexibl cluster method .
the low level we do micro cluster .
we explor the cf featur and birch tree structur .
it preserv the inher cluster structur of the data .
at the high level we do macro cluster .
it provid suffici flexibl for integr with other cluster method .
so thi method act , impact to mani other cluster method and applic for larg data set .
there ar some concern .
on is the , the birch tree is still sensit to the insert order of the data point .
anoth is , sinc the leaf node ha a fix size , the cluster obtain mai not be as natur .
and also , the cluster tend to be spheric given the radiu and diamet measur as the major paramet .
still , it is pretti interest algorithm and it can gener quit effect cluster .
hi .
in thi session , we're go to introduc an interest hierarch cluster algorithm extens , call chameleon , which is a graph partit on the knn , mean k nearest neighbor graph of the data .
so thi graph partit base approach wa develop in <num> by a group of research in univers of minnesota .
chameleon actual ha some interest point .
it measur the similar base on a dynam model .
essenti , two cluster ar merg onli if the interconnect and the close between the two cluster ar high rel to their intern interconnect of the cluster and a close of item within the cluster .
chameleon is a graph base two phase algorithm .
in the first phase , it us a graph partit algorithm .
essenti , it cluster object into a larg number of rel small sub cluster , also we call graphlet .
then in the second phase , it us an agglom hierarch cluster algorithm to find the genuin cluster by repeatedli combin these sub cluster or these graphlet .
let's look at the overal framework of chameleon .
suppos we have a larg dataset , then we can us k nn graph to merg them into spars graph , like thi .
what is k nn graph ?
essenti we can sai that two point , p and q ar connect .
if q is among the top k closest neighbor of p , and base on thi , k nearest neighbor , you can circuit thi graph .
then we will partit thi graph into a good number of small subclust , or call graphlet , 'kai ?
after thi partit , we ar go to merg , it , them into high qualiti cluster .
how we merg those graphlet back to the bigger cluster , essenti the merg is base on these two measur .
on we call rel interconnect , anoth we call rel close .
rel interconnect mean the connect of c sub <num> and c sub <num> over their respect intern connect , 'kai .
then the rel close is defin as close between cluster c sub <num> and c sub <num> over their respect intern close .
let's look at the method in some detail , 'kai .
the first is knn graph .
what is knn graph ?
let's look at the exampl , 'kai .
thi is the origin , two dimension dataset , so the first and nearest neighbor , <num> nearest neighbor graph , is for each point , each , you know , object , you just find the top on closest neighbor .
for exampl , thi on's closest neighbor could be here .
thi on's closest neighbor could be here .
then you construct a sub graph base on these , you get a <num> knn , you get a <num> <num> nearest neighbor graph .
for <num> nearest neighbor graph , is you , for each point , you look there two , top <num> nearest neighbor in construct graph like thi .
<num> nearest neighbor graph is for each point there look at their top three nearest neighbor , and you construct the graph like thi .
'kai , then we look at the absolut interconnect between c sub i and c sub j , okai ?
these absolut in , interconnect is essenti the sum of the weight of the edg that connect vertic in c sub i to vertic in c sub j .
essenti , you look at the we , weight sum of these edg .
then the intern connect of your cluster , c sub i , is essenti size of it min cut bisector .
the weight sum of these edg partit roughli equal part .
then you look at the weight sum of the edg of these partit .
then the rel interconnect , ri , essenti is for c sub i and c sub j , is defin by their ab , absolut interconnect divid by we can sai normal line by the averag of their respect interconnect .
then we look at the rel close and the merg of subclust .
rel close between a pair of cluster c sub i and c sub j is defin as follow .
the first is what is absolut close between c sub i and c sub j .
the absolut close essenti is averag weight of the edg connect these , the vertic thi , between these two class .
then their intern essenti , the intern close , like thi intern close of c sub i , or intern close of c sub j , is the averag weight of the edg that , that belong to the min cut bisector of cluster c sub i and c sub j respect .
that mean you look at what is min cut bisector to cut thi cluster or cut thi cluster , then you'll find eh , there averag weight of the edg , okai .
then you probabl can see where rel is a , is the absolut close , normal with their improportion defin intern close .
then how do we merg subclust ?
essenti is we were onli merg those pair of cluster whose rel interconnect , and rel close , ar both abov some user specifi threshold .
that mean we onli merg those maxim the function that combin rel interconnect and the rel close .
that mean even we find those pair unmerg we still want to find these merg , we're maxim the function .
then , base on the implement and experi we probabl can see chameleon actual can cluster complex object like thi , 'kai .
thi actual ik , these ar all the differ point with differ densiti eh , differ shape as well as lot of nois point , 'kai .
even with differ shape , differ densiti , and differ shape and differ nois , we still can see chameleon actual gener pretti high qualiti cluster , okai .
that simpli sai chameleon , us thi graph partit , and the merg method is pretti robust and gener a pretti high qualiti cluster .
even , it is in the spirit of hierarch cluster , it doe do a lot of good with thi graph base method .
hi , in thi session we ar go to introduc an interest hierarch cluster algorithm extens call chameleon which is a graph partit on the knn , mean kenya's neighbor graph of the data .
so , thi graph partit base approach wa develop in <num> by a group of research at the univers of minnesota .
chameleon actual ha some interest point .
it measur the similar base on a dynam model .
essenti two cluster ar merg .
onli if the interconnect and the close between the two cluster ar high rel to their intern interconnect of the cluster .
and a close of item within the cluster .
chameleon is a graph base , two phase algorithm .
in the first phase , it us a graph partit algorithm .
essenti , it cluster object into a larg number of rel small sub cluster .
also , we call it graphlet .
then , in the second phase , it us an agglom hierarch cluster algorithm to find the genuin cluster by repeatedli combin these sub cluster or these graphlet .
let's look at the overal framework of chameleon .
suppos we have a larg data set .
then we can us k nn graph to merg them into spars graph like thi .
what is k nn graph ?
essenti we can see that two point , p and q , ar connect .
if q is among the top k closest neighbor of p , then base on thi k nearest neighbor , you construct thi graph .
then we'll repartit thi graph into a good number of smaller subclust , or call graphlet .
after thi partit , we ar go to merg them into high qualiti cluster .
how we merg those graphlet back to the bigger cluster ?
essenti the , the merg is base on these two measur .
on we call rel interconnect , anoth we call rel close .
rel interconnect mean the connect of c sub <num> and c sub <num> over their respect intern connect .
okai .
then the rel close is defin as close between cluster c sub <num> and c sub <num> over their respect intern close .
let's look at the , the messert in some detail .
the first is knn graph .
what is knn graph ?
let's look at the exampl .
thi is the origin two dimension data set .
so the first nearest neighbor , on nearest neighbor graph is for each point , each you know , object .
you just find the top on closest neighbor .
for exampl , thi on closest neighbor could be here , thi on close neighbor could be here .
then you construct a subgraph base on these you get a on , you get a on , on nearest neighbor graph .
for two nearest neighbor graph is you , for each point , you look their two , top two nearest neighbor , construct a graph like thi .
three nearest neighbor graph is for each point , you look at their top three nearest neighbor in a construct graph like thi .
okai ?
then , we look at their absolut interconnect between c sub i and c sub j .
okai .
these absolut in , interconnect is , essenti , the sum of the weight of the edg .
that connect vertic in c sub i to vertic in c sub j .
essenti you look at the weight sum of these edg .
then the intern connect of the cluster c sub i essenti the size of it min cut bisector , the weight sum of these edg partit the graph into two roughli equal part .
then you look at the weight sum of the edg of these partit .
then the rel interconnect , ri , essenti is for c sub i and c sub j is defin by their ab , absolut interconnect divid by or you can sai normal by the averag of their respect interconnect .
then we look at the rel close and the merg of sub cluster .
rel close between a pair of cluster c sub i and c sub j is defin as follow .
the first is what is absolut close between c sub i and c sub j .
the absolut close essenti is the averag weight of the edg connect these , the vertic between these two cluster .
then their intern essenti the intern close like thi intern close of c sub i intern close of c sub j , is the averag weight of the edg that belong to the min cut bisector of cluster c sub i and c sub j , respect .
that mean you look at what is min cut bisector cut thi cluster , or cut thi cluster .
then you'll find , their averag weight of the edg .
okai .
then you probabl can see the rel is , is the absolut close normal with their inproportion defin intern close .
then how do we merg sub cluster ?
essenti is we will onli merg those pair of cluster whose rel interconnect and rel close ar both abov some user specifi threshold .
that mean we onli merg those maxim the function that combin rel interconnect and rel close .
that mean even we find those pair ar mergeabl , we still want to find these merg , we're maxim the function .
then base on the implement and experi we properli can see , chameleon actual can cluster complex object like thi .
okai .
thi actual , these ar all the differ point with differ densiti differ shape , as well as lot of nois point .
even with differ shape , differ densiti and differ shape and differ nois , we still can see chameleon actual gener pretti high qualiti cluster .
okai .
that simpli sai chameleon us thi graph partion and emerg method is pretti robust and gener a pretti high qualiti cluster .
even it is in the spirit of hierarch cluster , it doe do a lot of good with thi graph base method .
in the last session of thi lectur , we ar go to introduc probabilist hierarch cluster .
what is probabilist hierarch cluster ?
let's first examin the problem of algorithm hierarch cluster .
the first problem for algorithm hierarch cluster , it is , it is nontrivi to choos a good distanc measur .
for exampl , we alreadi see the singl link , the complet link , the averag link and the sensual link .
thei're all differ measur , but it's hard to choos a good distanc measur base on the applic problem .
the second problem is it is hard to handl miss attribut valu , becaus on , the attribut valu is miss , it mai impact on the qualiti of classroom .
is the optim call of algorithm hierarch cluster .
it's not so clear , becaus it's a heurist algorithm .
it more reli on local search .
now there's anoth approach call card probabilist hierarch cluster .
thi method , essenti us probabilist model to measur distanc between cluster .
there is larg a gener model , mean it regard the set of data object to be cluster as a sampl of the underli data gener mechan to be analyz .
then you final , find the paramet , find out how thei gener thi data set .
it is easi to understand , it ha the same effici as algorithm agglom cluster method .
and also , it can handl partial observ data .
in practic , we can assum the gener model adopt common distribut function .
for exampl it , we can assum is a gaussian distribut or bernoulli distribut and govern by a set of paramet .
let's look at the gener model .
in the gener model , we can assum given a set of on dimension point x .
we can assum thei ar gener by a gaussian distribut , like thi .
okai .
that mean , you essenti us differ mu and sigma you mai gener the distribut accord to the typic normal distribut or gaussian distribut formula .
okai .
then the probabl of a singl point x sub i , which is on point in the , the set x is gener by the model .
essenti , it's what's the probabl to gener x sub i under the condit of mu and sigma is base on thi formula .
then for the likelihood thi whole set of x gener , essenti is the likelihood for uh , uh , for the normal distribut to gener thi set of x point .
essenti is the probabl of a set of point x under the condit of mu and sigma squar .
for i from on to n , all these probabl multipli togeth , we get the probabl that gener the dataset x .
then the task of learn thi gener model , essenti to try to find the paramet mu and sigma .
so the for thi particular distribut , we try to find the maximum likelihood thi set of x is gener accord to thi , to paramet .
so the gaussian distribut usual i think most peopl know , for exampl , for bean machin if you drop the ball with pin if you us mani , mani ball final , you will find the distribut is gaussian distribut .
then if we have mani point , we can assum these point actual gener by the gaussian distribut like thi .
actual , for on dimens gaussian , thi differ mu , differ mean and differ varianc , you probabl can see thei gener differ distribut .
for two dimens gaussian , it , it mai gener in the , in the 3d space you could clearli see the , the distribut of these 2d gaussian .
then a probabilist hierarch cluster algorithm , essenti is we suppos we alreadi got partit the whole set of point into m cluster c sub <num> to c sub m .
then the qualiti of the class rank can be measur us thi qualiti function , that's the product of all those probabl .
then if we want to merg two cluster , c sub j1 and c sub j2 , then the chang in qualiti of the overal cluster is the word criteria .
okai .
so you probabl can see , thi is the merg cluster .
you gener a new cluster and these origin two cluster disappear from the origin set of cluster .
so that's the new cluster .
that's the origin cluster , becaus you merg thi , essenti the sum of the squar arrow will be increas , so that's the reason you try to minim such increas .
that's a probabl of pc sub i , now you get a probabl of these two merg togeth .
okai .
then you have to minu the origin probabl i from <num> to n .
then the distanc between cluster c1 and c2 is thi is the probabl of c1 and c2 togeth as on cluster , these ar the independ cluster .
if thi distanc is less than zero , we should merg these two cluster .
so that's the idea or the essenc of probabilist hierarch cluster algorithm .
so now , we summar the whole lectur .
fi , we introduc the hierarch cluster method .
we first introduc the basic concept of hierarch cluster , then we introduc agglom cluster and divis cluster .
we also studi the extens to hierarch cluster , especi we discuss birch , cure and chameleon algorithm .
final , we introduc the concept of probabilist hierarch cluster .
these ar set of book and paper .
for exampl , these three paper is birch , cure and chameleon .
that's the origin paper .
most of other , other textbook discuss differ measur and , and altern of hierarchi cluster algorithm .
thank you .
welcom to lectur <num> , method for cluster valid .
we have been work on cluster measur , but a veri import thing is how to evalu whether the cluster there is good , is stabl or is valid .
so we want to introduc the basic concept of cluster renov , then we will studi how to evalu cluster qualiti .
especi we will introduc extern measur , intern measur and the rel measur .
for extern measur we ar go to introduc match base measur , entropi base measur , and pairwis measur .
we will also studi cluster stabil and cluster tendenc .
we first introduc some basic concept of cluster valid .
we know cluster is unsupervis method .
in a sens , we do not have ani expert to judg whether thi cluster is good , what should be cluster .
so it is import to evalu cluster qualiti that mean evalu the good of the cluster .
also we want evalu cluster stabil .
that mean to understand the sensit of the cluster result to variou algorithm paramet .
for exampl , the number of cluster .
that mean what number of cluster is realli good base on our evalu .
the third on is cluster tendenc .
that mean whether it's suitabl to do cluster .
in thi session , we ar go to studi anoth import issu call cluster tendenc .
what is cluster tendenc ?
that mean we want to studi whether the data sai we have realli contain cluster .
that mean whether data contain some inher group structur .
otherwis , even us random gener data , you mai find the certain number of cluster , howev it doe not realli make sens .
that mean we want to assess whether the data is suitabl for cluster .
howev , to determin the cluster tendenc or cluster is a hard task .
just becaus there ar so mani definit of cluster .
for exampl , we studi partit method , hierarch method , densiti base method , graph base method .
all these differ method mai have differ definit of cluster .
that's why to studi whether the data is cluster or not is pretti hard .
howev , even when we just fix the cluster type , for exampl , we just studi the partit method , it is still hard to defin appropri null model for a data set .
howev , there's still some studi on cluster assess .
for exampl , there ar method like spatial histogram method , distanc distribut method or hopkin statist .
the gener philosophi , thei try to compar their measur with the measur gener from random sampl to see whether thei ar rather differ .
for exampl , for spatial histogram method , is try to contrast the histogram of data , with those , the histogram gener from the random set to see whether thei ar rather differ .
for distanc distribut is try to compar the pairwis point distanc from the data , and the pairwis distanc from the randomli gener sampl .
hopkin statist is to us a spars sampl test for spatial random .
and sinc thei carri a quit similar spirit , we were onli introduc on spatial histogram method .
for spatial histogram approach , the gener philosophi is we try to construct a d dimension histogram of the input .
for exampl , thi is a 2d , we mai construct a two dimension histogram for the input dataset d with a histogram gener it from random sampl .
that mean we won't compar the figur gener from the left and to that gener from the right figur .
okai ?
the data is cluster if these two histogram distribut ar rather differ .
that mean thei ar realli gener veri differ distribut then like thi dataset is cluster , okai ?
the concret method is we try to divid each dimens into an equi width bin .
then we try to see each cell here , how mani point ar in thi set .
then we can gener a empir joint probabl mass function , okai .
then for the random gener dataset , we'll do the same , we'll gener empir joint probabl mass function .
then what we will do is we try to compar whether these two , thei differ quit a lot .
that mean we're go to compar how much thei differ .
a typic method is we us kl diverg .
that mean we calcul the kl diverg valu .
kl diverg is kullback leibler diverg is defin base on some formula .
i'm not go to introduc the detail of kl diverg .
if you want to learn more , you can check ani textbook in statist or machin learn or data mine .
in gener , thei introduc thi measur .
then you find if thei ar veri differ , then from the random sampl , then you will sai thi dataset is cluster .
now we come to the lectur summari .
thi lectur we discuss cluster valid then we introduc basic concept why we want to valid cluster .
especi we discuss why is how to measur the cluster qualiti .
and we have the extern measur and that mean we do have .
in that case , we introduc match base measur , entropi base measur and a pairwis measur .
then we also studi when we do not have we will us some intern measur .
and we also studi the rel measur that simpli sai you have differ paramet , you gener differ cluster , you want to compar them .
then we also studi cluster stabil issu , we studi cluster tendenc issu .
okai ?
final , i'm go to introduc a few textbook , chapter or gener paper .
thei discuss the cluster valid measur .
especi where it show thi on is a , a summari of mani research paper , and zaki's book , it give a lot of detail on other measur we did not cover in thi lectur .
thank you .
welcom to lectur eight , cluster high dimension data .
in thi lectur , we ar go to first discuss challeng of cluster high dimension data .
then we'll introduc method for cluster high dimension data .
especi , we'll discuss subspac cluster method , includ subspac search method , correl base method , bi cluster method .
within bi cluster method , we ar go to discuss theta bi cluster and theta pcluster .
we also briefli introduc dimension reduct method .
so for the first session , we're go to discuss challeng of classifi high dimension data .
so why we have to cluster high dimension data ?
so the first question is how high is high dimension in cluster ?
in our previou discuss , mani exampl we give actual onli contain on to three dimens .
the most popular exampl is onli two dimens .
for the , for show for exampl , k mean , kernel k mean , em algorithm , but these method mai not work well when the number of dimens grow to <num> .
not to sai in mani applic , such as text document or dna micro arrai data .
if we take everi keyword as on dimens in text document , or we take everi word in the micro arrai dataset , then we mai need to handl ten of thousand of dimens .
so there ar some challeng at handl high dimens data .
for exampl , there could be mani irrelev dimens that mai mask the real cluster .
the second problem is that thi distanc measur mai becom meaningless due to equidist .
we're go to introduc thi a littl more .
moreov , mani cluster , mai realli exist onli in some subspac .
if you take all the dimens into consider , you mai not be abl to find a quit meaning cluster .
let's look at it .
the curs of dimension problem .
usual , data in on dimens is rel pack .
it could be quit dens .
howev , when you stretch it into sai , two dimens , then the data origin project on 1d now becom 2d data , it becom more far apart .
and you add more dimens the data even becom more further apart .
so , in high dimension space , the data could be veri spars .
everybodi mai have such experi .
for exampl , you watch the night sky on the dome .
you're pretti much see , there's some star , some bright or not so bright .
thei ar veri close on the sky , but at onc you realli studi like us telescop or ask astronom , you probabl know these veri close star in the real 3d space , it could be veri far apart .
okai .
that's mean the 2d project you see is readili pack , but the on that go to 3d , it mai becom pretti far apart .
then the distanc measur becom meaningless when you go to veri high dimens , just becaus in dimens a , you mai find that these two is veri far apart .
in dimens b , it could be just nois or someth .
but onc you see , there ar mani , mani dimens , there ar differ could be you have data in thi dimens and i have data in the other dimens .
and our measur , you measur is <num> , <num> as a unit .
i measur , onli like ten as a unit .
so , in that sens , in mani case , the distanc measur mai becom meaningless .
so some tradit data distanc could be domin by nois in mani dimens .
now , if we summar the curs of dimension problem , there ar five differ aspect refer to the curs of dimension .
the first aspect is optim .
that mean when the data , you know , get to a higher and higher dimens , then to do global optim becom more and more difficult .
the complex mai increas , you know , exponenti .
so that's a reason , you know , optim mai becom a challeng .
then the second problem is distanc concentr effect of the lp norm .
the lp norm , for exampl , if ly , we just us the manhattan distanc or two is just euclidean distanc , we studi thi .
but the distanc concentr problem , mean far and a close neighbor mai have similar distanc just becaus the , when the dimens is veri high , their differ probabl just on , whether you have data in thi dimens or we have differ dimens .
we have differ measur , so their rel contrast of lp distanc could diminish , you know , when the dimens increas .
that's a distanc concentr problem .
the third problem is irrelev attribut .
for exampl , you want to give a word to student .
there ar mani dimens could be irrelev .
for exampl , whether student height or student bodi weight or the student telephon number or student first place .
those probabl relev for your , a work composit .
so , in that sens , if you put all these irrelev attribut togeth , actual and final , you mai realli interfer your real purpos of cluster .
therefor , impact the perform .
the fullest problem in thi correl attribut that mean in mani case , you get a subset attribut , thei mai have strong correl .
okai .
then if we take all the dimens , the dimens veri high .
but sinc the strong correl , these set of attribut can be repres by onli on or two dimens that's a dimension reduct problem .
just give you a simpl exampl .
suppos you want to give student a word , mayb you want to look at gpa .
of cours , you can look at everi cours , includ their mid term and finer and homework .
mani , mani cours actual have strong correl with their gpa .
so you probabl just want to look at their gpa or their major gpa , instead of look all those attribut .
that mean the intrins dimension of dataset can be consider lower than the embed dimens .
that mean the real number of featur of the dataset .
when in intrins dimens could be like a gpa or the qualiti or the perform of the student .
but on the other hand , the embed dimens could get all the detail everi cours , their perform .
okai .
so that's the reason you want to do dimension deduct .
okai .
the fifth aspect for the curs of dimension problem is data sparsiti .
we ar us the exampl the show .
the data volum , the high dimens space could be extrem spars .
becaus in a veri high damag in space , no sum mai not even have a point , becaus that point doe not have that high damag in .
okai .
that's a data sparsiti problem .
so those differ aspect combin togeth , we can sai , thi is in realli curs of dimension .
we have to invent effect method to overcom thi problem , you , other to do effect cluster .
in thi session we will briefli outlin the method for cluster high dimension data .
so , the method for classifi high dimension data can be group into two categori .
on categori call subspac cluster .
that mean instead of find cluster includ all the high dimension space , we actual want to search for cluster onli exist in some subspac .
for exampl , cliqu , we ar alreadi introduc as a grade base approach , it is also a subspac cluster approach .
other approach includ pro class , includ by cluster approach wagon to introduc .
then the second categori call dimension reduct approach .
thi approach essenti is we realli consid all the dimens but we want to construct a much lower dimension space by some kind of transform a combin of mani dimens .
then we search for cluster in the correspond transform low damag in space .
for exampl , spectral cluster , non neg matrix factor and variou dimension reduct vassil you'll see in the literatur .
on the other hand , cluster shouldn't not onli consid dimens , but also consid featur or we call attribut .
okai .
there ar featur select method .
that mean , you try to find a particular subspac .
the data mai have nice cluster , mai have , veri nice featur .
anoth approach call featur transform , that mean , if you find mani , mani dimens irrelev , the current featur mai not be in the right spot base on the data .
you try to find for certain transform you mai be abl to reduc number of damag .
for exampl like a princip compon and asset , or singl valu decomposit , these measur .
if the featur highli correl or redund these method can do transform to reduc the number of dimens into a small set .
in thi session we ar go to first give a gener overview of subspac cluster method .
why do we want to do subspac cluster , just becaus mani cluster mai exist onli in some subspac .
subspac cluster at least try to find cluster in all the subspac from the low subspac like onli on d , to two d , to three d , you know , as far as we can go .
here we just show some pictur for exampl the origin data set visual you can see thei ar four color mean four cluster .
howev , thei intermingl togeth .
if we project onli on dimens a , we mai find it red and blue more were separ .
but the other color is somehow cluster , mix togeth .
but if we predict dimens b will have differ view , dimens c will also have differ view .
but if we predict dimens a and b , in thi two dimens space we find the green and red actual is wai far separ from the other part .
so , if we can see project on differ dimens you get differ cluster .
subspac cluster method .
in gener , we can get into axi parallel versu arbitrarili orient subspac .
axi parallel mean there's a subspac we're go to find , we're go to search in parallel with some axi .
like in parallel with x axi , y axi , z axi , or their combin , like x y plane .
arbitrarili orient subspac mean actual the real cluster could be , you know , not correl with x and y , these kind of plane , but it , still that in some wai , you mai be abl to do some transform .
subspac search method essenti is just to pace on axi parallel subspac , then we try to search and find cluster .
we can us bottom up approach , then start from 1d , then combin them into 2d , 3d space and the , you know , higher on .
top down approach is , mean you first find it in a higher dimens space and you try to find the lower on .
then search and cluster in arbitrarili orient subspac essenti is we can do correl base cluster method .
for exampl , we do pca base approach where we can introduc a littl more detail later .
so that's what you mai find if you transform into certain orient .
you mai reduc the dimens .
you mai do final cluster .
we will also studi bi cluster method .
especi bi cluster method could be optim base or enumer base method .
in thi session we ar go to introduc the first kind of subspac cluster method call , subspac search method .
what is subspac search method ?
that mean we try to search variou subspac to find cluster .
the measur can be categor into two categori call bottom up approach and top down approach .
for bottom up approach , it start from the low d , the lowest d , of cours , it's on dimens subspac .
then you try to find dens region , and then you try and search higher , higher dimens like two d , three d .
onli when you can find cluster or dens region in the low dimens space and then you go to the high damag to find their candid .
so there ar variou kind of prune techniqu to reduc the number of high dimens space to be search .
for exampl , cliqu is introduc as a grid base approach by agarw there's a typic bottom up approach .
the top down approach is start from the full space and search smaller sub space recurs .
it is effect onli if the local assumpt hold .
that mean we restrict our search to those sub space of cluster .
becaus the sub space can be determin by the local neighborhood .
if we don't have thi assumpt , don't have thi local assumpt , so the search mai not be effect .
a typic method is proclu method by cheryl aggarw .
a k medoid like method .
then we just review what we did for cliqu .
the cliqu , essenti is we start from on dimens space like salari or vacat , when these in <num> d , thei ar dens .
you try to find the , the great in the <num> d candid becaus we're correspond on dimens space ar pretti dens .
okai , so you find dens region in everi such space .
and then you try to gener their minimum descript like cluster .
then you onli us thi dens region to find promis candid in higher dimension space .
for exampl , that when salari and vacat in these part , ar dens , you find their intersect these ar candid to check whether thei ar dens .
onli when thei ar dens , then you try to find the higher dimension space candid .
so , cliqu actual automat identifi these subspac from low to high .
then it will termin when no more cluster or cluster candid can be found , so you actual found all the lower dimens subspac and here you cannot go even higher .
in thi session , we're go to introduc the second kind of subspac cluster method call correl base method .
essenti for subspac cluster method , the similar measur can be base on distanc or densiti , either .
but for correl base method , it is essenti base on advanc correl model .
for exampl , pca , what we call princip compon analysi .
thi approach , you probabl can see from the pictur on the right .
you see , suppos we have two dimens , x1 and x2 .
those point project into both x1 and x2 is certain distribut .
howev , if we get anoth axi e , we probabl can see , becaus all the point line up , all cluster around axi e .
so , if we us axi e and it's perpendicular axi , we probabl will see essenti , us a e axi , we will be abl to repres the data .
so that mean we want to find a project that captur the largest amount of variat in data .
then we can appli pca to deriv a set of new , uncorrel dimens .
that essenti we get a two dimension data predictor onto on dimens .
we did a dimension reduct .
then we can find cluster in the new space or it subspac , where we find dimens in the lower dimension space , find cluster .
other correl base method includ hough transform or fractal dimens , we ar not go to introduc these .
interest peopl can read wiki or some textbook .
here i give a littl simpl illustr of princip compon analysi in a littl detail .
suppos we have big n data point , data vector from small n dimens .
we want find a k orthogon vector , that mean princip compon .
usual the k is much smaller and n .
and then , if these princip compon can be best us to repres dat , a then thi transform will be veri meaning .
becaus we can find in a much lower dimens , we can find cluster .
so the first step , is we can normal the input data .
that mean each attribut will fall within the same rang .
then , we ar go to comput orthogon , or we call linearli uncorrel , or you can sai perpendicular unit vector .
that mean we want to find these k princip compon .
now each input vector , essenti it's a linear combin of these k princip compon vector .
then we can sort these princip compon base on the decreas signific order , or strength order .
that mean , the stronger on will be order higher than the lower signific on .
then we can elimin the weak compon .
that mean , those with veri low varianc , we can elimin them .
that mean we us the strongest princip compon to wrap in the dataset .
it is possibl to construct a good approxim of the origin data .
then we can find a cluster just on those strongest princip compon .
thi is princip compon and asset .
of cours , the detail how to comput it we're not go to introduc here becaus it would involv like comput covari matrix , comput their eigenvector .
it's a littl too mathemat .
if you want to know more , there ar lot of textbook you can read .
in thi session , we're go to introduc a third kind of subspac cluster method call bi cluster method .
what is bi cluster ?
bi cluster is to cluster both object and attribut simultan .
that mean we treat object and attribut in a symmetr wai .
let's look at exampl .
suppos we go to gene express data analysi .
we have row .
everi row repres a gene .
we have m column .
everi column repres a sampl or a condit .
essenti , these ar the real valu repres express level .
then , we have four requir to find bi cluster .
the first requir is , onli a small set of object particip in a cluster .
that mean , onli some gene particip in a cluster .
then , the second requir is , a cluster onli involv a small number of attribut .
simpli sai , onli some column ar involv in the cluster .
then the third line is an object mai particip in multipl cluster or doe not particip in ani cluster at all .
simpli sai , some gene , some row , some gene mai be in multipl cluster , but there could be mani gene that ar not in ani cluster at all .
the first on is an attribut mai be involv in multipl cluster or is not involv in ani cluster at all .
that mean some column like thi condit , thi sampl , could be involv in more than on cluster , but there could be mani column that ar not involv ani cluster at all .
then , a typic exampl is a gene express , or microarrai data .
what you got is thi matrix we call gene sampl , or gene condit matrix .
so each element in the matrix repres real number , it record the express level of a gene under a certain condit .
anoth exampl could be custom , and product , their relationship .
thi simpli sai , on custom mai be involv in these bi cluster , in more than on cluster .
but , at some product , also mai be involv in more than on cluster .
or some product mai not be involv ani cluster at all .
now we look at the type of bi cluster .
suppos we get a set of gene and a set of condit .
bi cluster essenti try to find a submatrix like thi .
the gene and condit follow some nice consist pattern .
we can look at differ ideal case .
could be we mai find on kind of bi cluster , or could be everi valu just like a constant .
like if thi on is <num> , these ar all 10s is , idea of constant valu sub cluster and thi bi cluster is veri idea but it bare can see such case .
the second case is the bi cluster with constant valu in row .
that mean each row in thi bi cluster , thei have a constant valu .
the third on could be , the bi cluster have some coher valu .
that mean thei have certain pattern .
thi , aka , mean as no , as pattern base cluster .
for exampl , we can see here , you mai observ some relat like if you've increas by <num> , so everyth just like a copi , and here increas <num> , everi year like a copi , decreas by <num> , everi row also like a copi .
that is also ideal case .
the last on could be if all these row in a column .
thei have the relationship .
it's hard to find .
for exampl , thi increas by <num> , thi by <num> , but thi is by <num> .
thi is by <num> .
if you find thi , what is the relationship ?
and we find the relationship is the increas or decreas follow the same trend .
simpli sai , you look at the correspond two cell , you will sai , if 0increas , the other on is also increas their product will be greater than zero .
okai , or , y , increas the other on at least maintain the same , so it's equal to <num> .
if thei ar decreas , that simpli sai , you get neg and neg .
thei multipli still posit .
so that mean , you're onli interest in up or down regul chang .
in that case , thi is anoth kind of pattern .
then , in the real world data , thei cannot be so ideal like we illustr abov .
that mean the real world data is noisi .
we try to find not ideal bi cluster , but we try to find approxim bi cluster .
essenti , there's two kind of method , on call optim base method , anoth call enumer base method .
the optim base method is try to find submatric onli on at a time but we try to achiev the best signific as a bi cluster .
simpli it sai , we try to find the best on we can find each time .
howev , the comput is veri costli .
try to find best on mai not be quit realist .
the best is to us a greedi search to find local optim bi cluster .
that mean in , local , thi is best , global , it mai not be .
okai .
then , on such algorithm call data bi cluster algorithm develop by cheng and church in ismb , publish in intern symposium of molecular biologi inspir from ethic confer .
anoth wai is enumer base method .
the method essenti is , to us a toler threshold .
that mean , as long as thei ar approxim , the follow within certain threshold , we can specifi the degre of nois allow in the bi cluster .
then we try to enumer all these sub matric as bi cluster that satisfi the requir .
try to find thi is like a pattern analysi , so on algorithm call delta p cluster , algorithm develop by wang's group , publish in <num>'s sigmod confer .
their further refin , call mapl , wa publish by jan pei and other in icdm confer .
in thi session , we ar go to introduc on bi cluster method call delta bi cluster .
bi cluster is quit us for micro arrai data analysi .
let's look at thi figur <num> , which we us <num> color <num> object or you can think as <num> by okai .
if you link those gene valu togeth , okai .
what you is , you couldn't find ani obviou cluster or pattern , becaus in gene a , b , c you pretti see some goe up and some goe down , is a littl like irregular it , it veri hard to find a good pattern .
howev , if you pick up some gene , some subset of gene .
then you line them up togeth you'll find a pretti nice pattern .
for exampl thi figur b show a shift pattern , becaus you shift thi green on down .
you'll find it's overlap with the blue on .
if you up a littl you'll find it overlap with the red on .
okai .
even figur c , what you see is if you scale a littl , like , the red on , scale a littl , you get a overlap or almost overlap with a green on , and , if you scale a littl more , you overlap with a blue on .
so these ar the shift pattern and scale pattern .
the problem is , can we find a nice subset of object , and subset of gene ?
thei realli can have rhythm in nice cluster .
now we see what could be the algorithm .
thei ar in a bi cluster algorithm , it's on of such algorithm find such nice cluster .
and we can defin for ani sub matrix i time j , that mean you pick up sub row and sub column , okai ?
subset of row and column .
you can calcul mean of thi row , essenti is you take row , so you pick all the the column of thi submatrix .
you perceiv and add all the valu togeth , divid by size of these , how mani column you get .
the mean of the row .
similarli you can get a mean of j's column .
and you can get mean of the whole submatrix .
then we want to judg the qualiti of thi sub matrix as a bi cluster .
what we can do is we take thi part as a residu .
and the residu essenti is you take ani item i j , okai .
thi particular cell you look at , thi sub matrix you look at i's row mean and j's column mean and the whole matrix mean .
then you look at thi valu , the differ between thi , okai .
then you take the squar .
we call mean squar residu .
then you try to add them up and divid by the size of i time j .
okai .
for the qualiti of thi , if thi valu is within more special data , then we can sai thi on is data bi cluster , but thi data is usual a human set up valu , usual is a small valu if thi delta is <num> , then thi submatrix is a perfect bi cluster , with all the coher valu .
howev , in mani case , it cannot be so ideal .
but then , you give a threshold greater than <num> .
you can set up a toler threshold .
essenti you want to find the averag nois per element against a perfect bi cluster .
thi is why we us thi residu valu , we can either judg whether the cluster is a good or nice bi cluster .
then the problem is , how can we find an effici method to find a maximum delta bi cluster ?
what is maxim on ?
the maxim mean , you enlarg a littl more no matter how you enlarg it their qualiti will go beyond thi dereha threshold .
so within thi dereha threshold , it's the largest on , of cours the subset in mani case it could be a nice pattern bi cluster but find the largest on will make more sens .
howev to comput thi , to find the maximum delta bi cluster is veri costli .
thei develop a heurist greedi search method to obtain local optim maximum delta bi cluster .
so the maxim contain <num> phase , <num> call delet phase , anoth call addit phase .
in the delet phase you start from the whole matrix .
you sai , oh , thi on , whole matrix is a nice delta bi cluster , in most case , it is not .
what you need to do is you iter remov some row and some column if their mean squar residu valu is over delta's threshold , of cours , how do you pick up thi ?
you put thi on into an iter loop , so for each row or column , you just examin their mean squar residu valu .
you look at the , thi row's mean squar residu valu and column's mean squar residu valu then you see which on , which column or which row , actual ha the largest mean squar residu .
you just cut them off , becaus thi is the worst case , or the least case thei mai for a nice delta bi cluster .
that's why i chop those row and column .
howev , if you chop thi , you quit often thi chop is too aggress becaus you chop it the other wai and some subset of row and column thei still form nice bi cluster .
so , in that sens you need addit phase , thi on is probabl too aggress .
you want to grow a littl .
how do you grow ?
that mean we try to expand iter thi delta bi cluster obtain from the delet phase .
okai ?
and as long as when we expand thi delta bi cluster requir is still maintain .
that mean , it's still within the delta structur .
that mean , we try to calcul all the row and column not involv the current bi cluster sub matrix .
the charact there mean we ar residu .
then the smallest mean squar residu , if thi column or thi row , thei have the smallest mean squar residu .
you try to add them back .
to get the delta bi cluster to see whether thei ar still a delta bi cluster , okai ?
in thi delet addit phase , you actual onli find on delta bi cluster .
but you want to find mani delta bi cluster , you need to run thi phase is multipl time .
and how do you run thi multipl time ?
that mean you just replac the element in the output bi cluster by random number .
that mean you add thi up , you , the smallest on , what about you add the second smallest on ?
third smallest on ?
you can do thi in a random number .
you can add a good qualiti while you find other cluster .
so thi is quit a costli search process , but it's still quit interest .
you'll find a quit a good number of their bi cluster .
in thi session , we ar go to introduc anoth interest bi cluster method , call delta pcluster .
we just studi delta bi cluster , cheng and church's approach .
thei publish their paper in ismb year <num> .
their approach is an optim base approach .
essenti is you first try to chop anyth with quit a big residu , you try to find a small on , actual chop quit mani in the delet phase .
then you try to add , on by on , row and column to add them back to still maintain their xmo , thei're a bi cluster .
okai .
that's quit costli .
so an effici algorithm call delta pcluster is explor pattern similar us enumer approach done by hijin wang's group , thei publish in year <num> sigmod confer .
that measur of the gener philosophi is , we get submatrix i time j if we want to defin thei ar perfect coher , we just look at i1j1 , i2j1 , these cell is differ , then look at i1j2 , i2j2 .
these two cell , thei're rather differ , okai ?
so , that mean for ani two by two matrix , we can defin p score as follow .
you get thi , smallest two by two matrix , you want see thei ar two row , differ differ column then look at their differ .
if their differ is the absolut valu , it's veri small .
simpli sai the p score is pretti small .
that mean thei ar almost perfect coher .
okai , then we can defin bigger on like i time j submatrix .
okai .
so , thei start a pcluster or we can sai thei're a pattern base to close , cluster .
essenti if you look at it , everi two by two submatrix thei ar p score if it's within their threshold .
thi delta is a user specifi toler threshold , toler of the nois .
then thi p score control the nois on everi sub element , submatrix of the delta bi cluster , which is actual a littl more rigor compar to the delta bi cluster algorithm thei us mean squar residu .
thei captur the , the averag nois .
thi on is captur everi <num> by <num> matrix thei ar submatrix or everi submatrix , thei ar nois .
then thi delta pcluster ha a nice monoton behavior .
that mean , you look at at the i time j submatrix .
actual , everi , their submatrix , that mean for everi x time y , x and y greater than <num> .
thei ar <num> by <num> , <num> by <num> , <num> by <num> , all of these submatrix .
thei ar also delta bi cluster .
so it's quit nice we have the monoton .
with thi monoton , we can us , like , a averag base algorithm .
like in pattern discoveri , we can find it .
howev , we onli want find the maxim y .
that mean if no more row and column can be ad to still make it a delta bi cluster , then we sai thi on is a maxim .
our algorithm onli need to comput the maxim delta pcluster .
actual , there ar some addit advantag .
on advantag is , it contain no outlier .
becaus the origin delta bi cluster algorithm is take the averag of these ratio .
but the outreach actual it still , it mai contain the outlier just becaus you get a quit big and quit small valu onc the averag , thei still maintain valu in the middl .
so thei mai still , within the delta threshold , so it mai not be so nice .
anoth interest thing is if we look at scale pattern like thi pattern , the down part of the right of thi corner , thi pattern .
we per see , if we take , for thi scale pattern , if we take a logarithm on thi valu , becaus scale mean , if you take the ratio , the ratio should be less than delta .
but , if you take a log , the origin divis becom the differ .
so it take exact p score form , the onli differ is all of these valu just take their log .
okai ?
then , thi actual , you do not need a new algorithm , you also can find the scale pattern , which is quit nice .
a year later jian pei work togeth with haixun wang , thei develop anoth algorithm call mapl to further improv the mine effici , base on a pattern base approach , like f p growth like method .
but thei actual just need to find a frequent close itemset .
that method is base on the ca , condit of the combin you can find a maxim subset of gene .
thei're still delta bi cluster .
then you , base on thi idea , you can further improv the process effici .
in thi session , we're go to introduc some dimension reduct method .
the dimension reduct measur .
the gener concept is in some situat , it could be more effect to construct new space , that mean we transform into some reduc number of dimens , instead of us some subspac of the origin data .
that mean the space should be chang .
essenti , origin m time n , you want to get smaller on .
let's look at thi figur , okai ?
thi figur , if we us the origin ax , for exampl x and y , you predict on x and y , that you can now find nice cluster becaus all these point actual overlap on both x axi project and y axi project .
howev , if we get a new , thi dash axi , okai , thi y , like , i turn the y , x , you know , the axi <num> degre .
what you can see is these point where you , you project thi axi to form three nice , separat cluster .
that mean sometim we transform a littl , but we can reduc the number of dimens .
in the meantim , we can find nice cluster .
that simpli sai , in mani case , featur select or featur extract mai not focu on find cluster structur .
howev , we mai us dimension reduct method to reduc the dimension us some mathemat transform .
there ar two veri popular approach , on call nonneg matrix factor .
we will briefli outlin the idea in the next slide .
the gener idea is for on high dimension spars nonneg matrix , we mai be abl to factor them approxim into two low dimension matric , okai ?
anoth method call spectral cluster , thi we will introduc a littl more detail in the next lectur , lectur ten , is to us a spectrum of the similar matrix of the data to perform dimension reduct .
so final , we mai find cluster in a much fewer number of dimens .
thi method essenti is to combin featur select and the cluster togeth .
there ar two popular spectral cluster method , on call normal cut by shi and malik .
anoth is ng jordan and weiss method , led by andrew ng publish in nip <num> .
we also call thi on njw method .
let's look at nonneg matrix factor , okai ?
for exampl , you can think about you get quit a larg set of document and thei have a lot of word , okai ?
then the word frequenc in document from a pretti high dimens nonneg matrix .
nonneg becaus the word frequenc can be either a posit valu or zero , mean there's no such word exist in thi document .
okai .
so , for thi nonneg matrix , it could be quit spars .
now , we can do some transform .
that mean we can automat factor , i mean to tune the low dimension nonneg matric , like u and v .
okai , origin it's n time d , is number of word and number of document .
now , you get n time k .
k could be much smaller valu .
and you get anoth on , it's k time d .
okai , that mean on matrix , a with n time d dimens is factor into u and v , these two matrix multipl .
but on is n time k , anoth , k by d , okai ?
so , thi is , that mean that on matrix a is factor into two factor , u and v .
of cours , sinc it's approxim , that mean you mai have a residu matrix r repres the nois in the underli data .
thi r actual is the differ between the origin high dimens nonneg matrix and the factor , the two factor matric .
their multipl , okai , their product .
so thi transform essenti is we can consid it is constrain optim .
that mean we can determin u and v as an optim problem so that the sum of the squar of the residu in r is minim .
that mean we try to find thi r as small as possibl .
that mean thi u time v as close as possibl .
actual , from the semant mean , u and v actual simultan provid the cluster on the row and the column .
row , you can think these ar the document , column , other word .
so thi becaus you simultan cluster on row and column , so it's kind of co cluster .
that mean for u as an n by k matrix , these essenti ar the compon of each of the , these n object and word , map into each of the k newli creat dimens .
okai , then for v , k time d , you can consid as each of the k newli creat dimens in term the origin d dimens .
so the advantag is thi factor get result is rather interpret , in the sens becaus it did not realli transform their dimens .
thei either repres it base on the word or repres base on the document .
okai , so the data point , it can be express as a nonneg linear combin of the concept , that mean word and document , in the underli data .
thei do not realli transform the whole data set into a new axi .
so in summari , in thi lectur , cluster high dimension data , we discuss the challeng of high dimens data cluster .
we also introduc the gener method for cluster high dimension data .
especi we focus on subspac cluster method .
we discuss subspac search method , correl base method , bi cluster method .
for bi cluster , we discuss delta bi cluster algorithm and delta p cluster algorithm .
also , we briefli introduc the dimension reduct method .
for the rec , recommend read these ar mostli the paper we've discuss in thi lectur .
so you mai find a nice origin paper you can read .
and final , arthur zimek got a , a nice summari call cluster high dimension data chapter in charu aggarw's book you mai like to read to find some new or addit method we did not discuss in thi lectur .
thank you .
now we get into cluster evalu , measur cluster qualiti .
measur cluster qualiti is an import issu just becaus cluster is unsupervis measur .
we want to evalu the good of cluster result by either some intern or extern measur .
unfortun there's no commonli recogn best suitabl measur in practic .
but there ar three categori of measur we call extern measur , intern measur and rel measur .
for extern measur , we can consid thei ar supervis , emploi criteria not inher to the dataset itself .
that mean we mai have some prior or expert knowledg .
for exampl , some ground truth .
then we can compar the cluster result against the prior or expert specifi knowledg , us certain cluster qualiti measur .
then the second kind of measur ar call intern measur , which is unsupervis .
that mean the criteria deriv from the data itself .
in that case , we will evalu the good of cluster by consid how well the cluster ar separ and how compact the cluster ar .
for exampl , we can us silhouett coeffici .
the third on is a rel measur .
that mean we can directli compar differ class ring us those obtain via differ paramet set for the same algorithm .
for exampl , for the same algorithm , we us differ number of cluster .
we mai gener differ cluster result .
we mai want to compar those result to , for exampl , we can us silhouett coeffici to see how nice , by set a certain number of cluster , the cluster ar better separ and you , within each cluster thei ar more compact so thi is a rel measur to compar differ paramet set .
welcom to lectur <num> .
constraint base cluster .
in thi lectur we're go to discuss why do we want to do constraint base cluster .
we will studi differ categori of constraint .
we will studi how to do constraint base cluster .
for exampl , how to handl hard constraint , how to handl soft constraint , how to encopp constraint , and distanc measur .
final , we will discuss user guid constraint .
that mean take user hint as constraint .
so at first discuss , why do we want to do constraint base cluster ?
constraint base cluster mean cluster by take user specifi constraint into consider .
usual , thi kind of cluster , also call semi supervis cluster .
becaus thei ar often in the form like cannot link , must link .
cannot link mean these two object must be in separ cluster .
must link mean these two point or two sub cluster must be link togeth , must be in the same cluster .
constrain base cluster can be much broader .
for exampl , we can take a distanc constraint , user guidanc constraint , user mai specifi differ number of cluster , mai specifi the granular of cluster .
all of these can be taken as constraint .
constraint base cluster actual is veri us .
it's highli desir .
why ?
becaus user usual know there applic the best .
also you us more user desir constraint and guidanc .
you can take less paramet .
there ar mani , mani applic thei mai like to us , user guid or constraint base cluster .
for exampl , if you want to group student into differ group , peopl mai ask why do you want to group those student ?
ar you consid differ kind of award ?
or do you want to organ parti ?
you think differ food or differ costum ?
and we probabl can look at thi , map .
suppos we want to alloc some deliveri center .
for exampl , you want some up or feder express .
you want to have differ center , you can deliv it to the hous , in a citi .
howev when you think about thi , you cannot just us euclidean distanc to comput the center of a cluster .
becaus usual the car cannot fly over the river .
thei have to go through the highwai , thei cannot cross mountain or lake .
so thei have to go around it .
also there could be avail road , thei mai even consid traffic , and other thing .
so that mean , we , in mani applic , will have to consid our constraint when we do cluster .
in thi session , we just introduc differ categori of constraint .
constraint can be categor base on what kind of constrain it is , okai ?
so the constraint can be instanc object .
for exampl , you can specifi how a pair or set of instanc , those object should be group in the cluster analysi .
usual the most typic on ar must link versu cannot link constraint .
must link of x and y mean the object x and y must be group togeth into on cluster in your cluster analysi .
for exampl , you can think these two orang color object .
onc we sai must link that mean in your finer , in your cluster analysi when you try to form cluster these two object should be in the same cluster , okai ?
constraint also can be defin as variabl .
for exampl , you can sai cannot link of x , y if the distanc of x , y is greater than the predefin distanc d .
for exampl , you can sai these two object , differ color , point of orang color , other is blue color .
if we sai these two object cannot be link togeth simpli sai .
you can sai these two object should be in differ cluster but onc you specifi these constraint .
you basic sai , ani kind of object , x and y , if their distanc is greater than d then cannot link , simplic cannot be link togeth .
the constraint on cluster mean you want to specifi requir of the cluster .
that mean final when you form cluster , what the cluster should satisfi ?
okai , for exampl , you mai specifi a minimum number of object in the cluster .
simpli sai , thi cluster must contain at least sai <num> optic .
the maximum time of a cluster simpli sai these cluster , the diamond cannot be over sai <num> .
the shape of cluster , for exampl the cluster must be in the shape of convex .
the number of cluster , for exampl the k .
also the constraint can be on distanc measur , that mean we can specifi requir , the distanc calcul .
the distanc function , distanc calcul must respect such a constraint .
for exampl , we can sai eh , when you ar think about hous , household we us set up deliv center .
you can sai the car must be drive on the road when you calcul the distanc .
and thei must observ obstacl that mean thei cannot cross river and lake us euclidean distanc .
thei have the think about a cross bridg or go around .
so these ar the popular us , the differ categori of constraint .
in thi session , we ar go to discuss constraint case cluster method .
we will first discuss how to handl hard constraint .
what ar hard constraint ?
simpli sai , we want the algorithm to strictli respect the constraint in the cluster assign .
that we were introduc on interest algorithm call cop k mean algorithm .
essenti it is k mean but in the constraint process it must observ like a must link and a cannot link constraint .
so , to follow the must link constraint as the hard constraint , what we can do is we regener the super instanc , i mean super object .
suppos you got a five object thei link togeth in some wai , okai ?
so you , what you can do is you first us the must link , you base on a link to b , b link to c , c link to d then abcd should be a transit closur of these object .
so you comput the transit closur of the must link constraint then you can wrap in the , all those , object .
thei follow these constraint , you wrap them as a superset , that mean to wrap in such a subset of the object , we can replac all those object in the subset by on super object .
the object you can take the mean , as a center , as thi object of locat and these super object should carri a weight .
if you get a four object , thei final merg into on super object then the weight should be four which is a number of object it repres .
then for the remain , we can just wake up the k mean to do all the cluster process .
then , how can we modifi the k mean on to observ the cannot link constraint ?
that mean , if these two object , thei have a constraint sai these two object cannot be link togeth , how can we do it ?
what do we need to do is adjust it to modifi the center assign process the k mean .
it simpli sai , you have to sign these object to the nearest feasibl center .
that simpli sai , if you sign on object to it near , near , nearest center , the oth , other object just cannot be sign to the center .
we have to send , assign to anoth center , anoth feedabl center to the closest on , okai ?
that mean everi time the object is assign to the nearest feasibl center , so the assign will respect all cannot link constraint .
but thi is the modif of the k mean , to handl hard constraint for must link and cannot link constraint .
in thi session we ar go to discuss how to handl soft constraint in constraint base cluster .
soft constraint mean you have must link and cannot link constraint .
you better observ it , becaus you pai less cost .
if you aren't not observ thi constraint to to go over thi constraint , you have to pai some penalti .
so that mean we will treat constraint base cluster as a optim problem .
that mean when the cluster violat a soft constraint , a penalti should be impos on the cluster , okai .
then the overal object is we want to optim the cluster qualiti and also minim the constraint violat penalti .
let's look at on exampl algorithm call constrain vector quantiz error algorithm , or call cvqe algorithm .
that algorithm is to conduct a k mean cluster while enforc constraint violat penalti .
let's look how these method work , okai .
so there's a object function .
that mean we us some of the distanc us in the k mean but it is adjust by the constraint violat penalti .
for exampl if the cluster violat the must link constraint .
what the penalti should be obtain is if object x and y must be link that mean thei have must link constraint ti with it .
thei should be assign to the same cluster base on the must link , howev now suppos you assign them to two differ cluster , c on and c two .
then you should add a penalti distanc between c on and c two , is ad to the object function as the penalti when you optim it , you try to reduc the penalti .
that mean you try to , you know , assign them to differ cluster .
then what is penalti for a cannot link violat ?
that mean if object and y cannot be link but now you put them into the same cluster .
that mean you assign to a same cluster center c .
then we should have object to function ad to it as a penalti .
the ob , the ad y is the distanc between c and c prime , where c is the current assign center , c prime is the closest center to c that can accommod x or y .
in thi session , we will discuss constraint base cluster with constraint on distanc measur .
constraint on distanc measur is quit a real problem .
there is on research paper develop by anthoni tung , in icd , <num> , spatial cluster in the presenc of obstacl .
thi is pretti real problem in the sens , if suppos you want to find deliveri center .
like at up and we want to deliv to differ household or commerci center .
so you don't want to assum ye , the , the , the car can fly over the river or highwai or fly over even the lake .
whether you want to observ the constraint on those obstacl and you have to go over it by cross the bridg then you can deliv your passer to the other side , of the bank .
so to observ thi constraint we have think about how to do it .
the first thing is we probabl should us a k medoid algorithm instead of k mean becaus k mean mai final locat the center of the servic .
right in the middl of the lake or right in the middl of the river or right on the highwai .
okai , so thi mai not be quit desir but a k medoid object it just sai to be here .
howev , to construct such cluster , actual it is quit costli to comput .
the reason is everi time we us you have to comput the distanc of all the nearbi object assign to the center .
calcul <num> cent , sai which on is closer and your center by thi object reassign , you'll recalcul center , center will chang .
when the center's chang , all thi distanc ha to be recomput so thi is veri costli process .
how to effici process it ?
okai , the first interest thing is we should think about triangul and a micro cluster .
triangul essenti is try the comput the visibl graph becaus you us triangl within thi triangl , all thing ar visibl .
you can base it on distanc to comput it but onc you go to the other triangl sometim you have to go over the vertic .
so you cannot do directli comput be there you creat distanc becaus thei mai have to cross over the obstacl .
then anoth import thing is build those join indic that is to pre comput the shortest pass .
why we have to build these join indic ?
just becaus you don't want to comput it again , again us , you know , the veri similar kind of thing to do recomput .
if we comput the vv index , the vertex to vertic index for obstacl , that mean you , you , get , you get thi obstacl .
anoth obstacl you want to see their vertex to vertex , how far awai ? becaus in some case you have to go over these vertex eh , then final get into anoth point to calcul their distanc , okai ? .
anoth is mv index , that mean you think about your micro cluster , you're think you do the micro cluster within the triangl .
and then you don't have to comput so mani point , you take micro cluster , each cluster like on super object , okai ?
so but we have to comput a micro cluster to the obstacl index .
for exampl , you comput thi micro cluster to thi on then you can you will be abl to comput their distanc in effici wai .
becaus everi micro cluster to thi obstacl index then thei link togeth is the distanc between these two micro cluster .
so the index sure will speed up comput becaus you don't have to recomput it .
let's just look at exampl , we actual have two , the same group of micro cluster and same group of obstacl .
but on comput is we assum thi deliveri center can fly over the river to deliv thing , probabl in the futur those drone mai be abl to do it , okai , ?
but it , the , the second on , the right side on , is the on , that mean you have to observ the , the obstacl .
for exampl , thi center if thei want to deliv to the hous over here thei have to observ , thei can onli go over those vertex then thei can go down there .
so , by observ these obstacl as constraint , as actual hard constraint then you'll probabl get a rather differ kind of distanc comput and differ kind of cluster , okai ?
actual , thi studi did not consid highwai , there ar some subsequ studi even consid you can onli drive on the highwai then you can re comput the distanc .
anywai , the gener idea is the distanc comput mai follow some constraint .
so thi is constraint base cluster with constraint obstacl distanc measur .
in thi last session of thi lectur , we will discuss user guid cluster .
or , we can sai take user's hint as constraint .
user guid cluster is a veri special kind of constraint , but it is also veri us .
for exampl , we look at relat databas .
suppos you got the student , you have relat databas repres the student's name , offic and posit thei link to advic , link to professor , to research group , and link to their cours and public .
so for thi typic relat databas , we mai want to do cluster across multipl relat becaus the student have a lot of featur store other relat .
howev , user link to so mani differ relat when you want to cluster student , you mai be puzzl .
what is the purpos for cluster those student ?
everi user mai give you the best hint , for exampl , when user want to organ a parti thei mai sai , oh i want to studi like the student , you know , the nation or their perman address .
we want to you know , to look at which field thei should find advisor .
you'll probabl want to studi user , group and area .
when you want to give a student award , you make look at their grade .
okai .
so , the user usual have the best wai to tell you the purpos of the cluster .
then , suppos you want to cluster student base on their research area , then the student can be link to mani relat you have , base on user hint .
that mean it's easi for user to provid some easi hint rather than to give you a good train set , or give you veri clear constraint .
okai .
so , that extent the user hint can be us as a user guidanc , we base on thi user guidanc we can do effect cluster .
thi kind of user guid cluster is differ from classif .
just look at thi pictur .
suppos thi is research group area .
okai .
the user hint sai thi is import .
then , you probabl don't want to take thi as a class label .
the reason is clear , becaus you paint three color , you mai want to get <num> cluster becaus thi is , suppos you , you sai thi is ai , thi is data inform system .
actual you can further partit them into thi is natur languag process as a group .
and then machin learn is a group .
then some search and reason could be anoth group .
okai ?
so , but anywai , thi user hint give you some good guidanc to see where to search .
that mean you mai want to us addit featur even cross to other relat .
try to find relat thing you want to group them to the desir number of cluster .
now , howev , thi user guid cluster is also differ from semi supervis cluster .
becaus semi supervis cluster usual sai , oh these two student must be in the same group .
these two student cannot be link togeth , that mean thei mai belong to a differ cluster .
the first thing is you can onli get expert for a veri small number of student .
okai , anoth thing is you still do not know the real purpos of cluster .
but user guid cluster is quit us becaus you just sai the sphere is quit import .
and then user , it's easi to specifi and it is not like thi cannot link or must link these kind of constraint .
in the meantim it is , you will be abl to search other more relev featur to make a good decis in cluster , okai .
that mean there's as much inform in other relat , need to judg instead of just take thi just sai cannot link or must link .
and also the user usual is not abl to specifi a good set of constraint , like a must link or cannot link thing , but it's easi to provid hint .
then let's look at it in the real life .
when user provid the hint you have to go over mani differ relat to search the relat us featur .
that mean we should search , start from user specifi featur like thi research area .
then you search the neighborhood the connect relat to find a pertin featur .
and then we can expand the search gradual .
okai ?
and how to expand search ?
you can start from thi field .
you look at the tupl id .
the id actual it's like a foreign kei or kei relationship .
you can link into the other relat and then you can , you can search further and further .
okai ?
then onli those interest relat , their further link need to be search .
okai , so thi is usual the propag you can go through via join pass .
so , base on thi philosophi , it will work out a cross class algorithm .
that mean we'll us user guid cluster across multipl relat .
the first is we try to base on user hint .
we can find some similar from other relat .
thei ar co relat , or thei actual inform to find that differ pertin featur across multipl relat .
okai ?
that usual we can us a heurist method to search for pertin featur .
the start from user specifi featur gradual expand the search rang and we can us tupl id propag to creat featur valu .
that mean we grab to , to the other relat , grab a most pertin featur .
and then we can explor cluster algorithm to find nice cluster , for exampl , we can us k mean , k medoid , hierarch cluster method .
how to find featur across multipl relat , usual is you follow those join path , okai .
then you try to find a particular field to see whether thei ar co relat with the thing you want to cluster , okai .
so if thei ar strongli correl with the student , you want to cluster .
so thi is an interest featur that you want to includ them .
for exampl , okai , suppos the featur ar the research group then you find the student who ar join in certain group actual quit a correl to the number of cours taken in differ research area .
then you mai find that thi featur and the student take cours in the research area these two thing ar quit correl so you want to link them togeth you know , think thi is import featur for cluster .
and search for pertin featur mai final get a veri interest on .
for exampl , when you want to see the research area , you mai find the research group area , confer of paper , advis .
these ar veri us thing to do cluster .
and when you want to organ parti you mai see the perman address , nation could be quit a relat .
when you look at it for award , look at it for academ perform , your gpa , gi score in number of paper ar more relat .
so the featur will convei same aspect of inform , usual can cluster tupl in a more similar wai , like a research group area vers confer of public .
then given user specifi featur , we'll be abl to find pertin featur by comput the featur similar .
then for cluster with multipl relat featur , we were base on the pertin featur .
then we were comput the similar us thi kind of formula .
we can weigh a featur base on , whether how close , how similar with other pertin featur in the featur space .
we mai us differ cluster algorithm like a claran , k mean , agglom , hierarch cluster .
actual the experi compar with other like subspac cluster algorithm , proclu , or ilp cluster algorithm like rdbc .
then we can see base on the research paper , if you like to read , you probabl see no matter you us which cluster method , crossclu will lead to better cluster accuraci , just becaus you , you us user's guidanc , the user will have the right view on what kind of thing mai form nice cluster .
so , these thing thi cluster accuraci actual must valid that , us the ground truth .
it mean us , given label on the real dataset .
final , we'll summar thi lectur .
in thi lectur we discuss constraint base cluster .
we first argu why constraint base cluster could be import .
we look at a differ categori of constraint .
we , then we studi constraint base cluster method like how to handl hard constraint , how to handl soft constraint .
how to handl constraint on distanc measur .
final we discuss a special kind of constraint call user guid cluster , that mean we take user hint as constraint .
we believ thi is veri us in practic .
so , final , i will list a bunch of research paper .
and some ar the book on constraint base cluster .
some ar the chapter on semi supervis cluster or constraint base cluster .
but some ar the real origin research paper work constraint base cluster algorithm .
in thi session , we will give a gener introduct on extern measur for cluster valid .
we know cluster is unsupervis .
in thi sens , we realli want to us some good judgement .
extern method is to give some expert knowledg or some prior truth .
so to that extent , we call these as ground truth t .
so given ground truth t , then we want to measur the qualiti of cluster c , we us the qualiti measur q c , t .
the function q ct is good if it satisfi the follow four essenti criteria .
the first on call cluster homogen .
that mean we want the cluster form to be pure .
that mean thei're in the same cluster .
so , the purer the better , of cours .
the second on , call cluster complet .
that mean we want to assign object belong to the same categori in the ground truth to the same cluster .
that mean you mai get all these object ar a categori a , but you assign them to two , to three cluster .
that's no good .
we want to be complet .
the third on is rag bag better than alien .
simpli sai , you mai have some heterogen object .
'kai .
if you put them into a pure cluster , you mix the other categori up .
that's no good .
you want to penal thi .
it's better even to put them into a rag bag .
rag bag mean it's in the miscellan or other categori .
it's better than you mix up with some pure cluster .
then the fourth measur of cours , small cluster preserv , simpli sai , if you got a pretti small cluster alreadi , you do want to further split them into piec becaus those piec like repres nois .
in that case , you better split a larger categori into smaller piec .
the reason is , the larger categori like , thei mai belong to differ cluster , or differ other smaller categori .
that's small cluster preserv .
then , we will see what we ar go to examin on those extern measur .
we often us thi graph , thi color graph .
those brown point , thi dark brown point , suppos thei ar ground truth partit t1 .
that mean expert label those point as t1 , okai .
then the ground truth , the other ground truth mai be thei label as blue on .
that mean t2 dark blue on , actual is anoth categori .
'kai .
that mean for ground truth , for expert , thei know that differ color belong to differ truth .
'kai .
howev your cluster algorithm mai gener thing circl by the light brown on or orang on , or the red on .
that mean thei ar not ideal , thei mai not be good enough for the match what expert judgement .
so , we want to measur whether your cluster method is good .
'kai .
so , the measur on we call match base measur .
we , we have sev , sever measur , like puriti , maximum match , or f measur .
anoth kind of measur call entropi base measur .
there ar condit entropi , normal mutual inform , nmi , or variat of inform .
the certain kind of measur call pairwis measur .
for exampl , we mai have four possibl , true posit , fals neg .
fals posit , true neg .
okai .
then , we actual have jaccard coeffici , rand statist , fowlk mallow measur .
the fourth categori call correl measur .
essenti , thei ar discret huber static and normal discret huber static .
and those mark as to be cover .
we will discuss them in more detail in the subsequ session .
hi , in thi session we ar go to discuss the first group of extern measur , call match base measur .
for match base measur , we will first introduc puriti versu maximum match .
we still us similar notat .
suppos we have ground truth we get three categori .
t1 actual mark the brown point , t2 mark the green point , t3 mark the black point .
these ar the ground truth .
then the cluster we final got is cluster c sub <num> , actual is mark as the , the ellips of the orang color , okai .
c2 mark as surround by the red color .
and c3 is surround by the black color .
so what is the puriti ?
actual quantifi the extent of that cluster c sub i contain the point onli from on ground truth partit .
suppos we have r cluster final .
so for puriti sub i , actual is , you first count how mani point in thi cluster .
then you try to see the maximum number of the ground truth here .
the maximum number of ground truth , actual the black point .
you will see the max number is on , two , three , four , five , six , seven .
so you get thi on seven but the total number is nine so you get a puriti of seven over nine .
then what is the total puriti of the whole cluster c ?
essenti thi total puriti is each on puriti .
suppos we have r cluster .
each on's puriti get their correspond proport .
so you add them togeth , so that's the total puriti .
or we can transform thi on veri easili into thi on , okai .
of cours , the perfect cluster is in the case when the puriti's on .
that mean you , everyth's veri pure .
you add them togeth , it's on .
and the number of cluster ob , obtain is the same as the number of group in the ground truth .
then we look at these two tabl , okai ?
let's first look at the green tabl .
the green tabl , the puriti on , that mean the first cluster , you get a maximum <num> .
so the puriti is <num> over the total number of point , is <num> .
so you get <num> over <num> .
puriti <num> obvious you get <num> over <num> and puriti <num> is <num> over <num> is realli <num> .
then the total puriti for the whole class rang , is accord to thi formula , we get a maximum y as <num> plu <num> plu <num> over the total number point you get <num> . <num> .
interestingli , if you look at the orang tabl , orang color tabl , you will get exact number the same thing is <num> over <num> , <num> over <num> , <num> over <num> , then final get that puriti is exactli the same formula .
but , you mai find on undesir thing for thi orang color tabl .
becaus as you see the ground truth , t2 total get <num> .
the actual partit into two cluster , c1 and c2 that mean both c1 and c2 would sai , we belong to ground truth t2 .
but on the green tabl , you probabl can see it's differ , becaus c1 more pair with t3 , and c2 more pair with t2 .
so that motiv peopl propos maximum match .
that mean on cluster can onli match to on partit .
if we have thi rule , then we have to find a pairwis match .
that mean when we look at thi , we look at the weight , we will allow these element onli belong to on cluster .
the ij and ij .
the ij must be the exact same .
then if thi element belong to m , then it onli belong to on .
so then we need to look at what is the maximum weight match .
the maximum weight match simpli sai you look at the whole group .
and then you look at the all over point .
you try to find the final match togeth should be maxim .
for exampl , if you look at a green on , the max match is <num> pertain to t3 , t2 pairwis with c2 , p1 pairwis with c3 .
that's why green's maximum match is equal to puriti <num> . <num> .
howev , for thi orang color tabl you pertain c is , if you sin t2 to c1 then you can assign t2 to c2 then you have to assign t3 to c2 then final you get thi match <num> .
howev , if you assign t3 to c1 , t2 to c2 , t1 to c3 you'll get match at <num> . <num> .
to that extent , thi assign is better becaus the max match the valu is bigger .
so that's the reason for the max match , you do have restrict , you have the pairwis assign .
then anoth popular us match base measur call f measur .
f measur ha been veri popular us in inform retriev .
f measur essenti is comput by precis and recal .
let's look at how the precis is defin , okai .
the precis is defin is for thi particular match what you want to see is for precis c sub i what you want find that is what's a maximum on match thi particular cluster .
then divid by the number of , the total number of point in thi cluster .
so in that sens , you're posit actual is the same as puriti .
that mean if you get a fraction of c sub i from the major partit of c sub j .
then the j's partit contain the maximum number of point from c sub i then you get thi formula .
for thi formula you get thi , the precis and the puriti .
actual , thi point is the same .
you get <num> over <num> , <num> over <num> and <num> over <num> .
'kai ?
that's the precis .
what about recal ?
recal actual is the fraction of the point in the partit share in common with cluster c sub i .
simpli sai , if you assign t3 to c1 , what you want to see is you get <num> , but t3 actual got <num> ground truth , you onli captur <num> .
that mean your recal is <num> over <num> .
that's exactli you probabl can see the formula .
similarli for the second partit if thi time branch is t2 to c2 what you can see is you onli get a <num> over <num> so their recal is <num> over <num> .
similarli for , for the third on , when you assign the ground truth of t1 to c3 , the <num> over <num> .
that mean that the fraction of the point in thi partit share in common with the cluster c sub i .
then you get thi recal .
then what about f measur ?
f measur actual is a harmon mean of precis and recal .
so , f measur for cluster c sub i is the harmon mean of precis sub i , and recal sub i .
if you transform the formula , it will becom like thi on .
then for f measur for the whole cluster .
what you need is , you just need to averag all the cluster , the f measur .
that mean , suppos you get r cluster , then you get f measur from on to r .
you add them togeth divid by r , okai .
so for thi green tabl what you can see is why you get f1 is becaus you got <num> time <num> divid by total is these two add togeth you get <num> .
then for f<num> you get <num> time <num> divid by these two ad togeth .
the f3 , of cours , is <num> , no matter what you calcul .
then , your averag of these <num> , you get f measur thi number .
in thi session we ar go to introduc anoth group of extern measur call entropi base measur .
we know entropi is a veri us inform theori measur .
not onli is inform theori also us in data mine , mission learn quit a lot .
entropi essenti is repres the amount of other of the , of the inform in all the partit .
so we still us thi graph , thi figur to repres conceptu we have ground truth repres by differ color point , we have cluster repres by differ ellips .
for entropi of cluster c .
suppos we got r cluster .
it's , it's essenti thi is the entropi of everi particular cluster .
then we add all these cluster togeth we get entropi for the cluster c .
then for each cluster , actual the entropi is base on the probabl of c sub i .
the probabl of cluster c sub i is base on the number of point in thi cluster divid by the total number of point .
then entropi of partit t is essenti defin a similar wai .
suppos we have ground truth j from <num> to k , these k group .
okai ?
so for each ground truth is , pt sub i we actual can get a , the probabl of the ground truth as defin by these p sub t , sub i , okai .
then we add all their entropi togeth for all the k ground truth we get a ground truth for the whole partit .
then what we're interest in is the entropi of t with respect to cluster c sub i .
that mean we want to see how the ground truth is distribut within each cluster .
so you probabl can see thi j repres the ground truth , and the i repres the realli cluster .
so we probabl want to see such distribut so we can work out the entropi of t with respect to the cluster c sub i .
then if we want to get to the condit entropi of t with respect to whole cluster c , then we just add all these togeth .
becaus thi on is just for cluster c sub i , but we total have r cluster , we add all these up proportion , then we will get the whole condit entropi of t with respect to the whole cluster of c .
conceptu , we can see the more of a cluster's membership ha split into differ partit , the higher the condit entropi .
that's the less desir .
you probabl can see if , if you're partit , wide spread into differ cluster it is no good .
okai .
for perfect cluster the condit entropi valu should be <num> .
the wors condit entropi valu , is log k .
we can us a transform formula like thi .
we can transfer on thi condit entropi to be the joint entropi , minu the cluster's entropi .
so , you probabl can see , that's the condit formula transform .
we're not get to detail , but you can check it .
anoth veri us measur us for extern measur is normal mutual inform .
we us a similar figur to sketch the idea .
the mutual inform is also defin in , inform's theori , introduc there .
but it's also veri us in machin learn and data mine .
okai .
essenti , the mutual inform quantifi the amount of share inform , between the cluster c and the partit t .
so you can probabl see the formula , we have r cluster , we have k ground truth .
so thei ar mutual inform essenti so ad all these up togeth .
thi is a similar formula as entropi , but it's differ becaus you can probabl see thi part is realli , thi ij's probabl divid by thi cross ring properti and a partit's properti .
that mean we want to measur the depend between the observ joint probabl p sub ij of c and t , and the expect joint probabl of pc sub i and pt sub j .
under the independ assumpt .
of cours , if c and t ar realli independ that mean thei realli want equal distanc to thi .
then you probabl can see thi is on actual when you take log thi becom <num> , okai .
of cours in thi case , thi is no good becaus it impli these ground truth actual scatter r around differ cluster .
howev there's no upper bound on the mutual inform , which is less desir .
that's why we need to introduc a normal mutual inform .
that mean we want to normal the rang from <num> to the highest wine on .
then the valu close to <num> actual indic a good cluster , a valu close to <num> mean thei ar almost accompani to random independ assign .
okai , then thi normal mutual inform essenti is you take the mutual inform .
divid by the entropi of cluster and divid by entropi of partit .
you product them togeth , take their squar root , or you can transfer in thi wai .
so thi is , quit us , becaus you will know onc you're cluster base on your extern measur .
your cluster becom perfect if their valu is veri close to on .
in thi session , we're go to introduc a certain kind of extern measur call pairwis measur .
what ar pairwis measur ?
that mean we take ani two point .
we see whether thei agre between their cluster label and a partit label .
so there ar four possibl case for ani pair .
we have true posit , fals neg , fals posit and true neg .
then we look at the case .
for exampl , you get ani two point x sub i and x sub j .
if thei belong to the same partit t and thei also in the same cluster c , in that case , thi is a true posit .
for exampl , we look at thi case .
the definit is the true posit is a number of such case , okai ?
for exampl , for ani two point x sub i and x sub j , if thei have the same true partit label and thei also have the same cluster label , that mean thei belong to the same cluster , then that's the case of true posit .
for exampl , we just look at thi case .
for these two blue point , thei belong to the same ground truth t2 and also thei belong to the same cluster c2 .
that's true posit .
okai .
then what is fals neg ?
fals neg mean thei have the same ground truth partit label , but on the other hand thei ar not in the same cluster .
for exampl , you just look at these two brown point .
thei have the same ground truth t1 , but thei belong to differ , cluster .
so that's the fals neg case .
well , what is fals posit ?
fals posit mean thei actual have differ partit label , but thei ar in the same cluster .
for exampl , just look at thi blue on and thi brown on .
thei actual do not have the same partit label but thei ar in the same cluster , okai .
what is true neg ?
true neg pair actual mean that thei do not have the same partit label but thei ar also not in the same cluster .
for exampl , if you look at thi black on and thi blue on .
these two point , thei do not have the same partit label , but thei ar also not in the same cluster .
so that's a good case .
then we see how we can calcul the four measur .
first , given n point in the data set , the possibl pair you need to examin , actual the n choos two , so that's the reason you have thi formula .
then for the true posit case is you get all the , the partit and the cluster , if thei agre to each other , that's the case you have n i j , you choos ani , these case you choos ani two , you get that mani case .
then what is fals neg ?
fals neg mean you get that mani partit case , but thei do not belong to the true posit , okai .
what is fals posit ?
that mean you have so mani cluster case , but thei do not belong to the true posit .
then what is true neg case ?
that mean for all the case , thei do not belong to ani on of the abov three case , then thei ar true neg .
okai , so that comput we can just simpli us those formula .
with the introduct of true posit , fals neg , these four measur , then we can calcul other measur like the jaccard coeffici and rand statist .
then , we still take thi figur as our illustr exampl .
then for jaccard coeffici , rememb we defin the jaccard coeffici befor .
and thi on is the definit for the pairwis measur .
and thi jaccard coeffici have the similar kind of flavor .
you probabl can see .
you have true posit divid by all the case except that you ignor the true neg case , okai .
that mean the fraction of the true posit point , but after ignor the true neg case .
therefor , thi comput , posit and neg ar differ to the , our asymmetr measur .
howev , for perfect cluster jaccard coeffici should be on becaus , you know , all the case you actual cover .
you don't have those fals ca , case .
for a rand statist is you take all the true case , true posit plu true neg divid by all the possibl pair .
okai ?
that mean you don't cover anyth like a neg .
okai ?
that's why , if it's perfect cluster , rand statist should be <num> .
it is symmetr becaus you take a posit and a neg case just , equal .
then there is anoth interest measur call fowlk mallow measur .
thi measur is a geometr mean of precis and recal .
rememb we just studi f measur , which is a harmon mean of precis and recal .
for geometr mean fowlk mallow measur , essenti is precis time recal , you get their squar root .
and onc we introduc all these measur , if we want to calcul for exampl ani tabl like thi on thi green tabl , we can us thi measur to calcul all the number .
we will leav thi calcul as an exercis , instead of spend time lectur here .
in thi session , we were introduc intern measur for cluster valid .
rememb , we just discuss sever kind of extern measur .
the lucki thing for extern measur is we do have we have expert knowledg , we have a prior .
unfortun , in mani case , we do not have that's the reason we have to reli on intern measur .
the intern measur base on the concept of cluster .
it mean we want the point within the same cluster .
that mean intra cluster , thei ar veri compact .
thei ar close to each other and we want point in differ cluster that mean inter cluster distanc .
we want them as far apart as possibl .
that mean we want them to be far separ .
that's the reason we want to maxim the intra cluster compact and also maxim the inter cluster separ , but we want a tradeoff between these thing .
so let's look at how we defin thi .
suppos we give a cluster c with k cluster , c sub 1to c sub k .
for each cluster c sub i , it contain n sub i number of point .
then we introduc the function w of s , r , which is the sum of the weight of all the edg with on verdict in s , the other in r .
for exampl , you'd get a two point .
you sai , thi point is in thi cluster .
the other point is in the other cluster .
you want to look at their weight .
the weight usual can be defin like a euclidean distanc or whatev you ar us for cluster .
then base on thi , we can sai , the sum of all the intra cluster weight over all cluster should be defin thi wai .
the reason is you can see thi is a c sub i , thi is a c sub i .
that mean both vertic ar in the same cluster , so that's why it's inter cluster .
but at on point a and the other point b , you actual us it will calcul twice , becaus a to b , b to a , you're actual calcul twice .
that's why you have to divid thi by two .
okai .
for all the cluster sum up togeth is i from on to the k's cluster , then we defin the sum of all the inter cluster weight should be calcul thi wai .
there , which is veri obviou , becaus on point , on vertex is in the cluster c sub i and the other on is not in the c sub i .
simpli sai , in the other cluster , that's why it's intro cluster .
but sinc the two point , you calcul it twice , that's why you have the divid by <num> .
then we sum up all these essenti , it's i from on to k , all the cluster .
we get a wout , okai ?
that's the inter cluster weight sum them togeth .
now we also can see the number of distinct intra cluster edg , becaus for i's cluster if n sub i number of point there intra cluster link , you have end point choos two .
that's why you get thi formula .
then get all the cluster on to k , you sum up , that's why you get n sub m .
then for the inter cluster n sub out is notat by on point is in cluster i , the other point is cluster j .
thei never ar in the same cluster .
so you want to calcul all the edg , among differ cluster .
so that's the number of distinct inter cluster edg .
then for betacv measur is defin as wi divid by n and you can think thi is the mean intra cluster distanc and you get wout divid by number of out , you can think is a mean inter cluster distanc .
so the betacv essenti the ratio of mean inter cluster distanc versu the mean intra cluster distanc .
of cours , for thi measur is the smaller , the better cluster .
becaus a smaller mean , thi is pretti small is quit a compact .
and thi is the bigger , it mean it's more separ .
that's a betacv measur defin , then we look at some other definit .
on popular us on call normal cut .
normal cut is defin as follow .
the normal cut of thi ratio is defin by sum up of all the k cluster .
for everi cluster , you calcul in thi wai or more explicitli , we can interpret in thi wai .
thi is wai of the inter cluster .
thi is a weight to sum up of , of the intra and inter cluster weight .
and for thi ratio , you actual sum from i from on to k , you'll get these normal cut ratio .
of cours , base on thi , you want to the inter cluster to be far separ , compar to the intro on .
that's why the bigger normal cut valu , the better the cluster .
then there's anoth definit call modular .
thi is especi popular for graft cluster .
it is defin in thi wai .
you proce , thi part is the same as normal cut , cut formula .
okai .
like thi , but thi part is the squar of thi .
okai .
so that simpli sai , thi on is the observ the fraction of the weight within cluster .
thi is the expect on .
so modular , actual measur the differ between the observ on and the expect on .
so the smaller the valu , the better the cluster , becaus the inter cluster distanc ar lower than the expect on .
so thi is the modular definit us for graph cluster .
in thi session , we ar go to introduc anoth kind of measur for cluster valid call the rel measur .
we alreadi introduc extern measur when we have we also introduc intern measur when we try to measur the intro cluster , thei ar veri compact , and inter cluster , thei ar far apart .
now , we want to look at what is rel measur .
rel measur actual is try to direct compar differ cluster .
especi for those obtain us the same algorithm but differ paramet set .
for exampl , like if we all us k mean to cluster a group of thing but in that case , we want to measur for differ cluster instanti .
which on gave you the best cluster or a differ case for k from on to certain number , which k give you the best cluster .
in that case , we were measur as us rel measur .
and , on interest measur call silhouett coeffici can be us for rel measur , okai ?
but , actual , silhouett coeffici also popularli us as intern measur .
that mean we will be abl to us it to check cluster cohes and separ .
so , i first introduc it as an intern measur befor i introduc it as an extern measur .
so for everi point , x sub i , we can calcul it silhouett coeffici , s sub i , okai ?
how we calcul it ?
we us thi formula thi formula actual , mu sub n , is the mean distanc to the point of it own cluster .
simpli sai , if we find thi point in the blue cluster , okai ?
we're go to see thi x sub i , actual to all the point in it's own cluster that distanc we want to reach them .
we get a mean distanc that's mu sub n .
then what is mu sub out's mean , okai ?
that mean we want to see thi blue point to it closest cluster which is thi orang on , to it closest cluster .
all the point it closest cluster , we calcul their mean distanc , that's thi valu , mu sub out mean , okai ?
then what we want to see is their distanc , you probabl can clearli see for ani point x sub i .
you want that the intern distanc , it rhythm is small but even to it closest cluster , thi averag reason be big .
so , to that extent , thi valu usual should be posit and the more posit the better , okai ?
then we , we normal it , we divid by the maximum valu between thi mu out mean .
and the mu n , and if we pick up the two , so it simpli sai that thi divis must be smaller than on .
but on the other hand , if thi cluster to on , simpli sai , it's better becaus the intern is veri compact to the outsid is the reason big .
thi is onli for on point , what is for the whole cluster ?
okai , so silhouett coeffici for the whole cluster is you take all the end point .
you calcul everi on's silhouett coeffici s sub i .
i from <num> to n you calcul all the s sub i , you normal it , you divid it by the number of point n .
you get the whole cluster silhouett coeffici .
of cours , these silhouett coeffici , if it's closer to plu <num> , it , it impli good cluster .
becaus the intern on to it own point is rather close to the other cluster , even to the closest on , is realli far .
so we also can us thi silhouett coeffici as a rel measur .
for exampl , we can us it to estim the number of cluster in the data , sai , what is the best of k .
we can pick a valu that yield the best at cluster , that mean yield the high valu for sc and sc sub i , okai ?
for exampl , you can , think about , you did a lot of cluster , okai ?
for all the i's , you want to see which on give you the best cluster ?
that mean which on give you the highest silhouett coeffici ?
then you can pick up that instanti , pick that k .
so that's the usag of silhouett coeffici as a rel measur .
in thi session , we ar go to studi anoth issu for cluster assess call cluster stabil .
cluster stabil basic sai , whether you got the right paramet , you get the veri stabl cluster result .
that mean you can assum you get the data set d , you take a sampl get sever dataset from the same data set d , you do the cluster .
and you should assum if you're cluster final get a veri stabl cluster result , that's a good cluster .
usual , you can us thi to find a good paramet valu for a given cluster algorithm .
for exampl , you can find a good valu of k , which is a good number of cluster .
so how do you find thi ?
you probabl just look at thi graph , you probabl can see .
if you set k as three or even as two , you probabl can find it quit reason cluster .
but if you set as four or five , no matter how smart you ar , probabl thi cluster you find mai not be quit stabl .
so to that extent , you mai try to test the cluster's stabil , we introduc on method call bootstrap approach to find the best valu of k , judg base on stabil .
how can we do thi ?
thi bootstrap basic sai , from d , you take tt sampl of size n , but the , the sampl is sampl with replac .
then it's everi time you take almost from the same d , you take it t time .
then you get to the sampl d sub <num> , d sub to d sub j to d sub t .
okai .
then when you run the same cluster algorithm with k valu from two to the maximum of the k you like .
okai .
then you can compar the distanc between all the pair of cluster .
for exampl , for each k , you mai try to see whether the d sub i and d sub j , these two differ sampl dataset .
you will see whether finer you get the , the sampl , you want to comput the expect pairwis distanc for each valu of k .
that mean suppos k you get a ten , you get at least ten cluster , you want to see their pairwis distanc .
okai .
then what you can see is thi valu k , if it exhibit the least deviat between cluster .
that mean you do cluster those sampl d sub i or sampl d sub j , thei have the least deviat .
okai .
that k should be the most stabl on .
thu , you mai want to have thi k valu .
that's on applic to test the cluster stabil to find the best k .
actual , there ar mani method to find the k , the appropri number of cluster .
on method call empir method , actual peopl give you thi empir formula .
for exampl , for total data set of n point , you mai take the squar root of half of thi n point .
okai .
for exampl , if n is <num> , the expect valu you will need to get number of cluster should be <num> .
of cours , thi mai work out for a small number of point .
if you get a realli big number of point , for exampl , you get a , too mani point , then you get thi valu , you'll get a k as a solvent , probabl you mai not want to get a solvent cluster even for too mani point .
anoth method , peopl us call elbow method .
elbow method mean you tri to base on the number of cluster that go on , two , three , four .
you get number of cluster , then you get it , the sum of the within squar varianc .
that mean you just to look at the averag of within to cluster varianc , you try to look at thi to see , know what is the best , the number of k that you want to see the elbow point , the turn point .
so that's on method .
anoth method of find the best number k is us cross valid .
that mean you divid a given dataset into m part .
so you take on part as a test data , the remain part to do the cluster .
okai .
you can do thi m time , so you can check their overal qualiti of the cluster .
then how we test the , the qualiti of the exampl ?
okai .
what we do is , is , suppos we us thi m minu <num> part to do the cluster , we find the k cluster .
then for each point in the test set , you try to find the , it's closest centroid .
then you , base on thi , you try to find for all the point in the dataset , you try to find the sum of the squar distanc .
that mean you try to find some of the squar error sse , as we introduc .
then usual , you try to see whether thi , you get the best fit of the test dataset .
that mean you get the smallest sum of squar distanc .
sinc for cross valid you repeat thi m time , so then what you can do is you can compar the overal qualiti measur with respect to differ k's , then you'll find what is the best number to fit the data best ?
that mean you get the overal qualiti measur , you get the lowest sse for thi particular k .
usual , thi is the right number of cluster .
welcom to lectur ten , cluster graph and network data .
in thi lectur , we ar go to first introduc basic concept of graph and of network cluster .
we'll introduc graph and network and their represent .
we'll introduc typic evalu measur , discuss the approach for graph cluster .
then we'll focu on spectral cluster and a scan , algorithm of densiti base cluster of network .
first , we ar go to discuss graph and network cluster , some basic concept .
first , we know most real world data ar inter connect , form gigant network or graph .
in among all kind of network , we can categor them into homogen network versu heterogen network .
homogen network ar those whose vertic and edg have onli on type .
for exampl , for web search engin , we look at click through graph or web page graph .
thei ar of on type .
for social network , friendship network , co author network , thei ar also on type , vertic and edg .
for heterogen network , vertic and edg ar of multipl type .
for exampl , just look at two type graph , for exampl , custom and product , thei could be link togeth , author and confer , thei could be link , link togeth .
of cours , author and author also can be link togeth .
but essenti , you can see is two type graph mean you onli have two kind of vertic .
for multipl type graph it simpli sai , within a network you mai have multipl type node and edg .
just give you a simpl exampl like research network .
a research network like dblp , you mai have author , paper , venu and keyword .
thei mai interconnect to each other .
like the author mai write paper publish in certain venu and us certain keyword .
similar thing as medic network , you mai have doctor , patient , treatment , drug , all these thing .
freebas can contain much more differ to , type of entiti and thei interact and thei link to each other .
so , we want to distinguish some concept like cluster , graph partit and commun discoveri .
cluster , usual you try to group object into group .
thei can be hard , like a k mean , or soft like em , and it can be complet or partial .
it can be balanc or skew , howev , for graph partit , most peopl mean it's hard partit , hard group , and it's complet .
that mean all these node and edg in the graph is somehow partit to differ part , differ partit , and typic thei ar balanc .
thei , you try to balanc them , instead we get a singl type graph .
for commun discoveri usual can be partial , mean even some node you ar not includ certain commun is quit okai , becaus we often interest in onli find the dens connect compon , we ar not interest in the cluster assign of everi vertex .
let's look at the major challeng for graph cluster .
that mean how to find good cut .
thei ar veri challeng becaus the first thing is high competit or cost .
mani graph cut algorithm or problem ar computation expens .
you need to trade between effici , scalabl and the qualiti of cluster .
and also , the graph can be quit sophist .
it , it mai involv weight , involv cycl or sale cycl .
it could be veri high dimension sinc on graph can have mani vertic .
okai .
if you think everi vertex is a dimens , the dimension of the matrix actual is a number of vertic in the graph , so thi could be realli big .
anoth issu is sparsiti .
sinc a larg graph is often veri spars , that mean each vertex on averag onli connect to a veri small number of other vertic .
if you think about a complet , connect mean everybodi connect to everybodi els .
in a larg network , usual , it's not the case .
to that extent , a similar matrix , from a veri larg spars graph , it can be a veri spars matrix .
so , we have to find effici method to solv all these problem or challeng issu .
in thi session , i'm go to introduc you pathsim path base similar measur for heterogen network .
similar search is veri import in , cluster analysi .
in the heterogen network , like dblp , peopl mai want you to do nice cluster , or answer some similar queri .
for exampl , who ar the most similar to christo faloutso , who is a professor in carnegi melon univers .
then if we take these dblp schema , we mai sai it depend on what do you mean who is more similar to christo faloutso .
we can take a part of schema to , to express such a similar .
for exampl , we can sai what we mean , similar mean author , paper , author , mean the share on common paper , then these two author ar most similar , that mean me essenti indic a meta path a p a , suthor , paper , author .
what is meta path ?
meta path is a meta level descrip , descript of a path between two object , the path , actual , is along the natur schema .
it denot an exist , or concaten relationship between two object type then differ meta path mai gener differ semant .
okai , for exampl , if we follow meta pass author , paper , author we mai work out the result look like thi christo faloutso is most similar to himself .
then the next on is spiro papadimitri , then jimeng sun , or jure leskovec , those peopl , actual ar mostli christo' fellow student or close collabor .
so that's the similar base on author paper author .
what about if you ask , you sai thi is not what i meant .
what i mean actual is thei mai have , thei mai be in the similar field , thei publish similar number of paper .
so , then what you want to do , actual you look at author paper venu paper author .
that mean these two author , if you want to sai thei ar similar , becaus thei work on paper , those paper publish in similar venu .
if you want to do that , you will , probabl can see , thi is a set of author who actual publish a similar number of paper in similar venu , therefor , thei share thi apvpa .
thi similar then is differ meta path mai lead to differ semant of similar .
so , we see the meta path is veri us .
then , besid meta path we also want to see what should be good measur to judg such similar .
then we can review on popular us of that measur is random walk .
the random walk basic sai you start from node x .
you randomli walk to node y .
what's the probabl you will go through thi ?
meta path , then if we mathemat we sai xy similar actual is defin by the summat of the probabl of p follow those meta path .
that mean there could be mani concret path with follow the same meta path , we want to see what's the probabl x for to go on .
thi wai is us in random walk and also us in person page rank propos by jeh and the widom in <num> .
such random walk actual favor highli visibl object , mean the object with larger degre so you have higher chanc to go down there then we look at a pairwis random walk .
the pairwis random walk it defin if we start from x and y , end at a common object z , follow a meta path p1 and p2 so , in that sens we defin similar let's sai the similar of x and y actual defin by the summat of such probabl .
the probabl mean you go from x goe to z and from y goe to , to z .
actual , if you think about from x goe to y thi y is thi p2 actual is kind of revers path .
but anywai , you , if you think p1 p2 these small p1 p2 is in the meta path of p1 p2 thei follow these probabl of trivers thei're probabl to sum up togeth you get xy similar .
thi similar also been us in simrank definit we introduc befor .
thi kind of similar favor pure object .
that mean , the object with highli skew distribut in their in link or out link .
if , if you check the simrank definit , you'll get the same conclus , okai .
howev , these origin definit of person pagerank and simrank do not distinguish object type and the relationship type .
thei origin defin in the homogen network or graph .
howev , thei can be us in heterogen network but we want to see whether these ar veri good measur .
then we look at anoth measur defin by us call pathsim measur .
the pathsim measur actual favor peer .
the peer mean those object with strong connect and a similar visibl with a given meta path , okai .
so , suppos we get a meta path is p , okai .
then the similar of x and y is defin by you , look at how mani path , how mani concret path follow thi meta path from x goe to y .
okai ?
then , it wa normal or is divid by the averag of these two meta path .
what's these two met the number of these two meta path ?
thei ar averag , okai ?
then , how to defin thi meta pass .
thi meta pass is x goe back to x .
thi is y goe back to y , okai ?
follow thi meta pass , okai ?
so , if we defin thi wai , whether it's a good measur .
we look at a simpl exampl , you .
suppos we follow these apcpa .
alwai a apbpa .
thi c mean confer , v mean venu .
if you look at thi on , then we probabl can see mike actual publish two paper in sigmod , on paper in vldb , but zero paper in icdm , zero paper in , in kdd .
in thi tabl , you probabl will sai bob is more similar to mike becaus bob also publish two paper in sigmod and on paper in vldb but zero paper in the other two venu .
now who's the second most similar y probabl we'll sai mari becaus you know thei're differ is minor .
okai , but what about jim ?
your peer sai no .
jim , he could publish <num> paper in sigmod and <num> paper in vldb .
how jim and mike could be veri similar ?
howev , if you us person page rank , simrank , random walk , and pairwis random walk , you them .
you previous the oral conclud mike , the most similar to mike is jim .
you probabl would not quit agre becaus jim like to be a full professor , and mike mai be just a phd student .
okai .
so , if you look at pass us apcpa , you probabl will find the most miner which is exact similar , actual then the second most similar could be mari .
you probabl would agre .
okai .
that simpli sai thi pass similar , us thi formula realli captur the semant of peer similar , thi is in mani case , is these ar , if you want to find nice cluster , you want to find these other peer .
thei ar veri similar .
you want to group them into the same cluster .
then , we probabl give you a concret exampl .
just pick on research , anhai doan , who is a professor .
in univers of wisconsin , in comput scienc depart in the databas area , he receiv hi phd from the univers of washington in year <num> .
okai .
then you sai , who is most similar to anhai doan ?
if you us person page rank , your program will find philip yu , jiawei han , hector garcin molina , and gerhard okai .
you probabl not quit agre becaus anhai wa not as senior as these peopl .
if you us simrank , you'll probabl find even those author .
thei mai not be in the same field as david's system , but if you us pastsim , you find the most jignesh .
then you can have more , deshpand you get jun yan and rene miller .
you probabl would agre if you look at their , public .
at least you look at their gener cv , you probabl sai , oh jignesh is realli , realli veri similar becaus thei ar in the same field .
thei ar even in the same depart .
okai , snd thei got a phd in similar year .
so , you'll probabl see around five hundr thousand author .
thi pathsim realli captur someth quit subtl and sensit so it's not a bad measur .
it's pretti nice on .
then , there's a price to comput path base similar it can be costli just becaus the path could be long , and the over , overal cost , it could be high just becaus if you think about these the path ar form by joint you follow a long pass you actual will join mani relat if you do it down in the rear time unlock actual could be rather costli .
how can we calcul the similar in a effici wai ?
of cours , on interest wai is pre comput .
that mean the overal cost can be reduc by store commut matric for short path schema , and comput top k queri onlin .
the gener philosophi is nobodi want to linearli rank <num> , <num> author if you just sai who is most similar to becaus you know peopl will not want to see <num> , <num> author you know give a linear rank .
well thei want to see just top k , mayb top five , top ten , top fifti , so what you realli want to do is you can gener some coclust why you gener co cluster ?
just becaus if you want , onli want find top k , unlik peopl work in , sai , eh , algorithm and theori , or comput architectur , eh , eh , could be veri similar to a height on .
so thei , the co cluster essenti cluster is similar .
author , you know there ar similar thing along the same path togeth .
so , it's much easier to find cluster and also similar author .
so , gener co cluster then we can materi commut matric for featur object and target object .
then if we want to do thi , anoth interest thing is us short path schema , becaus a veri long on is still could be quit big , but a shorter on you could store small , you now , much smaller materi as a relat .
sinc we have co cluster comput , the size actual would be somewhat smaller and a cluster .
we can deriv upper bound for similar between object and target cluster , and between object and object , okai .
then we can safe prone target cluster object if the upper bound similar seem lower than the current threshold .
in the meantim , we can dynam updat top k threshold simpli sai we can have some upper bound it for those you know cannot reach such upper bond we can prune them safe just becaus we onli want to find top k result .
thi is a effici framework for implement pass in in a larg data set .
in thi session we'll discuss user guid meta path select for cluster in heterogen network .
why we want user guidanc in cluster ?
simpli sai differ user mai like to get differ cluster for differ applic .
for exampl , we mai want to cluster author base on their connect in the network .
suppos we get eight author and four of them came from uiuc .
the other four ar from msr , microsoft research .
then we mai have four differ author publish paper in kdd .
the other four publish paper in sigmod .
then how can we cluster thi eight author ?
it realli depend on what do you want .
for exampl , if you follow the meta path aoa , that mean common organ , you will group <num> , <num> , <num> , <num> into on cluster <num> , <num> , <num> , <num> as anoth .
howev , if you sai the venu is more import , the public venu .
you will group differ , you will get a <num> , <num> , <num> , <num> or the kdd author group in on cluster .
<num> , <num> , <num> , <num> thei will be in anoth cluster .
okai , howev , you sai both of them ar import .
you probabl will get <num> , <num> as on cluster becaus thei ar from uiuc , thei publish in kdd .
so that's rather differ .
howev , which meta path we should choos we mai not know , we want to ask user .
that's why user guidanc determin the cluster result .
that mean differ user prefer mai lead to the choic of differ meta path .
the problem is how the user can give you the right meta path or differ weight .
actual , the best is you ask user to give some seed to give you the hint on desir cluster .
for exampl , okai , if user sai , i do not care but i want <num> and <num> to be in differ cluster .
you mai get , set of cluster like thi but you sai no , the user want to <num> , <num> , <num> , <num> in four differ cluster , you mai get the reader like thi , you mai work with a meta path that is aoa plu ava .
so the research problem becom we want user guid cluster with meta path select it simpli sai when we input target type like the author .
the number of cluster , k and the seed in some cluster , that mean you give some exampl .
then you give a set of candid meta path .
then , what we want to work out is the weight of each meta path and also the cluster result that consist with user guidanc .
thi is the basic idea of pathselclu , a path select do cluster .
it is a probabilist model approach .
for the first part , we'll remodel the relationship gener .
that mean a good cluster result should lead to high likelihood in observ the exist relationship .
that mean we want high qualiti relat to count more in the total likelihood .
now how we should do thi , actual , we need to model the guidanc from user .
that mean , the more consist with the user guidanc , the higher probabl of the cluster result .
then , the probabl model ha anoth featur it's model the qualiti weight of meta path .
that mean we want the more consist with the cluster result , the higher qualiti weight .
so that mean we can work out thi in a systemat wai but i will ignor the detail , you mai read the paper .
but the gener philosophi , i can show it in thi yelp data .
the yelp data for busi review , you mai have review as the star center .
it count as a set of term .
a setup busi , and a setup user .
okai .
then you mai have differ relationship type .
that we mai have , sai we mai have two candid path .
b r u r b mean these two busi , thei ar review , share some common user .
then b r t r b mean these busi , thei review share , share some common term , okai .
suppos we want to get our target object is cluster result the restaur .
we want to get six cluster .
so you probabl can see the test on the rear data set .
you onli select the <num> sampl or <num> sampl or <num> sampl .
then , if we look at accuraci or normal neutral inform .
we proce these path select clu , thi algorithm , in ani case thei gener far better with that in some other algorithm .
why ?
becaus the user give some hint or give some seed base on the data set , you will work out a nice wai to the meta path .
and anoth interest discoveri actual is if you look at the restaur to cluster restaur or cluster shop busi for exampl sell cloth then you will find their meta path weight ar rather differ .
for exampl for these , meta path b r u r b that mean whether it's busi share or common user .
for restaur , it weight much less compar for the term .
okai .
and also , compar to cluster shop busi .
why ?
becaus you probabl can see the differ peopl , actual the user , mai like to try differ kind of food so thei mai like to go to differ restaur .
howev , the user mayb share more by the shop busi like shop cloth just becaus for some veri expens cloth store , you mai attract on group of user more regularli .
for some veri cheap store , you mai like to attract anoth group of user like .
but the restaur peopl actual mai try more varieti .
that's the reason you get differ weight .
so it is veri subtl and in mani case you have to deriv the weight base on the data instead of base on your own think .
so in summari , in thi lectur we discuss cluster analysi in heterogen network .
we introduc the concept of heterogen inform network and how to do data mine in heterogen inform network .
we discuss rankclu and netclu as two interest rank base cluster measur in heterogen network .
we also discuss a new similar measur call pathsim , meta path base similar measur for heterogen network .
we then discuss how to us user guidanc to do meta path select then we can do effect cluster in heterogen network .
here i give you a set of interest research paper .
if you like to dig deeper , you mai like to read those paper .
and thi is still a young theori , there ar lot of open research problem peopl can work on .
thank you .
welcom to the last lectur of thi cours .
lectur <num> , advanc topic and applic .
in thi lectur , we ar go to discuss some advanc topic , includ cluster data stream .
we will first introduc a k median base approach , then we will introduc the clustream approach .
after that , we will briefli list some advanc theme of cluster analysi .
then we'll reintroduc some applic of cluster analysi .
we're discuss name disambigu us cluster analysi method .
and we also discuss how to my evolut of heterogen network us cluster analysi method .
after that , we will briefli introduc some explor of broad applic of cluster analysi , that will conclud our cours .
first , we will discuss cluster data stream , a k median base approach .
what ar data stream , why thei ar so import we want to discuss it ?
actual , data stream ar veri popular in current big data ag .
becaus there ar lot of sensor or video camera and mani other thing .
thei actual record data , an onlin data stream version that mean the data were flow into your video camera and flow out .
now , you mai not regist everyth , okai ?
especi in those observ system .
the major featur is thei ar continu , order , keep chang .
thei come and go veri fast but in huge volum .
with such fast data stream , a veri import requir for the algorithm is the singl scan .
that mean you ar not go to have time or chanc to store all the data and keep index and also try to access in a random wai .
you basic get a scan , you want to repli them in real time .
so , here is a typic stream process system architectur .
so , we ar face multipl data stream flow in , like those oper system .
then thi system where take those multipl data stream where process ar in some scratch space like main memori or some disk , as well , okai ?
then in the meantim , user user or applic program usual pose queri in the form like a continu queri .
in the sens , you have a request , alarm me when sort institut happen .
then the stream process system take thi mark for onlin data stream .
we're try to return the result to check the core return result in the real time .
then the problem becom , how can we perform cluster analysi effect in such fast chang data stream ?
on propos approach for stream cluster is a k median base approach .
thi method wa publish in icd <num> , done by o'callaghan and sever other research .
thi method is base on the k median method , that mean the data stream point come , thei ar in the form of metric space so you can easili comput median .
the goal is to find k cluster in the data stream such that the sum of the distanc from the data point to their closest center is minim .
so , the whole thing is similar to what the requir of typic cluster algorithm .
then , becaus it's a data stream , so usual thi method , actual design the wai like constant specter approxim valu .
it us a readm small memori space .
it's a simpl two step algorithm , that mean for each set of m record , like a s sub i .
you ar not go to regist all those m record becaus it's flow in veri fast , potenti infinit .
okai , but we try to find o k center in the stream , s sub <num> to s sub l .
thi o k mean usual you want to find the k center , but you want to store multipl k's .
it's a local cluster , that mean you want to assign each point in s sub i to it closest sector .
let s prime be the center for thi current , with each center weight by the number of point assign to it .
simpli sai these point , you onli regist point but you also regist weight .
becaus thi point actual could be summar by mani underli data stream point .
then we take thi s sub <num> to s sub l , these center .
then we will try to us thi s prime to do further cluster to find k center .
so , we can draw a hierarch cluster tree , we can illustr the method , okai ?
the method is we see mani , mani data point but we do not have capac to keep store all the data point .
what we want to maintain is at level i , we will maintain at most m , level i median .
that mean , we onli maintain the median point and also the weight .
that mean , how mani point realli aggreg ?
final , you onli regist median .
then if we see m such median come , we will gener on more level .
thi level is essenti the level i plu <num> is still case multipl but the weight is equal to the sum of the weight of those median .
that mean , if you look at a level i plu <num> thi median point , the weight essenti mean how mani point that it realli repres is the sum of weight of level i median .
and level i median is the sum of the weight of the , how mani real data point actual summar and repres by thi meet point .
so , it work but the problem could be the qualiti mai suffer for evolv data stream .
becaus when the data stream keep come , if you onli regist the m level i median , in mani case it mai not be suffici .
and you do not know whether , what you regist ar all veri low point ar just thei're recent point .
then you'll provid the limit function for discov and explor cluster over differ portion of the stream over time .
so that's the major thing , we probabl need a more robust and more sensit cluster method .
in thi session , we ar go to introduc an interest approach , for cluster data stream call the clustream approach .
thi approach wa develop by ibm , and the uic group work togeth .
and publish the paper in vldb <num> .
the gener goal of clustream is we try to get high qualiti cluster , of evolv data stream with rich function .
we follow the stream mine philosophi , that mean there's onli on pass over the stream data , with veri limit space usag , but still have high effici .
thi cluster methodolog contain three major point , on call tilt time frame work , t hat mean we store more recent data in a fine scale .
when the time becom more and more remot , we store the data in the closer , and closer scale .
becaus we can not store all the previou histori , but if we do not store in thi wai , we will lose dynam chang .
then , the second philosophi is micro cluster .
that mean we store the data in a fine cluster scale .
it's not confin to k , it's not even link to k , but we have much better qualiti than the k mean , and the k median method .
becaus it is increment , it ha onlin process , and it facilit cluster mainten .
then the third major point is , we have two stage process .
we have micro cluster for maintain onlin data stream , and it histori .
and for the macro cluster , we will be abl to answer adhoc queri .
therefor , with the limit overhead , we will be abl to achiev high effici , scalabl , qualiti of the result , and the power of evolut chang detect .
let's first look at thi tilt time model .
essenti , tilt time frame is a trade off between space , and granular of time .
that mean we want to decid , at what moment the snapshot of the statist inform should be store .
so the gener design , we'll have natur tilt time frame , logarithm tilt time frame , and a pyramid tilt time frame .
the natur tilt time frame , the gener philosophi is we follow the natur time boundari .
for exampl , everi quarter we store a frame .
then after four quarter , everi hour store a frame .
and everi <num> hour , everi dai we store on frame .
after <num> dai , everi month we store a frame , and so on .
so , that will maintain the most recent on , actual store in a fine scale , and gradual we get a closer and closer scale .
the logarithm tilt time frame maintain a similar philosophi , but it jump in a logarithm wai .
that mean everi t time we store a frame , and then , later we store everi 2t , then everi 4t , everi 8t , everi 16t , everi 32t .
we jump in an exponenti wai .
so , we have limit time frame , but we can store the realli , realli , long you know , the pass .
pyramid tilt time frame wa design by clustream .
it's a interest design thi wai .
for exampl , just give you a small exampl .
suppos we onli have six frame .
each frame mai onli be abl to store three snapshot , okai .
then the , suppos we give a snapshot number sai now , we store <num> <num> , suppos the <num> count .
if the frame number <num> count , it could not mod by ani <num> to the power d , if d is <num> , or over .
so , becaus it's an odd number , so , we'll store it in frame <num> , okai .
then frame <num> we're kee , becaus there , there ar alreadi three frame , we'll kick out <num> .
okai ?
so if , it , it's , <num> come , <num> mod <num> to the power <num> , you get mod <num> .
so then , we'll store <num> in the frame number <num> .
okai .
by do so , you probabl can see the most reason , why we almost store everi singl on .
then later , we start becom everi <num> everi <num> , everi <num> , and everi <num> .
so , it gradual chang , but most recent on , we store in quit some detail .
okai .
then , these kind of snapshot maintain a micro cluster , we can store in the pyramid pattern .
and then , thei can store differ level of granular depend on the recenc .
then the snapshot can be classifi into differ order vari from <num> to log t .
and the , the i th order snapshot , will occur at the level of alpha to the power of i .
onli the last alpha plu <num> snapshot ar store .
so , it's a gradual increas the gap , and decreas the qualiti , or timestamp to be maintain , okai .
then , these cluster framework is a micro cluster approach .
essenti , underneath the structur is a birch cf tree structur .
that on , we alreadi introduc in the birch in the hierarch cluster .
gener , a cf tree is a height balanc tree that store the cluster featur .
the non leaf node store sum of the cf of their children .
okai .
then , of cours , we could not store everi singl point , to that's why their so call leaf node , will still store at some minimum level , okai ?
we call these as a micro cluster .
okai ?
so , the micro clu , cluster is store in the cf tree .
it maintain the statist inform of data local .
it ha temporari extens , of cluster featur vector becaus it contain multi dimension point with timestamp .
then , each point will contain d dimens .
then , for the micro cluster for n point is defin by these <num> d plu <num> tupl .
okai ?
you probabl can see these ar the vector .
these ar addit <num> point , sinc you have d dimens .
so , thi vector , the length actual it's d .
that's why you get a <num> d plu <num> tupl .
we have onlin , and offlin compon for cluster evolv data stream .
the onlin compon essenti , is micro cluster mainten .
the offlin compon essenti , serv as queri base macro cluster .
that mean , when the data stream come , we will store thi summari statist about data stream .
we will increment store into thi micro cluster structur , okai , us the cf tree .
initi , we can creat a q micro cluster .
the q usual is signific larger than the number of natur cluster .
for exampl , suppos the num , number of natur cluster is k , and thi q is substanti bigger than k .
but of cours , your main memori will still be abl to hold it .
then , when the onlin increment updat come then , you will check i , if thi new point is within the max boundari , you'll insert thi on into micro cluster .
otherwis , you can creat a new cluster , if it's out of the boundari .
and then , these cluster can be further , everi time you insert them , sometim you mai need to merg to maintain the size of the .
with a timestamp , we will be abl to delet some obsolet micro cluster , and also , we can merg two closest on into a littl bigger micro cluster , okai ?
then , maintain those micro cluster , we will be abl to answer queri base on macro cluster .
that mean , when the user queri come , you can ask him mani differ kind of queri , okai ?
and then , base on store summari statist in thi cf tree , you will be abl to further cluster those micro cluster to gener the macro cluster .
that mean , base on a user specifi time horizon .
for exampl , you sai , i want to see last ten base .
okai ?
and the number of macro cluster k .
okai , suppos i want to gener the <num> cluster .
then , we can comput a macro cluster us like a k mean , or some other cluster algorithm .
so , thi is a veri effect structur to handl onlin data stream .
and we can gener qualiti cluster , with minim overhead , and a veri limit space .
so , thi is the design of cluster .
in thi session , we will briefli introduc some other advanc theme on cluster analysi .
on import interest on call ensembl cluster .
what is ensembl cluster ?
ensembl cluster is try to combin the result of mani cluster model to creat a more robust cluster .
sinc cluster is unsupervis , there's no singl model or criterion mai truli captur the optim cluster .
but if we can ensembl the model , it mai provid a more robust solut .
the gener method of knowledg is first we gener k differ cluster or we call these ensembl compon .
then after that , we will combin those differ result into a singl and more robust cluster .
so , we'll first to see how to gener thi k differ cluster , okai ?
usual we can us a model base or data select base .
the model base mean you gener us differ model or differ paramet .
you mai gener differ compon , thi is you can select those ensembl compon .
the data select mean you can data from differ portion or differ sampl for the whole partit of the data .
then you can gener those ensembl compon .
then we can combin them togeth .
now how to combin those , these differ ensembl compon , gener there ar two major propos .
on call , hypergraph partit that mean you will consid each data point is a vertex .
and that a cluster actual in ani ensembl compon , you can think thei ar hyper edg .
then those feed point will be wrap into , will be link by mani hyper edg .
then you can think thi on is hypergraph then you do hypergraph partit you mai get those ensembl final robust cluster result .
anoth on , call meta cluster , meta cluster is a graph base approach , compar to the typic point base , graph base cluster .
the vertic actual now is associ with each cluster in the ensembl compon , okai ? .
so these ar the typic approach for detail we will introduc some refer .
then we will briefli outlin , how cluster is us for effect data mine ?
first , cluster is can be us for data summar , why ?
becaus cluster is base on similar , actual thi is a veri natur form for summar .
becaus summar in the most natur wai is base on the notion of similar .
that's why when you cluster data you essenti summar the data .
then the second major effect data mine task cluster can contribut is outlier analysi .
actual , we can view these outlier , anomali , ar those data point that ar far awai from ani particular cluster .
that mean we can first do cluster and onc we see there's some isol data point .
thei're far awai from ani particular cluster , we will sai those ar the outlier .
then the third contribut that cluster can do is for classif .
the classif , if we first preprocess data to get a cluster , we will reduc the number of data point the classifi should handl , okai ?
and a typic on , for exampl , a cluster can speed up , kenya's neighbor classif .
for kenya's neighbor you try to find their neighbor but sometim you get too mani data point .
then we can replac the data point with the centroid of fine grain cluster belong to a particular class .
that simpli sai , if there ar mani , mani data point , thei all belong to a posit class .
we actual can find cluster and we just us centroid to repres some fine grain centroid .
so wrap in the mani data point if thei belong to the same class , okai ?
so that mai effect reduc the complex and number of data point to handl in classif .
and cluster also popularli us for damag dimension reduct .
in our lectur , the previou lectur we alreadi briefli introduc spectral cluster , non neg matrix vector , probabilist latent semant index .
those ar the typic dimension reduct method us cluster , okai ?
final , cluster can be us for similar search and index .
cluster is base on notion of similar that's why we studi similar measur , dissimilar .
if we can do hierarch cluster , then we mai creat index structur like cf tree .
that will support similar search and index effect .
final we will discuss cluster big data , so call big data is data in the realli , realli larg scale process .
you , you probabl know the hadoop , the mapreduc , there ar mani , mani such new methodolog to handl veri big data .
and on interest on call apach spark .
we have a pointer to their websit , it's a fast and gener engin for larg scale process .
it is on of the most activ project on big data , so it ha lot of contributor and lot of product deploy .
it ha contain user friendli and express api in python , java , scala , and r .
it ha fault toler in memori and on disk comput .
it hold data in memori and provid fast access dure iter and it contain some recov node failur .
and it ha mani featur rich standard compon for machin learn , graph comput , sql , and data frame and stream process .
for cluster algorithm , it support k mean , stream k mean , power iter cluster gaussian mixtur , latent dirichlet alloc and in mani more .
so i provid some pointer you mai click there , you mai watch some mooc video to learn more about how thei're cluster big data .
in thi session , we ar go to introduc you on interest applic of cluster analysi call name disambigu .
name disambigu actual is on task in data clean .
in data clean , peopl mai do object reconcili versu object distinct .
object reconcili mean the same object , sometim thei have multipl name .
for exampl , uiuc you mai also call the univers of illinoi at urbana champaign .
then object distinct mean the same name mai repres multipl real object .
for exampl , differ peopl or differ object mai share the same name .
just give you exampl , when we studi thi problem in the year <num> , we check allmus . com .
there were <num> song and <num> album have the exact the same name , forgotten or the forgotten .
and in dblp at that time , we check that there were <num> paper written by at least <num> differ peopl with the same name , wei wang .
so there's a new challeng for object distinct just becaus if you just look at the textual similar , thei have exactli the same name .
so you cannot us textual similar to distinguish them .
especi in dblp , if you cannot refer their websit or mani other piec of inform , then you'll probabl find it's realli hard .
howev , in thi point , link analysi mai take advantag of redund of those link and neighborhood to facilit entiti cross check and valid .
if thi is the case , we work out an algorithm call distinct .
it's object distinct by inform network base cluster analysi , and we show the power of cluster analysi .
thi paper wa publish in icd <num> .
let's look at thi distinct problem , the wei wang challeng in dblp .
when the student first propos thi problem , i told him thi is veri hard , becaus i even have three differ wei wang who work with me .
and we actual publish in differ year almost in the same confer .
so how could you distinguish them ?
then he actual later gave me thi pictur .
and he sai , if you look at thi wei wang , thi wei wang wa at a ucla and then he went to ibm and a here .
so if you look at the co author at differ year , there's a clear pattern .
wei wang around to thi time publish substanti with jiong yang , dick muntz .
and later with jiong yang , philip yu .
and later with , for exampl , with her own student .
but there is a clear pattern and a clear connect .
then you look at thi wei wang , actual wa first publish with hongjun lu , and then later with xuemin lin , substanti .
so that also ha some veri clear pattern .
and if you look at thi wei wong , it's more like publish a lot with the peopl in fudan univers , or someth .
so if you look at these differ wei wang , you see just base on the year and the collabor , you will be abl to find the cluster .
mayb we will be abl to us cluster analysi to help distinguish those case .
so that's the case which will becom quit interest , how to design a methodolog to distinguish those differ peopl with the same name .
then we want to work out the similar measur between the refer .
we check , there's a link base similar .
that mean just look at linkag between refer .
the refer to the same object ar more like to be connect .
then we can us random walk probabl to calcul thi link base similar .
anoth on is neighborhood similar .
neighborhood similar mean that for each refer , their neighbor tupl can indic some kind of similar between their context .
then the major problem becom , we got similar , we can group them into differ cluster .
but what is a boundari , how we can cut those differ link to get into distinct cluster ?
so we actual found that there's on interest wai to do train .
self boost , that mean we do train within the same bulki data set , by us rel clean case .
clean case mean those object mai have veri distinct last name or full name .
us thi , we think thei ar clean , thei ar just the same , refer the same author .
then we us them to studi the behavior .
then we us refer base cluster .
that mean we group refer accord to their similar .
then we got a littl detail on how to do train with the same data set .
that mean we want to build a train set without expert , we do it automat .
how can we do it automat ?
there's distinct name .
we just go to dbrp , if you just look at a name , look at their statist , you'll find those pretti long and pretti rare last name like johann gehrk and mike stonebrak .
then we assum those peopl , thei ar distinct .
thei ar clean , unambigu exampl .
then we look at their collabor behavior .
within the same commun , we assum the collabor behavior within the same commun , thei do share some similar .
then these train paramet can be us from those unambigu exampl .
we us svm to learn a model for combin differ join path .
then join path is us as two attribut with link base similar and neighborhood similar .
and the model is a weight sum of all those attribut .
then we do data cluster base on thi measur similar between cluster .
we actual try differ case to see what is the best measur , what is wai to do best cluster .
we first tri singl link , which is get highest similar between point in two cluster .
that mean between the two cluster , the closest on , the most similar on as the link .
that's a singl link .
howev , us the singl link we found is no good becaus the refer to differ object can be easili connect .
then we tri a complet link .
that mean try the minimum similar between them .
that mean link the two cluster to us the most farawai point .
howev , thi is no good either becaus a refer to the same object mai be weakli connect , as well .
so then we tri averag link .
that mean look at the averag similar between point in two cluster .
we found thi is a far better measur .
moreov , we refin thi link by think about averag neighborhood similar and the collect random walk probabl , we take both factor , we averag them .
so we found thi averag link us thi refin measur gave us the veri best result .
then we test on the real case .
we look at dblp popular name , like thi is on set .
we test it .
then like hui fang , got three author here ar the same name , and wei wang , got <num> author here ar the same name .
and these ar the number of paper thei publish .
and then we found actual some , we got perfect result .
some will get close to perfect result , for exampl , like joseph hellerstein .
and actual , <num> paper is a pretti larg number .
actual , there were two joseph hellerstein .
but interestingli , in dblp , actual thei have a differ middl initi .
you realli can't distinguish them .
we actual remov the middl initi .
we test our algorithm to see whether we can get those case , which joe hellerstein work on which paper , back .
actual , we found our f measur , it's not perfect , but it's not bad .
but then you look at wei wang .
for thi wei wang , you mai see thi f measur is not that good .
but it's pretti decent , as well .
we check for wei wang , what ar the problem , which case we got right , or which case we got wrong .
actual , we found if you realli look at these <num> differ wei wang who author more than on paper , we found there's on case , thi suni binghamton wei wang , wa complet miss .
and that two paper wa merg into fudan univers wei wang .
howev , fudan univers wei wang got five paper with credit to unc wei wang , but unc wei wang lost six paper to suni buffalo wei wang .
then we look at the detail case .
we found actual on author xian pei , almost thei publish in the same confer , same time .
he actual work with fudan univers wei wang , unc wei wang , and the suni buffalo wei wang at the same time .
that's <num> paper , essenti , wa all the paper collabor with him .
so that's a case we mess up .
otherwis , actual , it's a decent cut for the differ case .
so that simpli sai of cours we need further studi us addit techniqu .
and some peopl follow up thi work actual did more us addit signal to get even better result .
that paper just show cluster actual work , even for object distinct .
in thi session , we're go to studi anoth applic of cluster analysi .
evolut of heterogen network .
for data mine a veri interest thing is to studi evolut dynam of heterogen network becaus network evolv with time .
for exampl , for dblp network , the network mai form sequenc base on paper public year .
let me see you take everi year as on snapshot , you will see the sequenc of the network ar chang .
so the motiv is , we , we're try to model evolut of commun in heterogen network .
that mean we were , we want to automat detect the best number of commun in each timestamp , then model the smooth between the commun of adjac timestamp .
then we can model the evolut of structur explicitli , like the birth , death split of subnetwork .
we work at a algorithm call evolnetclu , it's evolut network cluster .
it's cluster of model evolut of dynam heterogen network .
that mean we want to see the co evolut of multipl type object within a commun .
simpli sai , within thi commun , within thi cluster , it contain heterogen multi type object and link and then we discov the overal evolut structur among differ commun .
the studi wa publish in machin learn and graph is mlg <num> and the later version were appear in algebra transact in <num> .
the gener idea is from natur sequenc , we want to find evolut commun or subnetwork .
you know , each commun consist of multipl object type cluster .
we have time stamp t sub <num> , t sub <num> , two t sub <num> and then we have author , we have paper , we have venu and keyword .
those ar the evolut compon , thei have sub cluster or network .
so we want to find these network if we do cluster , we probabl will find that there's a databas cluster .
there's an ai cluster and over the year some mai , some author or confer mai split and emerg into a new cluster call data mine cluster .
and over the year , each cluster , onc thei form , thei will evolv as well .
so we work out a graphic model , which is a gener model to model these network .
essenti , it is a dirichlet process mixtur model base gener model .
that mean at each timestamp , a commun is depend on histor commun and a background commun distribut .
thi graphic model repres by a sequenc of these gener graphic model .
you proce the time is g sub <num> to thi graph g sub <num> to g sub t .
and thi graph actual wa gener base on , you know , a lot of thing you can see .
for exampl , author , venu , firm and you know , the research paper entri , you base on object you can see , you us those paramet you would be abl to gener the , the real model how thing ar evolv .
then thi gener and model infer , essenti is to gener a new paper o sub i .
we need to decid whether thi on will join an exist commun or join a new on .
okai .
to join a exist commun k that we mai have probabl n sub k divid by thi .
and to , to join a new commun k with a probabl of the other on , okai ?
then we can decid it prior , either from a background distribut lambda or histor commun <num> minu lambda time pi  , the probabl is sub k with differ probabl and then we will be abl to draw our attribut distribut from the prior .
then we will be abl to gener the the new paper object o sub i accord to the attribut distribut us thi formula , essenti is kind of a chain rule and each on is a product of their probabl .
then we can us a greedi infer for each timestamp .
that mean we us collaps gibb sampl try to sampl cluster label for each target object .
for exampl , paper .
then base on thi , we can work out the object evolut discoveri error , which is essenti em algorithm .
the step on is gener the prior group , that mean we calcul the posterior probabl of the hidden label .
then in a step two , we put thi on into a iter process us em algorithm to do hidden commun label assign .
then at thi step three , we final we're do the commun distribut estim us those paramet .
so if you you look at the overal framework , thi algorithm we'll call paramet estim algorithm , essenti is an em style algorithm .
we'll give a differ timestamp to network and differ paramet , then we want to get commun assign vector and the paramet and their distribut .
so , at the veri begin , we assign each object into their prior group , then we get into thi em loop .
essenti , thi for each object , we do the e step , assign thi object to , to the commun with the maximum posterior probabl in either exist commun k or a new commun k plu <num> .
you'll get m step as we updat the relev statist and if the commun is too old , it contain no object , we will remov thi commun .
then we repeat to thi process until the cluster chang chang threshold is reach .
after that , we estim finer paramet for each commun .
then we look at a case studi , for exampl , for dblp data .
we will be abl to check the databas commun evolut and you can see we do from <num> to year <num> , everi three year , we partit as on group , then we studi the multi type object dure evolut .
okai .
for exampl , you mai see the , the top part of the blue on ar the venu and the red on ar the keyword .
so you can see the , from <num> to <num> , actual in the middl around <num> befor reach <num> , we've found actual , there's evolut .
some peopl actual , also get into the conferenc ai and the softwar , then you probabl can't see thei ar new keyword in thi subcommun focus on softwar and foregon knowledg .
around year <num> <num> to year <num> , we found anoth major split that besid the regular databas confer their keyword , thei , actual , there ar peopl join like ir and kdb commun so us the keyword like inform , retriev , data mine , text , quit a lot .
so we probabl can see the evolut of thi commun along the year in 1990s .
so , it's a clear indic thi commun ar evolv and if you , you know , studi the differ commun , you will see their interconnect and their link and their evolut behavior .
usual , we also observ the more type of object us .
that mean instead of just us , like venu , we actual us venu , us keyword , us paper and author .
the more object type us , the better accuraci .
and also , you get better histor prior in your result and better accuraci as well .
we also studi the evolut of social event base on delici . com .
okai .
the delici . com , the schema as wa you see , the user usual tag some event us tag on differ websit .
we studi in year of <num> , just on month of januari .
that mean we start from januari the first week of januari up to the fourth week of januari .
we want to studi , we found a differ cluster .
for exampl , these three cluster .
the first cluster c sub <num> is more intern polit and the sub sub <num> is more on the techknowledg .
the sub three is more like a and social life .
so you see the first cluster .
the first week of januari in <num> , there were lot discuss on the terror in secur travel airport .
the reason wa dure the christma time previou year in year <num> , there wa a terrorist who , who would have like to blow up the plane when the plane wa land in the detroit airport .
so you probabl see , that's why we got lot of discuss on the travel airport safeti , secur terror .
howev , it's just on week later , you probabl see there ar lot of thing chang .
we still see some terror entri , but on the other hand , the major ship of the discuss is that googl wa leav china due to the secur internet privaci polit and censorship .
so you probabl can clearli see the trend is chang the discuss on the intern polit and thi issu wa discuss in the follow two week as well .
okai .
then you see like a , a for cluster two , their discuss on the mac appl , iphon tablet and it's quit a lot , these four week .
their discuss did not chang that much on the focu .
then if you look at it the social part of the first week wa discuss more on the harri dispatch depress sleep .
but it just a week later , you probabl see thei start come out of haiti , then you got a haiti earthquak photographi and then there ar lot of thing in disast .
then you'll probabl see much more discuss in the follow week .
so just within these sever week , mani thing ar chang the network evolv quickli , e , especi you watch the keyword .
so , even we look at the count , we also see everi week , someth mai chang quit a lot .
that simpli sai , thi cluster method is quiet effect at find network evolut regular in heterogen network .
now we come down to the final session of thi lectur , also the final session of thi cluster analysi cours .
we were just touch a letter on explor of broad applic of cluster analysi .
we know cluster analysi ha veri broad applic .
here i just give you a few , just outlin in a veri brief wai .
for exampl , we see there lot of studi on custom segment and collabor filter us cluster analysi techniqu .
that mean we can cluster custom base on their similar of their profil and shop behavior .
there ar system call recommend system , the method call collabor filter base on cluster of custom rate of product .
now text mine us cluster analysi quit a lot .
the typic topic model is co cluster of word and document .
actual , there ar mani recent studi on how to do cluster , type , profil and in depth analysi of larg collect of text or web document .
there ar also lot of research work on cluster of multimedia and social media .
for exampl , summar multimedia data , like video data and also integr analysi of new and tweet .
there ar more applic , for exampl tempor and sequenti data analysi , like a cluster web log click stream to find pattern of user and reorgan web page structur .
or do like a cluster biolog sequenc data to construct the biolog model and biolog network and discov anomali .
in social network analysi , cluster analysi ha been popularli us , for exampl , cluster to find hidden social commun call commun detect .
although , subsequ analysi for anomali detect , classif , influenc analysi and link predict .
there ar mani , mani more emerg applic .
thei ar veri excit .
thei , thei show a cluster analysi , that's veri import for data mine and for broad social applic .
there ar mani thing wait for you to contribut .
we hope you will plai some veri activ role in explor broad applic of cluster analysi .
final , we summar thi lectur in thi advanc topic and applic .
lectur , we discuss cluster data stream .
we introduc k median base approach and the clustream approach .
we also touch a littl on advanc theme of cluster analysi , then we get into applic of cluster analysi .
we introduc two interest applic , name disambigu and evolut of heterogen network .
then we touch a littl broad applic of cluster analysi where , and we hope you will contribut more on cluster analysi and it broad applic .
final , i'll just show you a few refer book and research paper we cover and us in thi lectur .
hopefulli , you will enjoi program assign if you select to do so .
and you enjoi , enjoi the remain of thi cours .
hopefulli , you will get on more data mine cours in coursera and you enjoi other coursera cours .
thank you and goodby .
in thi session , we're go to introduc graph and network , their basic concept and their represent .
so what is a network or graph ?
a network or graph usual treat like synonym .
a , a graph g is written by a set of vertic , v , or is also call node for some literatur , and a set of edg e , or some peopl will call it a set of link .
e , actual it's a subset of v cross product with v and n is the number of vertic also call order of g , and it's the number of edg also call the size of g .
the multi edg mean if there exist more than on edg link between the two vertic , for exampl on , two , if there were more than on edg we call thi is a multi edg graph .
okai .
also , the loop mean edg connect a vertex to itself .
usual we call thi as a self loop .
for exampl , if you get a y , you get anoth edg also connect to y directli .
that's a self loop .
for simpl network we will not consid self edg or multi edg .
then , if we want to repres thi simpl network , we rep it as adjac matrix .
if i and j is on simpli sai there is a edg connect between vertex i and j .
'kai .
then , otherwis it's a zero .
direct graph simpli sai the edg mai have direct , like tail , point to head .
the , in that case , if the adjac matrix a sub i , j equal on , mean there's an edg from j to i .
it's direct .
otherwis thi is zero if there is no such direct edg .
'kai .
sometim the graph mai not just a on or a zero , thei mai have certain weight with everi edg .
weight w sub i j usual it's a real number is associ with each vertex each edg of v i , sub i j .
then thi graph is a weight graph .
then we see how to defin the vertex degre .
suppos we get a network g consist of set of vertic .
and edg .
for undirect network essenti there's no direct like thi graph .
then the degre of a vertex is defin as how mani edg from the other on go down here .
go in and out is just the same becaus it's undirect .
for exampl , okai .
we give it a .
obvious the number of edg contain , connect to a is four .
that's why the degre is four .
similarli , the degre of h is two .
then we look at direct network .
direct network , the in degre is how mani , edg point to thi v sub i .
'kai .
for exampl , if you look at the n degre of a you probabl can see there's three edg point to a .
that's why n degre of a is three .
and n degre of b is two obvious .
out degre just revers .
so look at how mani edg actual go out of it .
okai ?
from v sub i .
okai , then if you look a's outgo edg you will find onli on .
but at b's outgo edg you can find two .
so that's the basic concept of vertex and degre .
in thi session , we're go to discuss typic evalu measur for graph cluster .
so , there ar some commonli us measur for graph cut .
for exampl , mincut , or minimum cut , ratio cut , normal cut , conduct , modular .
the sum of these have been cover , actual in the lectur seven , but we were just a mention them briefli here .
also we were introduc the similar measur across network .
essenti , simrank will be cover in thi session .
thei're also other measur like a person pagerank .
for a minimum cut thi is the first , first introduc a graph cut method .
okai , the gener philosophi can be express in thi graph .
if you want to cut the minimum number of edg to partit thi graph into two , or into more , okai .
howev , you probabl can see if you cut here , you mai break them into two graph , but thi is not balanc at all , becaus you find to the left onli a singleton node , which mai not be desir .
okai ?
howev , if you cut here , even you cut two edg , actual you make the whole result two sub graph somewhat balanc .
so , thi on is a better cut , the mincut .
let's see how the mincut is defin .
that mean that given a number k .
here the k could be <num> , okai .
choos a partit c sub <num> to c sub k , which partit a network into k partit .
the measur essenti is , you look at the sum of the weight of the edg connect c sub i , thi partit , and other partit .
sinc you have c sub i , you try to get everyon .
essenti , thi link to other you count twice .
that's why you need to divid by two .
okai , howev , we alreadi see the mean cut is not a good cut , in mani case .
becaus you mai cut these two part , it's highli unbalanc .
then peopl propos other measur try to make the partit balanc .
on propos is call a ratio cut .
the ratio cut basic is sum of edg weight link partit c sub i and other partit , and you want some of the weight to be normal , it mean everi weight divid by it size of the partit c sub i .
okai , sinc you add those partit twice , that's why you divid by two , okai .
so , it sai normal , but it's far better becaus you probabl can see , if you cut thi on into singl on , thi size is onli on , so then the whole ratio could be rather big , rather than if you cut them rel balanc , thi on could be rather big when you divid by thi you make the weight much smaller so thi , that's why the , the smaller of the ratio cut the better .
normal cut carri veri similar idea .
the onli differ is , instead of a normal by the size of the , eh , cut partit , you normal by the degre of the cut partit .
that mean the total degre of cluster in sub i .
there ar sub graph in sub i .
so , the lower the normal cut , it mean the better of commun .
just becaus thei ar well connect among the commun or the cluster selv , but thei ar , thei ar , spars connect to the remain of the partit .
the conduct carri , actual , also similar methodolog .
the differ is normal by the the minimum between these two , and thi on is a degre of the , the cluster , and thi on is the degre of the other on .
you just pick up the minimum on .
so , thi is , on conduct , but if you get , all the whole partit you get into k cluster c sub <num> to c sub k .
then the whole conduct is just the sum of everi on .
okai .
there's anoth measur of co modular .
the modular definit is somewhat differ .
it's the sum of the differ of these .
the differ , the first differ you probabl can see , thi is your intern connect .
c sub i connect to sub , c sub i , normal by the number of edg .
thi on , actual is their degre .
their degre normal by these 2e , becaus the degre when you add them togeth , actual thi degre should be normal by 2e .
the definit of the modular is the sum of the differ from <num> to k , for everyon like thi .
but for everi cluster , what do you studi ?
is the differ between the , the fraction of the intern address and the fraction of those edg to be , to be insid a random cluster .
if my cluster is far better than the random on the qualiti is better .
that's why the optim cluster of the graph will maxim the modular .
the interpret is a modular or cluster of a graph .
actual , it's the differ between the fraction of all edg fall into individu cluster and the fraction of those edg that do , so if the graph vertic were randomli connect .
unfortun , all the previou and the current defin these function , the object function , thei ar normal , but to optim them is np hard problem .
that's why you try to get a greedi algorithm , or try to get some sub optim solut instead of get absolut optim on .
now , we look at in a structur , like in a graph , you get ani two node between x and y , you want to see how to judg xy .
ar , ar similar , 'kai ?
that essenti can base on random walk model base , gener philosophi defin is by simrank , is you try to look at the structur context similar .
that mean you look at similar between x and y , these two node .
okai , for exampl , suppos you like two vertic , u and v .
thi similar actual is defin by it neighbor .
thi x , y .
it mean x is the , in neighborhood of u , and y is in neighborhood of v .
of cours , if you us out , out neighbor , you , you can get a formula veri similar to thi on .
simpli sai if you look at u and v , how similar thei ar , you just sum up the similar among those neighbor .
okai , then you would normal them by us in degrad or vs in degrad .
'kai .
when you initi them , you can sai eh , for ani particular node , u and v , if thei ar , thei ar the same , thei ar ident , then their similar is y .
if thei ar not ident , their similar is zero .
so , then you can do thi in a recurs wai .
simpli sai you want look at ani particular node u and b , how similar thei ar .
you look at how similar all those neighbor pair ar .
okai ?
so , that mean you can comput s , i plu on .
you comput it base on i .
so , of cours , thi comput could be rather costli , just becaus you look at the two node , you calcul their similar .
you look at their neighbor' similar .
actual , there could be mani neighbor .
and then you look at their neighbor' neighbor similar so that chain can go recurs , can go pretti long .
so , it's costli to comput simrank measur .
i've heard there ar some effici comput method , includ some approxim comput method that have been propos and studi .
we'll not get too detail into thi .
in thi session , we will briefli introduc the approach for graph cluster .
there ar mani method propos for graph cluster , we will just introduc a few .
on approach call partit with geometr inform .
for exampl , on algorithm call geometr bisect algorithm .
the gener idea is you will partit the graph accord to their mesh .
howev , in mani real graph , especi social network and mani other abstract graph , such geometr inform is not avail , is not realli quit relat .
so that algorithm cannot be appli .
then there ar some other method call graph grow and a greedi algorithm .
the gener philosophi is try to start from the smaller graph and it grow into the bigger on .
on interest algorithm in thi bigger categori call kernigham and lin algorithm , we call k l , which wa propos in <num> .
the gener philosophi is we take a random partit , the graph to the requir partit , then we can appli k l algorithm to it .
thi k l algorithm mean at each iter we try to swap the pair of vertic if we can increas the gain .
so , essenti if we can keep swap pair of vertic to maxim the gain , then we can get the desir partit .
then the third group of algorithm call agglom and divis cluster .
thi is similar to the algorithm we studi in the hierarch cluster method .
on repres on wa propos by mark newman in <num> , the gener idea is merg small commun if it increas the graph's modular .
okai .
and he propos on algorithm call fast modular algorithm .
then the first categori is us spectral cluster .
thi on is quit popular cluster method introduc and it popular keep increas .
so we will cover the spectral cluster in detail in the next session .
then the fifth categori call markov cluster .
the gener idea is we have transit probabl matrix , then we'll iter appli expand and inflat these two step on transit probabl matrix .
after that , we ar go to prune awai the smaller valu in each column and renorm thi matrix .
then next iter , we'll do thi again .
due to limit time , we will in all these graph cluster method we'll introduc the most popular on .
spectral cluster in the next session .
in thi session , we ar go to introduc spectral cluster method .
why do we want to do spectral cluster ?
becaus spectral cluster have some uniqu advantag .
so first spectral cluster make no assumpt on the shape of cluster .
so you can cluster , like ani kind of a shape , even intertwin spiral , you'll be abl to find nice neat cluster not to sai other shape .
howev , mani other cluster algorithm , like a k mean or em tend to find non convex shape cluster .
and anoth advantag is usual em or like a k mean , those algorithm requir an iter process to find local minima and thei ar veri sensit to initi , so you usual need multipl restart to get a high qualiti cluster .
howev , spectral cluster where you have no such burden , okai .
the gener process of spectral cluster is partit into these three step .
the first step is construct a similar graph , for exampl , we can us k nearest neighbor base on their neighborhood us like a euclidean distanc or other distanc , you like to do your cluster .
okai .
and for all the point you can construct a such similar graph , for exampl , if you look at thi illustr exampl , you can see the origin data point , if you do , symmetr k nearest neighbor .
you will find thi k nearest neigh , neighbor graph like the follow .
then we can emb the data point in a low dimension space us spectral cluster embed .
what is thi ?
thi essenti is we can do graph laplacian , comput the eigenvector of such graph laplacian .
after thi embed the cluster , structur becom more obviou .
then we will be abl to appli classic cluster algorithm , like a k mean on thi embed .
that mean we will be abl to partit the embed graph into nice cluster , so thi is the gener idea .
now we look at how we do it step by step .
first , suppos we have a graph .
thi usual is obtain by the neighbor , then we get a weight .
suppos the weight , w sub <num> is the weight connect the vertic <num> and <num> .
okai .
then the adjac matrix is a n by n symmetr matrix , like thi on .
then the , for each element , what you've got , the , the assign is if there's an edg between , like <num> and <num> , then you can base on the edg weight , w sub <num> , <num> , you can get the weight , w sub ij is the weight of thi cell .
okai .
if there's no connect edg , like on , four is not connect , then on and four , thi cell will be assign a zero .
okai .
then we can perform laplacian on thi matrix .
essenti if we us a diagon matrix of degre minu thi adjac matrix , we will get a laplacian .
okai .
what is diagon matrix of the degre ?
simpli sai , you , the degre is defin by , for each row you just sum up all the degre , all the weight .
or you sai , all the share valu in , in thi row .
for exampl , for row on , what you get is w sub <num> <num> plu w sub <num> .
you get the weight , that's the degre you will get .
okai .
that's d1 .
similarli for other degre , you just calcul it in the same wai .
laplacian , thi matrix valu is if thi on is diagon matrix , i equal j , then you get the d sub i .
okai .
that's the real valu of thi degre .
if the if it's other cell , like ij ha an edg , then becaus <num> minu thi w sub ij , you get minu w sub ij .
that's what you got .
if origin wa zero , you would still get zero .
okai .
so that's the valu you get from calcul thi laplacian matrix , then we can see how we can calcul the graph for eigenvalu and eigenvector .
okai .
so we probabl can see the eigen valu comput is base on these formula , which ha been us veri popularli in linear .
it's probabl in matrix comput , okai .
we will not get into veri detail , but gener philosophi is you get a matrix , a .
a lambda is eigenvalu of a and for some vector v , you'll follow thi , thi equat and a v is the eigenvector and the lambda is the eigenvalu of a .
so then for a graph , g , if we have n node , when it adjac you will have n eigenvalu .
and that thi n eigenvalu you can sort them accord to the descend order , you can , lambda on , lambda two to lambda sub n of thi n correspond eigenvector , where the x sub <num> , to x sub n .
you look at thi figur y , you probabl can see .
thi is the origin graph you , we have six vertic and thei're , thei're connect .
weight of the connect is mark us those weight , those valu , okai ?
then obvious , the , the origin matrix , adjac matrix is mark here , you probabl can see and that's the origin adjac matrix .
then we calcul the , thi origin a ei , thi matrix eigenvalu and you probabl can see thi is their eigenvector and these is there eigenvalu .
and you pro , you , you , you probabl see thi is mu on to mu six .
okai .
so that's the spect , spectrum of the graph .
okai .
then the eigenvalu and eigenvector of the graph , the graph laplacian of g is calcul , you , for thi graph .
essenti is you get lambda on , lambda two , lambda n and we know the valu lambda on , lambda two , lambda n , we can sort them in thi order .
okai .
so , if thi zero , zero you , you get a spectrum is you , you get that thi lambda valu , thi eigenvalu and eigenvector .
then eigenvalu reveal a global graph properti , which is not appar from the ed , edg structur itself .
essenti , if you get a zero thi eigenvalu with k differ to eigenvector , you probabl will see these , essenti it's a k k com , connect compon .
that mean if lambda y is zero that on connect compon .
okai .
then if graph is connect , then lambda two will be greater than zero and lambda two is algebra connect of g .
here you probabl can see lg , l sub g ha lambda on , lambda two , lambda three at zero , you get a three connect compon .
and if you have ar thi g1 and g2 , you probabl can see g1 is , ha onli on connect compon and g2 also ha on connect compon .
that's why lambda two is greater than zero .
howev , you also can see g2 is much denser is much more connect than g1 .
that's why g2's lambda two valu will be greater than g1's lambda two valu , then we can workout the partit via spectral method .
the gener philosophi is if we got thi origin graph , we map them into laplacian of thi origin graph is thi valu .
okai .
then we actual also know , we can calcul the second the , the second eigenvector and the second vec , eigenvector v sub <num> cor , correspond to lambda sub <num> .
and for lambda sub two , the smaller , the better qualiti of the partit , becaus it , if lambda is zero , mean it's not connect .
and if lambda two actual is greater , mean thei , thei ar more tightli connect .
if you want to get nice partit , you want to find the smaller lambda two .
then if we calcul thi for each node i in g , we can guess assign it the valu v2 , then we , v2 sub i .
so for , for exampl , thi note onc v2 is <num> , <num> . <num> , then if we want to find thi other six node , their correspond v2 valu .
then if we get to thi v2's valu we can get a greater than zero then we can assign to c1 and these v2's .
the valu is less than zero it could be anoth cluster , then we actual will get a nice partit c1 , c2 thi on , two , three will becom on cluster , four , five six will becom anoth cluster .
so we can extend thi algorithm to k partit .
the ng jordan and and weiss algorithm you know , work out by andrew ng , michael jordan and and thi publish in <num> wa normal laplacian .
essenti , the origin laplacian vertic is not normal .
but onc you us thi formula , we can normal the graph , so the degre , that part is all on and all the other row you add up .
actual it's minu <num> , so you final get a normal graph .
so we can comput the first k eigenvector v1 to vk , then we can calcul the , the matrix the , you , you in a similar wai , we can do the partit into , you know , becaus we take the i th row of u as it featur vector after normal to norm on .
then we can cluster the point with k mean into k cluster and thi method is commonli us as a dimens reduct , dimension reduct method for cluster .
in thi session , we're go to introduc anoth interest algorithm , scan , which it is densiti base cluster of network .
for social network analysi , peopl often interest in find click hub and outlier .
cliqu you can think is peopl know each other .
peopl ar tightli connect with each other .
hub could be the peopl who , who ar connect to mani differ group but thei mai not belong to a singl group .
now outlier , mayb thei do not belong to ani other cliqu , but thei mai have some few connect with some cluster or some group .
it would be nice to find the cliqu , hub , and outlier .
okai .
if we identifi thi , we will be abl to find some special group .
for exampl , famili , some social commun , or terrorist cell , or mani other thing .
then we introduc an interest algorithm call scan , which is densiti base cluster of network .
so , we first to see how scan defin the neighborhood of a vertex , okai , the neighborhood of vertex v essenti defin as v itself is neighbor of itself then directli connect us social link it direct neighbor .
so , gamma of v , essenti v and it neighbor connect those red , those yellow point ar the direct neighborhood .
then , if we want to comput the similar or connect between the two vertic v and w , suppos we have two vertic v and w .
we want to see how tightli thei ar connect or how similarli thei ar .
so essenti , if thei have mani common neighbor , okai , and thei're own size as a normal factor so you probabl can see , the more common neighbor thei share , and also the less other neighbor , essenti , if these ar all their neighbor .
so , thei ar veri tightli connect , or thei ar veri similar .
okai .
so , if you find on big star , thei have mani mani neighbor .
of cours , thei have some neighbor with w but , thei mai not be as similar as the other node .
so , base on thi , we can work out a structur similar , base on the notion of similar .
the structur similar usual is larg for member of a cliqu , but it's a small for hub and outlier , so we base on thi , we can work out a densiti base cluster .
so , the structur connect , we can think about it base on densiti connect .
we can work out a definit similar .
to dbscan in the sens .
we still have epsilon neighborhood .
it simpli sai , if these two , their similar is greater than epsilon's threshold , then we will sai thei ar connect .
then w is v's neighborhood , and it , then these neighborhood , actual the number of such neighbor is it's on neighborhood .
then for core essenti is if thi , if thi dbscan neighborhood , the number of such dbscan neighborhood is greater . . .
or equal to a minimum support threshold .
you can think of mu is a minimum , it's a threshold .
simpli said , if these neighbor is no less than a threshold , then v is a core .
base on thi , we will be abl to deriv directli structur reachabl .
directli structur reachabl mean v is a core and w is direct neighbor , then v will be directli reachabl to w .
okai , so you can probabl see that's a veri similar thing in dbscan , you also have direct reachabl , direct event that ar reachabl .
thi on call direct structur reachabl .
then structur reachabl , essenti , we write down reach of u and v , is a transit closur of direct structur readabl .
simpli sai if q can reach p2 , p2 can reach p and q will also be abl to reach p , that mean that you us a sequenc of the structur reachabl .
you can us transit closur , that's also reachabl .
okai , then what is a structur connect ?
structur connect simpli sai , if thi node p reachabl by o , and , q is also reachabl by o , then p and q ar connect .
the same thing is if we look at v and w if you can reach v , and you can reach w and if v and w ar connect .
so , that's a veri similar notion , but in the previou notion thi graph is a spacial reachabl the densiti .
okai , here the new definit in scan is structur reachabl base on the graph , base on the topolog structur .
okai ?
then we'll see how to get a structur connect cluster .
essenti , a cluster can be consid if thei structur reachabl , is connect , and thei ar maximum , that mean all the connect on form on cluster .
base on thi , you can get a cluster .
you also can get a hub , which is link .
it doe not belong to ani cluster , but link to multipl cluster .
you also can get an outlier , which doe not belong to ani cluster , but it is onli connect to veri few cluster .
so , then we look at how the scan algorithm is execut , okai .
suppos we want to find , suppos we said mu is <num> , epsilon is <num> , then we'll see how we comput thei're similar , thei're simplic , how thei're connect for note <num> and <num> .
we'll just give you a littl detail to see how <num> and <num> , thei're similar is <num> . <num> , okai .
then we can see becaus <num> directli reachabl .
the neighborhood is nine and <num> .
nine directli neighborhood .
directli reachabl , is <num> , <num> , <num> , and <num> .
that's the neighborhood .
then , we know <num>'s neighbor you have two , and nine ha five neighbor , okai ?
includ itself .
their intersect of their neighborhood is nine and <num> , becaus that's onli nine and <num> is common , okai ?
then , if you look at the , the size , the nine and <num> , the size is two .
then you past on the formula to comput the sigma essenti is you look at the size is two , and the other size is five , two time five is .
then you comput thi , you get <num> . <num> .
that's what you get it , okai ?
in a similar wai , you can comput the other .
so , let's comput , suppos <num> , so we can comput it similar to <num> , <num> and <num> .
well , you can see is thi two is abov the threshold .
that mean eight and nine , eight and <num> is abov threshold , the activ form a cluster .
okai .
then you can keep go .
if you keep go , we will see , we comput nine and <num> , how thei're connect .
we found that thei ar below the threshold .
okai .
but if you look at <num> how thei connect with the other node .
you will find thi on is actual abov the threshold .
you grab all these into on cluster .
you , actual you can see thi on realli form a nice cluster .
then , you calcul six .
how thei connect with their direct neighbor .
so , you find actual .
none of them go over the threshold <num> .
simpli so that thi on is pretti isol .
and then if you keep go , you will find <num> actual is a hub .
and then zero , on , two , three , four , five , these six node actual form anoth cluster .
so you'll find the hub .
color and two cluster .
okai ?
thi is just execut of thi small graph .
for sever real world , pretti big graph , xiaowei xu actual did a veri thorough experi to show a lot of real world data set can gener veri interest cluster .
so the major issu becom how can we smart set up mu and epsilon ?
there's some follow up studi on to decid mu and epsilon , how to make the algorithm less sensit to the paramet .
thi algorithm is pretti effici becaus you onli look at your veri close neighbor instead of try to find , to ask to anyth from the whole graph .
so the run time actual is a big oh of number of edg .
sinc for spars network each node were not connect to too mani edg .
so that's a reason for spars network .
the complex if over the size of vertic .
if you look at the run time in the rear data set you will find that the number of vertic .
grow bigger and bigger .
actual , the fast modular , it would take much longer time to finish .
but us thi scan algorithm , actual , it's still veri effici .
now , we finish thi lectur .
in thi lectur , we discuss how to do cluster graph and network data .
especi we introduc some basic concept of graph and network cluster .
we introduc the represent of graph and network .
discuss some typic evalu measur , outlin the gener approach of graph cluster .
especi we focu on spectral cluster .
and a densiti base cluster of network .
then here i will introduc you a few interest research paper and tutori and some gener chapter of the book .
interest reader , you mai like to read those thing to have deeper understand .
the materi cover in thi lectur .
thank you .
welcom to lectur <num> .
cluster analysi in heterogen network .
we have discuss cluster analysi in graph and in homogen network .
now we come down to cluster analysi in heterogen network .
we will first introduc what a heterogen inform network .
we will then discuss a rankclu algorithm , netclu algorithm , pathsim measur , that mean pathsim similar search base on pathsim measur for heterogen network .
we will final discuss the user guid meta path select for cluster in heterogen network .
first , we're go to introduc the basic concept of heterogen inform network .
what ar heterogen inform network ?
we first see what other network .
okai .
and network actual ar the node repres an entiti or we call vertex , okai .
like a actor in a social network and then we have link or we call edg , essenti it's a tie in a social relationship between entiti .
then we can consid each note or each link mai have attribut , label and weight .
okai .
link mai also carri rich semant inform .
for network , we can categor them into homogen versu heterogen network .
homogen network ar the singl object type and singl node type , like franc and franc link with franc .
like world wide web , you can think it's a collect of link web page .
so you get similar kind of web page .
and a web page link page by point to those page , so you ar look for singl object type and singl link type .
howev , in realiti , there ar mani thing ar heterogen , multi type network .
that mean the network actual consist of multipl type object and the link .
for exampl , in a medic network , you will see patient , doctor , diseas , contact and treatment and drug and all of these actual can interconnect togeth to form a medic network .
similarli for peopl in a bibliograph network , you mai see public and thei contain author , thei publish in certain venu and other inform , other piec .
okai .
so for exampl , for dblp , which is comput scienc bibliograph databas , it contain over 2million paper entri .
thi is a typic bibliograph network .
now we take the bibliograph inform network as an exampl , if we see there , we have a paper .
thei link to venu , thei link to term thei link to author and the paper mai cite other paper .
and for thi model , we can think how much in thi network .
actual , can often be deriv from their origin heterogen network .
for exampl , coauthor network .
you can think if the differ author , thei share a same paper id .
you're think thei actual ar form coauthor , but you can think thi coauthor network actual is a project of these more complet heterogen network .
similarli , citat network , paper citat network also can be deriv from thi origin heterogen network .
howev , heterogen network carri richer inform than their correspond project homogen network .
take a coauthor network as exampl .
if you just know a and b ar coauthor a paper , but you do not know whether thei onli cross at on paper or mani paper , what paper thei ar coauthor , discuss what kind of theme , publish what kind of venu , so you lose lot of inform if you just look at coauthor network .
so anoth import thing is type heterogen network versu non type heterogen network .
if you look at the , like , a .
if you don't type them , you think everi word , distinct word is a singl type .
so you get a , you connect them togeth .
you sai , thi word is similar to the other word .
actual , it's veri hard to make good us of such inform .
howev , onc you type them , these type node and link mai impli more structur in the network and mai us such structur to get a rich knowledg discoveri .
that's why our goal is we try to mine type semi structur heterogen inform network .
from there , we will studi cluster , rank , classif , predict , similar search .
we can do a lot of a veri power thing , we again to focu in thi lectur on cluster analysi in heterogen inform network .
in thi session , we're go to introduc graph , and network , their basic concept , and their represent .
so , what is a network or graph ?
a network or graph , usual treat like synonym .
a graph g is referenc by a set of vertic , v , or also call node , for some literatur .
and a set of edg , e , or some peopl call it a set of link .
e actual is a subset of v cross product with v .
then n is the number of vertic , also call order of g .
m is the number edg , also call the size of g .
the multi edg mean if there exist more than on edg link between the two vertic , for exampl on two .
if there were more than on edg , we call thi as a multi edg graph .
okai .
and also , the loop mean edg connect a vertex to itself .
usual we call thi is a self loop .
for exampl , if you get on , you get anoth edg also connect to on directli , that's a self loop .
for simpl network , we will not consid self edg or multi edg .
then if we want a rapid and thi simpl network we'll refer as adjac matrix .
if i and j is on it simpli sai , there's an edg connect between a vertex i and a j .
okai .
then otherwis it's a zero .
okai ?
direct graph simpli sai the edg mai have direct like tail upon to head .
in that case if the adjac matrix a sub i j equal <num> , it mean there is an edg from j to i is direct .
otherwis it's a zero , if there's no such direct edg .
sometim the graph mai not just a on or zero , thei mai have certain weight associ with everi edg .
weight w sub i j , usual it's a real number , it's associ with each vertex , each edg of v i , sub i j .
then thi graph is a weight graph .
then we see how to defin the vertex degre .
suppos we get network g consist of a set of vertic and edg .
for undirect network , essenti , there is no direct like thi graph , then the degre of a vertex is defin as how mani edg from the other on go down here .
go in and out ar just the same , becaus it's undirect .
so , for exampl , okai .
we give it a obvious the number of edg contain , connect to a is four , that's why the degre is four .
similarli the degre of h is two .
then we look at direct network .
direct network , the in degre is how mani edg point to these v sub i .
okai .
for exampl , if you look at the in degre of a , you can see there's three edg point to a .
that's why in degre of a is three .
the in degre of b is two , obvious .
out degre just revers , look at how mani edg actual go out of it .
okai .
from v sub i .
then if you look at a's outgo edg , you will find onli on .
but b's outgo edg , you can find two .
so that's the basic concept of vertex and degre .
in thi session , we ar go to discuss typic evalu measur for graph cluster .
so there's some commonli us measur for graph cut .
for exampl , a mincut or minimum cut , ratio cut , normal cut , conduct modular .
some of these have been cover , actual in the lectur seven , but we will just mention them briefli here .
also , we will introduc the similar measur across network , essenti simrank will be cover in thi session .
there ar also other measur like a person pagerank .
for minimum cut , thi is the first introduc graph cut method .
okai .
the gener philosophi can be express in thi graph , as you want it cut to the minimum number of edg to partit thi graph into two or into more .
okai .
howev , you probabl can see , if you cut here , you mai break them into two graph .
but thi is not balanc at all , becaus you final left onli a singl node , which mai not be desir .
howev if you cut here , even if you cut two edg , actual you make the whole result two subgraph somewhat balanc .
so thi on is a better cut than mincut .
let's see how the mincut is defin .
it mean , given a number , k .
here the k could be two , okai ?
choos a partit c sub <num> to c sub k , which partit a network into k partit .
the measur , essenti is you look at the sum of the weight of the edg connect c sub i , thi partit and other partit .
sinc you have c sub i , you try to get everi on , essenti these link to other you count twice .
that's why you need to divid by two .
okai .
howev , we alreadi see the mincut is not a good cut in mani case , becaus you mai cut these two part is highli unbalanc .
then peopl propos other measur , try to make the partit balanc .
on propos is call a ratio cut .
the ratio cut basic is some of the edg weight link partit c sub i and other partit and you want some of the weight to be normal .
it mean everi weight divid by it size of partit c sub i .
okai .
sinc you add those partit twice , that's why divid by two .
okai .
so it's a normal , but it's far better , becaus you probabl can see , if you cut thi line to singl term , the size is onli on .
so then the whole ratio could be rather big rather than , if you cut them rel balanc , thi on could be rather big .
when you divid by thi , you make the weight thi much smaller .
so thi , that's why the , the smaller of the ratiocut is the better .
now i'm go to ask how it carri veri center idea .
the onli differ is instead of normal by the size of the cut partit , you normal by the degre of the cut partit .
that mean the total degre of cluster c sub i , okai .
or subgraph c sub of i .
okai .
so the lower the normal cut , it mean the better commun .
just becaus thei ar well connect among the commun or the cluster selv , but thei ar spars connect to the remain of the parti .
the conduct carri , actual also similar methodolog .
the differ is normal by the minimum between these two and thi on is a degre of the cluster and thi on is the degre of the other on , we just pick up the minimum on .
so thi is on conduct , but if you get all , the whole partit , you get into k cluster c sub <num> to c sub k , then the whole conduct is just the sum of everyon .
okai ?
there's anoth measur call modular .
the modular definit is somewhat differ , it's the sum of the differ of these .
okai .
the first differ you probabl can see , thi is your intern connect .
c sub i connect c sub i normal by the number of edg .
thi y actual is their degre .
their degre normal by these 2e , becaus the degre when you add them togeth , actual thi degre should be normal by 2e .
the definit of the modular is the sum of the differ from y to k like thi .
well , for everi cluster , what do you studi ?
it is the differ between the fraction of the intern edg and the fraction of those edg to be insid a random cluster , if my cluster is far better than the random nice on the qualiti is better .
that's why the optim cluster of the graph , we're maxim the modular .
the interpret is a modular of a cluster of a graph and it's the differ between the fraction of all edg fall into individu cluster and a fraction of those edg that do so if the graph vertic were randomli connect .
unfortun , all the previou and the current defin these function , the object of function , thei ar normal , but to optim them is np hard problem .
that's why you would try to get a greedi algorithm or try to get some sub optim solut , instead of get absolut optim on .
now we look at in a structur like in a graph , you get ani two note between x or y .
you want to see how to judg x , y ar similar , okai .
that essenti can base on random walk model , it base it the gener philosophi defin as by simrank is you try to look at the structur context similar .
that mean you look at similar between x and y these two node , okai .
for exampl , suppos you look at the two vertic u and v , thi similar , actual is defin by it neighbor , thi x , y .
and it mean that x is the , in neighborhood of u and y is in neighborhood of v .
of cours , if you us out neighbor , you can get a formula veri similar to thi on .
simpli sai , if you look at u and v , how similar thei ar ?
you would just a sum up the similar among those neighbor , okai ?
then you will normal them by u's in degre or v's in degre .
well , you initi them .
you can sai , for ani particular node u and v , if thei ar the same , thei ar ident , then their similar is on .
if thei're not ident , then their similar is zero .
okai .
so then you can do thi in a recurs wai .
it simpli sai , you want to look at ani particular node u and v , how similar thei ar ?
you look at how similar all those neighbor pair ar .
okai .
so that mean you can comput si <num> , we comput it base on that .
so of cours , thi comput could be rather costli , just becaus you look at the two node , their similar , you look at their neighbor's similar .
actual , there could be mani neighbor .
and then you look at the neighbor's , neighbor similar , so that chain can go recurs , can go pretti long .
so , it's costli to comput simrank measur .
howev , there ar some effici comput method , includ some approxim comput method that have been propos and studi .
we're not get too detail into thi .
