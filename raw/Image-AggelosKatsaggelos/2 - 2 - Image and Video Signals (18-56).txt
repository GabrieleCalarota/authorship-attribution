Welcome back, in this segment we defined,
1D, 2D, 3D, and multidimensional signals.
We also distinguished between scalar and
vector value signals.
Whether the equation is a colored image a
two dimensional, or
three dimensional signal, or what's an
example of a four dimensional signal.
We will show representative examples
of images with different intensity
resolutions.
For example, binary images such as a text
image, through color images,
when each pixel assumes one of 60 million
colors, and pseudo color images.
When we decide the number of colors to be
used for visualization purposes for
a color but also for a black and white
image, there's pseudo coloring problem.
We also show examples of stereo images
emulating human perception
that is variable to perceived depth since
we use two eye.
If we were to use two cameras
at some reasonable distance, the so-called
base line,
then the two images or videos recorded
by the camera emulate our human visual
system.
We discussed what the term processing, in
the title of the course means.
The narrow definition of a processing
system is one that accepts
an image at the input and generates
another image at the output.
The broader definition however, the one we
adopt in this course, is that of a
system which for an image at the input
might generate an image at the output
or a decision based on the analysis of the
image, which might result in an action
such as to operate for example in a
patient if there is a medical image.
Or the extraction or segmentation of an
object from an image
based on its color or it's motion and
maybe even its classification.
So let's look at some of these examples in
this next segment.
Depending on the number of the independent
variables, signals can be grouped,
classified into one dimensional, two
dimensional,
three dimensional or in general
multi-dimensional.
So tone, speech, audio, biomedical are one
dimensional typically a function of time.
If I look at this continues or discreet
I'm calling this s of n.
So one independent variable.
Text and grayscale images are certainly
two dimensional
signals where they're independent
variables to present space.
I can have s, x, y, for example, I can
call it.
While color, multispectral, hyperspectral
images, we'll say
a few words about them, have two
special coordinates, and then one can look
at the amplitude in two different ways.
The one is to say that the amplitude is
not
scalar anymore, but the vector three by
one vector when
it comes to colors since they have three
different channels
or a seven by one, let's say, for a
multispectral image.
Or, they can viewed as three dimensional
images with two spacial and one spectral
coordinates.
Video is a 3D signal.
It has two spacial and one temporal
coordinates,
while a 3D volume has three spacial
coordinate x, y, z.
As an example of a four dimensional signal
I can look at their volume
and x, y, z signal that changes
over time, so time is the fourth
independent variable.
Some of the tools that we use to describe
signals carry over
from 1D to 2D to MD is the straight
forward extension one
just adds one more variable, and
everything remains the same in some sense.
On the other hand, there are certain
results that hold true
for undimensional signals, but they cannot
be generalized for higher dimensional
signals.
Images and videos are clearly the focus of
this
class and the images are two dimensional
signals but
can also be three dimensional, where we
talk about multispectral,
hyperspectral images while video is a
three dimensional signal.
So we are a dimension.
So let us look now at some examples,
representative examples of, of signals.
Tones are examples of one-dimensional
signals.
On top you see a sine wave that only has
one frequency present in it, at 660 hertz.
That's why it's called a pure tone.
Let us listen to that.
[SOUND]
At the bottom, you see a square wave.
It also has the frequency at 660 hertz.
However, it has additional frequencies,
the so-called harmonic.
Let's listen to that as well.
[SOUND].
It should be clear that the bottom square
wave is richer in the sense of
having additional frequencies down the
proton the sinus wave.
Here we have another example of another
one
dimensional signal, it's a piano piece
familiar to, I
assume most of you, and this is a
synthesized
piece, and we can listen to that as well.
[MUSIC]
Here are some examples of the images.
On the left, you see a binary image, that
is I only have two bits to
represent the different colors, I have a
black and a white value here.
Binary images, text images like this one
are used in the fax encoding when
I want to transmit such text from point a
to point b through the fax machine.
In the middle, you see the again, an eight
bits per pixel
image, while on the right is a 24 bit per
pixel image.
This is a true color image.
I have two to, to the 24 different color
values.
This should be around 16 million [SOUND]
different colors to represent such an
image.
There are different ways to represent the
color image, one of which is
shown here, in terms of three different
channels, the red, green and blue
channels.
This is an RGB decomposition of the image.
So, each of these channels is a, black and
white image.
I use eight bits to represent it, so each
of the channels here has two, 256
different values and eight bits per
channel times
three, therefore 24 bits to represent the
color image.
So, if we look at this you notice the nose
of the mandrill here is quite red.
And therefore, we see that the pixel
values in the red channel are quite high.
Why it is represented by height?
Values closer to 56, while 0-55 so close
to
55 while the darker values are closer to
zero.
So, the red nose has high values in the
red
channel and pretty small values in the
other three channels.
[INAUDIBLE].
On the other hand if I look at the cheeks
of the mandrill this is the variation of
blue not exactly the blue color, so I see
high values in
the red, I'm sorry, in the blue channel
and there are the smaller values, well
definitely smaller than the red and semi
somewhere
in the middle in the green channel, right?
Actually this particular value apparently
is a combination of green and blue.
The Landsat program is responsible for the
acquisition of satellite imagery of, of
the earth.
It started in 1972, and the most recent
satellite that Landsat 8 was launched in
this year.
LANDSAT 7 data, here's eight spectral
bands with special resolutions ranging
from 15 to 60 meters, and the temporal
resolution is 16 days.
The main instrument on-board LANDSAT 7 is
the enhanced thematic marker plus, ETM
plus.
The resolution is 30 meters, except band
six,
that has 60 meter resolution, and band
eight, that has 50 meter resolution.
Band eight by the way, is the panchromatic
band.
So you have low, or high rather
special resolution, 15 meters, while we
have low spectral resolution.
By the way, this is the visible range of
light here.
So this is the blue, green, red channel,
while the rest are infrared.
LANDSAT data have helped to improve our
understanding of earth, and due to LANDSAT
today,
we have a better understanding of things
as
diverse coral reefs, tropical
deforestation and Antarctica's glaciers.
So here's an example of a LANDSAT 7 image
of the city of Amsterdam.
Up here you see the blue, green and red
channels,
and the rest of the channels for other
channels are infrared.
By the way hyperspectral images are the
ones that have many more bonds, up to 200
and 300 bonds, but the main characteristic
of
the, of the bonds are much closer spaced.
Let's compare it to the multispectral
images.
Very often in order to be able to
appreciate
the LANDSAT the images, the visible bands
are combined.
This is what you see of the left image.
This is by the way, the LANDSAT image of
the City of London.
So, these natural looking images are
familiar with the human eyes.
So, the reservoirs here close to Heathrow
are shown in
dark blue, while the city itself is shown
in gray.
So these images offer very good views
of city infrastructure, sediment and also
by symmetry.
We can tell how, how deep the water is
over there.
however, the Band 1-3 image, it's not
useful, it's difficult
to distinguish between different types of
vegetation and between clouds and snow.
So, therefore in the middle image, we
combine
the three channels, channel four, two and
three.
In other words one infrared bond and the
green
and blue channels, this is a widely used
combination, that's
especially used for studies of vegetation,
since the different
types of plants, reflect the infrared
light in different ways.
Now another combination is the seven,
four, two shown on the right.
So two infrared bands plus green, and
this combination is especially useful for
geological and
agricultural studies, because band seven
can help
discriminate between various types of rock
and mineral.
And bright green here indicates vegetation
while
the water appears dark blue or black.
Now our natural world is three dimensional
while the images are two dimensional.
They present the projection of a three D
world on to the two D plane.
In order to perceive depth, we need two
eyes.
so, we need to cameras that will capture
the same
scene, and therefore they will emulate the
human visual system.
So, here you see on the left what the left
camera sees, and on the right, what the
right camera sees.
And the difference between these two
images is the so-called disparity map.
It tells us how each and every pixel moved
from
one image, and going from one image to the
next.
Or if I use disparity map, I can map the
left image onto the right image.
The disparity map relates to the depth in,
in the image.
So the two channels infuse the property
will give us the
depth perception, and the fusion could be
done through a red
and blue channel and therefore I can use
the color coded
glasses or I could use glasses with
different polarization and so on.
Here's another example of a stereo image.
In this case this is a LANDSAT image,
actually two LANDSAT images were used
that were acquired of the same scene that
were acquired one year apart.
So the satellite does not go exactly over
the same
position necessarily and the two images
are represent two different images
from the left and right channel of the
same scene, and
therefore they can be combined to give us
the depth perception.
You can obtain depth information also
using the kinect camera.
It projects a known pattern onto the scene
and infers depth from the information from
the pattern.
So the kinect combines structural light
with two computer
vision techniques depth from focus and
depth from stereo.
The kinect is intended to use with the
Xbox and therefore it is of
interest there to compute the depth map
but also infer the body position.
So you see here the skeleton of the person
on, on the left image, right.
So here is the person, this is the depth
map,
and also the kinect provides a visible
image shown here.
So you see the visible and the
corresponding depth image that showed
where the person stands and where the door
is and so on.
[BLANK_AUDIO]
We can see here [SOUND] short video but
shows both the visible
image as well as the depth image acquired
by the kinect camera.
[BLANK_AUDIO]
Very often we're interested in capturing
the three-dimensional structure of an
object.
So, instead of using two cameras as in the
stereo case, we use many cameras on a
specific rig.
So, as you can see here, the image of
this particular object is viewed from many
different angles.
Here's an example of a video.
A video consists of individual frames, and
one could argue
that a video is nothing else than a
collection of images,
and therefore, if I have an outgoing that
is effective in
processing an image, a still frame, as
it's called.
Also, I could apply the same algorithm to
frame after frame and I will be done.
However, what is special about video is
that these frames are highly correlated
and therefore, I can gain if in processing
such frames.
I take this correlation into account.
Of course these frames, if they're
displayed at some frame rate, 30 frames
per second, for example, one can perceive
the actual motion indice.
Finally, processing in the title of the
course means the manipulation
of the values of an image or a video by
computer.
So as the resulting image is more useful
to us, or has some desirable properties.
For example the result of processing might
be the removal of blur,
as is the case here you, you see, the
input to the system
is an aerial photograph that is blurred
with the motion between the
camera and the scene and the output is a
sharpened, a restored image.
Now if an image is input, an image is
output this has been the kind of narrow
definition of processing, or so but if I
do as filtering, and the broader meaning
of processing that we adopt here is that
an image or video can be input to
a system and we're interested in
extracting important
features from such an image or we're
interested
in making decisions based on the image.
The best example here shows this is a
chest x-ray
and the input and based on the analysis
that would be
performed whether there's a malignant
tumor or not, for example,
certain decisions and actions will be the
output of the system.
As time goes on, what we see is that the
boundary is between tradition and
separate areas become fuzzy or in other
words there's overlap between these
traditionally separate areas.
So, when it comes to signal processing and
more specifically between
with the video processing that we'll be
dealing with in this class.
There is overlap between the, this field
and the
fields of communication, computer vision,
machine learning and optimization.

