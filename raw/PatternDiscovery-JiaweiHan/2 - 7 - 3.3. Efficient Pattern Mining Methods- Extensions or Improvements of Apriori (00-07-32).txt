[MUSIC]
Since the proposal of the famous
Apriori algorithm there have been
many interesting extensions or
improvements to this method.
Let's just examine some of them, 'kay.
So the first line of work is
how to reduce the number of
passes of transaction database scans.
We know that database scan involve disc
accessing, which could be quite expensive,
but if you want to find
size ten frequent itemsets,
likely you have to scan
this database ten times.
That's quite expensive to
reduce a number of scans, so
there are several publications.
One is called partitioning method,
we're going to introduce, another one
called dynamic itemset counting method.
You probably notice the first
author search brand
who was a PG skilled in
of Stanford University.
The second year, 1998,
he occurred to propose one of
the most famous algorithm page rank and
set up a Google company.
So the, another line of the work is how
to shrink the number of candidates.
You generated very large number
of candidate sets in many cases.
How to shrink the number of candidate sets
to be generated could be a big saving.
Hashing method is interesting one for
mining max patterns.
Bayardo pro, also proposed pruning
by support lower bounding.
Another interesting method is Toivonen,
proposed sampling method.
The Surlines work is to explore
some special data structures,
like some tree projection method
using H3 to do H-miner, and,
hypercube decomposition is
another interesting method.
We're going to introduce a new
tree structure called FP tree.
Let's look the first one,
partitioning method,
partitioning method guarantee you only
need to scan data twice, you know,
before you generate all the frequent
itemsets, no matter for
how many case it is, so they observed
a very interesting property core.
Any itemset potentially frequent in the
transaction database must be frequent in
at least on of its partitions,
that means suppose you get a global tr,
big database, big transaction database,
you partition them into k partitions.
If an itemset X is not
frequenting any one of
these k partitions, it cannot be frequent
in this global transaction database.
Why?
The frequency.
If you got an itemset X,
which is not frequent in the first one,
not frequent in the second one,
not even, not frequent in the case one,
so you add them together.
You add all these support together
you get a global support.
You add all these database together
you get a global database.
You pretty can see if everyone is
smaller than sigma times its size,
then, the finally,
the global one will also smaller than six
sigma times the size of global database.
That's why at least one of these
database contain must be frequent.
'Kay, so with this observation,
'kay you can
see this partition method
we need only two scans.
The first scan is you partition
database into k partition database.
How do you partition them?
Each partition you want them
to fit into the main memory.
Why?
Because no matter how you scan them,
you are not involved in any database iOS.
So that's the reason you can scan once,
you put them in the database, you can work
on it, derive k frequent items that, for
no matter how many case you can get, okay.
That means you can derive all
the local frequent itemsets, okay.
Then based on this property, you can
do the second scan like this, okay.
You can say, since any frequent
itemset in one of these patron
actions can be potentially frequent so
the globally candidate sets, is,
which is frequenting any one of them,
it will be a global candidate itemset.
Then the second scan
just can't against each
database combos counts of
the global candidate set, okay.
Then you will get their count,
you add them together, you will be able to
either derive global frequent patterns,
that's why this method guaranteed
to scan database only twice.
So this is pretty interesting.
Another method or
I'm going to introduce you called di,
directly hashing and pruning.
This method is trying
to reduce a number of
candidate sets based on hashing and
pruning techniques.
The general philosophy can be like this,
okay.
The observation is if the k-itemset is
frequent, then in this hashing bucket,
it contains several potential itemsets and
must be frequent.
So if the hash count is not frequent,
then any one of them cannot be frequent,
the general philosophy like this, okay.
Why you are going to derive
the frequent one-item set?
In the meantime, you set of a hash entry
for the potential two-item sets, but
why you said hash entry rather than count
each one because there could be many,
many distinct items.
So the frequent two itemsets before you
get the frequent one, the candidate two
could be really, really big, but if you
set a hansh, hash entry to get several
items, you share one hash entry,
the whole hash table will not be too big.
Then the interesting thing is why
do you derive frequent one itemset?
You also can cut many two itemsets,
they can not be frequent.
For example suppose our minimum support
can be 70, then this one has only 35,
this one has only 58,
that implies those items,
none of those items can be frequent,
because they share this entry.
Their support come this here,
is still below the threshold.
So, only the second one, this bd,
be, de, the support is quite big.
Pass the support threshold these could be
This may contain the real candidate sets,
okay.
That's a reading you can
reduce the number of
candidates substantially using
this hashing and pruning method.
[MUSIC]

