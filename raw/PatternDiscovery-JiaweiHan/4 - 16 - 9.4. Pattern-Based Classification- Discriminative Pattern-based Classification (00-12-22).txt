[MUSIC]
Now we, we're introduce another
interesting pattern based classification
method called Discriminative
Pattern-Based Classification.
What is Discriminative
Pattern-Based Classification?
You probably see
rule-based classification,
like work out class association rules,
like what CBAC market are quite effective.
But it, in many cases, people still
like their own classification methods.
For example,
you may like C4.5 as a typical
decision tree classification method.
Or you may like support vector
machine like SVM method.
Can pattern-based classification
help those methods?
Actually the answer is yes.
Actually we just need to take
a discriminative patterns as features
we can feed into any classifier to improve
its classification performance, okay.
The general principle is we first mine
discriminative frequent patterns
as high quality features.
Then those features can be
applied to any classifier, okay.
This framework, called PatClass
framework is, goes as follows.
Okay.
The first is we, to feature construction
by frequent itemset mining.
And then based on these constructive
features, we do feature selection.
For example, we may use
Maximal Marginal Relevance, MMR method,
which I'm not going to get into detail.
Which is in the paper, and
also in many other references.
The general philosophies,
we may use these kind of measures
to select discriminative features.
That means those features
that are relevant, but
minimally similar to the previous
selected ones, okay.
That means, suppose you already
have a set of selected features.
If those features can
cover the current feature,
then this current new feature
will now become discriminative,
because they do not have
to make new contributions.
That means you want to find minimally
similar to the previously selected ones,
you will count those features as new
discriminative features to include
them, okay.
Then you will remove those redundant or
too closely correlated features.
That means you only find high quality,
discriminative features.
Then using this feature,
we can do model learning.
That means, we can use this
feature as a major features to
feed into typical classifier
like Support Vector Machine,
C4.5, to build classification model.
Then you may wonder why these
discriminative patterns may have
certain power.
Let's look at typical example.
We do a little detail study on one
typical measure people use
to core information gain.
And we will see the pattern length, that
means when we mine frequent k-patterns,
these pattern lengths is k, it's not 1.
We will see why the pattern
length is longer,
to certain extent,
it will have have high information gain.
Actually we use feature score,
in some other measures we got a very
similar distribution as well.
Now you look at this, okay.
This red mark, red line,
actually is the single feature.
And those dots are the information
gained of the single feature, okay.
We have three datasets.
Actually, we tried many machine
learning we possibly the,
the dataset is in those depository.
Like Austral, Cleve and Sonar.
You can see, for single feature,
their information gain is
somehow scattered, okay.
But, if you make the pattern length
a little longer like 2, 3, 4.
Immediately, you can see
those longer patterns
actually carry a lot of information gain,
okay.
Even you make the pattern longer to
such as, somewhat close to less than 10,
okay, they still have reasonable quality
to contribute to information gain.
That simply says,
on those real, real data sets,
we compute their information gain,
we found the k-itemsets for
k greater than 1, but
often less or equal than 10.
Least discriminative
power information gain,
is even higher than that
of the single features.
That implies if we just use like
decision tree or something,
you look at one feature at a time,
you may lose a lot of the valuable things.
But those things can be captured by
longer frequent pattern mining, okay?
So that's one thing we study,
the discriminative pa, power.
Then another thing is we
studied the frequency power.
The power of pattern frequency, okay.
We also watch the information
gain distribution, okay.
We still take some data sets in
machine learning repository,
like Austral, Breast, and Sonar.
Okay.
The typical information gain computation
formula appears as follows.
The information gain for, under condition
X, you look at this information gain on C.
Then this is unconditional,
you get this information gain.
And this information this is the entropy,
okay.
This is the entropy of the given data,
this is entropy based
on the condition of X.
We call conditional entropy based on X.
The information gain actually is a given
data, you compute their entropy,
then you compute a conditional entropy,
and there the condition of X.
Their difference is the information gain.
Then you probably can see,
if you stick with very low support,
the information gain could be low as well.
But you increase the information,
in, increase the pattern frequency,
actually, their gain increases as well.
You probably can see this, okay.
That means we need certain frequency
to capture more information gain.
Of course, we do not need too frequent
patterns, you probably can see,
if the pattern frequency's too high,
they do not have too much value.
But on the other hand, they are more
frequent than the very low ones,
they do contain lots of power.
That simply says,
we want to capture good information gain.
We also need to explore frequent
patterns as well, okay.
Actually the further computation, if you
take the derivative on this information
gain, you do the further computation, you
will find information gains upper bound.
This monotonically increases with pattern
frequency up to certain extent, okay.
So with this result we are convinced,
both discriminative and
frequent pattern actually pretty powerful
in capture the information gain,
or capture the discriminative power, okay.
With this, we construct the pattern class.
That means that we first grab, finding
frequent and discriminative patterns,
then we feed into those typical
classifiers like SVM and C4.5.
Then what do we get?
You probably see, this is a performance,
based on a set of data included in machine
learning repository in UC Irvine, okay.
These are the set many,
many people in machine learning and
data mining use as examples, okay.
Then, we look at SVM.
If we just use single features,
no matter we use different kind.
Like, this is all things,
these is the frequent sets,
this, using RBF theater, okay.
Then, you see, they get a good, like using
SVM, you get good classification accuracy.
But, on the other hand,
if we use frequent pattern.
This is using all the pattern,
this is using more,
selected patterns, simply says.
If you use frequent and discriminative
patterns selected based on the process,
you get the very best performance among
all the different possible classifiers.
That means, for the same SVM,
if you were use good features,
use frequent discriminate patterns
features, you really get a better game.
Okay.
The same thing happens for C4.5.
Of course, if you look at
overall classification accuracy,
C4.5 may not be as competitive as SVM.
However, if you use frequent
discriminative patterns,
you still see non-trivial gain of
the classification accuracy on C4.5.
That means, frequent and
the discriminative patterns could
be pretty powerful to get a better
classification accuracy on
almost any kind of classifier.
Then we look at scalability
test on discriminant patterns.
We know, mine very, very low support
will take a long time to mine them.
Then what about if we
mine reasonably frequent?
That means higher support patterns.
We'll get less patterns, but whether we
have much less power in classification.
So you probably can see, of course,
you see minimum support is 1,
everything is frequent.
You get too many patterns,
you just cannot finish, okay.
But if you raise the minimum support
on this Chess Data to 2,000,
we still generate 68,000 patterns.
It will take 44 seconds
to mine such patterns.
Then, you get a pretty high
classification accuracy
on the Chess Data for both SVM and C4.5.
But on the other hand, okay,
even you increase minimum
support from 2,000 to 3,000,
at this time your number of
patterns decrease quite a lot,
from 68,000 to only 136.
And the mining time also reduce quite
a lot because you find much less patterns.
But even you use 136 patterns,
you can sort your classifier by SVM and
C4.5.
You probably see, their classification
accuracy is still pretty high.
That simply says,
we don't need that many patterns, but
we do need frequent, and
the discriminative high quality patterns.
For almost a similar study on
waveform data, you probably can see,
you increase minimum
support from 80 to 200.
The number patterns down
from 26,000 to 2,000.
The running time for these pattern
mining down from 176 second to 8 second,
but still the classification accuracy
only decrease a tiny little.
But simply says,
if you want mine fast, scalable,
actually this frequent discriminative
pattern is also an interesting method.
[MUSIC]

