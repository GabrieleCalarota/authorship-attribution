[SOUND] Now we examines
the first strategy,
Simultaneously Inferring Phrases and
Topics.
We know the phrases integrated was topics,
may generate some meaningful topics and
a more telling phrases.
So, we will see how we
can do this in one shot.
That means we can do simultaneously
inferring phrases and topics.
The general philosophy access shown,
in this Bigram model, proposed in 2006.
The general philosophy is, you based on
the previous word, and the current topic.
You look at, you, you know,
what are the probability,
the next word will be generated,
using this probabilistic generative model.
Then you may form bigrams,
based on this method.
One generalization of this method,
called Topic N-grams or TNG.
This one, was done, one year later, 2007.
The, it is still a probabilistic
generative model,
that generate words in textual order.
Instead of just thinking about bigrams,
this one actually creates n-grams by
concatenating successive bigrams.
So this one is a generalization
of Bigram Topic Model.
The third one is called
Phrase-Discovering LDA,
or PDLDA, done by Lindsey in 2012.
This is viewing, every sentence.
There's a time series of words.
Then for this Phrase-Discovering LDA,
it essentially, is
trying to find generative parameter,
or topics, it changes periodically.
Use every word,
access drawn based on previous m words, or
based on the previous context,
and the current phrase topic.
This key word based on surrounding,
you know, words.
And the topic,
try to see whether the, the phrase or
the new words will be drawn, will be,
you know, naturally from a nice phrase.
So all these three massers have
some high modeling complexity,
because your due models to
draw every subsequent word.
Just a look at the previous m words, and
the topic and
try to call in this generating model.
It is really made expensive.
It has high inference cost and is slow.
But also it may be over fitting.
Let's look some examples.
This slides basically show the TNG,
they experiments on the research papers
will show a few topics it generated.
One topic called Reinforcement Learning,
and for
this if you compare the three rows.
The first row done by LDA,
the second row was generated by
n-gram one when n is two or more.
The third one is in gram one
is generate only the unigram,
the only one single word.
If you look at this reinforcement
learning, optimal policy,
dynamic programming, look this is
much more meaningful and telling.
Then the unigrams, then the LDA output.
Similarly, you look at
Human Receptive System.
You will also clearly see, the n-grams,
especially when n is two or more.
It's better than the unigram,
than the LDA output.
Similarly, we can see like for
the topic, speech recognition and
support reckon machine, you look at
these n-grams, when n is two or more,
it is much more telling and meaningful,
than the unique grams or RDA output.
Okay, [LAUGH] the support of
up back machine, you'll for
people from Mineral Waste,
the field proquency it is,
much more meaningful than you just
to look at the unigram and LDA.
Okay?
So, to that extent, we say it is
good when we do topic modeling,
we promote thinking about phrase
formation, phrase generation.
Doing topic modeling, you'll generate
more informative interesting topics.
So, this is the first
strategy we are learning.
Then we will proceed to
the next two strategies.
Thank you.
[MUSIC]

