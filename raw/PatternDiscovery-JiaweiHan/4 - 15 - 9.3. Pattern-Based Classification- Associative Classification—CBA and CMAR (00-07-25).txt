[SOUND].
Now we are going to introduce
the first pattern-based
classification method called
associative classification.
In particular, we are going to
discuss two methods, one called CBA,
another called CMAR.
CBA, also called classification
based on associations,
it was developed in 1998
by Bing Liu's group.
The general philosophy is,
we first mine high-confidence,
high-support class association rules.
What is class association rules?
That means this association rule,
the right hand side must be a class
label equals to a certain class.
That means for this attribute,
there's a class label.
For example,
whether it's cancer the diagnosis.
And then, the, this is a constant, means,
whether it's cancerous, or benign, okay?
Then the left hand side is
a set of conditions and
is predicated P sub 1 to P sub l.
Okay?
Then it, is associated with confidence
measure and the support measure.
Then with this set of
class association rules,
we are going to first sort those rule,
or you can say rank the rule,
in descending order of first confidence,
then support.
Then how to do classification once
these set of rules are mined,
it can be used as a classifier.
Then when you get a test case,
you get a tuple count, you want to test
whether this one should be positive or
negative, you want to get a class label.
You first apply the first rule
that matched the test case, okay.
If the first rule, because
the high-confidence and high-support,
you will say, if this one says class A,
then it should belong to class A.
Otherwise you can apply the default rule.
And interestingly they tested many
examples in some typical data repository.
They found this very simple class
association rule based on classifier,
can be more accurate than some
traditional classification methods.
Such as C4.5, which is a rather
sophisticated interesting
classification based on tree, you know,
it's a decision tree method, okay?
Then you may wonder why?
Why these high-confidence
associations among these
attributes can be to
decision tree classifier?
The, actually, what you probably
can see is, in many cases,
the high-confident class association,
that means if this one is very high,
usually the left hand side will contain
multiple association, multiple conditions.
Or you can think, you are exploring
multiple attributes at the same time.
But it, remember, many decision tree
algorithm, you are only testing one
attribute at a time to decide which branch
you want to construct or to go down.
So this one attribute at a time,
versus multiple
attributes you want to examine
based on their high-confidence and
high-support you probably can see
this simple rule may have it's edge.
Now we're going to study another
interesting classification method called
classification based on multiple
association rules, is represented by CMAR.
CMAR actually used two improvements
in comparing with CBA.
One is rule pruning.
We actually use some kind of
classification based happy tree method.
But whether the rule should be inserted
into the tree, we first do some test.
The test is given, two rules,
R1 and R2, if the an,
antecedent of R sub 1 is more
general than that R sub 2, and
also this R sub 1's confidence is
no weaker than R 2's confidence,
simply says R1 is more general and
more powerful in classification than R2.
So there's no use of R2 anymore,
we just prune this R2 from the tree.
Another thing we can prune the rules
could be if the rule's an,
antecedent and if the class labor
are not positively correlated,
based on chi squared test of
its statistical significance.
That means if this antecedent and the
class label are not positively correlated,
then we better remove those
rule because it's misleading,
it, it cannot derive good results.
Then another important thing
is why we do classification.
We are not so simple just based
on sorted confidence and support.
We may be based on multiple rules, okay.
Just give you a simple rule, rule for
the CMAR that's if there's only one
rule that satisfies the tuple X,
that means your current test tuple X,
only one rule can match it.
Then we just assign this rule's
class label, that simple.
But in some cases where you test this x,
you may find multiple rules,
or we say a rule set,
actually matches this condition, okay.
In that sense, we look at this rule set,
when not simple just based on
high-confidence to make a a final readout,
we will divide this S into groups
according to its class label.
That means if it positive,
it will go to positive class, if negative,
it go to negative one.
Then we use weighted chi square measure
to find the strongest group of the rules,
based on the statistical correlation
of the rules within a group.
That means, we see whi,
amount these two rule sets we partitioned,
which one is the strongest one?
Then we just assign, this X,
the class label of the strongest group.
So this one is a little more
careful at decision making, so
we find it's quite effective.
Based on our test, CMAR actually improves
the model construction efficiency
because we use tree, we use root pruning.
And also,
it improves classification accuracy.
We compare with CBA and
C4.5, we show this CMAR has
some multiple performance improvements
on classification accuracy.
For more you may like
to read those papers.
[MUSIC]

