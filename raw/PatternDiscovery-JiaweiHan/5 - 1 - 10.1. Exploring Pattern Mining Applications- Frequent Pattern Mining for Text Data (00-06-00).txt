[SOUND] In this new lecture we are going
to explore Pattern Mining Applications.
Instead of discussing all
kinds of applications,
because there are so many, we will
focus only on Wang in this lecture.
There's frequent pattern mining for
text data,
especially we're going to focus on
phrase mining and topic mining measures.
We will discuss three strategies.
One strategy called simultaneously
inferring phrases and topics.
That means, while we are doing
topic model we actually doing
simultaneously working
on the phrase as well.
Strategy 2 says,
post topic modeling phrase construction.
That means we first do topic modeling,
thinking the documents
is bag of single words.
Then, after this for each topic
model we try to construct phrases.
Then, for third strategy, is we first do
phrase mining then we do topic modeling.
'Kay.
So, for these strategy,
we're going to discuss in some depths.
But for other applications,
we put some applications to Lecture 11.
Some oth,
other applications like indexing, or
some other things we already
discussed in other lectures.
Now we look at the first session, eh,
about frequent pattern mining for
text data as a general introduction.
Why we want to do fre,
frequent pattern mining is for text data,
essentially, is we want to do
phrase mining and topic modeling.
What is phrase mining?
What is topic modeling?
Okay.
Topic modeling essentially
is you look at many,
many different documents for this corpus.
You try to work out how many topics or
you can say,
focus the discussion on certain things.
There, there could be multiple topics
in this set of documents, 'kay,
for these documents, for
these topics, each topic you get
a distribution of words.
If they are single words,
this considered as unigrams.
If they are mixture of a unigram and
marketgram,
usually they call these as n-grams,
or you can say phrases.
Okay?
Then, if you just read suppose that
get machine learning as one topic,
but you'll read this,
this distribution of single words,
sometimes you may get it confused.
For example,
machine could be anything you know,
it could be hardware machine,
software machine, mechanical machine
the vector could even belong to
mathematics like linear algebra.
And, there are many,
many other things can be, you know,
considered almost for each single word.
But if we generate the phrases, for
example, learning, support vector
machines, reinforcement learning,
most people were easy to recognize this
topics actually is really machine
learning, and those phrases are very,
very representative terms
from machine learning.
So that give us, you know, less ambiguous,
is more meaningful concepts for
machine learning.
Okay.
There are many strategies for
phrase-based topic modeling.
The first strategy is we
try to generate words and
tokens in topic modeling at the same time.
Okay, this kind of measures
include bigram topic model,
topic and gram model, and
face discovering topic model.
Okay, then another strategy second to one,
called post bag-of-words model,
model inference,
visualize topics with n-grams.
That means you first
do model construction,
topic modeling, using bag-of-words model.
It simply says you treat every
single word as an individual one.
Okay?
Then you do topic model.
After that, you try to combine
those words in the same topic.
You try to visualize those topics
with phrases, with n-grams.
These measure include label topic,
TurboTopic, and KERT, those are measures.
The third strategy is, we first do, we
call prior bag-of-words model inference.
That means we first try took, to phrase
mining, constructing phrases, okay.
Then we try to impose those phrases
constraints to bag-of-words model,
essentially we do
phrase-based topic model.
This one is done in,
it's going to be published
in to some 15 ToPMine algorithm.
We are going to discuss and
compare those methods.
[MUSIC]

