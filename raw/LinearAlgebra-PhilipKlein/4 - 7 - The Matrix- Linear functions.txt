
In the examples we just saw, we assumed 
for each function that the function could 
be represented by matrix-vector 
multiplication. 
In fact, in one of the examples that's 
not true. 
So how can we tell for a given function 
whether it can be represented by 
matrix-vector multiplication? 
We'll state two algebraic properties. 
If a function can be expressed as a 
matrix vector product, then it has those 
properties. 
If a function from Fc to Fr has those 
properties, then it can be expressed as 
matrix vector multiplication. 
We're going to state it in somewhat more 
general terms, however. 
Let big V and big W be vector spaces over 
some field. 
And suppose a function whose domain is 
big V and whose co-domain is big W 
satisfies the following two properties. 
Property L1 is it for every vector v in 
big V and every scalar alpha F of alpha 
times V is the same as alpha times F of 
V. 
And property two is for every two vectors 
u and v in big V, f of u plus v equals f 
of u plus f of v. 
If a function satisfies these properties, 
we call it a linear function. 
Now, suppose we have a R by C matrix and 
suppose the function is defined by. 
Multiplication by M. 
Then F is a linear function. 
How do we know? 
Well, the domain and code domain are 
vector spaces. 
We earlier showed that M times the scalar 
vector product, Alpha B equals Alpha 
times the matrix vector product, M times 
V. 
So that proves that F satisfies property 
1. 
We also showed earlier that m times the 
vector sum u plus v equals the vector sum 
of m times u and m times v. 
And this proves that the function 
satisfies property L 2. 
So a function defined by matrix vector 
multiplication, is a linear function. 
Now that we know that the functions that 
can be represented by matrix vector 
multiplication are linear functions, 
let's go back and look at some of our 
examples. 
Our first example was stretching by two 
in the horizontal direction. 
Property l one of linear functions would 
require that s of V 1 plus v 2 equals s 
of v 1 plus s of v 2. 
Is, is that true for stretching by two in 
the horizontal direction? 
We'll illustrate that it is true, using 
an example. 
Here's a vector, v 1, and here's its 
image under s, s of v 1. 
Here's another vector, v 2. 
And here's its image, s of v 2. 
Now if we add v 1 and v 2 We get this 
vector. 
V1 + V2 and its image of S of V1 + V2 
over here. 
But this same vector is also the sum of S 
of V1 + S of V2. 
So we've shown that S of V1 + V2 Equals s 
of v1 plus s of v2. 
The second property of linear functions 
property L2 requires that for any scalar 
alpha and any vector v in the domain s of 
alpha times v is the same as alpha times 
s of v. 
Let's illustrate this property using our 
vector v one from the previous example. 
Remember, here is the image from s of v 
one. 
We multiply v one by a scaler, say 1.5, 
and this is what we get. 
This is 1.5 times v one. 
And the image of that under s is this, s 
of 1.5 times v1. 
But notice that that image is the same as 
taking the image S of v1 and scaling it 
by 1.5. 
This shows that s of 1.5 times v1 is the 
same as 1.5 times s of v1. 
This isn't a mathematical proof, but it 
illustrates how you would go about 
proving this. 
And since the function, s, satisfies 
properties l one and l two, it is a 
linear function. 
You can similarly show that rotation by 
theta degrees is a linear function. 
What about the translation function? 
We'll see that this function violates 
property l one of linear functions. 
For example, let's plug in the vector 4, 
5 for v1 and 2, -1 for v2. 
The sum of these two vectors is 6, 4. 
Translating that by adding 1, 2 gives us 
7, 6. 
On the other hand, translation of 4, 5. 
Plus the translation of 2 minus 1 is the 
vector 5, 7 plus 3, 1 which is 8, 8. 
So they don't match up. 
Property L1 is violated. 
So this function is not a linear 
function. 
In fact you can show that this function 
doesn't satisfy property l two either. 
We now show And if f is a linear function 
with domain big u and codomain big b. 
Then it maps the zero vector of big U to 
the zero vector of big B. 
Here's the proof. 
Let zero denote the zero vector of big u 
and let zero sub v denote the zero vector 
of big v. 
Now let's apply the function f to the 
zero vector. 
f of the zero vector equals f of the zero 
vector plus the zero vector, which by a 
property of linearity, equals f of the 
zero vector. 
Plus F of the 0 vector. 
Now, we just subtract one of these, from 
both sides and we get, on the left side, 
we get the 0 vector and on the right, we 
get F of the 0 vector. 
That proves the lemma. 
Now, I'll show that linear functions, 
have this nice property, I call, pushing 
linear combinations through. 
So, for a linear function f, and for any 
vectors v1 through vn, and any scalars, 
alpha 1 through alpha n, f applied to the 
linear combination, alpha 1 vn plus and 
so on, is the same as the linear 
combination, alpha 1 times f of v1 and so 
on. 
So we've pushed the linear combination 
through. 
I'll show how this works in the case 
where n equals 2. 
When we apply f to the linear combination 
alpha 1 v 1 plus alpha 2 v 2, by property 
L2, we can split up the sum. 
So that's equal to f of alpha 1 v1 plus f 
of alpha 2 v2. 
And then by Property L1, we can move the 
coefficients outside. 
So this is equal to alpha 1 times f of v1 
plus alpha2 times f of v2. 
And the proof when, and is great than 2, 
is similar. 
Here's a quick example of pushing through 
the linear combination. 
We'll take the function to be matrix 
vector multiplication, where this is the 
matrix. 
Let's verify that f applied to the linear 
combination is the linear combination of 
the values of f. 
On the left, I've derived the value of 
this directly. 
And on the right, I've derived the value 
of this directly. 
And they turn out to have the same value. 
In the examples we showed how to derive a 
matrix from a function assuming the 
function could be defined by matrix 
vector multiplication. 
Now I'm going to show that that method is 
justified. 
So, given a function from, say, R n to R 
m, we want a matrix m such that f of x 
equals M times x. 
The method involved plugging in the 
standard generators e1 through e n And 
then defining the ith column of M to be f 
applied to the ith standard generator. 
Now we'll show that this method really 
works if such a matrix M exists. 
So if there is such a matrix. 
And we just showed that F is a linear 
function. 
That means it satisfies these two 
properties. 
Take any vector V. 
Let's say these entries are alpha 1 
through alpha M. 
We can right V in terms of the standard 
generators. 
So applying the function f to v we get 
this. 
Now we use the properties of linear 
functions. 
Using properties L1 and L2 we can convert 
this to this. 
We define the matrix m so that its first 
column. 
Is f of e1, its second column is f of e2 
and so on, so we, we've replaced these by 
column 1 of m and so on. 
So what we have is a linear combination 
of the columns of n. 
And by the linear combinations defintion 
of matrix vector multiplication, That's 
the product of M times the vector v. 
The kernel of our linear function F is 
the set of vectors V, who's image is the 
Zero vector. 
It's written ker of F. 
For a function defined by matrix vector 
multiplication, F of X equals M times X. 
The kernel of F is just the null space of 
the matrix M. 
Now we'll prove what I call the one to 
one lemma. 
A linear function is one to one if and 
only if its kernel is a trivial vector 
space. 
Here's the proof. 
Let f be a linear function whose domain 
is big U and whose co-domain is big V. 
There are two directions to prove. 
First, suppose kernel of F contains some 
non 0 vector. 
That is, it's not a trivial vector space. 
Then, F of that vector is the 0 vector by 
definition of kernel. 
Now, we've seen that a linear function 
always maps the 0 vector to the 0 vector. 
So f of 0 is also the 0 vector. 
So this shows that the function is not 
one-to-one. 
Equivalently, we've shown that if a 
function is one-to-one, then its kernel 
must be trivial. 
Here's the second part. 
Suppose, on the other hand, that the 
kernel is trivial, it contains only the 
zero vector. 
And let v1 and v2 be any two vectors that 
have the same image under f, f of v1 
equals f of v2. 
Moving this to the other side, we get f 
of v1 minus f of v2 is the 0 vector. 
Then by applying a property of linearity 
f of v1 minus v2 is the 0 vector. 
And that shows that v1 minus v2 is in the 
kernel of f. 
But, we said that the kernel of f is 
trivial. 
It consists only of the 0 vector, so it 
must be that v1 minus v2 is the 0 
vector. 
Which shows that V1 equals V2. 
So that shows that this function is in 
fact one-to-one. 
Let f be the function defined by matrix 
vector multiplication f of x equals a 
times x. 
If the kernel is trivial, 
that is, if the null space of A is 
trivial, 
then there's only one way to get a 
particular output B. 
B is the image under F of only one vector 
in the domain. 
That is, there's at most one vector U 
such that A times U equals B. 
In other words, the solution set of the 
matrix vector equation a times xx equals 
b has at most one vector. 
We've seen the criterion for telling 
whether a linear function is one to one. 
How can we tell if a linear function is 
onto? 
Recall that a image of a function f is 
the set of all images of elements of the 
domain. 
That is, it's the set of all f of v where 
v belongs to the domain. 
You might know this as the range, but 
that's an ambiguous term and so, we avoid 
it. 
The image of a function f is written m of 
f. 
Now, the image of a function is a subset 
of the co-domain of that function and 
asking whether a function is onto is the 
same as asking whether its image is, in 
fact, the entire co-domain. 
Here's an example from lights out. 
To construct the matrix, who's columns 
are the button vectors, for 2 by 2 lights 
out. 
What happens when we multiply this 
matrix, by a vector, alpha 1, alpha 2, 
alpha 3, alpha 4. 
The result is, a linear combination of 
the button vectors. 
Where the coefficients are, alpha 1, 
alpha 2, alpha 3, and alpha 4. 
Now let's define a function, f, that maps 
a vector, alpha 1, alpha 2, alpha 3, 
alpha 4, to this matrix-vector product. 
And the image of that function is the set 
of all configurations that can be 
expressed as linear combinations of those 
columns. 
So, saying that the function f is onto 
means that every configuration can be 
written as a linear combination of the 
button vectors. 
That is every configuration of 2 by 2 
lights out can be solved. 
Is this true? 
Can two by two lights out be solved for 
every configuration? 
And, what about five by five lights out? 
These questions amount to asking whether 
a particular linear function is onto. 

