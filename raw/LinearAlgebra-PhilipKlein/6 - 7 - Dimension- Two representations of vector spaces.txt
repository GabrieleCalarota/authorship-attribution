Long back, we saw that there were two
important ways to represent a vector
space, as the solution set of
a homogeneous linear system and
as the span of some vectors.
Equivalently, putting it in terms of
matrices, you can represent a vector space
as the null space of a matrix or
as the row space of some other matrix.
We've seen that each
representation has it's uses.
So it's important to ask if we've
specified a line through the origin
in our three by giving
a couple of equations.
How can we find a vector
that generates the line?
If we have specified
a plane through the origin
as the set of linear combinations
of a couple of vectors.
How can we find the equation for
the plane?
The same questions can be asked
more generally about affine spaces.
How can we go from the representation of
an affine space as the solution set of
a linear system to the its representation
as the affine hull of some vectors.
Or from its representation as
the affine hull of some vectors
to its representation as
the solution set of a linear system.
If we can do conversions in a vector
space case, the homogeneous case,
we can do conversions in the affine
space case, the inhomogeneous case.
As we've seen before, we can
translate inhomogeneous problems into
homogeneous problems, solve them,
and translate the solutions back.
I'll illustrate one such conversion.
From the representation of an affine space
as the solution set of a linear system
to its representation as
the affine hull of some vectors.
We start with a linear system, Ax equal b.
Let's assume this linear system
has at least one solution.
Our goal is to find vectors whose
affine hull is the solution set
of the linear system.
I'll illustrate the process with
this example linear system.
First, find one solution
u to the linear system.
Here's our solution for
this example system.
I haven't told you how we
can find such a solution.
We'll describe a couple of
methods in the coming weeks.
Next, consider the corresponding
homogeneous system.
This is the system obtained by just
setting these right-hand sides to 0.
We know that the solution set
of the homogenous linear system
is just the null space of this matrix
A and is a vector space, big V.
Suppose we manage to find generators,
b1 through bk for big V.
In our example system,
the null space of the matrix A is
generated by the set consisting
of a single vector, b1.
We can then infer that the solution set of
the original linear system is the affine
hull of the vectors u, b1 plus u,
b2 plus u, and so on, up to bk plus u.
In our example, the solution set is
the affine hull of u and b1 plus u.
I'll go through that example again.
One solution to this matrix
vector equation is this.
Now, the null space of this matrix
is the span of the set consisting
just of the vector b1.
So in this plot, the line is
the null space of the matrix, and
you can see it's generated
by this vector b1.
The solution set to the original
system is obtained by translating
this line by adding the vector u to
each of the vectors making up the line.
That is the solution set is u plus
the span of the set consisting of b1.
That's just the line through
the two vectors u and u plus b1.
So it can also be written as
the affine hull of u and u plus b1.
So let's return to the two
representations of vector spaces.
One representation is as the solution
set to the homogeneous linear system,
a1 dot x equals 0 through
am dot x equals 0.
Or equivalently as the null space of
a matrix whose rows runs a1 through am.
The second representation is as
span of vectors b1 through bk or
equivalently as the row space of
the matrix whose rows are b1 through bk.
How do we convert between one
representation and another?
Oh, there are two,
two conversion problems.
One, from left to right,
you, takes as input
the homogeneous linear system and
outputs generators for the solution set.
The other, from right to left, takes the
generators and produces a homogeneously
linear system whose solution set
equals the span of those generators.
Let's focus on the first conversion
problem, finding generators for
the solution set of a homogenous linear
system given by a vectors a1 through am.
The solution set of this homogeneous
linear system is the set of vectors u,
such that a1 dot u equals 0, and
so on up to am dot u equals 0.
We can write the homogeneous
linear system as a matrix vector
equation where the right-hand
sides are all 0.
The matrix A has as its rows
the vectors a1 through am.
So, finding generators for
the solution set is equivalent to finding
generators for the null space of
a matrix whose rows are a1 through am.
Suppose we had an algorithm for
this problem.
Let's call it algorithm X.
The input to algorithm X
is the rows of a matrix A.
And the output is the set of generators
for the null space of that matrix.
In fact, later in the course,
we'll see several algorithms
that can play that role.
Now, if u is a vector in the null space,
that is if u has a zero dot
product with every row,
then u also has a zero dot product with
every linear combination of the rows.
This motivates a new concept,
the annihilator of a vector space.
For a vector space big V, the annihilator
of big V is the set of vectors u,
such that u has a zero dot product
with every vector in big V.
The annihilator of big V is written
as big V with an o as a superscript.
For example, the annihilator of a, the
span of vectors a1 through am is the set
of vectors having zero dot product with
every linear combination of a1 through am.
Which is just the set of vectors having
zero dot product with a1 through am.
Which is the solution set of
the homogeneous linear system,
a1 dot x equals 0 through
am dot x equals 0.
Which is, the null space of
the matrix with rows a1 through am.
So, we can interpret our mysterious
Algorithm X as an algorithm that,
given generators for a vector space big V,
outputs generators for
the annihilator of big V.
Let's see some concrete
examples of the annihilator.
First, an example over the real numbers.
Let big V be the span
of these two vectors.
Then, I claim that the annihilator
of big V is the span of
the set consisting of just 1, 0, minus 1.
First, note that 1, 0,
minus 1 has a zero dot product,
which e, each of these two vectors.
Therefore, 1, 0,
minus 1 has a zero dot product with every
vector in the span of those two vectors.
Also, any scalar multiple of 1, 0,
minus 1 has a zero dot product
with every such vector.
We've shown that every
scalar multiple of 1, 0,
minus 1 lies in the annihilator of big V.
But, perhaps the annihilator
of big V contains other
vectors than those in the span of 1,
0, minus 1, no,
every vector with a zero dot
product with all of big V,
must be a scalar multiple of 1,
0, minus 1.
So, that tells us that the annihilator
is exactly the span of 1, 0, minus 1.
Now, an example over GF(2).
Let big V be the span
of these two vectors.
Then, I claim the annihilator of big V
is the span of the set consisting just
of the vector 1, 0, 1.
Note that over GF(2), 1, 0,
1 has a zero dot product with
each of these two vectors,
so it has a zero dot product with every
vector in the span of these two vectors.
Now, what other vectors have a zero dot
product with all the generators of big V.
Well, the zero vector has a zero dot
product with those vectors, but that's it.
Those are the only two vectors, and
that shows that this is really
the annihilator of big V.
Observe something about
these two examples.
In the first case, we were working
with vectors with three entries and
the dimension of big V plus
the dimension of big V equals 3.
In the second example as well, we were
working with vectors with three entries,
and again, the dimension of big V, plus
the dimension of the annihilator of big V,
equaled 3.
Here's one more example,
using vectors with four entries.
Big V is the span of these two vectors and
the annihilator of big V is
the span of these two vectors.
In this case,
the dimension of big V plus the dimension
of the annihilator of big V equals 4.
In general, if we have vectors with N
entries, the dimension of the vector space
plus the dimension of the annihilator
of the vector space equals n.
The proof is simple given
what we already know.
Let a1 through am be generators for
big V, and
let A be the matrix whose
rows are a1 through am.
Then, the annihilator of big V is
just the null space of the matrix A.
The Rank-Nullity theorem states that the
rank of A plus the nullity of A equals n.
Well, the rank of A is
the dimension of big V, and
the nullity of A is the dimension
of the annihilator of big V.
So the theorem is true.
Now that we know a little
about the annihilator,
let's return to thinking about converting
between representations of a vector space.
We originally said, that our
Algorithm X takes as input the rows
of a matrix A, and outputs generators for
the null space of A.
But, we can equally, well,
interpret Algorithm X as an algorithm
that takes as input generators for
a vector space big V and outputs
generators for the annihilator of big V.
As we've said before,
the conversion problem,
in which we go from a homogenous
linear system to generators for
its solution set,
is solved by this Algorithm X.
But, what about the other
conversion problem?
I've interpreted the left to right
conversion as giving generators for
the vector space big V, output generators
for their annihilator of big V.
Accordingly, I'm going to interpret
the reverse conversion problem as given
generators for the annihilator of big V
come up with generators for big V itself.
Maybe we could hope for
a mysterious Algorithm Y to
solve this conversion problem.
Well, crazy idea but
what would happen if we applied
Algorithm X to the generators for
the annihilator of big V?
Well, we know what would happen.
You feed in the generators for the
annihilator of big V to Algorithm X, and
it'll find you generators for the
annihilator of the annihilator of big V.
What good will that do?
It turns out,
this is exactly what we want.
Since, as we will show,
the annihilator of the annihilator
of big V equals big V itself.
This tells us that when we
are given generators for
the annihilator of a vector space big V,
and
we want to get back generators of big
V itself, we can just use Algorithm X.
We still have to prove this theorem.
So here we go.
Theorem, the annihilator of
the annihilator of big V is big V itself.
Here's the proof.
Let a1 through am be a basis for
big V, and
let b1 through bk be a basis for
the annihilator of big V.
Since b1 is in the annihilator,
it has a zero dot product
with every vector in big V.
So in particular,
it has a zero dot product with
the basis vectors a1 through am.
Similarly, each vector bi of
the basis of the annihilator
has a zero dot product with a1 through am.
Now, since a1 has a zero dot
product with b1 through bk,
it follows that a1 has
a zero dot product with
every vector in the span of b1 through bk.
We choose b1 through bk to be a basis for
the annihilator of big V.
So we see that a1 has a zero
dot product with every vector
in the annihilator of big V.
This shows that a1 is in the annihilator
of the annihilator of big V.
Similarly, a2 is the annihilator of
the annihilator of big V, and so on.
Therefore, every vector in
the span of a1 through am is in
the annihilator of
the annihilator of big V.
Thus, the span of a1 through am
is a subspace of the annihilator
of the annihilator of big V, but
that span is just big V itself.
So big V itself is a subspace of the
annihilator, of the annihilator of big V.
We're almost there.
We've shown the big V is a subspace of
the annihilator of the annihilator.
To show it is equal by
the dimension lemma,
we only need to show that
the dimension of big V equals
the dimension of the annihilator
of the annihilator.
Suppose we are working with
vectors having n entries.
Let's apply the annihilator
dimension theorem to big V.
It tells us that
the dimension of big V plus
the dimension of
the annihilator big V equals n.
Now let's apply the annihilator dimension
theorem to the annihilator of big V.
It tells us that the dimension
of the annihilator of big V plus
the dimension of the annihilator of
the annihilator of big V equals n.
Together, these equations show that the
dimension of big V equals the dimension of
the annihilator of
the annihilator of big V.
And that completes the proof.

