
[BLANK_AUDIO]. 
Now I'll define orthogonal complement. 
Let big U be a subspace of big W. 
And for each vector b in big W, we can 
break it up into its parallel part and its 
perpendicular part. 
The parallel part is the projection b 
onto the space 
Big U. 
And the perpendicular part is the 
projection of b orthogonal to the space 
big U. 
Note that b parallel is in big U, and b 
perpendicular is orthogonal to big U. 
That is it's orthogonal to every vector 
in big U. 
So, we can take every vector b in W and 
split it up this way. 
And we'll define big V to be the set of b 
perpendicular obtained this way, taking 
all the vectors b in big W. 
And we call this set big V, the 
orthogonal complement of big U in big W. 
Now, it's easy to see that every vector 
in big V is orthogonal to every vector in 
big U. 
That's how we constructed it. 
Also every vector b in big W can be 
written as the sum of a vector in big U, 
the parallel part, plus a vector in big 
V, the perpendicular part. 
Now, this second condition that every 
vector in big W can be written as the sum 
of a vector in big U. 
Plus a vec, vector in big V suggests that 
maybe there's a there's a direct 
sum here. 
So, that raises the question is big W the 
direct sum of big U and big V. 
Well, for the direct sum to even to be 
defined it has to be the case, that big U 
and big V share only the zero vector. 
So, let's see if that's the case. 
So, consider any vector w, in both big U 
and big V. 
It must be orthogonal to itself. 
It has to be both the parallel part and 
the perpendicular part. 
So, that means the inner product of w 
with itself is zero. 
But the inner product of a vector with 
itself is the square of its norm. 
By a property of norms, if a vector has 
norm 0, it must be the 0 vector. 
So, that shows that the only vector 
common to U and V is the 0 vector. 
And so, direct sum is, in fact, defined. 
And because of this property, big W is the 
direct sum of big U and big V. 
Now remember, that, that means that the 
dimension of big U plus the dimension of 
big V equals the dimension of the inner 
product U plus V, which is W. 
So, here's an example, let big U be the 
span of these two vectors and let big V 
denote the orthogonal complement of U in 
R to the forth power. 
What vectors form a basis for big V? 
Well, by looking at these generators we 
can see that any vector in big U has it's 
first two entries the same and it's last 
two entries the same. 
So, it has the form a, a, b, b. 
So, any vector of the form c minus c, d 
minus d is orthogonal to every vector in 
big U. 
So, lets consider the space span by these 
two vectors. 
Every vector in this space has this form 
c minus c, d minus d so is orthogonal to 
every vector in big U. 
So at, so we know atleast taht the span 
of these two vectors is a sub space of 
big V, the orthogonal compliment of, of 
big U. 
Is it in fact the whole thing? 
Is it all of V? 
Well, we know that the direct sum of U 
and V, is R4, so the dimension of U, plus 
the dimension of V, is the dimension of 
R4, which is 4. 
Now, these generators for big U, they're 
linearly independent. 
So, that shows that the dimension of U is 
2. 
Plugging that in, we see therefore that 
the dimension of V is 2. 
Now, these two vectors in fact are 
linearly independent so the dimension of 
their span is 2 as well. 
So, that shows that in fact this span 
covers all of big V. 
So, it's equal to big V. 
So, we found the orthogonal complement of 
U in R4. 
So, here's a use of orthogonal 
complement. 
Our goal is to find a basis for the null 
space of a matrix. 
Now, by the dot product definition of 
matrix vector multiplication, a vector V 
is in the null space of this matrix if 
the dot product of every row with v is 
zero. 
So, that tells us that the null space of 
the matrix equals the orthogonal 
complement of the row space in R4. 
Now, these three rows are linearly 
independent so the dimension of the row 
space is three. 
We know the dimension of R4 is 4, so the 
dimension of the orthogonal complement is 
4 minus 3, which is 1. 
And now here is a vector that has a dot 
product of, of zero with each of these 
rows. 
So, this vector forms the basis for the 
orthogonal complement. 
It's definitely in the orthogonal 
complement and the space it Spans has the 
right dimension. 
So, the space it spans must in fact be 
the orthogonal complement. 
So, we found a basis for the null space 
of this matrix A. 
Let's use these ideas to find the 
intersection of two planes. 
We take this plane, the plane spanned by 
these two vectors, and this plane, the 
plane spanned by these two vectors and we 
want to compute the intersection. 
Well, the orthogonal complement in R3 of 
this first plane is the span of this 
vector. 
That shows that the first plane equals 
the set of xyz, such that this vector 
dotted with xyz is zero. 
Now, the orthogonal complement in R3 
of this second plane is the span of this 
vector. 
Therefore the second plane is the set of 
xyz such that the dot product of this 
vector with xyz is 0. 
Therefore, the intersection of these two 
sets is the set of xyz that satisfy both 
conditions. 
This dot product is 0 and this dot 
product is 0. 
Okay, so we have expressed this 
essentially as the Null-Space of a 
matrix. 
By the Row-Space Null-Space duality a 
basis for this vector space is a basis 
for the Null-Space of this matrix. 
Notice that the rows of this matrix are 
the vectors in this formulation of the 
set. 
And the Null-Space of A is the orthogonal 
complement of the Span of its rows in R3, 
which we can determine is the Span of 
this vector. 
That shows that the intersection is Span 
of the vector 1, 2 minus 2. 
How would we actually compute the 
orthogonal compliment? 
Suppose we have a basis u1 through uk. 
For the vector space, big U, and a basis 
w1 through wn for the vector space, big 
W. 
How can we compute a basis for the 
orthogonal complement of big U in big W? 
Note that here, we're assuming that big U 
is a subspace of big W. 
One way is to use orthogonalize. 
Apply it to a vlist consisting of first 
the vectors u1 through uk that form a 
basis for big U, 
followed by the vectors w1 through wn 
that form a basis for big w. 
This seems like the strange thing to do 
because after all, the vectors u1 through 
uk are already Spanned by the vectors w1 
through wn. 
The vectors u1 through uk lie in big U, 
which is a subspace of big W. 
So it's counter intuitive to include them 
in this list, but you'll see why we need 
them. 
The output of orthogonalize is a list of 
vectors I'll write as u1* through 
uk* followed by w1* through wn*. 
And, these Span the same space as the 
input vectors. 
The space spanned by the input vectors is 
exactly big W. 
Big W has dimension n we see that it has 
a basis of cardinality. 
These Span the same space they for, 
therefore they span big W, so the space 
they span also has dimension n. 
We saw earlier that the non-zero vectors 
in the list output by orthogonalize form 
a basis for the space spanned by the 
input vectors. 
So, the non-zero vectors in this output 
list are a basis for an n-dimensional 
space, therefore there must be n of them. 
The number of non-zero vectors in the 
output is exactly n. 
Now, the vectors u1 star through uk1 Span 
exactly the same space as the input 
vectors u1 to uk. 
And they are all non-zero vectors because 
u1 through uk are linearly independent. 
They form a basis for big, big U. 
So, they account for k, so u1 star 
through uk* account for k non-zero 
vectors in the output. 
So, exactly n minus k of the remaining 
output vectors, w1* through wn* 
are non-zero. 
Now, every one of those output vectors is 
orthogonal to u one through un. 
So, they're orthogonal to every vector in 
big U. 
So, they lie in the orthogonal complement 
of big U. 
Now, by the direct sum dimension Lemma, 
the dimension of the orthogonal 
complement is n minus k. 
So, those remaining non-zero vectors, 
among u, among w1* to wn* are 
exactly a basis for the orthogonal 
complement. 
[BLANK_AUDIO] 

