
Let's see how we can use this important 
theorem. 
We'll revisit the question of when a 
linear function is invertible. 
We'll prove the following theorem. 
Let f be a linear function from big V to 
big W. 
Then, f is invertible if, and only if, 
its kernel is equal to 0, and the 
dimension of the domain equals the 
dimension of the co-domain. 
I like to interpret that condition as 
saying, the domain and the co-domain are 
basically the same size 
so that they can be matched up. 
Here's the proof. 
Well, we saw before that a linear 
function f is one-to-one if the dimension 
of it's kernel is 0 and is onto, if the dimension of its 
image equals the dimension of its 
co-domain. 
So, a function that f is invertible if 
both conditions hold. 
That is, if the dimension of the kernel 
is 0, and the dimension of the image 
equals the dimension of the co-domain. 
But now, we have an additional tool, the 
kernel image theorem, which tells us that 
the dimension of the kernel of f plus the 
dimension of the image of f equals the 
dimension of the domain. 
So, using that, the dimension of the 
kernel equals 0 and the dimension of the 
image equals the dimension of the 
co-domain, if and only if the dimension 
of the kernel equals 0 and the dimension 
of the domain equals the dimension of the 
co-domain. 
And that completes the proof of the 
linear function invertibility theorem. 
Now, we'll prove an analogue of the 
Kernel-Image Theorem specifically for 
matrices. 
Apply the Kernel-Image Theorem to the 
function f of x equals A times x. 
Then, the kernel of f is just the null 
space of the matrix A, and the dimension 
of the image of f is the dimension of the 
column space of A, which is just the rank 
of A. 
Now, we define the nullity of a matrix to 
be the dimension of the Null space of A. 
Then, the Rank-Nullity Theorem states 
that for any n-column matrix A, the 
nullity of A plus the rank of A equals n. 
This is a direct translation of the 
Kernel-Image Theorem into the terminology 
of matrices. 
With what we now know, let's return to 
the checksum problem. 
Recall, we defined a checksum function 
that takes n-vectors over GF(2) and maps 
them to 64-vectors over GF(2). 
The function is defined in this way using 
dot products with 64-vectors, a1 through 
a64. 
We had an original file we represented by 
an n vector p and the transmission error 
represented by a vector e. 
So, the corrupted file is p plus e and we 
were interested in finding out the 
probability that the corrupted file had 
the same checksum as the original file. 
We saw that if the the air was chosen 
according to the uniform distribution, 
the probability that the corrupted file 
has the same, same checksum as the 
original file was 2 to the dimension of V 
over 2 to the n 
where V is the Null space of the matrix 
whose rows are these vectors, a1 through 
a64. 
Well, it's easy to choose a1 through a64 
so that the rank of this matrix a is 64. 
In fact just choosing random vectors will 
probably work. 
Now, the rank-nullity theorem tells us 
the rank of A plus the nullity of A 
equals n. 
Now the rank of A we said was 64. 
The nullity of A is the dimension of this 
vector space V, the null space of the 
matrix. 
So, that's equal to n, and that shows 
that the dimension of v is n minus 64. 
Plugging that into our formula, you get 
that the probability that the corrupted 
file has the same checksum as the 
original file is 2 to the n minus 64 
divided by 2 to the n. 
Which is 1 over 2 to the 64. 
A tiny, tiny number. 
So, there's a very tiny chance that a 
random change will go undetected by the 
checksum function. 
Now, let's use the Rank-Nullity Theorem 
to help understand when a matrix is 
invertible. 
Let A be an R by C matrix. 
Then A is invertible if and only if the 
cardinality of R equals the cardinality 
of C and the columns of A are linearly 
independent. 
That is A has to be a square matrix. 
Here's the proof. 
Define the function f, by f of x equals A 
times x. 
Then, A is an invertable matrix, if and 
only if, F is an invertible function. 
The function F is invertible, we know, if 
the dimension of its kernel is 0 
and the dimension of its domain equals 
the dimension of its co-domain. 
That is, if the nullity of the matrix A 
is 0, and the number of columns equals 
the number of rows. 
Well, the nullity of a matrix A is 0, 
space of A is 0, 
which is true if, and only if, the Null 
space of A consists just of the 0 vector. 
That is, if the only vector x, such that 
Ax is the 0 vector, is the 0 vector, 
itself. 
And that's true if, and only if the 
columns of a are linearly independent by 
the linear combinations definition of 
matrix vector multiplication. 
And that completes the proof. 
So, for example, this matrix cannot be 
invertible because its not even square. 
This matrix is square and its, and its 
columns are linearly independent. 
So, it is an invertible matrix. 
And here is a matrix that is square, but 
its columns are not linearly independent. 
You can get the third column by adding 
the first two. 
So, it's not invertible. 
Now we can show that the transpose of an 
invertible matrix is itself an invertible 
matrix. 
Here's the proof. 
Suppose A's an invertible matrix. 
We've written it in terms of its columns. 
The matrix is square, and the columns are 
linearly independent. 
Let n be the number of columns. 
Then, the rank of A equals n. 
And because A is square, it also has n 
rows. 
Here they are. 
And by the rank theorem, its rows are 
also linearly independent. 
Now, the columns of the transpose of A 
are the rows of A. 
So, they're linearly independent as well. 
And of course, the matrix is square, its 
columns are linearly independent. 
So, we can conclude that it's invertible. 
Let's see what else we can learn about 
matrix invertibility. 
Earlier, we proved that if A has an 
inverse, then the product of A with its 
inverse is an identity matrix. 
And we might be tempted to conjecture the 
the converse, that is, if B times A is an 
identity matrix, then A and B must be 
inverses. 
Well, that doesn't, that doesn't turn out 
to be true. 
But if we add an additional condition, it 
is true. 
And that condition is that the matrices 
have to be square. 
So, suppose A and B are square matrices, 
such that B times A is an identity matrix. 
Then, A and B are inverses of each other. 
To show that A is invertible, since we 
know it's already, we already know its 
square, all we have to do is show that 
its columns are linearly independent. 
Well, to show that the columns of A are 
linearly independent, let u be any vector 
such that A times u equals 0. 
That is, A times u is a linear 
combination of a columns of A. 
And we need to show that the 
coefficients, the entries of u, have to 
therefore all be 0. 
Well, using this equation, multiplying 
this equation by B on both sides, gives 
us B times A times u equals B times the 0 
vector, which, of course, is the 0 
vector. 
On the other hand, this quantity is equal 
by associativity to this quantity. 
Now, B times A is the identity matrix. 
So, B times A times the vector u is the 
identity matrix times u, which is u 
itself. 
Well, that shows that u must be equal to 
the 0 vector. 
That shows that for any linear 
combination of the columns of A that 
equals 0, the coefficients of that linear 
combination must be 0. 
And that shows the columns of A are 
linearly independent. 
So, we've been able to show then, that A 
has some inverse, it's written A to the 
power of minus 1. 
Now, we have to show that B is that 
inverse. 
Well, we know A times A inverse is an 
identity matrix. 
We know that B times A is the identity 
matrix. 
Let's multiply that on the right by A 
inverse. 
We get B times A times A inverse equals 
the identity matrix times A inverse. 
We can simplify the quantity on the 
right, the identity matrix times a 
inverse is just a inverse. 
We can use associativity on the left. 
And now, we have a times a inverse, which 
we know is the identity matrix. 
And B times the identity matrix is B. 
So, we've shown that B is equal to a 
inverse, and that completes the proof. 

