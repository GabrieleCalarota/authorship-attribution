
But now we turn to an application of 
these algebraic properties and the null 
space. 
Now error correcting codes are used in 
cell phones, flash drives, and many other 
devices. 
Hamming came up with a code that's now 
called a linear binary block code. 
Linear because its based on linear 
algebra, binary because it uses zero's 
and one's. 
And block code because it encodes blocks 
of fixed length using the error 
correcting code. 
Then here is how a linear binary block 
code works. 
Suppose I want to send a message. 
Let's say my message is 4 bits. 
I encode that as a block of, in this 
case, 7 bits. 
Then I transmit it through what's 
abstracted as a noisy channel. 
This is this is a channel that sometimes 
introduces errors during transmission. 
At the other end of the noisy channel, a 
7 bit block is received that might differ 
from the, the block that's injected into 
the channel. 
And then that's decoded hopefully to 
obtain the original 4 bits that was 
transmitted. 
In a linear code, the block that's 
actually injected into the noisy channel, 
is called a code word, and we'll denote 
it by C. 
The block that's received at the other 
end might not be a code word. 
We denote that by c tilde. 
We let big C denote the set of code words 
allowed. Now Hamming's code was a linear 
code. 
And what that means is we represent the 
4-bit and 7-bit blocks as 4-vectors and 
7-vectors over GF2. 
We can write the seven bit block that was 
received, c tilde as c plus e, e is the 
error vector. 
Since we're using GF2, e has ones 
precisely in the positions where c tilde 
differs from c. 
The key idea in a linear code, is that the 
set of code words, big c, is the null 
space of a matrix h. 
The advantage of doing this is it makes 
the receiver's job easier. 
Now the receiver gets c tilde, and needs 
to figure out what the error vector is. 
How can the receiver do that? 
The receiver multiplies c tilde by the 
matrix H. 
H times c tilde equals, substituting for 
c tilde, H times c plus E. 
And now, using one of our algebraic 
properties, we can split this sum up. 
H times c plus E is H times c plus H 
times E. 
Using the fact that the set of code words 
is precisely the null space of the matrix 
H, we know that H times c is the zero 
vector. 
So we end up with just H times E. 
So the receiver can compute from c tilde 
the product of H times E, and from that 
needs to determine the error vector E. 
How can he do that? 
In Hamming's original code, this is the 
matrix H, now you might notice something 
interesting about the columns and their 
and the order. 
Well suppose that that noisy channel 
introduces just 1 bit error then e, the 
error vector has only 1 one the rest of 
its entries are 0. 
Can we determine the position of that 
bit, from H times e? 
Let's say for example that e has a 1 in 
its third position. 
So e looks like this. 
H times e is then the linear, using the 
linear combinations rule. 
H times e is then the third column of H, 
which is 0, 1, 1. 
And that tells us where the error occurs. 
As long as e has, at most, one bit error. 
The position of that bit can be 
determined from the product H times e. 
This shows that the Hamming Code allows 
the receiver to correct one bit errors. 

