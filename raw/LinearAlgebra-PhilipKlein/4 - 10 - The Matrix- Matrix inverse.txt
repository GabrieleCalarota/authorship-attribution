Now we've seen that matrices A and
B correspond to functions.
And the product corresponds
to another function.
If the functions that
result from these matrices.
Are functional inverses of each other.
We say the matrices are matrix
inverses of each other.
So here's an example.
This is a,
an elementary row addition matrix.
The function, applied to a vector,
with entries x1, x2, x3.
Adds twice the first
entry X1 to the second.
Therefore the functional
inverse subtracts the,
twice the first entry from the second.
So here's the functional inverse.
That tells us that the inverse
of the matrix A is this matrix
which is also an elementary
row-addition matrix.
But this time subtracts twice
the first entry from the second entry.
If A and B are matrix inverses of
each other, we say that A and B.
Are invertible matrices.
And it's not hard to show that each
matrix has at most one inverse.
And we refer to the inverse of
a matrix A as A to the power minus 1.
So you'd read this A inverse.
Why do we care about invertible matrices?
Here's one reason.
A matrix vector equation is guaranteed to
have exactly one solution if the matrix
is invertible, let's see that.
Let A be an m by m matrix, and define
the function f by f of x equals A times x.
Now suppose A is an invertible matrix.
Then f is an invertible function.
That is f is one-to-one and onto.
Since it's onto, for any vector b in
the co-domain of f, there is some vector
u in the domain of f such that
the image of u under f is b.
That is, there is at least one solution
to the matrix vector equation,
A times x equals B.
Now since f is one-to-one, for
any vector b in the co-domain of f,
there is at most one vector u in
the domain that maps to b under f.
That is, there's at most one solution.
To the matrix vector equation,
a times x equals b.
So, if a is invertible, then, for
every right hand side vector b.
For every vector b in
the co-domain of this function f.
The matrix vector equation,
ax equals b, has exactly one solution.
Here's an example the industrial
espionage problem.
Given the vector b, specifying
the amount of each resource consumed
our goal was to figure out how much of
each product the factory was producing.
So we set this up as
a matrix vector equation.
If we solve the vector matrix equation,
for
some right hand side B,
giving the amounts of resources consumed,
are we guaranteed to get the true number
of products produced by the factory?
Well, is there a unique solution?
If there are multiple solutions,
our industrial espionage will fail.
We can't be certain that we found
the true number of products produced.
But it turns out that M transpose,
here, is an invertible matrix.
So, the function is an invertible
function, and there's a unique solution.
For every vector b that's the case.
The sensor node problem
gives us another example.
Remember that we're trying
to find a current draw for
each of four hardware component and
let's say we use three test periods.
We record the total power consumed in
each of those test periods in a vector b.
And for each test period,
we have a vector specifying for
that test period what portion of
the time each hardware component is on.
Now, to find the vector
giving the current draws for
each component, we want to solve a matrix
vector equation, Ax equals b, where.
A is a matrix whose rows
are these duration vectors.
Does this yield the current draw for
each hardware component?
It turns out in this case that
this matrix is not invertible.
In particular the function that maps
x to a times x is not one-to-one.
So there's no guarantee that there's
only one solution to the equation.
And therefore by using solve to get
a solution to this we might not
get the right solution, we might not get
the solution giving the current draw for
each of the hardware components.
We have to add more test periods.
Let's use four test periods instead.
In this case.
The matrix consisting of the corresponding
duration vectors is an invertible matrix.
And so, there's only one solution to
the matrix vector equation Ax equals b.
And therefore, in principle at least,
we'll find the correct current draw for
each of the hardware components.
Another reason for
considering invertible matrices is that
the matrix vector equation Ax equals
B is somewhat easier to solve in
the case when A's an invertible matrix.
Later we'll learn two algorithms,
to solve such equations.
But we'll also learn how to cope in the
case when A is not an invertible matrix.
And finally, invertible matrices play
a key role in an idea called change of
basis which we'll see later.
Change of basis is a very
important part of linear algebra.
And we'll see it being used in
rendering an image with perspective.
Or in actually synthesizing a perspective.
Free image from an image with perspective.
>> Here is a simple but useful
proposition about invertible matrices.
If A and B are invertible matrices and
their product A times B is defined,
then it too is an invertible matrix.
We'll see some examples.
Here are two matrices, A and B.
Each corresponds via matrix vector
multiplication to a function from r2
to r2.
The matrix A corresponds to
the function that adds the second
entry to the first entry.
And this function f is clearly invertible.
Because the function that
subtracts the second entry from
the first entry is its inverse.
The matrix B corresponds to the function
that adds the first entry to
the second entry.
This function is also clearly invertible.
Since the functions f and
g are invertible,
their composition is also invertible.
And by the matrix multiplication Lemma
their composition corresponds to
the matrix product A times B.
Which is this matrix, so
that matrix is also invertible.
In the second example,
the two matrices are slightly bigger.
But like the previous two matrices, these
are elementary row addition matrices.
Multiplication of mat, vector by this
matrix A adds four the times the first
element to the second element.
And This function is clearly invertible
because the function that subtracts four
times the first element,
from the second element is its inverse.
Now multiplication by the matrix B
adds five times the first element to
the third element.
And that function is similarly invertible.
Again, by the matrix multiplication Lemma,
multiplication by the matrix A times B
corresponds to application of
the composition of the two functions.
So the composition is also
an invertible function.
So the product A times B
is an invertible matrix.
One more example.
Here are two matrices A and B.
[SOUND] Here's the matrix product,
it's not an invertible matrix.
So the proposition implies
that at least of one of A and
B is not an invertible matrix and in
fact that first one a is not invertible.
For the proof, we just follow
the same line of reasoning we used in
the first two examples.
Given the matrices A and B, let f and
g be the corresponding functions, the ones
defined by matrix vector multiplication.
Now suppose A and
B are both invertible matrices.
Then the functions f and
g are invertible functions.
I'm assuming the matrix product A times
B is defined, the functions f and
g can be composed.
And since f and
g are both invertible functions
the composition is also invertible.
So, by the matrix multiplication Lemma.
The corresponding matrix is A times B and
that's an invertible matrix.
>> Now here's a nice observation
about matrix inverses.
If a matrix A has an inverse then
the product of A with its inverse.
Is the identity matrix.
Here's the proof.
Let B denote the inverse of A.
As usual,
we define the corresponding functions,
the function f which takes x to A times x.
And the function g which
takes y to B times y.
By the matrix multiplication Lemma
the composition f composed with g
has the property that g
composed with g applied to
x is the same as taking the product
of AD and multiplying it by x.
On the other hand.
Because we've said that A and
B are matrix inverses of each other,
it must be that f composed with
g is the identity function.
So, the product AB must
be the identity matrix.
What about the converse?
We've said that if A has an,
an inverse, then A times A inverse,
is the identity matrix.
You might be tempted to conjecture, that
if AB is an identity matrix, then A and B.
Are inverses of each other.
But here's a counterexample.
A matrix A and a matrix B.
Their product is an identity matrix.
But in fact the functions corresponding
to A and B are not invertible.
In fact, we can see here that multiplying.
A by this vector, and
multiplying it, A by this vector,
both give you the zero vector.
So, the function corresponding to matrix,
matrix multiplication is not even onto.
The null space of A is not trivial,
and we know that that implies
that the function is not onto.
So, f is not an invertible function.
So, it doesn't the matrix
A doesn't even have an inverse.
So, just because the product
of two matrices is
the identity matrix doesn't mean these
matrices are inverses of each other.
So, it's a false conjecture.
However, we can show.
That matrices A and
B are inverses of each other if and
only if both the product AB and
the product BA are identity matrices.
Here's the proof.
Suppose A and
B are inverses of each other.
By the lemma A times B
is an identity matrix.
Applying the Lemma with
the roles reversed tells us
that BA is also an identity matrix.
On the other hand, suppose AB and
BA are both identity matrices.
We define the functions in the usual way.
f is the function that
maps y to A times y.
And g is the function
that maps x to B times x.
Because AB is the identity matrix, by
the matrix, matrix multiplication Lemma,
f composed with g is
the identity function.
Because, the product BA
is an identity matrix,
it follows using the same Lemma that g
composed with f is an identity function.
This proves that f and
g are inverses of each other,
functional inverses of each other.
And therefore A and
B are matrix inverses of each other.
How can we tell if a matrix is invertible?
Well.
A matrix is invertible if
the corresponding function is
an invertible function.
And we know that an invertible, a function
is invertible if it's one-to-one and onto.
Now, we have a criterion for
linear functions being one-to-one.
And the function corresponding to matrix
vector multiplication we've seen is always
a linear function.
So, by the one to one Lemma
the function is one-to-one if and
only if the kernel is
trivial which is the same as saying
the null space of the matrix is trivial.
What about onto?
We're not yet
in a position to characterize
linear functions that are onto.
So we can't quite answer this question.
But if we did know how whether
a linear function is onto,
we could get a nice criterion.
For whether a matrix is invertible.
[SOUND]

