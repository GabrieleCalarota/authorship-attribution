
Building on our orthogonalized procedure, 
we're going to develop the QR 
factorization. 
We're going to be able to show that 
certain matrices can be written as the 
product of matrices in special form. 
Now in general matrix factorizations are 
an important part of computation in 
linear algebra. 
They're important mathematically, because 
they provide insight into the structure 
of matrices. 
And they're important computationally, 
they're the basis for several algorithms 
in linear algebra. 
Now notice that a matrix whose columns 
are mutually orthogonal has a special 
property. 
If you take the transpose of, of this 
matrix and multiply it by the matrix 
itself. 
Then you get a matrix whose only non-zero 
elements are the diagonal elements, and 
those diagonal elements are the square 
norms of the columns. 
Why is that? 
Well let's use one of our definitions of 
matrix-matrix multiplication. 
The entry of the product in the first row 
in the first column is the dot product of 
the first row of the first matrix. 
Times the first column of the second 
matrix and so on. 
The off diagonal elements are all zero, 
because these vectors are mutually 
orthogonal. 
Now we can change things around, so that 
instead of getting this diagonal matrix 
we get the identity matrix by normalizing 
the columns. 
What does normalizing mean? 
Normalizing a vector, means scaling it, 
multiplying it by a scaler, so as to make 
its norm one. 
And here's the definition of normalize. 
It just takes a vector and divides it by 
its norm, which is the square root, of 
its inner product with itself. 
So, for example the vector with entries 
1, 1, 1, once it's normalized, its norm 
squared is 1, and the entries are 1 over 
root 3. 
Now, if we take these columns and we 
normalize them, renaming q1 through qn. 
Then we take the transpose of that matrix 
multiple it by the matrix because the 
norms of the columns are all 1. 
The diagonal elements of the product are 
also, all 1. 
So we see that if the columns of a matrix 
Q are mutually orthogonal, and they all 
have norm 1 then Q transposed times Q is 
the identity matrix. 
Well we say that vectors that are 
mutually orthogonal are orthonormal. 
And if a matrix Q has orthonormal 
columns, we call Q a column orthogonal 
matrix. 
Now, I know it should be called column 
orthonormal, but that's the way it goes. 
And if Q is a square matrix, and column 
orthogonal 
we call Q an orthogonal matrix. 
It really should be called an orthonormal 
matrix, but this is the convention. 
And we see that if Q is an orthogonal 
matrix its inverse is its transpose. 
We'll learn a little bit more about these 
orthonormal vectors. 
Suppose q1 through qn are orthonormal 
vectors. 
The projection of a vector b onto one of 
these vectors, say, qj, b parallel is a 
scalar multiple sigma j times qj. 
And the scaler sigma j is the ratio of 
the inner product of qj with b by the 
inner product of qj with itself. 
The inner project of qj with itself is 1, 
because these vectors all have norm 1. 
So it's just the inner product of qj with 
b. 
So the vector of these scalars, the 
vector sigma one through sigma n can be 
written using the dot product definition 
of matrix-vector multiplication. 
As sigma 1 through sigma n which is the 
dot product of q1 with b through the 
dot product of qn with b. 
That's the product of this matrix times 
the vector b. 
This matrix has as its rows, the vectors 
q1 through qn. 
And the sum of these projections is the 
linear combination sigma 1 times q1 plus 
up to sigma n q n. 
Can be written as this matrix, whose 
columns are the vectors q1 through qn, 
times sigma 1 through sigma n. 
Now, we'll start to develop procedure for 
producing the QR factorization. 
Remember, that the result of 
orthogonalize is this relationship. 
Here's the matrix whose columns are the 
original vectors supplied to 
orthogonalize. 
Here's the starred versions, and they're 
related by matrix multiplication with 
this upper triangular matrix. 
Now, these starred vectors are mutually 
orthogonal, but they're not orthonormal. 
So, let's assume that the input vectors 
linearly independent. 
In that case, the star vectors are all 
non-zero. 
So we can normalize them. 
We'll normalize these vectors, replacing 
them with their normalized versions and 
we'll we'll call the normalized versions 
q1 through qn. 
Now to maintain this matrix relationship, 
because we're normalizing these, that 
means dividing each of these by it's 
norm, we have to multiply the corresponding 
rows of this matrix by the norms of this 
vectors. 
And so we get a slightly different 
triangular matrix. 
Instead of 1's along the diagonal, it 
has the norms of the vectors v1* 
through vn star. 
And the other entries are modified as 
well. 
This is called the QR factorization of 
the matrix formed by the original vector 
v1 to vn. 
In this case the matrix whose columns are 
q1 to qn is called Q. 
And the upper triangular matrix is called 
R. 

