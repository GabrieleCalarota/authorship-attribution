
Given a point b and a plane, we want to 
find the point in the plane that's 
closest to b. 
So here's plane and a point. 
And we want to find the point of the 
plane that's closest to this point. 
Now, by translating the plane and the 
point, we can ensure that the plane 
contains the origin, in which case, it's 
a vector space. 
It has a basis, let v1, v2 be the basis 
for this vector space. 
Then our goal becomes, given a point b, 
and vectors v1 and v2 find the point in 
the span of v1 and v2 that's closest to 
b. 
Here's the point. For example suppose v1 
and v2 are these 2 vector and our point b 
is this. 
Then, the point in the plane closet to b 
is, 6 minus 3,0. 
So we want to come up with an algorithm 
that will solve problems of this form. 
More generally, given a vector b and 
vectors v1 through vn. 
We want the algorithm to find the vector 
in span of v1 through vn that's closest 
to b. 
Now as a special case, we could use such 
an algorithm to decide whether b lies in 
the span of these vectors v1 through vn. 
After all, if, if the vector in span of 
v1 through vn closest to b is b itself, 
that means that b is in the span. 
If not, then b is not in the span. 
So let A be the, the matrix who's columns 
are v1 through vn. 
And using the linear combinations 
interpretation of matrix vector 
multiplication, a vector in the span of v1 
through vn can be written as A times x. 
Thus testing if b is in the span of v1 
through vn, is equivalent to testing 
whether the matrix-vector equation, Ax 
equals b, has a solution. 
More generally, if Ax equals b possibly 
doesn't have a solution, we can use the 
algorithm to find a point in the set, 
it's closest to b. 
Now, so far we've just discussed finding 
the point itself, but we also want to be 
able to find out how to express it as a 
linear combination of v1 through vn. 
That is, we want to find the vector x 
that solves this equation, or that 
minimizes the distance between Ax and b. 
So as I'm saying it's not enough to find 
the point in the span of these vectors 
that's closest to b. 
We want to find the representation of 
that point in terms of those vectors. 
So, our goal is to find the coefficients 
x1 through xn such that the linear 
combination x1 v1 and so on 
is the vector in the span in these 
vectors closest to b. 
I can write that as minimizing the norm 
of the difference between the vector b 
and the product of this matrix with this 
vector. 
And that's equivalent to minimizing not 
the norm but the square of the norm. 
And we can rewrite this matrix, not in 
terms of its columns, but in terms of its 
rows. 
We've written it here, where the rows are 
the vectors a1 through am. 
And this problem is equivalent to 
minimizing this sum of square 
differences. 
And this is the problem that we addressed 
in the with gradient descent. 
All right, so, finding the vector x that 
minimizes this norm squareed. 
Which is equivalent to finding a vector x 
that minimizes this sum of square 
differences. 
This is called least squares., it goes 
back to Legendre. 
Sometimes attributed to Gauss. 
An equivalent way of formulating the 
question is, given a matrix-vector 
equation Ax=b. 
In case that equation doesn't have a 
solution, find the best solution 
available. 
Find that solution that minimizes the 
error where the error is, the difference 
between the target vector b and the vector Ax that you found. 
You want to make that error have as small 
a norm as possible. 
Now, there is an algorithm based on 
Gaussian elimination. 
But, we're going to develop a, an 
algorithm based on the notion of 
orthogonality. 
And we'll see that you can get solutions 
much,much faster than say, gradient 
descent. 

