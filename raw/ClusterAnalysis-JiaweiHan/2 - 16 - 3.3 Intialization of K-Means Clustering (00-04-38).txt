[MUSIC]
>> As we just discussed,
the quality of K-Means clustering,
is quite sensitive to
the initialization status.
Then we need to see how to do
good in the initialization, so
we find a good quality clustering,
using K-Means method.
I just give you a simple
example you probably can see.
Different initializations may generate
rather different clustering results.
Okay.
Some may not be optimal at all.
Just give an example, suppose we have only
four points we want to find two clusters.
If we initialize in these solid
circle that means these two
points getting to one cluster.
Like if we get to the center like this
two they will attract these two and
these two, in one cluster.
Then actually it becomes stabilized
because you calculate the center.
The center is still here,
it will not move.
So then you find a pretty ugly
cluster because you precede these.
If you vertically group things into two
clusters, the sum of the square distance,
or SSE actually become rather small.
From this point of view,
we know the quality of clustering
could be very sensitive
to the initialization.
Originally, MacQueen, in 1967, said we
should select the k seeds randomly.
Then we can, we need to run the algorithm
multiple times, using different seeds.
In some software package, they may even
say run these algorithm like 200 times.
Finally, you find out which one you derive
the best K-Means cluster simply says you
want to find a SSE,
the sum of a square error, is minimized.
There are many other methods also
proposed for better initialization.
For example, there is one called
K-Means++, proposed in 2007.
Essentially this ++
initialization is as follows.
The first centroid,
you can select it randomly.
But then,
the next centroid to be selected,
you try to find the farthest point
from the currently selected points.
Okay, that means a selection then you
based on weighted probability score.
You base on different probability, you
select which is the best for the next one.
Then this selection continues until
the k centroids are obtained.
That's initialization, it's done.
The we can run the K-Means algorithm.
Now I'll give you a simple
example you probably can see,
poor initialization may lead
to some poor clustering.
For example, we take the same data sets,
those black points as we've shown before.
Now we give you two centroids, initialized
as the red ones here, the blue ones here.
Okay, with this initialization,
we can get no like clusters,
we can assign these objects into two
different clusters in different colors.
Okay, one the upper part,
those points assigned to the red cluster,
the lower part assigned
to the blue cluster.
Then we can recalc the center again.
You probably can see the center actually moved.
And with this recalculation
we can reassign those points.
You can run this a few rounds,
you probably can see.
Finally, they stable like this,
but actually this is not
a very good clustering at all.
Okay?
So that means that this run of the K-Means
generates some poor quality clustering.
But simply giving us a simple shell,
we do need some smart initialization.
Multiple random initialization,
then we select the best ones.
Okay?
So this is the initialization of K-Means
clustering.
[MUSIC]

