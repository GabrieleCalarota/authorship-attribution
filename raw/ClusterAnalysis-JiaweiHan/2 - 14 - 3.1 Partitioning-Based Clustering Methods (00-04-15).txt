Hi, welcome to Lecture 3,
Partitioning-Based Clustering Methods.
In this lecture,
we're going to introduce concepts and
most popular methods of
partitioning-based clustering.
The first is we want to introduce
what are the partitioning algorithms.
Then we will introduce one of the most
popular clustering methods called
K-Means clustering.
Then we will discuss how to
initialize K-Means clustering.
As alternatives of K-Means measures,
we are going to introduce
K-Medoids Clustering Method, K-Medians,
and K-Modes Clustering Methods.
Finally, we are going to introduce
the Kernel K-Means Clustering Method.
So first we will introduce Basic Concepts
of Partitioning Algorithms.
The partitioning method, essentially
to discover the groupings in the data.
That means you get a, like K-groups, if
you want to partition them into K-groups
by optimizing a specific object function
figuring per sum of the square distance.
And then we iteratively improve
the quality of such partitioning.
So, the K-partitioning method is,
with partitioning,
dataset D of n objects
into a set of K clusters.
Then we definitely can
iteratively improve it so
that an object function is optimized.
For example,
the object function could be the sum
of the squared distance is minimized,
where as c sub k is the centroid or
medoid of cluster C sub k.
So a typical objective function is sum of
squared errors is often written as SSE.
So a clustering, okay, for
the SSE is sum of k clusters.
K is from one to k.
Then, for each such cluster,
if an object i is in this cluster,
then the square error.
This simply says the distance
of sum of the squares.
These such distance, the whole function,
is a k clusters and each cluster get its,
points to its center the sum of such
square, distance of square errors.
We want to minimize this
objective function.
Then in general what we can think
conceptual we can think of this.
If you get a very nice K-cluster,
then all the objects to the center
is somehow shorter, okay?
If it is shorter,
then the sum of the square distance or
square errors, actually
will be pretty small.
That's why we can get within
the cluster center the sum of
the square errors is pretty small.
Then if we get a k such seen, we add them
together, the whole thing will be smaller.
For the K-partitioning method,
essentially the problem is given
a k, the number of clusters,
We want to find a partition
of these K-clusters that
optimizes the chosen
partitioning criteria.
For example, the sum of square distance.
However, to find that global optimal,
actually we need to exhaustively
enumerate all partitioning.
Because the potential number of
partitioning actually is exponential.
So more realistically we
should get heuristic measures,
that means that greedy algorithms.
For example, the typically used K-Means,
K-Medians, and
K-Medoids and all the other methods
are all greedy algorithms or
heuristic methods to find
such a nice partition
[MUSIC]

