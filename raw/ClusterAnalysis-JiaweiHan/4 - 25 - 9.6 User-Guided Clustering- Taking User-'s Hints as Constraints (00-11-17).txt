[NOISE] In this last
session of this lecture,
we will discuss user guided clustering.
Or, we can say taking user's
hints as constraints.
User guided clustering is a very
special kind of constraint, but
it is also very useful.
For example,
we look at relational database.
Suppose you got the students,
you have relational database
represent the student's name,
office and position they link to advice,
link to professors,
to research groups, and
link to their courses and publications.
So for this typical relational database,
we may want to do clustering
across multiple relations
because the student have a lot of
features storing other relations.
However, user linking to so
many different relations when you want
to cluster students, you may be puzzled.
What is the purpose for
clustering those students?
Every user may give you the best hints,
for example,
when user want to organize a party
they may say, oh I want to study
likely the student, you know, the
nationality or their permanent address.
We want to you know, to look at which
field they should find advisors.
You'll probably want to study users,
group and areas.
When you want to give a student awards,
you make look at their grades.
Okay.
So, the user usually have the best way to
tell you the purpose of the clustering.
Then, suppose you want to cluster
students based on their research areas,
then the students can be linked to many
relations you have, based on users hints.
That means it's easy for user to
provide some easy hints rather than to
give you a good training set, or
give you very clear constraints.
Okay.
So, that extent the user hints
can be used as a user guidance,
we based on this user guidance
we can do effective clustering.
This kind of user-guided clustering
is different from classification.
Just look at this picture.
Suppose this is research groups area.
Okay.
The user hint says this is important.
Then, you probably don't want
to take this as a class label.
The reason is clear,
because you paint three colors,
you may want to get 20 clusters
because this is, suppose you,
you say this is ai,
this is data information systems.
Actually you can further partition them
into this is natural language processing
as a group.
And then machine learning is a group.
Then some search and
reasoning could be another group.
Okay?
So, but anyway, this user hints give you
some good guidance to see where to search.
That means you may want to use additional
features even cross to other relations.
Try to find related things you want
to group them to the desired number
of clusters.
Now, however,
this user-guided clustering is also
different from semi-supervised clustering.
Because semi-supervised
clustering usually say,
oh these two students must
be in the same group.
These two students cannot
be linked together,
that means they may belong
to a different clusters.
The first thing is you can only get expert
for a very small number of students.
Okay, another thing is you still do not
know the real purpose of clustering.
But user guided clustering is
quite useful because you just say
the sphere is quite important.
And then user, it's easy to specify and
it is not like this cannot link or
must link these kind of constraints.
In the meantime it is,
you will be able to search
other more relevant features to make
a good decision in clustering, okay.
That means there's as much
information in other relations,
needed to judge instead of just taking
this just say cannot link or must link.
And also the user usually is not able
to specify a good set of constraints,
like a must link or cannot link things,
but it's easy to provide hints.
Then let's look at it in the real life.
When user provide the hints you have
to go over many different relations
to search the related useful features.
That means we should search,
start from user specified
feature like this research area.
Then you search the neighborhood
the connected relation to
find a pertinent features.
And then we can expand
the search gradually.
Okay?
And how to expand search?
You can start from this field.
You look at the Tuple ID.
The ID actually it's like a foreign key or
key relationship.
You can link into the other relations and
then you can,
you can search further and further.
Okay?
Then only those interested relations,
their further link needs to be searched.
Okay, so this is usually the propagation
you can go through via join pass.
So, based on this philosophy,
it will work out a cross class algorithm.
That means we'll use user guided
clustering across multiple relations.
The first is we try to base on user hints.
We can find some similarity
from other relations.
They are co-related, or
they actually informative
to find that different pertinent
features across multiple relations.
Okay?
That usually we can use a heuristic method
to search for pertinent features.
The start from user-specified feature
gradually expand the search range
and we can use tuple ID propagation
to create feature values.
That means we grab to, to the other
relations, grab a most pertinent features.
And then we can explore clustering
algorithms to find nice clusters, for
example, we can use k-means, k-medoids,
hierarchical clustering methods.
How to find features
across multiple relations,
usually is you follow those join paths,
okay.
Then you try to find a particular fields
to see whether they are co-related
with the things you want to cluster, okay.
So if they are strongly correlated
with the student, you want to cluster.
So this is an interesting feature
that you want to include them.
For example, okay, suppose the features
are the research groups then you find
the students who are joined in
certain group actually quite
a correlated to the number of courses
taken in different research areas.
Then you may find that this feature and
the student taking courses in the research
area these two things are quite correlated
so you want to link them together
you know, thinking this is
important feature for clustering.
And searching for pertinent features may
finally get a very interesting ones.
For example,
when you want to see the research areas,
you may find the research groups area,
conferences of papers, advisers.
These are very useful
thing to do clustering.
And when you want to organize party
you may see the permanent address,
nationality could be quite a related.
When you look at it for awards,
look at it for academic performance,
your [INAUDIBLE] GPA, GI score in
number of papers are more related.
So the features will convey
same aspect of information,
usually can cluster tuples
in a more similar way,
like a research group area verses
conferences of publications.
Then given user-specified feature,
we'll be able to find pertinent features
by computing the feature similarity.
Then for clustering with
multiple relational features,
we were based on the pertinent features.
Then we were compute the similarity
using this kind of formula.
We can weigh a feature based on,
whether how close,
how similar with other pertinent
features in the feature space.
We may use different clustering
algorithm like a CLARANS,
K-means, agglomerative,
hierarchical clustering.
Actually the experiments compared with
other like subspace clustering algorithm,
PROCLUS, or
ILP clustering algorithms like RDBC.
Then we can see based on the research
paper, if you like to read,
you probably see no matter you
use which clustering method,
CrossClus will lead to
better clustering accuracy,
just because you, you use user's guidance,
the user will have the right view on what
kind of things may form nice clusters.
So, these thing this clustering
accuracy actually must validate that,
using the ground truth.
It means using,
given labels on the real datasets.
Finally, we'll summarize this lecture.
In this lecture we discussed
Constraint-based clustering.
We first argue why constraint-based
clustering could be important.
We look at a different
categories of constraints.
We, then we study constraint-based
clustering methods like how to handle
hard constraints,
how to handle soft constraints.
How to handle constraints
on distance measures.
Finally we discuss a special kind of
constraints called user-guided clustering,
that means we take users
hints as constraints.
We believe this is very
useful in practice.
So, finally,
I will list a bunch of research papers.
And some are the books on
constraint based clustering.
Some are the chapters on semi-supervised
clustering or constraint base clustering.
But some are the real original
research papers working
constraint based clustering algorithms.
[MUSIC]

