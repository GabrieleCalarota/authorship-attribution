[SOUND]
In this session we will briefly outline
the methods for
clustering high-dimensional data.
So, the methods for
classifying high dimensional data
can be grouped into two categories.
One category called subspace clustering.
That means instead of finding clusters
including all the high dimensional space,
we actually want to search for
clusters only existing in some subspaces.
For example, Clique, we are already
introduced as a grade based approach,
it is also a subspace clustering approach.
Other approaches including pro-class,
including by-clustering
approach wagon to introduce.
Then the second category called
dimensionality reduction approach.
This approach essentially is we really
consider all the dimensions but
we want to construct a much
lower dimensional space by
some kind of transformation
a combination of many dimensions.
Then we search for
clusters in the corresponding
transformed low damage in space.
For example, spectral clustering,
non-negative matrix factorization and
various dimensionality reduction
vassilates you'll see in the literature.
On the other hand, clustering shouldn't
not only consider dimensions,
but also consider features or
we call attributes.
Okay.
There are feature selection methods.
That means,
you try to find a particular subspace.
The data may have nice clusters,
may have, very nice features.
Another approach called feature
transformation, that means,
if you find many,
many dimensions irrelevant,
the current features may not be in
the right spot based on the data.
You try to find for
certain transformation you may be
able to reduce number of damages.
For example like a principal component and
assets, or single value decomposition,
these measures.
If the feature highly correlated or
redundant these methods
can do transformation to
reduce the number of
dimensions into a small set.
[MUSIC]

