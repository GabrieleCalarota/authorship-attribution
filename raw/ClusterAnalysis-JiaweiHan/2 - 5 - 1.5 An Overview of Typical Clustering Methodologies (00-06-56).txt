[SOUND].
Hi.
In this session we're going to
provide a brief overview of typical
clustering methodologies.
There are many clustering methods.
They can be roughly categorized into
a few clustering methodologies.
The first methods are called
distance-based methods, the, essentially,
there are two kinds of method,
one called partitioning algorithms,
the other we call hierarchical algorithms.
Partitioning algorithms
essentially is partitioning the,
the data in the high dimension
space into multiple clusters.
The typical methods includes K-Means,
K-Medoids, K-Medians.
Those methods we are going to introduce.
For hierarchical algorithms,
essentially we can either doing
agglomerative or we call bottom up.
Merging many, many points into clusters.
Men, merging small cluster into bigger
ones Then from hierarchical clusters.
Or we can use dividing measures.
You start with a single big all
the data is encased in one cluster.
Then we try to split them,
divide them using the top-down splitting
into smaller and smaller clusters.
Then the second methodology
called density-based methods,
the third one called grid-based methods,
of course they have some linkages as well.
Densit- based measure essentially is,
we can think that data space
may be at a higher level granularity,
may think about a,
a with certain grade or
certain structures or certain neighbors,
we may find certain density.
For the dense small regions we can
merge them into bigger regions.
In this way we will be
able to find the clusters,
those are dense regions
with arbitrary shape.
The grid-base method,
essentially is partitioning the data
space into grid like structures.
Each grid has got a summary
of the characteristics of
the data in the lower grid or lower cells.
Then the third method is called
probabilistic and generative models.
This means we can model data
from a generative process.
We can assume underneath there's certain
distribution, for example,
a mixture of Gaussian, okay.
Then with the data we can think
the current points are generated
from these underlying generative models,
underlying, you know,
generative mechanisms.
Then we can model parameters,
using expectation maximization algorithm,
we are going to introduce. Based
on the available data sets,
we may try to find
the maximum likelihood fit.
Based on this fit we will be able to
estimate the generative probability
of the underlying data points.
And based on this, we may find the models.
Then, in many cases data may be
sitting in a high dimensional space.
We may need to study high
dimensional cluster methods.
One influential method called
subspace clustering.
You essentially try to find
the clusters of various subspaces.
So that method can be categorized
into bottom-up methods,
top-down high-dimensional clustering
methods or correlation based methods.
We will also introduce some interesting
methods like vertical clustering.
Then, for high-dimensional clustering,
there are many methods developed for
dimensionality reduction.
Essentially, we can think the,
the high dimension
is in a vertical form they
are contained in lots of columns.
And for those columns when we perform
clustering, essentially, the columns
are clustered, the rows or columns can be
clustered together, we call co-clustering.
There are several typical measure.
Typical measures.
One is,
probabilistic latent semantic indexing.
Later, people develop another measure
called Latent Division, Allocation.
So, PLSI, or LDA are typical topic
modeling method for text data.
Essentially we can think the text can
be clustered into multiple topics.
Each topic is a cluster.
And each topic is associated
with a set of words.
Or you can think of their dimensions and
also the set of documents you can
think they are rows simultaneously.
Then the second popularly study in method
called non-negative matrix factorization,
NMF.
This is a kind of co-clustering
method you can think is,
you get a non-negative
matrix because the word,
like a word frequency
documents are non-negative.
Okay, they are zero or, or more, but
they are non-negative,
you know, values in the matrix.
for this non-negative matrix,
we can approximately factorize
it into two non-negative lower
rank matrices like U and
V that will reduce
the number of dimensions.
Another very interesting method we're going
to study is called spectral clustering.
That means we use a spectrum of the
similarity matrix of the data to perform
dimensionality reduction, the higher
dimension reduced into the lower,
fewer dimensions.
Then we can perform clustering
in fewer dimensions.
That's spectral clustering methods.
In this course, we're going to study many,
low dimension and
high dimension clustering methods and
study their different variations and
applications as well.
[MUSIC]

