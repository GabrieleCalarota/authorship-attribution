[MUSIC]
Welcome to the last
lecture of this course.
Lecture 12, Advanced Topics and
Applications.
In this lecture, we are going
to discuss some advanced topics,
including clustering data streams.
We will first introduce
a k-median-based approach,
then we will introduce
the clustream approach.
After that, we will briefly list some
advanced themes of cluster analysis.
Then we'll reintroduce some
applications of cluster analysis.
We're discussing name disambiguation
using cluster analysis method.
And we also discuss how to my evolution
of heterogeneous networks using
cluster analysis methods.
After that, we will briefly
introduce some exploration of broad
applications of cluster analysis,
that will conclude our course.
First, we will discuss clustering data
streams, a k-median-based approach.
What are data streams, why they are so
important we want to discuss it?
Actually, data streams are very
popular in current big data age.
Because there are lots of sensors or
video cameras and many other things.
They actually record data,
an online data stream version that means
the data were flowing into your
video camera and flowing out.
Now, you may not register everything,
okay?
Especially in those observation systems.
The major feature is they are continuous,
ordered, keep changing.
They come and go very fast but
in huge volume.
With such fast data streams,
a very important requirement for
the algorithms is the single-scan.
That means you are not
going to have time or
chance to store all the data and
keep indexing and
also try to access in a random way.
You basically get a scan,
you want to reply them in real time.
So, here is a typical stream
processing system architecture.
So, we are facing multiple data streams
flowing in, like those operation systems.
Then this system where take those
multiple data streams where
processes are in some scratch space like
main memory or some disks, as well, okay?
Then in the meantime, users users or
application programs usually pose query
in the form like a continuous query.
In the sense, you have a request,
alarming me when sort institution happens.
Then the stream processing system taking
this mark for online data streams.
We're trying to return
the results to check
the core return results in the real-time.
Then the problem becomes, how can we
perform cluster analysis effectively
in such fast changing data streams?
One proposed approach for stream
clustering is a k-median based approach.
This method was published in ICDE 2002,
done by O'Callaghan and
several other researchers.
This method is based on the k-median
method, that means the data stream points
coming, they are in the form of metric
space so you can easily compute medians.
The goal is to find k-clusters in
the data stream such that the sum
of the distances from the data points
to their closest centers is minimized.
So, the whole thing is similar to
what the requirement of
typical clustering algorithms.
Then, because it's a data stream,
so usually this method,
actually designed the way like
constant specter approximation value.
It uses a readme small memory space.
It's a simple two-step algorithm,
that means for each set of M record,
like a s sub i.
You are not going to register all those M
records because it's flowing in very fast,
potentially infinite.
Okay, but we try to find o k centers in
the stream, s sub 1 to s sub l.
This o k means usually you want
to find the k centers, but
you want to store multiple k's.
It's a local clustering,
that means you want to assign each point
in s sub i to its closest sector.
Let s prime be the centers for
this currently,
with each center weighted by
the number of points assigned to it.
Simply says these points,
you only register points but
you also register weight.
Because this point actually could
be summarized by many
underlying data stream points.
Then we take this s sub 1 to s sub l,
these centers.
Then we will try to use this s prime to
do further clustering to find k centers.
So, we can draw a hierarchical
clustering tree,
we can illustrate the method, okay?
The method is we see many,
many data points but
we do not have capacity to keep
storing all the data points.
What we want to maintain is at level i, we
will maintain at most m, level i medians.
That means, we only maintain
the median points and also the weights.
That means,
how many points really aggregated?
Finally, you only register medians.
Then if we see m such medians coming,
we will generate one more level.
This level is essentially the level
i plus 1 is still case multiple
but the weight is equal to the sum
of the weights of those medians.
That means, if you look at a level
i plus 1 this median point,
the weights essentially
means how many points
that it really represents is the sum
of weights of level i medians.
And level i medians is the sum of the
weights of the, how many real data points
actually summarized and
represented by this meeting point.
So, it works but
the problem could be the quality may
suffer for evolving data streams.
Because when the data streams keep coming,
if you only register
the m level i medians,
in many cases it may not be sufficient.
And you do not know whether,
what you register are all very low
points are just they're recent points.
Then you'll provide the limited
functionality for discovering and
exploring clusters over different
portions of the stream over time.
So that's the major thing,
we probably need a more robust and
more sensitive clustering method.
[MUSIC]

