In this session, we are going to
discuss typical evaluation measures for
graph clustering.
So there's some commonly used measure for
graph cutting.
For example, a mincut or
minimum cut, ratio cut,
normalized cut, conductance modularity.
Some of these have been covered,
actually in the lecture seven,
but we will just mention
them briefly here.
Also, we will introduce the similarity
measure across networks,
essentially SimRank will
be covered in this session.
There are also other measures
like a personalized Pagerank.
For minimum cut, this is the first
introduced graph cutting method.
Okay.
The general philosophy can be expressed
in this graph, as you want it cut to
the minimum number of edges to partition
this graph into two or into more.
Okay.
However, you probably can see, If you cut
here, you may break them into two graphs.
But this is not balanced at all,
because you finally left only a single
node, which may not be desirable.
However if you cut here,
even if you cut two edges,
actually you make the whole resulting
two subgraphs somewhat balanced.
So this one is a better cut than mincut.
Let's see how the mincut is defined.
It means, given a number, k.
Here the k could be two, okay?
Choose a partition C sub 1 to C sub k,
which partition a network
into k partitions.
The measure, essentially is you
look at the sum of the weights of
the edges connecting C sub i,
this partition and other partitions.
Since you have C sub i,
you try to get every one,
essentially these linking to
others you counted twice.
That's why you need to divide by two.
Okay.
However, we already see the mincut
is not a good cut in many cases,
because you may cut these two
parts is highly unbalanced.
Then people propose other measures,
try to make the partition balanced.
One proposal is called a ratio cut.
The ratio cut basically is some of the
edge weights linking partition C sub i and
other partitions and you want some
of the weights to be normalized.
It means every weight divide by
its size of partition C sub i.
Okay.
Since you add those partitions twice,
that's why divided by two.
Okay.
So it's a normalization, but it's far
better, because you probably can see,
if you cut this line to single term,
the size is only one.
So then the whole ratio could
be rather big rather than,
if you cut them relatively balanced,
this one could be rather big.
When you divide by this,
you make the weights this much smaller.
So this, that's why the,
the smaller of the RatioCut is the better.
Now I'm going to ask how it
carries very center ideas.
The only difference is instead of
normalized by the size of the cutting
partition, you normalize by
the degree of the cutting partition.
That means the total degree
of cluster C sub i, okay.
Or subgraph C sub of i.
Okay.
So the lower the normalized cut,
it means the better community.
Just because they are well connected among
the community or the clusters selves,
but they are sparsely connected
to the remaining of the party.
The conductance carry,
actually also similar methodologies.
The difference is normalized by
the minimum between these two and
this one is a degree of the cluster and
this one is the degree
of the others ones,
we just pick up the minimum one.
So this is one conductance,
but if you get all,
the whole partition, you get into
k clusters C sub 1 to C sub k,
then the whole conductance
is just the sum of everyone.
Okay?
There's another measure called modularity.
The modularity definition
is somewhat different,
it's the sum of the difference of these.
Okay.
The first difference you probably can see,
this is your internal connections.
C sub i connective C sub i
normalized by the number of edges.
This y actually is their degree.
Their degree normalized by these 2e,
because the degree when you add them
together, actually this degree
should be normalized by 2e.
The definition of
the modularity is the sum of
the differences from y to k like this.
Well, for every cluster,
what do you study?
It is the difference between
the fraction of the internal edges and
the fraction of those edges to
be inside a random cluster,
if my clustering is far better than
the random nice one the quality is better.
That's why the optimal clustering of
the graph, we're maximize the modularity.
The interpretation is a modularity
of a clustering of a graph and
it's the difference between
the fraction of all edges falling into
individual clusters and
a fraction of those edges that do so
If the graph vertices
were randomly connected.
Unfortunately, all the previous and
the currently defined these functions,
the object of functions,
they are normalized, but
to optimize them is NP-hard problem.
That's why you would try to
get a greedy algorithm or
try to get some sub-optimal solutions,
instead of getting absolute optimal one.
Now we look at in
a structure like in a graph,
you get any two notes between X or Y.
You want to see how to judge X,
Y are similar, okay.
That essentially can base on random walk
model, it base it the general philosophy
defined as by SimRank is you try to look
at the structural-context similarity.
That means you look at similarity
between X and Y these two nodes, okay.
For example, suppose you look
at the two vertices u and v,
this similarity, actually is defined
by its neighbors, this x, y.
And it means that x is the,
in neighborhood of u and
y is in neighborhood of v.
Of course, if you use out neighbor,
you can get a formula
very similar to this one.
Simply says, if you look at u and
v, how similar they are?
You would just a sum up the similarity
among those neighbors, okay?
Then you will normalize them by
u's in degree or v's in degree.
Well, you initialize them.
You can say, for any particular node u and
v, if they are the same,
they are identical,
then their similarity is one.
If they're not identical,
then their similarity is zero.
Okay.
So then you can do this
in a recursive way.
It simply says, you want to look
at any particular node u and v,
how similar they are?
You look at how similar all
those neighbor pairs are.
Okay.
So that means you can compute Si+1,
we compute it based on that.
So of course,
this computation could be rather costly,
just because you look at the two node,
their similarity,
you look at their neighbor's similarity.
Actually, there could be many neighbors.
And then you look at the neighbor's,
neighbors similarity,
so that chain can go recursively,
can go pretty long.
So, it's costly to
compute SimRank measure.
However, there are some efficient
computation methods, including some
approximate computation methods that
have been proposed and studied.
We're not get too detailed into this.
[MUSIC]

