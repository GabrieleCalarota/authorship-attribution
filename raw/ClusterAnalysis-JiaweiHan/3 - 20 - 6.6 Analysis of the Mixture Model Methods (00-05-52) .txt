[SOUND]
This is the last session of this lecture.
So we will study the analysis
of the mixture model methods.
First, we want to discuss
the relationship between k-means and EM.
K-means, actually can be considered
as a special case of EM.
In the sense, k-means is a hard-EM.
That means if you look at these
two conditional probability,
actually the probability of x sub j, this
object belongs to the cluster C sub i.
It's either one or zero.
That means, if this object is,
is closest is closest to this center,
then it belongs to this cluster.
Okay.
So the assignment is crisp,
is hard is one.
Otherwise, it does not belong to this
cluster, the probability is zero.
Similarly for the probability of
cluster C sub i under
the condition of x sub j,
if x sub j is in this cluster,
then the probability is one.
That means,
if this x of j is closest to this center,
that is 100% belonged to this cluster.
Otherwise, it's zero.
To that extent, we say,
k-means can be viewed as a hard-EM.
In the E-step, in the expectation step,
we take the local minimum
instead of a distribution.
That means, we assign this cluster
this object to 100% to one cluster,
as long as it's closest to
the center of this cluster.
And the Gaussian Mixture Model is the soft
version of k-means, because we calculate
the distribution instead of the most
likely one in the E-step and
then use weighted sum to compute
the new centers in the M-step.
Okay.
To that extent, Gaussian Mixture Model
introduced the variance to learning,
but the clusters in the k-means
have the same variance.
Now we look at the other issues,
the initialization and
speed-up of expectation-maximization.
And we already know hard and
soft clustering assignment.
K-means is hard assignment clustering and
probabilistic clustering is soft
assignment points to cluster.
Then if we compare with k-means,
the EM algorithm for
Gaussian Mixture Model will take many
more iterations to reach convergence.
Because every time you soft assignment,
this assignment is pretty gradual is
pretty progressive, but
take many more iterations.
Then if we want to speed up
the convergence of Gaussian Mix Model,
we can get a good initialization.
The typical method could be we can first,
run the k-means algorithm,
because it's fast.
Okay.
Then we can choose the means and
the covariances of the clusters
based on the result of k-means.
Then we will get the fraction
of data points assigned to their
respective clusters to
initialize the mean,
the covariance matrix and
the probability, the prior probability.
Sometimes, a Gaussian component.
A Gaussian cluster may collapse
to a particular data point.
To that extent,
it would cause a singularity problem.
Then when we detected a Gaussian
component to this cluster is collapsing,
we can reset its mean and covariance,
then continue with the optimization.
Now we summarize the mixture model,
their strengths and weaknesses.
The strengths is a mixture
models are more general
than the partitioning methods or
fuzzy clustering methods.
The clusters can be characterized
by a small number of parameters.
The result may satisfy the statistical
assumption of the generative models.
However, the weakness of the mixture model
is it may converge to local optimal.
This is similar to k-means.
To overcome this problem
similar to k-means,
we can run the mixer model multiple
times with random initialization.
Also, for mixer model,
like EM for Gaussian mixer model,
though computationally
it is quite expensive.
If the number distribution is large or
the dataset contains very
few observed data points.
That also means it,
it takes a larger dataset to run it.
And the finer problem is it is hard
to estimate the number of clusters.
This is similar to k-means.
Actually all these studies,
there are some follow up research.
They tried to propose different
measures to overcome such difficulties.
[MUSIC]

