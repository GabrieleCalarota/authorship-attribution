[SOUND] In this session,
we're going to introduce
the second kinds of subspace clustering
methods called correlation based methods.
Essentially for
subspace clustering methods,
the similarity measure can be based
on distance or density, either.
But for correlation-based methods,
it is essentially based on
advanced correlation models.
For example, PCA,
what we call principal component analysis.
This approach, you probably can
see from the picture on the right.
You see, suppose we have two dimensions,
x1 and x2.
Those points projected into both x1 and
x2 is certain distribution.
However, if we get another axis e,
we probably can see,
because all the points line up,
all cluster around axis e.
So, if we use axis e and
it's perpendicular
axis, we probably will see essentially,
using a e axis,
we will be able to represent the data.
So that means we want to find a projection
that captures the largest
amount of variation in data.
Then we can apply PCA to derive a set
of new, uncorrelated dimensions.
That essentially we get a two-dimensional
data predictor onto one dimension.
We did a dimensionality reduction.
Then we can find clusters in
the new space or its subspaces,
where we find dimension in the lower
dimensional space, finding clusters.
Other correlation-based methods
including Hough transform or
fractal dimensions,
we are not going to introduce these.
Interested people can read Wiki or
some textbooks.
Here I give a little simple
illustration of principal component
analysis in a little detail.
Suppose we have big N data points,
data vectors from small n-dimensions.
We want find a k orthogonal vectors,
that means principal components.
Usually the k is much smaller and n.
And then, if these principal components
can be best used to represent
dat,a then this transformation
will be very meaningful.
Because we can find in a much lower
dimension, we can find clusters.
So the first step,
is we can normalize the input data.
That means each attribute will
falls within the same range.
Then, we are going to compute orthogonal,
or
we call linearly uncorrelated, or
you can say perpendicular unit vectors.
That means we want to find
these k principal components.
Now each input vector,
essentially it's a linear combination
of these k principal component vectors.
Then we can sort these
principal components
based on the decreasing significance
order, or strength order.
That means, the stronger one
will be ordered higher than
the lower significance one.
Then we can eliminate the weak components.
That means, those with very low variance,
we can eliminate them.
That means we used the strongest principal
components to wrap in the datasets.
It is possible to construct a good
approximation of the original data.
Then we can find a clusters just on
those strongest principal components.
This is principal component and assets.
Of course, the detail how to compute it
we're not going to introduce here because
it would involve like computing covariance
matrix, compute their eigenvectors.
It's a little too mathematical.
If you want to know more,
there are lots of textbooks you can read.
[MUSIC]

