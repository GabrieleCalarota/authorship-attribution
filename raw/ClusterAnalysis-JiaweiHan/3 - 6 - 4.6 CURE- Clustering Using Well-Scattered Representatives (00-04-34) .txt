[SOUND]
Hi.
In this session, we're going to
introduce an interesting hierarchical
clustering algorithmic extension,
called CHAMELEON, which is a graph
partitioning on the KNN, means
k-nearest neighbor graph of the data.
So this graph partitioning
based approach was
developed in 1999 by a group of
researchers in University of Minnesota.
CHAMELEON actually has
some interesting points.
It measures the similarity
based on a dynamic model.
Essentially, two clusters are merged
only if the interconnectivity and
the closeness between the two
clusters are high relative
to their internal interconnectivity
of the clusters and
a closeness of items within the clusters.
CHAMELEON is a graph-based
two-phase algorithm.
In the first phase,
it uses a graph-partitioning algorithm.
Essentially, it cluster
objects into a large number
of relatively small sub-clusters,
also we call graphlets.
Then in the second phase,
it uses an agglomerative hierarchical
clustering algorithm to find
the genuine clusters by repeatedly
combining these sub-clusters or
these graphlets.
Let's look at the overall
framework of CHAMELEON.
Suppose we have a large dataset,
then we can
use K-NN Graph to merge them
into sparse graph, like this.
What is K-NN Graph?
Essentially we can say that two points,
p and q are connected.
If q is among the top k
closest neighbor of p, and
based on this, k-nearest neighbor,
you can circuit this graph.
Then we will partition this
graph into a good number
of small subclusters, or
called graphlets, 'kay?
After this partitioning,
we are going to merge,
it, them into high-quality clusters.
How we merge those graphlets
back to the bigger clusters,
essentially the merge is
based on these two measures.
One we call relative interconnectivity,
another we call relative closeness.
Relative interconnectivity means
the connectivity of c sub 1 and
c sub 2 over their respective
internal connectivity, 'kay.
Then the relative closeness is defined as
closeness between clusters c sub 1 and
c sub 2 over their respective
internal closeness.
Let's look at the method in some detail,
'kay.
The first is KNN graph.
What is KNN graph?
Let's look at the example, 'kay.
This is the original,
two-dimensional datasets, so the first and
nearest neighbor, 1-nearest neighbor
graph, is for each point, each,
you know, object, you just find
the top one closest neighbor.
For example,
this one's closest neighbor could be here.
This one's closest neighbor could be here.
Then you construct
a sub-graph based on these,
you get a 1 KNN,
you get a 1 1-nearest neighbor graph.
For 2-nearest neighbor graph, is you,
for each point, you look there two,
top 2-nearest neighbor in
constructing graph like this.
3-nearest neighbor graph is for each point
there look at their top three nearest
neighbors, and
you construct the graph like this.
'Kay, then we look at the absolute
interconnectivity between C sub i and
C sub j, okay?
These absolute in, interconnectivity
is essentially the sum of the weight of
the edges that connects vertices
in C sub i to vertices in C sub j.
Essentially, you look at the we,
weighted sum of these edges.
Then the internal connectivity
of your cluster, C sub i,
is essentially size of
its min-cut bisector.
The weighted sum of these edges
partitioned roughly equal parts.
Then you look at the weighted sum
of the edges of these partition.
Then the relative interconnectivity, RI,
essentially is for C sub i and
C sub j, is defined by their ab,
absolute interconnectivity
divided by we can say normalized
lines by the average of their
respective interconnectivity.
Then we look at the relative closeness and
the merge of subclusters.
Relative closeness between
a pair of clusters C sub i and
C sub j is defined as follows.
The first is what is absolute
closeness between C sub i and C sub j.
The absolute closeness essentially is
average weight of the edges connected
these, the vertices this,
between these two classes.
Then their internal essentially,
the internal closeness,
like this internal closeness of C sub i,
or
internal closeness of C sub j,
is the average weights of the edges that,
that belong to the min-cut bisector of
cluster C sub i and C sub j respectively.
That means you look at what is
min-cut bisector to cut this
cluster or cutting this cluster,
then you'll find eh,
there average weights of the edges, okay.
Then you probably can see
where relativeness is a,
is the absolute closeness,
normalized with their improportionality
defined internal closeness.
Then how do we merge subclusters?
Essentially is we were only merging
those pairs of clusters whose
relative interconnectivity,
and relative closeness,
are both above some
user-specified threshold.
That means we only merge those
maximizing the function that
combines relative interconnectivity and
the relative closeness.
That means even we find those pairs
unmergeable we still want to find
these merged,
we're maximizing the function.
Then, based on the implementation and
experiments we probably
can see CHAMELEON actually can cluster
complex objects like this, 'kay.
This actually ik, these are all
the different points with different
density eh, different shapes as
well as lots of noise points, 'kay.
Even with different shapes, different
density, and different shapes and
different noises, we still can see
CHAMELEON actually generated pretty
high-quality clusters, okay.
That simply says CHAMELEON,
using this graph partitioning, and
the merging methods is pretty robust and
generate a pretty high-quality clusters.
Even, it is in the spirit
of hierarchical clustering,
it does do a lot of good with
this graph based method.
[MUSIC]

