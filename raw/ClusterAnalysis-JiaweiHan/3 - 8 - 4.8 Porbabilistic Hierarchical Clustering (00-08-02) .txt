[MUSIC]
In the last session of this lecture,
we are going to introduce
probabilistic hierarchical clustering.
What is probabilistic
hierarchical clustering?
Let's first examine the problems of
algorithmic hierarchical clustering.
The first problem for
algorithmic hierarchical clustering,
it is, it is nontrivial to
choose a good distance measure.
For example,
we already see the single link,
the complete link,
the average link and the sensual link.
They're all different measures, but
it's hard to choose a good distance
measure based on the application problems.
The second problem is it is hard to handle
missing attribute values, because one,
the attribute value is missing,
it may impact on the quality of classroom.
[INAUDIBLE] is the optimization call
of algorithmic hierarchical clustering.
It's not so clear,
because it's a heuristic algorithm.
It more relies on local search.
Now there's another approach called card
probabilistic hierarchical clustering.
This method,
essentially uses probabilistic
models to measure distance
between clusters.
There is largely a generative model,
means it regards
the set of data objects to
be clustered as a sample of
the underlying data generation
mechanism to be analyzed.
Then you finally, find the parameter,
find out how they generate this data set.
It is easy to understand,
it has the same efficiency as
algorithmic agglomerative
clustering method.
And also,
it can handle partially observed data.
In practice, we can assume the generative
model adopt common distribution functions.
For example it, we can assume
is a Gaussian distribution or
Bernoulli distribution and
governed by a set of parameters.
Let's look at the generative model.
In the generative model, we can assume
given a set of one-dimensional points X.
We can assume they are generated by
a Gaussian distribution, like this.
Okay.
That means,
you essentially using different mu and
sigma you may generate the distribution
according to the typical
normal distribution or
Gaussian distributing formula.
Okay.
Then the probability of
a single point x sub i,
which is one point in the,
the set X is generated by the model.
Essentially, it's what's
the probability to generate x sub i
under the condition of mu and
sigma is based on this formula.
Then for the likelihood this
whole set of X generated,
essentially is the likelihood for
uh,uh, for
the normal distribution to
generate this set of X points.
Essentially is the probability
of a set of points
X under the condition of mu and
sigma squared.
For i from one to n, all these
probability multiplying together,
we get the probability that
generated the dataset X.
Then the task of learning
this generative model,
essentially to try to find
the parameters mu and sigma.
So the for this particular distribution,
we try to find the maximum
likelihood this set of
X is generated according to this,
to parameters.
So the Gaussian distribution
usually I think most people know,
for example, for bean machine
if you drop the ball with pins
if you use many, many balls finally,
you will find the distribution
is Gaussian distribution.
Then if we have many points,
we can assume these points actually
generated by the Gaussian
distribution like this.
Actually, for one dimension Gaussian,
this different mu, different mean and
different variance, you probably can see
they generate different distribution.
For two dimension Gaussian,
it, it may generate in the,
in the 3D space you could clearly see the,
the distribution of these 2D Gaussian.
Then a probabilistic hierarchical
clustering algorithm,
essentially is we suppose
we already got partition
the whole set of points into m
clusters C sub 1 to C sub m.
Then the quality of the class rank can
be measured using this quality function,
that's the product of
all those probabilities.
Then if we want to merge two clusters,
C sub j1 and C sub j2,
then the change in quality
of the overall clustering
is the word criteria.
Okay.
So you probably can see,
this is the merged cluster.
You generate a new cluster and
these original two cluster disappear
from the original set of clusters.
So that's the new clustering.
That's the original clustering,
because you merge this,
essentially the sum of the square
arrows will be increasing, so
that's the reason you try to
minimize such increasing.
That's a probability of PC sub i,
now you get a probability of
these two merged together.
Okay.
Then you have to minus the original
probability i from 1 to n.
Then the distance between cluster C1 and
C2 is this is the probability of C1 and
C2 together as one cluster,
these are the independent cluster.
If this distance is less than zero,
we should merge these two clusters.
So that's the idea or
the essence of probabilistic
hierarchical clustering algorithm.
So now, we summarize the whole lecture.
Fi, we introduced the hierarchical
clustering methods.
We first introduced the basic
concepts of hierarchical clustering,
then we introduced agglomerative
clustering and divisive clustering.
We also studied the extensions
to hierarchical clustering,
especially we discussed BIRCH,
CURE and CHAMELEON algorithms.
Finally, we introduced the concept of
probabilistic hierarchical clustering.
These are set of books and papers.
For example, these three papers is BIRCH,
Cure and CHAMELEON.
That's the original papers.
Most of others, other textbooks
discussing different measures and,
and alternatives of hierarchy
clustering algorithms.
Thank you.
[MUSIC]

