[SOUND] Welcome to Lecture Eight,
Clustering High-Dimensional Data.
In this lecture,
we are going to first discuss challenges
of clustering high-dimensional data.
Then we'll introduce methods for
clustering high-dimensional data.
Especially, we'll discuss subspace
clustering methods, including subspace
search methods, correlation-based methods,
bi-clustering methods.
Within bi-clustering methods,
we are going to discuss theta
bi-clustering and theta pClustering.
We also briefly introduce
dimensionality reduction methods.
So for the first session,
we're going to discuss
challenges of classifying
high-dimensional data.
So why we have to cluster
high-dimensional data?
So the first question is how high
is high-dimensional in clustering?
In our previous discussion,
many examples we give actually only
contain one to three dimensions.
The most popular example
is only two dimensions.
For the, for showing for example, k-means,
kernel k-means, EM algorithms,
but these methods may
not work well when the number
of dimensions grows to 20.
Not to say in many applications,
such as text documents or
DNA micro-array data.
If we take every keyword as one
dimension in text documents,
or we take every word in
the micro-array datasets,
then we may need to handle tens
of thousands of dimensions.
So there are some challenges at
handling high-dimension data.
For example,
there could be many irrelevant dimensions
that may mask the real clusters.
The second problem is that this distance
measure may become meaningless due to
equidistance.
We're going to introduce
this a little more.
Moreover, many clusters,
may really exist only in some subspaces.
If you take all the dimensions
into consideration,
you may not be able to find
a quite meaningful clusters.
Let's look at it.
The curse of dimensionality problem.
Usually, data in one dimension
is relatively packed.
It could be quite dense.
However, when you stretch it into say,
two dimensions,
then the data originally
projected on 1D now
become 2D data, it becomes more far apart.
And you add more dimensions the data
even become more further apart.
So, in high-dimensional space,
the data could be very sparse.
Everybody may have such experience.
For example,
you watch the night sky on the dome.
You're pretty much see, there's some
stars, some bright or not so bright.
They are very close on the sky, but
at once you really study
like use telescope or
ask astronomers,
you probably know these very close
star in the real 3D space,
it could be very far apart.
Okay.
That's mean the 2D projection
you see is readily packed,
but the ones that go to 3D,
it may become pretty far apart.
Then the distance measure
becomes meaningless when you
go to very high dimension,
just because in dimension A,
you may find that these
two is very far apart.
In dimension B,
it could be just noise or something.
But once you see, there are many, many
dimensions, there are differences could
be you have data in this dimension and
I have data in the other dimensions.
And our measure,
you measure is 1,000 as a unit.
I measure, only like ten as a unit.
So, in that sense, in many case, the
distance measure may become meaningless.
So some traditional data distance could be
dominated by noises in many dimensions.
Now, if we summarize the curse
of dimensionality problem,
there are five different aspects
refer to the curse of dimensionality.
The first aspect is optimization.
That means when the data, you know,
getting to a higher and higher dimensions,
then to do global optimization
becomes more and more difficult.
The complexity may increase,
you know, exponentially.
So that's a reason, you know,
optimization may become a challenge.
Then the second problem is distance
concentration effect of the Lp norms.
The Lp norms, for example, if Ly,
we just use the Manhattan distance or
two is just euclidean distance,
we studied this.
But the distance concentration problem,
means far and
a close neighbors may have similar
distances just because the,
when the dimensions is very high,
their difference probably just on,
whether you have data in this dimensions
or we have different dimension.
We have different measure, so
their relative contrast of
Lp distances could diminish,
you know, when the dimension increases.
That's a distance concentration problem.
The third problem is
irrelevant attributes.
For example,
you want to give a word to students.
There are many dimensions
could be irrelevant.
For example, whether student height or
student body weight or
the student telephone number or
student first place.
Those probably relevant for
your, a work composition.
So, in that sense, if you put all
these irrelevant attribute together,
actually and finally, you may really
interfere your real purpose of clustering.
Therefore, impact the performance.
The fullest problem in this correlated
attributes that means in many cases,
you get a subsets attributes,
they may have strong correlation.
Okay.
Then if we take all the dimensions,
the dimensions very high.
But since the strong correlated, these
sets of attributes can be represented
by only one or two dimensions that's
a dimensionality reduction problem.
Just give you a simple example.
Suppose you want to give student a words,
maybe you want to look at GPA.
Of course, you can look at every course,
including their mid-term and
finer and homeworks.
Many, many courses actually have
strong correlation with their GPA.
So you probably just want to look
at their GPA or their major GPA,
instead of looking all those attributes.
That means the intrinsic
dimensionality of datasets
can be considerably lower
than the embedded dimension.
That means the real number
of features of the datasets.
When in intrinsic dimension could
be like a GPA or the quality or
the performance of the student.
But on the other hand,
the embedded dimension could get all the
detailed every course, their performance.
Okay.
So that's the reason you want
to do dimensionality deduction.
Okay.
The fifth aspect for the curse of
dimensionality problem is data sparsity.
We are use the example the show.
The data voluming, the high dimension
space could be extremely sparse.
Because in a very high damage in space,
no sum may not even have a point,
because that point does not
have that high damage in.
Okay.
That's a data sparsity problem.
So those different aspects
combined together, we can say,
this is in really curse of dimensionality.
We have to invent effective
method to overcome this problem,
you, other to do effective clustering.
[MUSIC]

