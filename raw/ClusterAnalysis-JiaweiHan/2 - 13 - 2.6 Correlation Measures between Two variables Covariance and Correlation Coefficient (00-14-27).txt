[MUSIC]
In this session,
we're going to discuss correlation
measures between two variables.
Especially we will reintroduce
covariance and correlation coefficient.
Before we introduce covariance
between two variables.
We will first examine or
review the variance for a single variable.
What is variance?
I think you all know what is average, for
example the average salary
of employees in a company.
Okay.
The average also called mean mathematically.
It represent as mu. So
mu actually is the expect value of x.
However, if we just use mean, we may not
be sufficiently represent the trend or
the, the spread of
the value in variable x.
For example, we may not only like to know
the average salary of in a company,
but we also like to know
how these value spreads.
That means, whether they are very
close to the middle or the mean, or
they're widespread.
They have a many very high salaries and
many very low sal, salaries.
In that case, we introduce the concept
of variance which actually is
to measure how much the value of X deviate
from the mean or the expected value of X.
Essentially, we use sigma square
to graph in the variance X.
Sigma is called standard deviation.
Variance of X actually is expected
value of X deviate from the mean.
If we take the square, simply says,
no matter if it's positive or
negative, we always change
them into positive.
Okay?
Then, if X is discrete variable,
then the formula is written like this.
It simply says we use
some of this function.
Okay?
This function is, actually is, x deviate
from the mu, from the mean value, take
the square times x density function, okay?
If x is a continuous variable,
so we will take integral
where the range is from
minus infinity to infinity.
To that extent,
we say variance is the expected value
of the square deviation from the mean.
For this formula,
we can also do a little transformation.
For example can transform for
the expected value of X,
you know, square deviation from the mean.
We can write down its expected value of
X squared minus, mean, means squared.
This transformation actually has
been introduced in many textbooks.
It's quite simple when I
continue to introduce here.
In many cases, if we write in this form,
it may lead to more efficient computation,
especially when you want to
do incremental computation.
If we take a sample then the sample
variance is actually the average
squared deviation of the data
value Xi from the sample mu hat.
So the sample variance is
reading as sigma hat squared.
So we often use this formula or use
a similar transform formula to compute it.
Now we introduce a covariance for
two variables.
Once we get two variables, x 1 and x 2,
we want to see how these two variables,
they change together.
Whether they are going up together,
going down together.
Which could be the positive covariance.
Let's look at the definition.
The definition actually original
definition is X 1 minus mu 1 square.
Now we see actually these two
variable we s, want to see X 1.
The difference from its mean value of X1.
X2, the difference from X2's mean value or
expected values.
And we look at their expectation.
Okay.
Mathematically we also can
transform this into this form.
If we get a sample covariance, we look at
the sample covariance between X1 and X2.
So the,
the sample covariance is calculated
by their difference from the sample mean,
okay?
So that's also popularly used.
Actually the sample covariance
can be considered as
a generalization of sample variance.
For example, a reason we will want to
look at the two variables x1 and x2.
They're a covariance.
But if we think these to X 1 and
X 2, we replace it by X 1,
that means we just look at the two
variable X 1 X 1, what is the covariance?
Then we can represent this 2 by 1.
In that case the sample
covariance the formula.
We look at this formula we change
the variable from i2 to i1 and
mu 2 to mu 1's hat.
Though then we derive this formula.
And this formula essentially is
sigma 1's hat and its square.
Okay, so,
we probably can easily see the sample
variance is just the special
case of sample covariance,
when the two variables are just the same.
When this,
the covariance value is greater than 0,
we say it is a positive covariance.
If its, its value is less than 0,
it is negative covariance.
If these two variables are independent,
then their covariance is 0.
However, the converse is not true.
That means now, when the covariance is 0,
it does not mean X1 and
X2 are always independent.
Only under certain additional assumptions,
for example
if the data follows multivariable
normal distributions.
In that case, the,
the covariance of 0 implies independence.
Now we look at a, a concrete example.
Suppose we have two stocks X1 and X2.
They have the following
values in one week,
like these five values,
five pairs of values.
Then the question is,
if the, the stock affect,
whether the stock, okay,
affected by the same industry trends.
That means whether their price
will rise or fall together.
Okay, then we care for their covariance.
We will be able to know whether they are
positively correlated or negative [INAUDIBLE], okay?
So if we look at the covariance
formula especially we use a more
simplified computation formula.
Then we can calculate the expect
of X1 which is a mean value of X1.
The expected value of X2
which is a mean value of X2.
Then we look at their covariance
actually as we use this
formula we look at their product,
their dot product, the sum of the,
and divide by sum of them divide
by the number of variable pairs.
Then we minus this is expected value
of xy and expected value of x2.
Then we get the final value is 4.
Okay.
That means,
the covariance is greater than 0.
That means X1 and
X2 they rise or fall together.
Then if we want to normalize them we
will introduce correlation coefficient.
That means for two numerical variables
we want to study their correlation,
which essentially is
a standard covariance.
That means we want to
normalize the covariance value
with the standard
deviation of each variable.
So it is defined as
correlation coefficient
is the covariance divided by the product
of their standard deviation.
Or you can say the covariance
is divided by the product,
the variance, get their square root.
So if we look at sample correlation for
two attributes X1 and X2,
then essentially we get,
we get a row one two's hat
is equal to the, the sigma 1 two's hat.
This is essentially their sample.
Covariants divide by their
sample standard deviation.
In a concrete formula
we can write in this way.
Okay.
Then if this co, correlation coefficient
is greater than 0, that means A and
B are positive correlated.
That means,
X 1's values increase as X 2's.
The higher value greater than 0,
the stronger correlation, okay?
If rho12 equals 0,
that implies they are independent
under the same assumption as
discussed in the co-variance.
If they are less than 0,
they are negatively correlated.
Then we can look at a, a set of variables.
We can see for example for
two variables when they
are perfectly negative correlated,
they line up like this.
Their correlation coefficient is minus 1.
Okay.
Then if they gradually become not so
perfectly negatively correlated,
you will see their trend.
When this value is 0,
you could not see anything like
a positive correlated or a negative correlated.
But when you gradually grow
these correlation coefficient
you'll see their value will become more and
more correlated.
When they are perfect correlated then
their correlation coefficient is 1.
That simply said the correlation
coefficient value range is from
minus 1 to 1.
'Kay.
Then, if we draw this in the scatter plot,
we will see the set of points,
their correlation, coefficient changes
from minus 1 to plus 1 in this shape.
In many cases, we may want to write,
for two variables,
we want, we may want to write their
variance and the correlation information
into the two by two
covariance matrix form.
Okay.
For example, you may say the variable one
is self correlation there, essentially
there are variance is this one.
And for their covariance between 1 and
2 is defined here between 2 and
y is defined here.
And then that's,
that's variable two's variance.
So this is a typical you know,
2 X 2 covariance matrix.
In general, if
we have T, D numerical attributes.
That means, suppose we, you,
you, we find a data sets.
It has N co, rows, and D columns.
That means we really have
D numerical attributes.
Then their covariance matrix
essentially is written in this form.
You probably can see that this
is the variance of variable one.
This is the variable of the second.
This is the variable of the D dimensions.
Okay?
And their covariance for
each one will be lining up like this.
So, in summary, in this section,
in this lecture,
we discussed several important concepts.
We introduced this similarity
measure between objects, okay.
Especially for
numerical data we introduced Minkowski distance, for
symmetric and asymmetric binary variable.
We studied their proximity measure,
especially Jaccard coefficient.
We introduced the distance measure
between categorical attributes,
ordinal attributes and mixed types.
We also introduced cosine similarity as
proximity measure between two vectors.
We introduced covariance and
correlation coefficient as correlation
measures between two variables.
And we give a few interesting
additional reading.
These are the several books
that contain interesting
chapters discussing
the different measures.
Thank you.
[MUSIC]

