[MUSIC]
In this session we're going to
introduce Gaussian Mixture Models.
That means we'll assume
all the clusters formed
based on the Gaussian distribution
with different parameters.
We'll first look at a Gaussian
distribution for univariate, that means
for single variable and for multivariate,
that means for multiple variables.
We first draw some plot and
contours for Gaussian distribution.
For example, we can see for this plot
this is a Gaussian Distribution for
univariate, for the single variable
the parameter like this one is mu,
the mean is 0 sigma
the standard deviation is 1.
This is a typical standard bell curve.
Then for mu equals 5 and sigma equals
2 we can draw a curve like this.
Then for 2-D Gaussian distribution,
we give mu and
a sigma We draw the contours for for
the one set of parameters mu and
sigma here.
For another set of mu and sigma
covariant matrix we'll draw like this.
So this is a typical way to
have a visualization of different
parameters for Gaussian distribution.
Now we study Gaussian Mixture Model.
We assume every cluster C sub i,
is characterized by
a multivariate normal distribution,
where f sub i of x is
a probability distribution
of x attribute to cluster C sub I.
That means we can represent this density
function of x under the cluster c sub i.
This parameter is mu sub I and
sigma sub I.
Mu is a mean and sigma sub i is
a covariance matrix for the cluster i.
And we can write in the typical Gaussian
distribution form for multivariate.
Then we can assume the probability
density function of x
is given by all the k cluster normals
from a Gaussian mixture model.
That means we will say the density
function f of x essentially
defined by summation
of all the k clusters.
So each k cluster,
the density function this f sub i of x.
The probability of this cluster is P,
probability of cluster C sub i.
Then we can represent in
the parameter form is mu sub I and
sigma sub I we can say for
all the k clusters.
Each cluster is a parameter is under the
conditioning of mu sub i, and sigma sub i.
Then the prior probability,
probability P of C sub I.
If for all the k cluster they added
together their probability should be 1.
So we call this is mix your parameters.
Then our task is try to get maximum
likelihood estimation of you
know the model parameter.
Essentially is if we want
to say given the data set D
what's the probability under
the model parameter of theta,
we can generate this whole set of D.
Since we assume all these number,
these n points are independent.
We can think their probability is their
probability density function of x sub j.
They all multiply together for
all the end points, we get this.
If we write this in log form, so
we can take the log of this function and
these multiplication
by taking log, it becomes summation, so
the computation could be more efficient.
And this f of x sub j, we can use
a previous one to this function here.
Then maximum likelihood
estimation is to choose
the parameters set theta.
So to make this probability
to be maximized.
Or we can say we can thinking
to maximize the log-likelihood.
That means we want to find
a set of parameters theta,
so the log of these
probability generation,
the theta data set D under
the parameter theta is maximized.
However, to directly maximizing this log,
log likelihood over theta is hard.
It's very expensive.
So we can use EM approach to find
the maximum likelihood estimation for
the parameter theta.
The EM approach essentially
consists of two steps iteratively
one is expectation
another is maximization.
The expectation is given
the current estimate of theta,
we try to compute the cluster
posterior probability.
That means, we want to com,
compute what's the probability
of the cluster c sub i under the con,
condition of x sub j.
And we can use the Bayes theorem to write
in the Bayes theorem form that means we
want to find the probability of c
sub i under the condition of xj.
It is actually is the function
essentially the numerator
is actually the density
function of x sub j for
cluster sub i times
the probability of cluster sub i.
Then the denominator is
all the k clusters for
each cluster that generates a j.
Then we were thinking
about the probability
to generate the cluster C sub a.
Then for all these k clusters,
we add all these together.
That's our denominator.
So for maximization step we can,
essentially is using
the computed cluster posterior
probability to re-estimate
the parameter's theta.
That means we want to re-estimate
mu sub i, sigma sub i,
and the probability of c sub i,
for each cluster, c sub i.
[MUSIC]

