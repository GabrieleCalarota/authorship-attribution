[MUSIC]
In this session,
we will briefly introduce some
other advanced themes
on clustering analysis.
One important interesting one
called ensemble clustering.
What is ensemble clustering?
Ensemble clustering is trying to
combine the results of many clustering
models to create a more robust clustering.
Since clustering is unsupervised,
there's no single model or
criterion may truly capture
the optimal clustering.
But if we can ensemble the models,
it may provide a more robust solution.
The general method of
knowledge is first we generate
k different clusterings or
we call these ensemble components.
Then after that,
we will combine those different results
into a single and more robust clustering.
So, we'll first to see how to generate
this k different clusterings, okay?
Usually we can use a model-based or
data selection-based.
The model-based means you generally use
different models or different parameters.
You may generate different components,
this is you can select
those ensemble components.
The data selection means you can
data from different portions or
different samples for
the whole partition of the data.
Then you can generate
those ensemble components.
Then we can combine them together.
Now how to combine those,
these different ensemble components,
generally there are two major proposals.
One called, hypergraph partitioning
that means you will consider
each data point is a vertex.
And that a cluster actually
in any ensemble components,
you can think they are hyper-edge.
Then those feed points will be wrapped
into, will be linked by many hyper-edges.
Then you can thinking this one is
hypergraph then you do hypergraph
partitioning you may get those ensembled
final robust clustering results.
Another one, called meta-clustering,
meta-clustering is a graph based approach,
comparing to the typical point based,
graph based clustering.
The vertices actually now is
associated with each cluster
in the ensemble components, okay?.
So these are the typical approaches for
details we will introduce some references.
Then we will briefly outline,
how clustering is used for
effective data mining?
First, clustering is can be used for
data summarization, why?
Because clustering is based on similarity,
actually this is a very natural form for
summarization.
Because summarization in the most natural
way is based on the notion of similarity.
That's why when you cluster data
you essentially summarize the data.
Then the second major
effective data mining
task clustering can contribute
is outlier analysis.
Actually, we can view these outliers,
anomalies,
are those data points that are far
away from any particular cluster.
That means we can first do clustering and
once we see there's some
isolate data points.
They're far away from
any particular cluster,
we will say those are the outliers.
Then the third contribution that
clustering can do is for classification.
The classification, if we first
preprocess data to get a clustering,
we will reduce the number of data points
the classifier should handle, okay?
And a typical one, for
example, a clustering can speed up,
Kenya's neighbor classification.
For Kenya's neighbor you try
to find their neighbors but
sometimes you get too many data points.
Then we can replace the data
points with the centroid
of fine-grained clusters
belonging to a particular class.
That simply says, if there are many,
many data points,
they all belong to a positive class.
We actually can find clusters and
we just use centroid to represent
some fine-grained centroid.
So wrapped in the many data points if
they belong to the same class, okay?
So that may effectively
reduce the complexity and
number of data points to
handle in classification.
And clustering also popularly used for
damage dimensionality reduction.
In our lectures,
the previous lectures we already
briefly introduced spectral clustering,
non-negative matrix vectorization,
probabilistic latent semantic indexing.
Those are the typical dimensionality
reduction method using clustering, okay?
Finally, clustering can be used for
similarity search and indexing.
Clustering is based on
notion of similarity
that's why we study similarity measure,
dissimilarity.
If we can do hierarchical clustering,
then we may create indexing
structure like CF-Trees.
That will support similarity search and
indexing effectively.
Finally we will discuss
clustering big data,
so-called big data is data in the really,
really large-scale processing.
You, you probably know the Hadoop,
the MapReduce, there are many,
many such new methodologies
to handle very big data.
And one interesting one
called Apache Spark.
We have a pointer to their website,
it's a fast and
general engine for large-scale processing.
It is one of the most active
projects on big data, so
it has lots of contributors and
lots of product deployments.
It has contains user-friendly and
expressive APIs in Python,
Java, Scala, and R.
It has fault-tolerant in-memory and
on-disk computation.
It holds data in memory and provides
fast accessing during iterations and
it contains some recovered node failures.
And it has many feature rich standard
components for machine learning,
graph computation, SQL, and
data frames and stream processing.
For clustering algorithm,
it supports k-means, streaming k-means,
power iteration clustering Gaussian
mixtures, latent Dirichlet allocation and
in many more.
So I provide some pointers you may
click there, you may watch some
MOOC videos to learn more about
how they're clustering big data.
[MUSIC]

