[SOUND] In this session,
we're going to introduce
another interesting algorithm, SCAN,
which it is density-based
clustering of networks.
For social network analysis,
people often interested in
finding clicks Hubs and outliers.
Cliques you can think is
people know each other.
People are tightly
connected with each other.
Hubs could be the people who,
who are connected to many different groups
But they may not belong to a single group.
Now outliers, maybe they do not
belong to any other clique, but
they may have some few connections
with some cluster or some group.
It would be nice to find the cliques,
hubs, and outliers.
Okay.
If we identify this, we will be
able to find some special groups.
For example, families,
some social communities, or
terrorist cells, or many other things.
Then we introduce an interesting
algorithm called SCAN,
which is density-based
clustering of networks.
So, we first to see how scan define
the neighborhood of a vertex, okay,
the neighborhood of vertex V
essentially defined as V itself is
neighbor of itself then directly connected
using social links its direct neighbor.
So, gamma of v,
essentially v and its neighbor
connected those red, those yellow
points are the directed neighborhood.
Then, if we want to compute
the similarity Or connectivity between
the two vertices v and w,
suppose we have two vertices v and w.
We want to see how tightly they
are connected or how similarly they are.
So essentially, if they have
many common neighbors, okay, and
they're own size as a normalized factor So
you probably can see, the more common
neighbors they share, and also
the less other neighbors, essentially,
if these are all their neighbors.
So, they are very tightly connected,
or they are very similar.
Okay.
So, if you find one big star,
they have many many neighbors.
Of course,
they have some neighbors with w but, they
may not be as similar as the other nodes.
So, based on this,
we can work out a structure similarity,
based on the notion of similarity.
The structure similarity
usually is large for
members of a clique, but
it's a small for hubs and outliers, so
we base on this, we can work
out a Density-Based Clustering.
So, the structural connectivity,
we can think about it based
on density connection.
We can work out a definition similar.
To DBSCAN in the sense.
We still have epsilon-Neighborhood.
It simply says, if these two,
their similarity is greater
than epsilon's threshold,
then we will say they are connected.
Then w is v's neighborhood, and it,
then these neighborhood,
actually the number
of such neighbors is it's on neighborhood.
Then for core essentially is if this,
if this DBSCAN neighborhood, the number
of such DBSCAN neighborhood is greater...
Or equal to a minimum support threshold.
You can think of mu is a minimum,
it's a threshold.
Simply said, if these
neighbor is no less than a threshold,
then v is a core.
Based on this, we will be able to
derive directly structure reachable.
Directly structure reachable
means v is a core and
w is direct neighbor,
then v will be directly reachable to w.
Okay, so you can probably see that's
a very similar thing In DBSCAN,
you also have directive reachable,
direct events that are reachable.
This one called direct
structure reachable.
Then structure reachable, essentially,
we write down REACH of u and
v, is a transitive closure of
direct structure readability.
Simply says if Q can reach P2,
P2 can reach P and
Q will also be able to reach P,
that means that you use a sequence
of the structure reachable.
You can use transitive closure,
that's also reachable.
Okay, then what is a structure connected?
Structure connected simply says,
if this node p reachable by o,
and, q is also reachable by o,
then p and q are connected.
The same thing is if we look at v and
w If you can reach v,
and you can reach w and
if v and w are connected.
So, that's a very similar notion,
but in the previous notion
this graph is a spacial
reachability the density.
Okay, here the new definition
in scan is structure reachable
based on the graph,
based on the topological structures.
Okay?
Then we'll see how to get
a structure-connected clusters.
Essentially, a cluster
can be considered if they
structurally reachable,
is connected, and they are maximum,
that means all the connected
ones form one cluster.
Based on this, you can get a cluster.
You also can get a hub, which is linking.
It does not belong to any cluster,
but linking to multiple clusters.
You also can get an outlier,
which does not belong to any cluster,
but it is only connected
to very few clusters.
So, then we look at how
the Scan Algorithm is Executed, okay.
Suppose we want to find,
suppose we said mu is 2, epsilon is 0.7,
then we'll see how we
compute They're similarity,
they're simplicity,
how they're connected for note 9 and 13.
We'll just give you a little
detail to see how 9 and
13, they're similarity is 0.63, okay.
Then we can see because
13 Directly reachable.
The neighborhood is nine and 13.
Nines directly neighborhood.
Directly reachable, is 8, 12, 10, and 13.
That's the neighborhood.
Then, we know 13's neighbor You have two,
and nine has five neighbors, okay?
Including itself.
Their intersection of their
neighborhood is nine and 13,
because that's only nine and
13 is common, okay?
Then, if you look at the, the size,
the nine and 13, the size is two.
Then you paste on the formula to
compute the sigma essentially is you
look at the size is two, and
the other size is five, two times five is.
Then you compute this, you get 0.63.
That's what you get it, okay?
In a similar way,
you can compute the others.
So, let's compute, suppose 8, so we can
compute its similarity to 7, 12 and 9.
Well, you can see is this
Two is above the threshold.
That means eight and nine,
eight and 12 is above threshold,
the active form a cluster.
Okay.
Then you can keep going.
If you keep going, we will see, we compute
nine and 10, how they're connected.
We found that they
are below the threshold.
Okay.
But if you look at 12 How they
connected with the other nodes.
You will find this one is
actually above the threshold.
You grab all these into one cluster.
You, actually you can see this
one really forms a nice cluster.
Then, you calculate six.
How they connected with
their direct neighbors.
So, you find actually.
None of them go over the threshold 0.7.
Simply so
that this one is pretty isolated.
And then if you keep going,
you will find 6 actually is a hub.
And then zero, one, two, three, four,
five, these six nodes actually
form another cluster.
So you'll find the hubs.
Colors and two clusters.
Okay?
This is just execution
of this small graph.
For several real-world, pretty big graph,
Xiaowei Xu actually did a very
thorough experiments to
show A lot of real-world data sets can
generate very interesting clusters.
So the major issue becomes how can
we smart set up mu and epsilon?
There's some follow-up
studies on to decide mu and
epsilon, how to make the algorithm
less sensitive to the parameters.
This algorithm is pretty efficient
because you only look at your very
close neighbors instead of trying to find,
to ask to anything from the whole graph.
So the running time actually is
a big oh of number of edges.
Since for sparse network each node
were not connected to too many edges.
So that's a reason for sparse network.
The complexity if over
the size of vertices.
If you look at the running time in
the rear data sets you will find that
the number of vertices.
Grows bigger and bigger.
Actually, the fast modularity,
it would take much longer time to finish.
But using this scan algorithm,
actually, it's still very efficient.
Now, we finish this lecture.
In this lecture, we discussed how to
do clustering Graphs and network data.
Especially we introduce some basic
concepts of graph and network clustering.
We introduce the representation
of graphs and networks.
Discuss some typical evaluation measures,
outline the general approach
of graph clustering.
Especially we focus on
spectral clustering.
And a density-based
clustering of networks.
Then here I will introduce you
a few interesting research
papers and tutorials and
some general chapters of the book.
Interested readers, you may like to read
those things to have deeper understanding.
The material covered in this lecture.
Thank you.
[MUSIC]

