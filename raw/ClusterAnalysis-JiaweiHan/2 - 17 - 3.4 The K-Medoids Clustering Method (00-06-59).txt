[SOUND] Now I'm going to introduce you
another interesting K-partitioning
clustering method called
the K-Medoids Clustering Method.
Why we need to study
K-Medoids clustering methods?
Just because the K-Means algorithm
is sensitive to outliers.
Because the mean is sensitive
to the outlier itself.
Just give you a simple example.
If you look at a company salary,
if you add in another very high salary,
the average salary of the whole company,
it can be shift quite a lot.
So, let's look at a K-Medoids,
what is K-Medoids?
That means that instead of taking
the mean value of object in a cluster
as our centroid,
we actually can use the most
centrally located object in a cluster,
or we call medoids.
That means the K-Medoids clustering
algorithm can go in a similar way as we
first select the K points as
initial representative objects,
that means initial K-Medoids.
The difference between K-Means is K-Means
can select the K virtual centroid,
but this one should be the K
represented of real objects.
Then we put this one into a repeat loop,
we can assign similarity we assign each
point of the,
to the cluster with the closest medoid.
Then we can select a, randomly select a
non-represented object, supposes o sub i.
We're see whether we's,
use o sub i to replace one medoid m.
Whether it will improve
the quality of the class rank.
That means the total cost
of swapping is negative.
Simply it says, if we're swapping,
we can reduce some of the square errors.
Then we are going to swap m with object
o i to form the new set of medoids.
Then we need to do,
redo the assignment, and
here, this process goes until,
the convergence criterion is satisfied.
Now we'll see a small example how
a typical K-Medoids algorithm is executed.
You look at the PAM as an example.
Suppose we are given ten small
number of points in this small graph,
okay, in this 2D space,
we want to find the two clusters.
At the very beginning we
arbitrarily choose K objects here,
we choose two objects as initial medoids.
Then we will find the clusters of
these medoids as follows, okay?
Then we will see whether we can randomly
choose another, object like o random.
Say this non-medoid object, we want to
see whether it could become a medoid
if it were reduced, the total cost,
or we say, we get a better SSE.
In this case,
suppose we choose one here, but
we found it does not really reduce
any you know, the total SSE.
Then we actually can get another one,
like we get this orange one.
Then we look at the cluster we can form,
we know this one, will reduce
the total SSE, that simply said
the quality of the cluster is improved.
Then we will do the swapping.
So this, essentially is,
is listed here is, we initially,
we select initial K-Medoids randomly,
then we will do object reassignment,
then we try to swap medoid m
with the random non-mediod
object o sub i,
if it improves the clustering quality.
Then we'll do it again and again until
the convergence criterion is satisfied.
So this is just a simple
execution to illustrate the ideas
of this K-Medoids, how it is executed.
Notice, see, these K-Medoids clustering
essentially is trying to find the K
representative objects, or
medoids, in the clusters.
And the typical algorithm PAM,
called partitioning around medoids,
was developed in 1987 by Kaufmann and
Rousseuw.
Starting from initial sets of medoids,
then we iteratively
replace one of the medoids
by one of the non-medoids,
if such a swapping improve the total
sum of the squared errors,
that means the total
quality of the cluster.
This method works effectively for
small data sets because it,
we can keep trying different swapping, but
it cannot scale well because the
computational complexity is quite high.
If we look at, into detail actually,
this computational complexity for
every swapping, actually it's, it's to
the square of the number of points.
This is quite expensive.
So how to improve its efficiency?
There's one proposal by same
authors in 1990 called CLARA.
Essentially, is PAM on samples.
That means instead of using the whole
points we choose a sample s,
s is a sample size,
then the computational complexity,
the square actually comes down
to the size of the sample.
However, if the sample,
initial sample selection is no good,
the final clustering
quality could be poor.
Then in 1994, there's another algorithm,
called CLARANS,
proposed is, every iteration,
we do randomized re-sampling,
that means we do not keep exactly the same
sample, we do randomized re-sampling,
that will ensure the efficiency and
the quality of clustering.
[MUSIC]

