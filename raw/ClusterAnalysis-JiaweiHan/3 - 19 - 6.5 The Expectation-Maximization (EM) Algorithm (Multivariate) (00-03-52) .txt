[SOUND] In this session, we are going
to examine the expectation, maximization,
algorithm for multivariate case.
And we probably can
look at initialization,
then we get into expectation and
the maximization step.
Each initialization is very much
like the single variable case,
is we randomly initialize mus.
That means for mu sub 1 to mu sub k,
these k clusters,
we just randomly initialize them.
Then for covariance, we for
all the k clusters we give them
identity matrix assigned to them.
And for the probability of c sub i
we give them equal probability for
all the k clusters.
Then we put this into repeat and
hear loop and
hear the sum of the changes of
the means across two iterations is no
greater than a pre-specified
threshold epsilon.
Then for the expectation step,
we just assign objects to clusters
according to the current parameters
of the probability clusters.
That means, for all the, clusters and for
all the objects j, from 1 to n.
We will calculate
the posterior probability for
c sub i under the condition of x sub j.
And these calculation is very
much the same, you use you,
you, the numerator part you will
get to the density function
of the, of under the condition of mu and
sigma, and then,
times the probability of c sub i.
Then, for the denominator,
we will calculate the same way for
every clusters, and just sum them up.
Then for maximization step,
this we try to find a new clustering or
parameters that minimize SSE or
the expected likelihood.
Then, for for cluster i from 1 to k,
we will recalculate in this way.
Re-estimate the mean.
Re-estimate the covariance matrix and
re-estimate priors.
And we will repeat this until the change
becomes really, really small.
Now we look at the,
the demonstration of these two,
of two dimensional data
set to see how these exam,
these EM exam algorithms is executed.
When we start,
this is the original data points.
Then we look at the initialization
you probably can see.
Even you get a two clusters, centers,
but they are not a distinguished.
They,they, you can hardly see,
the what things are going on.
But in the second iteration
you will see slightly change.
But when you get into
certain semi-iterations you
clearly see two centers.
Then they form nicely into clusters.
So this is in the, is a demonstration.
On the,
every step while things are changing and
here to the things become re- really,
really stable.
[MUSIC]

