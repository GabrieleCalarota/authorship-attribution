[SOUND] In this session,
I'm going to introduce the K-Medians, and
the K-Modes clustering methods,
as two interesting alternatives
to the K-Means clustering method.
Why we want to do K-Medians?
Because the medians are better than
means when we encounter outliers.
Medians usually less sensitive
to outliers comparing to means.
For example, in a large firm, okay,
there could be many employees.
We want to find the median salary,
even when you're adding,
when you add a few top executives,
the mean salary may change quite a lot.
The median could still be very stable.
So that's the reason,
instead of computing the mean value
of the object in the cluster, so
we take the medians as the centroid,
and we use L1-norm,
that means the Manhattan
distance is our distance measure.
The, the criterion function for the
K-Medians algorithm is written as this.
Essentially sum, the total sum should
be k from 1 to the number of cluster k.
And for each cluster, the object in
the class where you just look at
the difference, this, the different,
that take the absolute value
of their distance to the median.
The K-Me, K-Medians clustering algorithm
essentially is written as follows.
The first, at the very beginning,
we select the K points as
the initial representative objects.
That means as initial K-Medians.
Then we get into the slope, we assign
every point to its nearest median.
Then we recompute the median, using
the median of each individual feature.
Then this process repeats until
the convergence criterion is satisfied.
Then we look at a K-Modes as another
interesting alternative to K-Means.
K-Modes essentially is to
handle categorical data,
because K-Means can not handle
non-numerical categorical data.
It, we, of course we can map
categorical value to 1/0.
However, this mapping can not
generate quality clusters for
high-dimensional data.
Then people propose K-Modes measure,
which is an,
an extension to K-Means, by replacing
the means of the cluster with modes.
The dissimilarity measure
between object X and
the center of a cluster Z,
is written as follows.
Okay.
This is the di, the dissimilarity or
we can, we can ay distance measured,
distance function.
Okay.
For the j sphered of object X,
that means X of j.
And look at the j sphered
of the cluster center Z.
Okay.
What's the distance?
Essentially is,
if this object is not equal
to the center on the same attribute,
essentially say they are different.
Then we just say the distance is 1
because it is very dissimilar, or
the distance is largest.
However, when this value is equal
to this attribute center here,
then we give this formula.
The this formula actually means,
if there are many, many objects occur
the same in this cluster, okay, then
this value actually will be very small.
For example, if this, if the,
the attribute cluster, there is
only one variable, it is very frequent,
and this guy have to equal to this value,
then this division will be to 1,
then the distance is the smallest.
In most cases is the more frequent for
this j value,
and this one will be closer to 1,
so the distance is smaller.
And if this one is very rarely equal to
the very rare value, and the number
of distinct values is quite big, so
this, your distance is closer to 1.
Okay.
So this is quite reasonable definition.
That means this,
these dissimilarity measure distance
function essentially is frequency-based.
The more frequent, or the,
the closer, the less frequent,
a little far away,
even are equal to this value.
The whole algorithm,
we will not list here, but
is still based on the iterative one.
We first do a object cluster assignment.
Then doing a centroid update,
then we take this one into a loop
until the criteria are reached.
Then there's another alternative
method called fuzzy K-Modes method.
The fuzzy K-Modes method essentially is,
calculate the fuzzy cluster membership
value for each object, to its cluster.
Simply says, you know,
you give a fuzzy cluster value.
Okay.
If it's very close to this cluster,
the fuzzy value is closer to 1.
Okay.
It's far away from this cluster, and
the fuzzy value is somewhat closer to 0.
Okay.
And if we have the data,
some attributes
are categorical attributes,
some attributes are numerical attributes,
we can use a K-Prototype method.
Essentially, for
numerical one we can use a method,
the functions similar to the K-Means.
And the categorical one,
we can use similar to K-Modes.
And finally, we integrate this,
we can get a K-Prototype method.
[MUSIC]

