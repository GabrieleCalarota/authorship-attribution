[SOUND]
In this session,
we're going to introduce some
dimensionality reduction methods.
The dimensionality reduction measures.
The general concept is in some situations,
it could be more effective
to construct new space,
that means we transform into some
reduced number of dimensions,
instead of using some subspace
of the original data.
That means the space should be changed.
Essentially, originally m times n,
you want to get smaller ones.
Let's look at this figure, okay?
This figure, if we use the original axes,
for example X and Y,
you predict on X and Y, that you
can now find nice clusters because
all these points actually overlap on both
X-axis projection and Y-axis projection.
However, if we get a new,
this dashed axis,
okay, this Y, like, I turn the Y,
X, you know, the axis 45 degrees.
What you can see is
these points where you,
you project this axis to form three nice,
separatable clusters.
That means sometimes we
transform a little, but
we can reduce the number of dimensions.
In the meantime,
we can find nice clusters.
That simply says, in many cases,
feature selection or
feature extraction may not focus
on finding clustering structures.
However, we may use
dimensionality reduction methods
to reduce the dimensionality using
some mathematical transformation.
There are two very popular approaches, one
called nonnegative matrix factorization.
We will briefly outline
the idea in the next slides.
The general idea is for
one high-dimensional sparse
nonnegative matrix,
we may be able to factorize them
approximately into two
low-dimensional matrices, okay?
Another method called spectral clustering,
this we will introduce a little
more detailed in the next lecture,
lecture ten, is to use a spectrum
of the similarity matrix of
the data to perform
dimensionality reduction.
So finally, we may find clusters in
a much fewer number of dimensions.
This method essentially is to
combine feature selection and
the clustering together.
There are two popular
spectral clustering methods,
one called normalized cuts by Shi and
Malik.
Another is Ng-Jordan and Weiss method,
led by Andrew Ng published in NIPS 2001.
We also call this one NJW method.
Let's look at nonnegative
matrix factorization, okay?
For example,
you can think about you get quite a large
set of documents and
they have a lots of words, okay?
Then the word frequency in documents
from a pretty high dimension
nonnegative matrix.
Nonnegative because the word frequency
can be either a positive value or
zero, means there's no such
word existing in this document.
Okay.
So, for this nonnegative matrix,
it could be quite sparse.
Now, we can do some transformation.
That means we can automatically factorize,
I mean to tune the low dimensional
nonnegative matrices, like U and V.
Okay, originally it's n times d, is
number of words and number of documents.
Now, you get n times k.
k could be much smaller value.
And you get another one, it's k times d.
Okay, that means one matrix, A with n
times d dimensions is
factorized into U and
V, these two matrix multiplication.
But one is n times k,
another, k by d, okay?
So, this is, that means that one matrix
A is factorized into two factors, U and V.
Of course, since it's approximate,
that means you may have a residue matrix
R representing the noise
in the underlying data.
This R actually is the difference
between the original high-dimension
nonnegative matrix and the factorization,
the two factorized matrices.
Their multiplication, okay, their product.
So this transformation
essentially is we can consider it
is constrained optimization.
That means we can determine U and
V as an optimization problem so
that the sum of the square of
the residuals in R is minimized.
That means we try to find
this R as small as possible.
That means this U times
V as close as possible.
Actually, from the semantic meaning, U and
V actually simultaneous provide
the clusters on the rows and the columns.
Rows, you can think these
are the documents, columns, other words.
So this because you simultaneously
clusters on rows and
columns, so it's kind of co-clustering.
That means for U as an n by k matrix,
these essentially are the components
of each of the, these n objects and
words, mapped into each of
the k newly created dimensions.
Okay, then for V, k times d,
you can consider as each of the k
newly created dimensions in
terms the original d dimensions.
So the advantage is this factorization
get results is rather interpretable,
in the sense because it did not
really transform their dimensions.
They either represent it based on the
words or represent based on the documents.
Okay, so the data point,
it can be expressed as a nonnegative
linear combination of the concepts,
that means words and documents,
in the underlying data.
They do not really transform
the whole data sets into a new axis.
So in summary, in this lecture,
Clustering High-Dimensional Data,
we discussed the challenges of
high-dimension data clustering.
We also introduced the general methods for
clustering high-dimensional data.
Especially we focused on
subspace clustering methods.
We discussed subspace search methods,
correlation-based methods,
bi-clustering methods.
For bi-clustering, we discussed
delta-bi-clustering algorithm and
delta-p-clustering algorithm.
Also, we briefly introduced
the dimensionality reduction methods.
For the rec,
recommended readings these are mostly the
papers we've discussed in this lecture.
So you may find a nice
original paper you can read.
And finally, Arthur Zimek got a,
a nice summary called
Clustering High-Dimensional Data chapter
in Charu Aggarwal's book you may like
to read to find some new or additional
method we did not discuss in this lecture.
Thank you.
[MUSIC]

