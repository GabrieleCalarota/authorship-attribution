[SOUND] In this session,
we're going to discuss typical evaluation
measures for graph clustering.
So, there are some commonly used
measures for graph cutting.
For example, mincut, or
minimum cut, ratio cut,
normalized cut, conductance, modularity.
The sum of these have been covered,
actually in the lecture seven,
but we were just a mention
them briefly here.
Also we were introduced the similarity
measure across networks.
Essentially, SimRank will
be covered in this session.
They're also other measures
like a Personalized Pagerank.
For a minimum cut this is the first,
first introduced a graph cutting method.
Okay, the general philosophy
can be expressed in this graph.
If you want to cut the minimum number of
edges to partition this graph into two,
or into more, okay.
However, you probably
can see If you cut here,
you may break them into two graphs, but
this is not balanced at all, because
you find to the left only a singleton
node, which may not be desirable.
Okay?
However, if you cut here,
even you cut two edges,
actually you make the whole resulting
two sub-graphs somewhat balanced.
So, this one is a better cut, the mincut.
Let's see how the mincut is defined.
That means that given a number k.
Here the k could be 2, okay.
Choose a partition C sub 1 to C sub k,
which partition a network
into k partitions.
The measure essentially is, you look
at the sum of the weights of the edges
connecting C sub i, this partition,
and other partitions.
Since you have C sub i,
you try to get everyone.
Essentially, this [INAUDIBLE]
linking to others you counted twice.
That's why you need to divide by two.
Okay, however, we already see the mean
cut is not a good cut, in many cases.
Because you may cut these two parts,
it's highly unbalanced.
Then people propose other measures
try to make the partition balanced.
One proposal is called a ratio cut.
The ratio cut basically is sum of edge
weights linking partitions C sub i and
other partitions, and you want some
of the weights to be normalized,
it means every weight divide by
its size of the partition C sub i.
Okay, since you add those partitions
twice, that's why you divide by two, okay.
So, it says normalization, but it's far
better because you probably can see,
if you cut this one into single one, this
size is only one, so then the whole ratio
could be rather big, rather than if
you cut them relatively balance,
this one could be rather big When you
divide by this you make the weights much
smaller so this, that's why the,
the smaller of the ratio cut the better.
Normalized cut carries very similar ideas.
The only difference is, instead of
a normalized by the size of the,
eh, cutting partition, you normalize
by the degree of the cutting partition.
That means the total degree
of clusters in sub i.
There are sub graphs in sub i.
So, the lower the normalized cut,
it means the better of community.
Just because they are well-connected among
the community or the cluster selves,
but they are, they are, sparsely connected
to the remaining of the partition.
The conductance carry, actually,
also similar methodologies.
The difference is normalized by
the The minimum between these two,
and this one is a degree of the,
the cluster,
and this one is the degree
of the other ones.
You Just pick up the minimum one.
So, this is, one conductance,
but if you get,
all the whole partition you get
into k clusters C sub 1 to C sub k.
Then the whole conductance is
just the sum of every one.
Okay.
There's another measure of co-modularity.
The modularity definition
is somewhat different.
It's the sum of the difference of these.
The different,
the first difference you probably can see,
this is your internal connections.
C sub i connect to sub, C sub i,
normalized by the number of edges.
This one, actually is their degree.
Their degree normalized by these 2e,
because the degree
when you add them together, actually
this degree should be normalized by 2e.
The definition of the modularity is
the sum of the differences from 1 to k,
for everyone like this.
But for every cluster, what do you study?
Is the difference between the,
the fraction of the internal address and
the fraction of those edges to be,
to be inside a random cluster.
If my clustering is far better than
the randomized one the quality is better.
That's why the optimal clustering of
the graph will maximize the modularity.
The interpretation is a modularity or
clustering of a graph.
Actually, it's the difference
between the fraction of all edges
fall into individual clusters and
the fraction of those edges that do,
so If the graph vertices
were randomly connected.
Unfortunately, all the previous and
the currently defined these functions,
the objective functions,
they are normalized, but
to optimize them is NP-hard problem.
That's why you try to get
a greedy algorithm, or
try to get some sub-optimal solutions
instead of getting absolute optimal one.
Now, we look at in a structure,
like in a graph,
you get any two nodes between x and
y, you want to see how to judge xy.
Are, are similar, 'kay?
That essentially can base on random walk
model based, general philosophy defined
is by SimRank, is you try to look at
the structure context similarity.
That means you look at similarity
between x and y, these two nodes.
Okay, for example,
suppose you like two vertices, u and v.
This similarity actually is
defined by its neighbors.
This x, y.
It means x is the, in neighborhood of u,
and y is in neighborhood of v.
Of course, if you use out, out neighbor,
you, you can get a formula
very similar to this one.
Simply says if you look at u and
v, how similar they are,
you just sum up the similarity
among those neighbors.
Okay, then you would normalize them
by us in degrade or vs in degrade.
'Kay.
When you initialize them, you can say eh,
for any particular node, u and v,
if they are, they are the same, they
are identical, then their similarity is y.
If they are not identical,
their similarity is zero.
So, then you can do this
in a recursive way.
Simply says you want look at
any particular node u and
b, how similar they are.
You look at how similar all
those neighbor pairs are.
Okay?
So, That means you can compute S,
[INAUDIBLE] I plus one.
You compute it based on I.
So, of course,
this computation could be rather Costly,
just because you look at the two nodes,
you calculate their similarity.
You look at their neighbors' similarity.
Actually, there could be many neighbors.
And then you look at their
neighbors' neighbors similarity so
that chain can go recursively,
can go pretty long.
So, it's costly to
compute SimRank measure.
I've heard there are some efficient
computation methods, including some
approximate computation methods that
have been proposed and studied.
We'll not get too detailed into this.
[MUSIC]

