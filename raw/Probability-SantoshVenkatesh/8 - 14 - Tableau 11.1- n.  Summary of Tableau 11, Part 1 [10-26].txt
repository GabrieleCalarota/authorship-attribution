Summary of Tableau 11.
The fabulous limit laws.
Part One.
Chebyshev's enduring inequality and
the magisterial law of large numbers.
We begin with a random sample and
at the heart of a random
sample are two key features.
First, the idea of independence,
a rule of products.
Sample elements must be independent,
one from the other.
And second, the pernicious persistence
of subtle bias in real samples.
Where a call bias can erupt via
geographical or sampling bias.
Via refusal bias,
the fish refuses to bite,
we don't want to answer
a telephone poll and wording bias.
Implicit and a probabilistic
analysis of a problem is the,
a priori assumption that a sample's
bona fides have been established.
In other words,
the sample is independent and bias free.
Armed with this understanding of a random
sample, we begin with a model for a poll.
We have an underlying population for
a chance experiment
in which a subpopulation of
proportional size p exists and
we would like to estimate
the size of this proportion p.
Naturally enough, our random
sample constitutes a sequence of
Bernoulli trials whose
success probability is p.
And atarting with the sample, we form
the accumulated number of successes,
S sub n as the number of elements
in the sample of type one.
Naturally enough,
we understand that Sn has a binomial
distribution with parameters n and p and
there are good reasons to believe
that Sn has at its heart,
a reasonable estimate for
the underlying p.
We recall that the expectation
of Sn is n times p.
And therefore,
that leads to a principled estimate,
Sn over n for
the underlying unknown parameter of p.
We have all kinds of good reasons to
believe that this would be a reasonable
estimate.
The maximum likelihood principle of R.A.
Fisher, the expected value,
the concentration of the variables.
Okay.
But
to go from a qualitative to
a quantitative understanding,
we need to put down a mathematical
structure for the problem.
Any estimate when, will incur an error and
because the estimate is transbased,
there'll be a certain with which
you can make renouncements.
Tying these two together
is the sample size,
the amount of data available for
the problem.
What is it that we want?
Given an error tolerance
of up to epsilon and
a desired confidence of,
let us say, 1 minus delta,
we would like the probability that our
estimate Sn over n differs from the true,
but unknown p by more than
epsilon to be no more than delta.
So this is what we would like.
The analytical two, which allows us to
make such guarantees is Chebyshev's
subtle and beautiful inequality,
leading to the law of large numbers.
And what does it say?
It says that the probability that
a relative frequency of successes in
the sample.
Sn divided by n,
differing from p in absolute
value by more than epsilon
is bounded about by
the reciprocal of 4 times
n times epsilon squared.
And for any epsilon, however small as n
becomes large, this ratio goes to zero.
This is the content of
the law of large numbers and
this is responsible for much of
the intuitive content of the theory.
But directly, for our application in mind,
this estimate gives us
a guarantee that we seek.
For any desired confidence
parameter 1 minus delta.
We know that the right-hand side 1 over 4n
in epsilon squared will dip below delta if
n is big enough and that gives us a link
between sample size, error and confidence.
Immediately, we can conclude
that if n is larger than
the reciprocal of 4 times
epsilon squared times delta,
then our estimate provides an error,
which is,
does not exceed epsilon and
the confidence of
this pronouncement is
at least 1 minus delta.
If you promptly put in numbers,
we find, for example,
that we can guarantee an error
of no more than 3% in
the unknown p with
a confidence of at least 95%,
provided n is no more than about 5,500.
And immediately,
that gives us the kinds of guarantees
in applications that we're looking for.
Why do polls work?
Polls work, because we're going to
estimate an unknown p by Sn over n and
this estimate gives us an error of
no more than 3% with a confidence
of at least 95% if n is
just a little over 5,000.
Now this is a remarkable statement.
Consequently, the underlying
population could be enormous.
Could have hundreds of millions of people,
billions of people.
Nonetheless, a fixed,
relatively modest sample gives
us control of estimates over
these large populations.
A smaller variation of this theme
tells us why drug testing works.
Here we work not with one sample,
you see, because the underlying
efficacy of the drug,
the underlying propensity of a drug for
creating side effects, which
are deleterious is not a priori known.
How do we compare what the drug does with
what happens in the population at large?
To do this, we start with
a randomized double sample, one for
a drug, one for a placebo.
And we compare the discrepancy in
the relative frequencies in the two cases.
Now we should understand that here,
one more level of uncertainty
has been introduced.
Now we are comparing two random sequences,
one vis a vie the other.
We naturally anticipate that the sample
sizes needed to give guarantees
of similar area and confidence have
to double and indeed, they do.
So roughly speaking,
if a sample of signs above 5,000 works for
a poll, we would need to double it,
get a sample of about size
10,000 to get similar estimates
in a drug testing scenario.
But again, the sample size is fixed.
And with a relatively modest sample size,
we've got very good estimates.
At the heart, this is why polls work.
This is why drug testing works.
This is the good news.
This is encouraging.
From a pragmatic point of view however,
we might quibble.
Testing 5,000 people
is not at all trivial.
If you ask 5000 people for answers,
yes, we get accurate estimates.
But generating a random sample
of size 5,000 is very expensive.
And of course, generating for
a drug testing trial.
A random sample of size 10,000, one for
each of the drug and
the placebo is now quadrupled the stakes.
This becomes enormously expensive even
at these apparently modest numbers.
And so
from a purely pragmatic point of view,
our next question might be is it
possible to reduce the sample size
numbers further and
still get the same guarantees.
Indeed, it is going to be.
Okay.
But before we turn to see how
to reduce these numbers further,
we'll take a small digression.
Okay.
The law of large numbers is the oldest,
the most venerable and
perhaps the most intuitive of the fabulous
limit laws in the theory of chance.
We've seen it operating almost
serendipitously through
Chebyshev's marvelous inequality
in the context of polls.
We will take a small detour now
to pick up why this law is so
ubiquitous in a rather general setting.
This two done two is anxious to get to
the rest just stay might wish to skip.
The next top low 11 part two,
where you flesh these extensions
of the law of large numbers.
For the student, who wishes to actually
see how this law generalizes and
works in a very broad context, proceed,
but be warned that this is now a dangerous
bend section to be sampled with
discretion if time and inclination allow.
And before plunging in,
it would be advisable to first sample
the dangerous been section
tableau six part part two.
Because I'm going to setup a law
of large numbers in the continuum
to give a flavor of how a general
law can be constructed.

