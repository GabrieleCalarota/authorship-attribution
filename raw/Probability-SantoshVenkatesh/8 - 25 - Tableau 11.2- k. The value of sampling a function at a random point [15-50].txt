So, we are faced with a problem
originating from statistical
physics of evaluating an integral
in a large number of dimensions,
where, apparently,
the problem is physically intractable.
Now, let's remind ourselves
of what the setting is.
So, to begin, we are given a function.
A function is in one or more dimensions.
So, we're thinking of a function in,
let's say, on the unit interval,
on the unit squared, the unit cubed in
three dimensions or more generally,
a unit cube in d dimensions.
And, for regularitiy's sake, let's say,
the functions bounded in absolute value,
the precise size of the function,
the value of the bound, does not matter.
We'll keep things simple and say
the function is bounded by a unit value.
Our objective is to evaluate the integral
of the function, over its domain.
In other words,
over the D-dimensional cube.
A smidgen of vector notation.
We'll simplify our expressions and
allow everything to be put
together in bite-sized pieces.
So, let's restate the problem
in vector notation.
So, the idea again, we're given
a function, let's say in d dimensions.
With variables x1 through xd,
each of these variables takes
values in the unit interval.
To get there, then, the collection of d
variables takes values in what we
call the d dimensional unit cube.
And the usual notation for
this is, the unit,
unit interval with a superscript d
indicating that we have d variables.
Okay.
So,
we are given a function on
the d dimensional unit cube.
And, the function is bounded
in absolute value by, by unit.
What do we need?
We want to evaluate an integral.
And, we will compact the notation of
the integral using this vector notation.
By saying, I want to integrate,
iteratively,
over the d variables in the unit cube.
The function f of a bold faced x which
represents the d variables x1 through xd.
D of bold faced x representing d of
x sub d, dot dot dot through d of x1.
You see here, just a compact way
of writing the same integral
which you saw on the previous slide.
Our objective is to evaluate this and
we've seen that, numerically,
this is going to be very difficult.
The idea that was proposed by Fermi for
Norman and
Ulam, was to try
a chance-driven computation.
Take a gamble, they said.
What does that mean?
So, suppose we start by picking
up a collection of d variables,
by sampling at random from
the d dimensional cube.
What does this mean?
To put an image firmly in your mind,
imagine that you have a three dimensional
cube, you have three variables.
You're going to pick
a point inside the cube.
Eeny, meeny, miny, moe at random,
and out comes a value.
Of course, you want to do this in
a uniform way over the whole cube, so
that, no area is unfairly
represented over the other.
In other words, we have random sampling
according to a uniform distribution.
What does this mean?
It means that,
each individual variable X1, X2,
through X sub D are selected
by independent sampling.
They constitute repeated
independent trials
from a common uniform
density on the unit interval.
Now, this is a setting we well understand.
So, if we have repeated independent
trials, then, the probability law
governing the ensemble of the collection
is given by a rule of products.
You'll recall at the discrete domain,
probabilities multiply.
In the continuous domain,
densities multiply.
And so, accordingly, what this
means formerly in notation is that,
this vector of d variables, boldface x is
governed by a joint density
p of boldface x and
we obtain p as a product measure by
multiplying out the individual densities.
Recall that,
we had introduced a uniform density,
u of x, a function which takes value 1
in the unit interval and 0 outside it.
Each of our variables is governed
by that self-same density.
And so, to, to obtain p of X1 through XD,
we simply multiply out the uniform
density evaluated at X1 times the uniform
density evaluated at X2 and so on.
Now, of course, since the uniform density
takes value 1 only in the unit interval,
this product will be 1,
precisely when all the variables,
individually are the unit interval.
In other words,
this product is going to be
1 exactly inside the unit cube and
0 outside it.
So, this is the uniform
density in D dimensions.
Now, we've got this random
point by random selection.
So, we understand how we
can do this in principle.
All we have to do is generate D
independent uniform variants and
stick together a point in D dimensions.
That seems rel,
relatively simple even in 200 dimensions.
That's not a hard problem at all.
Once you've done that,
you've got a random point in the cube.
What do we do next?
Well, look up the value of
the function that is given to us
at that randomly selected point.
Naturally enough,
we want to call the value of the function
at that random point, let's call that Y.
Okay, so, Y is f of X?
Now, the difference here from
the normal setting is that,
this is Y evaluated at a random point X.
In other words,
Y itself is a chance variable.
Okay.
So, maybe we should compute
things about Y.
What can we say about Y?
So, pick a random point X,
evaluate the function at that point X.
That's one look up in our look up table,
one computation.
This is now a new chance variable,
Y is equal to f of X.
One of the questions we could ask is,
well, what is its expectation?
What is E of Y or what is the same thing?
The expectation of f at a random point X.
Now, the change of variable theorem
immediately rides to our rescue.
Now, how are we going to do this?
Pick any generic point little x
inside the d dimensional cube.
Evaluate the value of the function
at a little point, f of little x.
Weight this value for
the function according to the mass that
is placed by p near little x.
Integrate out of all possibilities as
X sweeps over the d dimensional cube.
And.
There you go.
This is an expression for
the expectation of f at a random point X.
Students might well object at this point
that, well, this is all well and good.
But you've given me another d dimensional
integral and you already told me that it,
computing a d dimensional
integral is going to be hopeless.
So, where do we go from here?
Well, I'm not actually going
to evaluate this integral, but
I do want to analyze it and
see where it leads us.
So, to begin, in principle, in an
integral, like what you see on the right,
the variables x1 through xD,
which comprise the vector boldface X
range over all possibilities
in the d dimensional space.
But, of course, in practice,
where they can range is constrained
by the density function, p of x.
Recall p of x,
is 1 inside the unit cube and 0 outside.
So, this big integral collapses into
just an integral over the unit cube.
There's no contribution
from outside the unit cube.
And inside the unit cube, p of x is
identically 1, and so, this integral
collapses to a slightly simpler looking
integral, still formidable but simpler.
Pause for a moment and take a look
at that expression on the right.
Do you recognize it?
Of course, you do.
This is precisely the value J for
the integral of the function
over the unit cubed.
This is what we want.
Oh, this is brilliant.
Understand what we have done here.
We selected a point at random,
we will evaluated the function value at
that point, but that's a chance variable.
The expectation of the chance variable is
exactly the thing we want to compute, J.
But, you know?
It's not going to be very satisfactory
to just pick out one point and
say, oh, evaluate the function here.
This is your answer.
Why?
Because, yes,
on average, the answer's going
to give you a value of J.
But, it's going to get
scattered all over the place.
And so, naturally,
we want to ask how much is that scatter?
How widely spread out around J is it?
So, naturally enough,
we want to take a look at the variance
of the value of the function
at a random point.
We'll have recourse to the change
of variable theorem again.
Pick a point, little bold-faced x,
inside the unit interval, any point.
Evaluate the function at that point.
That's f of x.
Ask, how far is f of x from
the expectation we have just computed,
which is that fixed but unknown J.
In other words, look at f of x minus J and
naturally enough,
since I don't care whether it's,
you're on the right or left of J,
we're going to look at the square of this.
And so, we're looking at the x,
the square to deviation of f,
at a given point x from J.
We'll rate it in a consular mass
that x puts near that point.
Integrate out of all possibilities,
and now we've got a variance.
Now, we're going to simplify this quickly,
and
I'm going to be greedy and I'm going to be
satisfied with very crude approximations.
But, to begin, this is a process
we've seen before, right?
To try to simplify it,
we'll take a look at the integrant.
And, inside the integral, we find that
there's a whole square in square brackets.
Something like an expression of
the form a minus b the whole squared.
Where a is identified with f of x,
b with j.
Well, we know from elementary school that
a minus b the whole squared is inevitably
a squared minus two a b plus b squared.
And so, we write those down.
But, moment we write that down,
we realize that I can compute the integral
by adding up three contributions.
One from the a squared term,
one from the minus 2 ab term, and
ones from the b squared term.
All right, let's write this down.
So, the a squared term contributes
an integral of the form f of x squared.
The 2ab term contributes well, since 2ab
is identified with 2 times f of x times J,
and 2 J is a constant,
it comes out of the integral.
And so,
I've got 2 J times an integral of f.
And finally, the last term b squared
will correspond to a J squared,
well that's a constant.
Fixed but unknown, yup.
But that constant comes out
of the integral as well and
that's our third set up.
We can simplify this quickly.
It looks like we're making things worse
here by expanding an integral into bigger
and bigger and bigger and messier things.
But, bare with me.
Take a good, hard look at the integral
in the second tab on the right.
What do you say?
Does that remind you of anything?
It should.
It's exactly the expression for
J that we wrote in the previous line.
So, the entire integral in the middle
is exactly the quantity
we want to estimate J.
What about the third term on the right?
We're integrating a density
over the entire range, and
densities are properly normalized to unit.
And, therefore, the integral must be 1.
Or, if you want, you can identify that
p of x is 1 only inside the unit cube.
So, we are integrating d
of x inside the unit cube.
We are looking at the volume of
the unit cube which is inevitably 1.
So, the third term becomes 1.
Now, we see what happens.
The middle term contributes minus
2J times J or minus 2J squared.
The third term contributes plus J squared.
Cancel 1J squared and now,
we've got a slightly simpler expression.
And, integral of f of x the whole squared,
weighted according to p of x from
which you subtract J squared.
A little house cleaning yellow
make things even simpler.
J squared is a positive quantity,
whatever it is.
And I'm taking away a positive
quantity on the right.
If I expurgate,
eliminate the term J squared,
I can only increase the value
of the term on the right.
And so, I've now got an inequality,
the variance of X is no more.
That the integral of the square of f
of x weighted according to p of x.
All right, having made one approximation,
let's go ahead and
make one more and
simplify life even further.
What do we know about f of x?
Well, we know it's bounded and
we presume it's bounded by unit.
Therefore, in absolute value
f of x is no more than 1.
And, therefore, the square of f of x
is also manifest lead no more than 1.
F of x the holds squared is less than or
equal to 1.
If, in the integrant,
I replace f of x squared by 1,
I can only increase
the size of the integral.
And so, I got this is no more than
the integral of just p of x dx.
And what is that?
Well, I'm integrating a density and,
therefore, by normalization,
it must inevitably again be a unit.
What's the moral here?
If I select a random point
in a d dimensional cube and
evaluate the function at that point,
I get a chance-driven variable.
But that variable has expectation exactly
J, the quantity that I want to determine.
And, the expected squared spread,
the variance of that estimate,
is no more than unit.
Okay, this sets the stage for a beautiful
and elegant resolution of our problem.
And, this will come up next.

