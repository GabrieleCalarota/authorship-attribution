Our next application of the principle
of inclusion and exclusion and
this delicate Poisson seat that
we've discovered is classical.
It's historical.
It goes back to the year 1708.
And de Montmort wrote about this,
amongst other problems of his
era that he had considered.
This one is called Le
probleme des rencontres.
And, translated,
it means it's a problem of matchings.
I'm going to phrase it for
you in its classical, whimsical statement.
I'll give you a statement.
Perhaps, a more sober reassessment
of what the problem really is.
Imagine a setting where
we have n individuals and
they're all wearing hats.
I don't know this promptly dates me
because who wears hats these days, but
perhaps, an Ascot, but let's suppose that
we have n individuals wearing n hats.
The hats are taken off, shuffled, and
handed back to the individuals at random.
Now, some questions now arise out of this.
One question could simply be,
if you hand back the hats at random,
what are the chances that
nobody gets their own hat back?
And, a more nuanced
version of this question,
what are the chances that exactly k out
of n individuals get their hats back?
Of course, this is purely whimsical.
I could imagine a group
of sailors on shore leave
who in a fit of playfulness
enter an establishment and
when they leave just grab somebody's hat,
put it on and leave.
And, now, you've got a jumbling of hats.
If you want a more sober version of this
problem, imagine that you have n articles,
each of which is associated with,
let's say a location, a place, an.
If these articles are allocated randomly,
what are the chances that none of the
articles goes to the intended location?
What are the chances a certain number,
k of them, go to the intended location?
So, this is sometimes
called a matching problem.
So, now, let's jump in.
And, doing an analysis of this
problem what is our starting point?
Inevitably, what is a sample space for
the problem?
Now, let's strip the problem of all
the irrelevant trappings, hats and
sailors and individuals and
so on and so forth.
What do we have at heart?
We have n individuals.
Let's label them, give them names,
one, two, three, up through n.
And, what do we have next?
Each of them is allocated a hat.
Let's call the hat allocated
to an individual 1, pi 1.
The hat allocated to individual 2,
pi 2 and so on.
The hat allocated to individual n, pi n.
Why pi?
We'll come to that in a moment why I
chose this particular nomenclature.
So, the first person could
perhaps get the third hat,
in which case pi 1 stands for 3.
The second person perhaps got the eleventh
hat, in which case pi 2 is 11.
And, perhaps the last person got
the second hat, in which case pi n is 2.
What can we discern about
the numbers pi 1 through pi n?
Well, each of them is a number
from 1 through n, and
no two numbers are the same.
The same heart can't go to two people.
In other words, we've discovered
that a sample point of this
experiment are permutations of
the numbers 1 through n, and
that explains the nomenclature,
why I chose pi, for permutation of course.
Each permutation then gives
rise to a sample point.
Remember to specify a sample point,
you have to give me an object
whose specification eliminates all
chance connotations for the problem.
The moment you tell me all the values pi 1
through pi n, I know precisely which hat
each of the individuals received, and
there's nothing now left to chance.
And, therefore,
these are exactly the sampled ones.
How many permutations are there?
Well, naturally in factorial.
So, this is a finite setting.
But, the number of sample points
grows very, very, very quickly
because the factorial function grows
quite rabidly as it increases.
Very well, so,
at least now we understand the sample
points are very simple to characterize.
They're given by permutations
of the numbers 1 through n.
What is our next question?
Oh, inevitably,
what is the probability measure?
And, of course, in this setting,
there is no reason to believe that any one
permutation is to be favored
over any other permutation.
And, therefore,
we have a combinatorial setting, where,
to each permutation we
allocate equal mass.
And, what is that mass?
Well, the reciprocal of the number of
terms that are in the sample space,
the size of the sample space.
In other words, each permutation is
allocated atomic mass 1 over n vector.
So far, so good.
This was not very hard at all.
But, of course, the next question,
the million dollar question,
what are the events of interest?
Well, naturally, whether
the individuals get their hats or not.
So, let's promptly put in place some,
some terminology, some notation here.
Let A sub one denote the event that
the first individual got his hat back.
In other words, pi 1 is 1.
Let A2 be the event that the second
individual got his hat back.
And, generically, let A sub j be the event
that the jth individual got his hat back.
We should very quickly take stock for
a moment.
What is an event?
Well, an event is a subset
of the sample space.
What are the sample space?
Well, systems of permutations.
What n is the event Aj?
Aj, then, corresponds to that
subset of permutations where
the j at the element of
the permutation was j itself.
In other words, the hats got spread out,
but the j of hat stayed where it belonged.
All the other hats could be assigned
willy-nilly in any way whatsoever.
And, therefore,
we identify with the event Aj,
those permutations where the jth element
of the permutation was j itself.
Okay, this is clean and simple.
Now, we're making progress here.
Before we try to do an analysis,
we should take a step back as it were and
take a look at the situation
that is developing.
These events are quite simple
to characterize individually.
The events seem to have
a certain symmetry property.
That is, every individual, as far as
their hats are concerned, has the same
look at the situation in some loose
sense that every other individual does.
The random allocation of hats is
conferring symmetries upon the problem.
But, these events are not independent.
And, to see why this is the case,
all we have to do is consider the simplest
of the settings where, say, n is 2.
If you only had two individuals, then,
if the first individual gets his hat,
in other words, A1 occurs.
That forces that the second
individual also gets their hat.
In other words,
when you have two individuals,
knowing A1 tells you everything about A2.
Of course, so these events
are now tightly coupled together.
The saving grace here is
going to be the following,
that if we have a large
number of individuals.
Then saying something about
one of them has a very modest,
perhaps negligible effect on
what happens to the others.
In other words, we're going to
discover that these events are only
weakly dependent.
And that is going to set the stage for
the emergence of what Poisson said.
But we are getting ahead of ourselves.
So, let's put in place some more
terminology, some more notation.
Let's promptly write down X sub j to
be the indicator for the event Aj.
X sub j is 1, precisely when the jth
individual gets his hat and 0 otherwise.
And the events that we are interested in.
First, that nobody gets their hats.
Well, A1 does not get his hat if
the event A1 complement occurs.
A2 doesn't get his hat if the event
A2 complement occurs, and so on.
The event that nobody gets a hat,
of course,
is the intersection of the complements
of each of these events Aj.
In terms of the indicated
variables this says that
the sum of the indicators is sadly 0.
A more nuanced version of this is when the
sum of indicators takes a given value k.
If this occurs, then precisely
k individuals get their hats.
And, of course, we'd like to try
to compute such probabilities.
So, let's start.
Naturally, our first question now is,
can we set up a proper inclusion
exclusion formulation?
And for this, we're going to have to take
a look at the inclusion-exclusion sums.
George Boole would recommend that we start
with the simplest version of the problem
that we don't yet understand.
And following his sage advice, yet
once again, let's start with the simplest
of the inclusion-exclusion sums,
that corresponding to a single event.
What can be said with the first term?
So pick a generic event, let's say A1.
What is the probability that A1 occurs?
Remember, A1 was the event that
the first individual got his hat back.
It is random allocation.
Of course, this is the probability of
that subset of n-tuples where the first
element is locked in at 1.
And all the others can be
anything else they want.
How do we compute this?
Well, the continuality of the event A1,
the size of the event A1,
the number of permeations that trigger
A1 is easy enough to characterize.
Remember, the first element is fixed at 1.
The first hat goes back to its owner.
But then you have n minus 1
remaining hats to allocate, and
they can allocated willy nilly.
So there are exactly n minus 1 factorial
sample points in the event A1.
Out of a total of n factorial sample
points, we have a common factorial
setting, and therefore the ratio
blacks out as it must to 1 over n.
The first individual has a chance
1 over n of getting his hat back.
Of course, the moment you,
you run down this particular analysis,
it's clear that exactly the same
analysis works for any individual.
The probability that A2
gets his hat to the game,
1 in n, because the second element
of the permutation is held fixed and
the remaining n minus 1
elements are allowed to vary.
And so on for any individual.
And therefore the inclusion-exclusion sum,
S1,
corresponding to singletons
is simplicity itself.
There are n possibilities for
the singleton, n ways of selecting
the individual who you're looking at.
For each such individual,
the chance of getting his hat
back is 1 over n and therefore,
the first term in the inclusion-exclusion
formulation s1 is exactly 1.
Okay, this was perhaps too simple.
We should look at a slightly
more complex setting to see
how we can settle our intuition.
So, let's look at the,
the case of the second term in
the inclusion-exclusion sum.
This is going to require pairwise
intersection probabilities, and
let's start with one for definiteness.
Lets look at the first two individuals,
one and two.
What is the chance that both
of them get their hats back?
What is the probability
of A1 intersection A2?
What does that event mean?
Well, it means that we're looking
at all subsets of permutations,
where the first two elements
in the permutation are fixed.
The first element has to be 1,
the second element has to be 2,
and the remaining elements
can vary arbitrarily.
And therefore A1 intersection
A2 has got exactly
n minus 2 factorial elements in it.
And therefore the probability of of this
event is n minus 2 factorial divided by n
factorial, and that gives you
the reciprocal of n times n minus one.
And some of you might recognize that
in the denominator what I really
have is a falling factorial.
It is n to the two folly.
Okay, a fancy way of describing
a simple object, but bear with me.
Okay.
Now once you've done this,
it now becomes clear what we have to do.
The second inclusion-exclusion sum, S2,
sums over all pairwise intersections,
but every pairwise intersection behaves
exactly like A1 intersection A2.
Each of them has exactly the same
probability of occurrence.
And therefore S2 is the number of pairs,
n choose 2 times the probability that
any particular pair of hats is returned.
And this gives us 1 over 2,
half 1 over 2 factorial.
Having seen the cases or the first and
second terms in
the inclusion-exclusion sums,
we're now coming to a right understanding
of how this thing is going to play out.
Let's very quickly jump
forward to a generic term,
say the jth inclusion-exclusion sum.
[INAUDIBLE] Means that they are going
to look at j-wise intersections
of the event at hand.
The events being, of course,
that the individuals get their hats.
Let's pick one particular j-wise
intersection for definiteness, so
we can get an understanding of how
this thing is going to play out.
So lets say that we pick sailors, 1, 2,
3, up until j, individuals 1 through j.
And ask, what is the chance that
all of them get their hats back?
What is the probability that these j
specified individuals get their hats back,
1 through j?
The story now is clear because this
means the intersection of A1 through Aj
corresponds to exactly
the following event.
All those permutations where
the first j elements are fixed.
The first element has to be 1,
the second element has to be 2.
The jth element has to be j.
And the last n minus j elements,
can be anything they want.
And so this immediately gives rise, so
a probability given by n minus
j factorial over n factorial,.
Cancel common terms in numerator and
denominator and
we discover a falling
factorial in the denominator.
The probability that j specified
individuals get their hats back
is the reciprocal of the falling
factorial n to the j falling.
With this in hand, we are ready for
the jth inclusion-exclusion sum,
let's just write it down.
Again, the symmetries in the problem tell
us that these probabilities are unaffected
by whichever group of j
individuals you choose to look at.
There are n choose j such
groups of individuals and
therefore the jth inclusion exclusion
sum facts out to be simply,
elegantly, beautifully 1 over j factorial.
We can't really have hoped for
anything much cleaner than this especially
when we started out the setting.
Where there was a worry that
the lack of independence in
the events A1 through An was
going to cause trouble.
No matter, we've got a simple
reciprocal factorial capturing
these inclusion-exclusion sums
that's promptly put it all together.
Let's begin with the simplest question,
what is the chance that none of
the individuals get their hats back?
So remember,
A1 complement is the event that the first
individual does not get his hat back.
A2 complement the event sadly that
the second individual does not get
his hat back.
A n complement the event that the nth
individual does not get his hat back.
The probability of
intersection of all of these
is given by the basic
inclusion-exclusion alternating sum.
We promptly put in our recipe for
factorials in here and now we're
got a plain and simple expression.
S0 is 1 by definition.
S1 is also 1 but we're going to
write it as 1 over 1 factorial.
And then we have a 1 over 2 factorial,
a 1 over 3 factorial and so
on with alternating
signs in front of them.
Does that remind us of something,
of course the numbers we have these
factorials in the denominator.
That is going to start suggesting
an exponential function
in which case we are looking
at a power in the numerator.
Well, what is a power?
Oh, this is the simplest of possibilities
we are simply taking negative
1 to integer powers.
Minus 1 to the power 0 is 1.
Minus 1 to the power 1 is minus 1.
Minus 1 to the power 2 is plus 1.
And so
negative 1 to integer powers is going to
give you alternating plusses and minuses.
And so what you've got is
a truncated exponential series
whose exponent whose parameter is minus 1.
But the factorials in the denominator
tell us that this series
is going to converge fearfully fast.
And therefore, these truncations
are already going to be very,
very close to their limiting object.
And therefore,
what is the limiting object?
It is just e to the power minus 1.
Very approximately 1.
In words, the chance that
nobody get's his hat back is
approximately 1 3ds, is the power minus
1 if you want to be more precise.
And this approximation is good even for
very small values of n because of
these factorials in the denominator.
Even for an equal to 4, 5 or
6 you'll find that the approximation
minus 1 is very, very
close to the actual inclusion-exclusion
alternating form that I have.
And so
we've discovered something beautiful,
there's a slogan that
De Montmort discovered in 1708.
The probability of
the obverse of this event.
The probability that at least
one individual gets his hat back
is approximately two in three.
1 minus e to power of minus 1 and
this is almost independent of the number
of people that you're looking at.
Even for very small you're going to
get this as a very good approximation.
This is unexpected perhaps,
like that is wow, that is neat.
I, I didn't quite anticipate this,
especially if we have a large
haggle of sailors and
you're tossing hats to them at random.
Wouldn't anticipate that something so
simple and
elegant popped out of this by
looking at an asymptotic analysis.
Let's look a more gentle case and it's
not going to be any much harder at all.
So let's just but
these inclusion-exclusion sums together.
What's the probability that
exactly k individual get hats?
We've gotted you inclusion-exclusion
alternating formulation.
Plug in the fact that
these inclusion-exclusion
sums are reciprocal factorial there
now is just a question of algebra.
Write it all out, you find that
various terms cancel in the numerator,
denominator.
Put it all together,
we find that every one of these terms
has got a reciprocal k factorial in it.
Factor it out, and then inside
the round brackets you discover,
now wait minute, I've just found
a truncated exponential series again.
And that entire thing in the round
brackets is going to be approximately
e to the power minus 1 and therefore.
There we go, Bob's your uncle.
The probability that k out of n
individuals get their hats and
nobody else does is approximately e to the
power of minus 1 divided by k factorial.
And this result is going to be valid
almost independently of the size of the,
you know,
very modest small values are there.
Now, the way I've developed it,
it looked accidental but
of course it's not accidental.
Because we realize, wait a minute,
let's take a look at what those
inclusion-exclusion sums really are.
They're of the form of a reciprocal
factorial, 1 over j factorial, okay?
But 1 is the same as 1 to the power j so
what we've got is these
inclusion-exclusion sums
are the form of a fixed number to a power
1 to a power divided by a factorial.
But this is exactly the province of
the cross on set and so here's our slogan.
The number of individuals who
receive your hats even for
modest values of n is covered by
a Poisson law with expectation 1.
I ask you to step back and savor such a
beautiful and elegant result which emerges
so unexpectedly from what looks like
a complicated mix and mess of a problem.
This also points out why we are really
doing a metaphorical sieve.
A lot of individuals don't
get their hats back.
In fact, most individuals don't.
The chance of getting your hat back,
in from modest values of n is
quite small it's only 1 in n.
This is a rare event and what governs
the number of rare events that occur?
It is a Poisson distribution and
in this case a Poisson distribution
with expectation unit.
Let me go on and
give you another classical example.
I must apologize in advance,
I'm feeling whimsical today.
This is going to be our next lecture.

