We can edge up to the continue via a
limiting sequence of discreet experiments,
much as we do in the calculus.
We saw for example,
by tossing a coin repeatedly,
in the limit we engender a continuous
space, the unit interval.
Now one can imagine other
experiments which don't quite so
gratuitously, pose at
equal mass everywhere.
And this leads to a very general framework
for experiments in the continuum.
In all such cases we can think
of these experiments as arising
as limiting sequences of
discrete experiments.
But it is frequently going to be
the case that it is much simpler and
easier to deal directly
with the continuum.
This process is much as we do in calculus
where we look at discreet approximations
to sums, and in the limit,
we end up with the Riemann integral.
These two intergal come to no harm if
she bears firmly in mind that all that
is happening when we transition from
a discreet experiment to a continuous
experiment is that discrete probability
masses segue into mass densities.
Where you have mass per unit length
spread over a region and that sums
segue naturally into their appropriate
intervals, all else carries through.
Positivity, normalization, and
most importantly, additivity.
So, let's launch directly into
the setting where we have an abstract
generic probability density function, for
instance, like the one I've shown you.
I've called this function p of x
to connect it to the idea of
a probability mass function.
Recall for a mass function the argument
was discrete, a set of integers.
But now we're going to deal with
a function of a real variable.
The implicit sample space here
is the continuum, the real line.
P of x now is a function of x if it
is a quantity to bonafide density.
What properties should it have?
Well naturally, it's a mass density and
therefore the function should
be non negative everywhere.
It would make no sense to
say that the mass density
is negative at a certain point.
We do not countenance
negative probabilities.
And it has to be properly normalized.
What does normalization mean?
In this case that the limiting sum,
the integral of the function is unit.
Another way of thinking about it is
that the area under the curve is unit.
Bear in mind here that
your p of x connotes
a mass density in units of
probability mass per unit length.
D of x multiplying it is
an infinitesimal length.
And we have a mass of
a length times length.
We end up in fact, exactly with mass.
Any genetic function, p of x,
which is nonnegative everywhere and
has got unit area under the curve.
Constitutes a bona fide probability
density for some chance experiment.
Now, once you have a density, what kinds
of chance experiment are we dealing with?
Let's take stock.
The underlying sample space is in
fact a continuum, it is a real line.
And in settings like this,
tradition compels us to use a letter
rather different from capital omega.
We use blackboard bold R to
denote the real line, and
of course that just means all real numbers
from minus infinity to plus infinity.
We think of this as a limiting experiment.
We think of this, for example,
as connoting the amount of rainfall in
a given day, the rate of sunspot activity,
the percentage of contaminants in
a reservoir, and so on and so forth.
All of these can be thought of
as limiting discreet experiments
giving rise to a continuous value.
Here is our sample space.
The sample points here are real numbers,
I should to adhere to our conventions,
call it lowercase omega.
But convention in this setting compels
us to use letters like x to denote it.
And so
by convention we use uppercase letters,
typically R, S, T, U, V, W, X, Y, Zed.
To represent chance outcomes
which are numerical.
A generic such letter of course is upper
case X and this will represent for
us a generic outcome
of a chance of an experiment which gives
rise to a continuum of possible values.
Since we now are dealing with a numerical
value it is natural to call such
a sample point a random variable.
All right, so upper case X is a generic
random variable for a chance experiment
described by the probability law which
is induced from that density of value.
What are the events of interest?
Well, the basic events
are naturally intervals.
So an interval of event a is
a form of an interval from a to b.
Now, this is a subset of the real life,
a bona fide event, but the context
here of an experiment which gives rise to
a real number, adds color to the picture.
And we can describe this event in
notation graphically by saying,
this is the event which corresponds
to all outcomes, random variables,
which take values between a on
the one hand and b on the other.
And this kind of explicit writing out
of the event is visually appealing and
of course it connects
well with our intuition.
We now want to assign
a probability measure to this.
Probabilities here
are identified with areas, so
we're interested in the probability
of the interval from a to b.
Or more verbosely,
the probability that a random variable,
the outcome of a chance experiment,
lies between a and b.
And we simply identify it
as the area under the curve from
a to b of the density function.
This is exactly the kind of process we
went through with a uniform distribution.
We can think of the integral of
the right as eliminating Riemann sum by
taking the interval and as we did before
splitting it up into infinitisimal pieces.
Adding up the probability
contribution from each interval in
into infinitisimal piece.
Which is of course
the height of the rectangle.
A tiny rectangle centered at that point.
Add them up, go to the limit,
you get an interval.
Now at this point it's much
easier to deal with the interval.
What about more general events?
Well a more general event is obtained
by taking various subsets and
various intervals on the line and
stitching them together like this.
So now we are going to have basic events
which engender more complex events
by a process of unions, intersections,
set differences, and so forth.
Let's say A is one such generic event.
I've shown it to you by three
shaded areas under the curve.
We're interested in the probability that
your random variable a chance outcome
lands in the set A.
And how do we compute the probability?
Simply by stitching together areas under
the curve and appealing to additivity.
The area under the curve of p of x in the
region A is what we denote as the integral
over A of p of x and this exactly gives
right to the probability of interest.
And all we've done here is we've taken a
density with units of mass per unit length
multiplied by infinitesimal lengths and
added to get a total probability mass.
Naturally a question now is, at this level
of abstraction, this is all well and good.
But what kinds of functions
represent densities in practice?
What kinds of functions arise
out of real chance experiments?
We shall turn to this next.

