So let's start with a fairly
artificial example.
All right, this is designed purely to
run through the algebra to make sure
we understand what
the definition is saying.
So, let us start with a sample
space of ten elements.
Lets call them one, two,
three, four, up to ten.
If you wish, you can think of this as an
experiment where we throw a ten sided die,
with the numbers one through
ten inscribed on the faces.
Naturally enough, for
the simple stylized experiment,
I'll assume that this ten sided
die has no bias towards any face.
In other words, we are randomly selecting
an element from one through ten.
Or in still other words,
the mass function associated with
the probability measure for
this problem is uniform.
It puts equal mass of one and
ten on each of the ten faces.
We've taking another
liberty with notation, and
we've done this again in the past.
To specify the probability measure in
these simple finite cases, as we know,
all we require is the mass function which
gives us the atomic probabilities for
every singleton set.
Formally, we should say,
here is a singleton set, the element i.
And then there is a set.
The probability, naturally enough,
we put round brackets around that set.
That's how we define probability measure.
The probability measure is a set function
which takes a set as an argument.
But this unnecessarily
complicates our notation.
Now we have brackets curly and
round floating all over the place.
Where there's no chance of ambiguity,
we'll simply drop the round brackets.
And we'll say the probability
measure P of the set
which arises in the curly
brackets following it,
in this case a singleton set,
i, is 1 in 10.
Now we've got the sample space,
we've got the probability measure.
To compute generic probability,
we simply appeal to additivity and
simply sum atomic probabilities.
What are the events of interest?
Well, let's put together just
this grab bag of events.
There's nothing particularly insightful
about the way I've constructed
these events.
They're just sets.
The target event A is a set comprised
of the elements 2, 3, 4, 5, 6.
And I'm giving you three other
possible events which could serve as
ancillary events for the purpose of
communicating side information about
what the experiment actually happened.
So I've given you three more events B,
C, and
D with various subsets
as I've given you there.
And your problem now is to evaluate
the conditional probabilities of A given
that B has occurred, A given that C has
occurred, A given that D has occurred.
And finally, A given a composite event
B intersection C has occurred.
Pause the lecture and
work your way through this to allow you to
settle concepts before you turn to
the next slide, and to the answers.
All right.
Now that you are back,
let's verify what you've got.
We should begin by writing down
the various events of interest and
their probabilities.
So let's start with a sample space.
The sample space omega comprise,
is comprised of the universal set
of the integers 1 through 10.
Normalization of probability measure
tells us that this entire set is certain.
It has got probability one.
The target event A is comprised
of the elements 2 through 6.
It has five elements.
We are dealing with
a combinatorial setting.
So for the chance of A by
additivity is 5 in 10, or 1 in 2.
B, C, D, and B intersection C are the
ancillary events of interest to us,
which are casting side information
upon the problem at hand.
Each of these events now can be
identified with a particular subset.
And each of these subsets then will
give rise to a particular probability.
6 in 10 for B, 5 in 10 for
C, 4 in 10 for D.
And B intersection C is comprised
of three elements, 4, 6, and 8.
And therefore,
it has got a chance of 3 in 10.
Good.
Now we've got the target event,
the ancillary events of interest, and
now we need to compute
further events which reside
in the intersection between the target
event A and the various ancillary events.
A intersection B, A intersection C,
A intersection D, and
A intersected with
the event B intersection C.
But again,
intersection is an associative operator.
The order of operation does not matter.
You just simply write it down as
A intersection B, intersection C.
And now we have four more sets and
their associated probabilities.
We're now all ready and equipped to write
on various conditional probabilities.
So let's start by writing down the first.
The probability of A given B.
Then the probability of A given C.
The probability of A given D.
And the probability of A given the
occurrence of the event B intersection C.
Okay, at this point there's,
there's nothing very deep about these
manipulations, it's purely arithmetic.
And whatever numbers turn out, turn out.
At this point we have just churned
through the definition mill,
if you will, and
provided numerical answers.
But we should pause, stop,
take a look at these answers and
see whether they cast any
light upon the situation.
Whether they tell us something more about
what our formal definition is doing.
And when you look at these
particular numerical answers,
we notice something interesting.
We start with the unvarnished
probability of A being 1 in 2.
We discover that conditioned
on the occurrence of B,
the chance of A has not changed.
B, apparently,
is not altering the chance of A.
It is still 1 in 2.
But conditioned on D,
the chance of A has decreased.
In fact, it's 0 now.
Whereas, conditioned on C,
the chance of A has increased to 3 and 5.
Very interesting.
What about conditioning
on B intersection C?
Remarkably, perhaps, the chance of A has
now increased even further to 2 in 3.
Wow, that is very interesting.
This is a political edge, and we need
slogans to encapsulate such an edge.
And so here's a slogan for us.
A, a salutary observation that
is always worth keeping in mind.
Our definition of conditional
probability is very, very simple.
It's a ratio of an intersection
probability to an unvarnished probability.
But, conditional probabilities
can affect native
unvarnished probabilities
in unanticipated ways.

