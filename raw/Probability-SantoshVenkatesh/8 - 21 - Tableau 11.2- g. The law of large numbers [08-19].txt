We are now equipped to deal with
the law of large numbers in
a fair amount of generality.
We begin with what
the inequality of Chebyshev says
in the case of a sum of
independent variables.
So let's take stock.
Chebyshev's inequality says that for
a generic chance variable X,
the probability that
the absolute deviation of X
from its expected value,
exceeding any given number tau is
bounded about by the ratio of
the variance of X to tau squared.
Now bear in mind that this is true for
any chance variable X.
In particular, what if we replace X by Sn?
Then all we'll have to do in
Chebyshev's inequality is strike out
some real references to X and replace
them all by S of N and we've got now
a version of the inequality for
a partial sum of independent copies of x.
Now let's focus on this.
This says something about the probability
of the absolute deviation of
Sn from its expected value.
And here is where additivity comes in,
triumphantly.
So let's take stock of
what the key issues are.
So to begin, we are starting with
repeated independent trials.
A chance variable, x, with expectation mu,
with area sigma squared.
And x1, x2, x3, and so
forth are independent copies of x.
We form partial sums,
Sn being the enif of these sums being the
sum of the first n coordinate variables.
At the moment we have a sum,
additivity comes into play.
To begin expectation is additive
without qualification and
therefore the expectation
of Sn is just n times mu.
And since the summons are independent,
variance also is additive and
so the variance of Sn is
n times sigma squared.
It is important to keep in mind
that the variance keynotes and
expected squared spread around the center.
The actual spread therefore is captured
by the square root of the variance or
the standard deviation.
And so, what can we say about
the standard deviation of Sn?
It is exactly the square
root of n times sigma.
Now, observe that both the expectation and
the standard deviation increase
unboundedly as n increases.
But it is important here to keep
an idea on the proper scale.
The expectation increases linearly with n.
The standard deviation, or
spread around the expected value,
increases only as a square root of n.
And this has got very
important consequences.
Consider, for instance,
when n is 100, the expectation of,
of S100 is 100 times
the expectation of one variable.
But the variance of S100 is
also 100 times the variance,
the standard deviation is only square
root of 100 times the standard deviation,
or ten times sigma.
So the expected value is moved by 100, but
the spread has only increased ten fold.
What if area is 10,000?
Though the expected value
is moved 10,000 steps,
the spread has only
increased a hundredfold.
And this suggests that we're
getting an increasing concentration
right around the center.
The spread around the center is increasing
much slower than the rate of movement
of expectation.
So let's focus on this and
now immediately go right,
try to write it down, a probability for
a deviation from a mean.
Remember, our estimate for the mean,
the expectation is Sn over n.
And what we'd like to know is, what
are the chances that Sn over n deviates
in absolute value from mu by
even a tiny amount epsilon?
Now the first thing we should
do is simplify the fractions by
clearing the denominator,
multiply throughout by n.
And the event that Sn over n minus mu in
absolute value exceeds epsilon is exactly
the same as the event that Sn minus n mu
in absolute value exceeds n times epsilon.
Now focus on the right-hand side.
The term n times mu is exactly
the expectation of Sn by additivity.
And we may identify that n epsilon
with the deviation parameter
tau in Chebyshev's Inequality.
And so, we just write down the answer.
This is bounded about by
Chebyshev's Inequality,
by the ratio of the variance of Sn,
2 tau squared but
tau is N times epsilon so
we get N epsilon the whole squared.
Additivity of variance because the sum
amounts are independent says the numerator
is N times sigma squared and
we find that there is a factor N in common
and numerator denominator cancel that.
And we've got an elegant and
simple expression.
Let's focus on this inequality.
It is difficult to ask for something
much simpler and compact than this.
The probability that an absolute
value Sn over n deviates from mu but
as small in amount as epsilon or
more is bounded about
by sigma squared divided
by N epsilon squared.
Now sigma squared is a variance of
the underlying chance experiment.
Epsilon squared is your fixed
error parameter squared.
These are fixed constants
in our application.
Remember we're looking at a whole
sequence of partial sums.
What happens to the right-hand side as
n becomes larger and larger and larger?
Well, eventually the right-hand
side tends to zero.
And this is content of
the weak law of large numbers.
So we start with a sequence of
independent repeated trials,
where each trial has got an expectation
mu, variant sequence squared.
Epsilon is your tiny, fixed deviation
parameter, your allowable error tolerance.
We form partial sums, and
the weak law of large numbers says
that these partial sums are increasingly
concentrated at mu in the following sense.
At Sn over n,
deviates from mu by even a tiny amount,
where the probability then goes to
0 as n becomes larger and larger.
And of course we should conclude this with
a nice little slogan, a pithy slogan,
which captures the essence of this
mathematically precise statement.
And here it is.
The probability that Sn over n
deviates from its expected value mu by
even a tiny amount is small provided
only that a sample size,
n, is large enough.
Or another way of saying this
is in a probabilistic sense,
Sn divided by n is a good estimate of mu.
Does that look familiar?
Of course it does right?
This is exactly the passage we went
through when you tried to defend why
sample means were good estimates of
population proportions in a pool but
now of course we have
it much more generally.

