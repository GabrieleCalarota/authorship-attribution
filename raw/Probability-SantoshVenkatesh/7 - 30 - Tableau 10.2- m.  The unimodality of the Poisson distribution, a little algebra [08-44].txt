Before we proceed with the analysis,
let's take quick stock.
Remember, we are starting with
the personal probability more elaborately.
k runs over all the non negative
integers for k times p of k.
And we murk this to get another
identity for k squared time p of k.
We'll put both of these
to good use shortly.
Now let's come back to
our problem at hand, and
imagine a chance experiment
which produces integer value x.
According to poisson distribution
parameter lambda what does this mean?
It means that the mass function
attached to x is given
by the poisson probabilities p of k or
more elaborately p of k and lambda.
On the right of your screen you see
a visualization of the distribution for
the case lambda is equal to 5.3.
Again we notice the unimodal character,
and
that the peak occurs in
the immediate vicinity of lambda,
in this case at a value k equal to 5,
just to the left of lambda equal to 5.3.
We see in this picture, again,
visible evidence that there is a centering
character, and so, appealing to
the kind of analysis we did for
the binomial, we come up with the idea
of a probabilistic center of mass,
or, more formally a mathematical
expectation for this variable.
This is, of course, what more informally
we call the mean of this variable,
or the expected value, or, of course,
the probabilistic center of mass.
But let's stick with the,
with the formal terminology,
this is the expectation
of a Poisson variable.
And the intuition here is that as before
the margin [INAUDIBLE] with
weights distributed on it.
What we are trying to
do is find the fulcrum,
the center of mass,
where things are in perfect balance.
How do we define this?
Well, simply enough we'll
call it as before, e of x,
the expectation of your
poisson variable x.
It is frequently abbreviated by
the lower case greek letter mu and
it's defined to be a weighted sum.
The values that the variable takes,
the integer k,
weighted by the mass attached to k.
Of course this is a natural extension
of the idea of an arithmetic average
to a setting where the weights
are of different values.
Observe that in the expression I've
hidden the parameter lambda, so
lambda is a fixed parameter,
which is informing this distribution.
Of course I expect the sum on the right
to somehow depend on the lambda, and
this is what we're going
to attempt to discover.
Now each term on the right
hand side is of the form
of an integer k multiplied
by a probability p of k.
Our first basic Poisson immediately
tells us how we can resolve this.
k times p of k must be
lambda times p of k minus 1.
And immediately simplifying, we observe
that 0 times p of 0 is, of course, 0.
1 times p of 1, with k playing the role
of 1, is lambda times p of 1e minus 1.
Two times p of two, where now k is two,
becomes lambda of p of two minus one,
and so on down.
Every term on the right now contains
the fixed parameter lambda which we factor
out of expression very naturally, and
then what remains we collect inside
square brackets, and we realize that what
is in the square brackets is a sum
of all the Poisson probabilities.
But of course, Poisson normalization tells
us, that a sum must indeed be 1, and
therefor the entire expression
on the right devolves and
gives us very simply just
the parameter lambda.
It's hard to imagine
a much simpler expression.
The entire [INAUDIBLE] here involves
just breaking down the probabilities.
Churning through the Poisson identity and
there you go.
What was your [INAUDIBLE]?
You have a simple expression.
Of course, when you try to write
these down compactly, it's simpler
to compact these various terms of
the sum into a summation expression.
And we could all these down in a very
compact mathematical notation as follows.
The expectation of x that we
call mu is the sum over all k,
of k times the weights b of k.
Replace k time p of k by the possion
additive lambda p of k minus 1.
Factor out lambda, the remaining
possion probability is at 1 and
we get back lambda.
What have we discovered?
If x is a chance variable.
Distributed according to a poisson
distribution with a parameter lambda
then the expectation of x is exactly
that poisson parameter lambda.
And therefore it is
completely permissible and
even encouraged to say that we have a
poisson variable with expectation lambda.
In place of saying that
we have Poisson variable,
whose Poisson distribution comes
equipped with for the parameter lambda,
knowing the expectation tells us
everything about the parameter lambda, and
therefore implicitly tells us everything
about the form of the weight functions,
p of k and lambda.

