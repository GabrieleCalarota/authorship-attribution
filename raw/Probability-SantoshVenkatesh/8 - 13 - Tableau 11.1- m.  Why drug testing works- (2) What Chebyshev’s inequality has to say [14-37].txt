The event at hand on the left
on the top of the screen is,
implies at least one of
the two events on the right.
So we want to compute a probability.
Monotonicity of probability measure tells
you that if you have a bigger event,
you get a bigger probability.
So the probability of
the event on the left
is certainly no larger than
the probability of the event on the right.
But on the right, I got a union of two
events, call them temporarily A and B.
The probability of the A union B,
on the other hand,
by what we call Boole's inequality.
And this will take you right back to the
basic properties of probability measure in
tableau five.
By Boole's inequality,
the probability that one or the other or
both event occur is bounded a bow by
the sum of the event probabilities.
Identify A with the first event
on the right, B with a second
event on the right, and
we immediately get an inequality.
Oh, this is lovely, okay?
And it looks like we
have complicated matters,
but we've really, really simplified it.
And one more step will make
it completely transparent.
We've got two probabilities on the right.
How do you deal with this?
Well, we realize that both Sn and
Sn prime are statistically the same.
What I mean by that is,
they're both governed by the self
same binomial distribution.
And if that is the case, the probability
law governing each of them, Sn and
Sn prime, is exactly the same,
which means the probabilities being
queried must be identically equal.
And therefore,
we simply get twice the probability, or
let's say one of them, say the first one.
This is getting simpler and simpler.
The expression on the right,
the probability,
looks very familiar and for a reason.
We've seen this before.
It asks, what is the probability that
the accumulated sum of Bernoulli trials,
the accumulated successes differs from
its expected value by some amount.
But this is exactly the probability in
the tails of the binomial distribution.
We know the distribution is going to
get concentrated around the center, so
this probability should be small.
And indeed,
Chebyshev's inequality gives us
an explicit characterization of how small.
So we just run Chebyshev's
inequality through here.
And here observe that the number delta,
the discrepancy that we know from our
experiment, divided by 2, plays the role
of the error parameter epsilon that
we had in Chebyshev's inequality.
Plug it in, and we've got now
an elegant and simple answer.
The desired probability that
in my gedanken experiment,
the relative frequencies of success
in the two groups exceeds the stated
discrepancy is no larger than twice
a term given by Chebyshev's inequality.
Let's simplify, right.
When you expand our delta to the whole
squared, you get delta squared over 4,
4 cancels out.
And now we've got a really elegant and
simple estimate.
The desired probability is no larger
than 2 divided by n times delta squared.
We'll have to be a very demanding
customer if we wanted something
even simpler than this.
This is beautiful, elegant, and simple,
and immediately it gives rise to
a pronouncement in terms of what
our test is supposed to tell.
Here's what we've got.
A probability of a deviation is no
larger than 2 over n delta squared.
Remember, delta is a known discrepancy
between the patients getting the drug and
the patients getting the placebo.
If you want to, to evoke some of
the notation we've used earlier,
then n times delta plays the role
of our error, n delta is the error.
And the expression on the right should
then play the role of a confidence
parameter, a delta where 1
minus delta has the confidence.
Without further ado,
let's just call it a lower case delta.
This is the confidence parameter.
Now, here is the basic setting.
The estimate on the right
tells us the chances
that random fluctuation gives
you a discrepancy as bad or
worse than what is actually observed.
What can we conclude from this?
We can conclude in two directions.
One, what if this fraction is small,
what if little delta is a small number?
A small value for little delta implies,
for example, let's say little delta is 1%.
It says it's rather unlikely
that chance fluctuations
will give rise to a discrepancy
as big as we have seen.
This then suggests that the drug
is actually causing complications.
It is enhancing the risk.
The chance of getting such a large
discrepancy by accident is small.
So small bound suggests that
there is a serious chance
that the drug is causing complications.
On the other hand,
if the value for the bound,
the confidence parameter, is relatively
large, say, 50%, 40%, something like that.
Then, such a large discrepancy is
what we observe in our patient pool,
is easily explicable by just
random chance fluctuations,
life happens nothing is certain.
Of course we do have to be a bit cautious
with second part of this pronouncement,
you see, because I don't,
have not given you an exact estimate,
an accurate estimate of the probability.
I've given you a bound.
It is conceivable that the bound
is to loose, it's a poor bound.
So a large number might not really
say that the probability is large.
So, if you want to treat that argument
a little further, get a more accurate and
definite pronouncement, then we'll
want to get a more accurate estimate.
I will defer that, right?
But at least, we have now
a suggested direction for interval.
We compute this likelihood, and
it's encapsulated in a fraction.
If a fraction is small,
we have reason to believe that
the drug causes health complications.
If the fraction is large,
we have some reason to believe
that the drug is incidentally
does not do any damage.
And so here we have a slogan.
The smaller the fraction,
2 times a reciprocal of n times
the observed discrepancy squared.
The smaller this fraction is,
the more likely that the drug
in question exhibits significant
enhanced side effects,
for example, cardiac arrests,
of the type being tested.
Again, it is hard to imagine a simpler and
easier pronouncement.
Before we proceed, you know,
we're probably well to do a little sanity
check and a test of your understanding.
So let's try this.
Consider two settings.
One where you have a sample
size of size 100, so
you have a double sample of 100 and 100.
100 given a drug, 100 given a placebo.
And you observe a discrepancy.
So what discrepancy level can
you conclude that there's a 50%
confidence that there are, in fact,
side effects from the drug.
At what discrepancy level can
you conclude that there is
a 90% confidence that
the drug causes side effects?
Do it again if the sample size
is bumped up from 100 to 3,000.
Pause the lecture.
Scribble with it.
Try to get your head
around these concepts,
and see if you can come up with numbers.
Restart the lecture when you're ready.
All that is requisite is to plug
into your favorite calculator,
and the values for
little delta being 1 minus the,
the confidence parameter,
right, and the value for n.
So let's do this without further ado.
Suppose n is 100, and
the confidence parameter is 50%.
In other words, we want a confidence
of 50%, and so delta is now 0.5.
We find that the discrepancy,
big delta, is now 0.2.
What if we increase
the confidence demand to 90%?
In this case,
1 minus delta is 90%, or 0.9.
So little delta is now 0.1.
Plug it into the expression, and we find
that the discrepancy now becomes 0.45.
In other words,
if our conference demands are quite small,
then a relatively small
discrepancy will trigger it.
If our confidence demands are high,
then our discrepancy creeps outward.
What happens if we
increase the sample size?
We go from a 100 to 3,000.
Now, before you take a look at the number,
you know,
we should understand what
increasing the sample size does.
When you increase n, the binomial
gets increasingly concentrated
right around its expected value.
In other words,
it gets highly likely that Sn and
Sn prime both behave very similar to
each other, both close to n times p.
So the difference is going to be
very close to 0, which then makes
even small discrepancies harder
to explain by random fluctuation.
So, a discrepancy of 0.04 is
already rare when the sample
size is 3,000 if you
want a confidence of 50%.
But if you want a confidence of 90%,
it creeps up from 0.04 to 0.08.
Let's step back.
So what have we really done,
what have we achieved here?
We've articulated a systematic process for
checking the bona fides of
a new putative wonder drug.
We're asking, does this drug cause
side effects of a particular type, and
the test involves simply
running a patient pool,
one given a drug, one given a placebo
under suitable safeguards to govern bias.
The test itself is simplicity itself.
All it requires is an understanding
of how much the discrepancy is, and
that's an easy calculation and asking how
much confidence do I need to enact policy.
And out of all of that,
comes an immediate conclusion.
At this point, you know, we should ask,
where do we go now, you know, quo vadis.
I mean, this is interesting,
it's beautiful,
a relatively elementary argument.
Yes, we did work hard to get here.
But if the student will go back and
take a look at it,
she will realize that the mathematics
wasn't that hard, and
step by step it was just essentially
a few simple algebraic steps.
The results of the analysis
give rise to very elegant and
easily implemented policy proscriptions.
We can do polls, and
we can even test drug efficacy.
Now, at this point,
the sample size requisite are modest.
If you remember, when we're doing a poll
to get accurate estimates, say for
about 3% error with a 95% confidence,
we needed by our estimates using
Chebyshev's inequality,
something like 5,000 sample size.
5,000 individuals give us good estimates.
When you move to drug testing,
we were laboring under the problem that
not only did we not know the underlying
success probabilities, but
we were comparing a double sample.
You see,
we were comparing a probability for
the drug with a probability for
the placebo, both of which were unknown.
And so, we end up effectively
estimating the probability for
the placebo while we
are doing our experiment.
We're going to have to pay a price for
this double estimation, and
this price is manifested in the factor
two in the numerator of amount.
So roughly speaking,
our sample size will double
to get the same kinds of confidences
we had when we're doing just a poll.
Which means, that if we needed 5,000
individuals to get a reasonable ac,
reasonably accurate estimate for a poll,
we're now going to need
10,000 individuals.
Now, at one level that's encouraging news,
right?
It's a fixed number.
At another level,
this is very disquieting.
I've told you how
expensive this process is.
10,000 individuals will cost
an enormous amount of money to monitor,
to check, to, to recruit.
And so, there is an incentive here
to see if we can improve the bounds,
reduce the sample complexity
demands on a process.
10,000 is a lot.
Can we bring it down to
something more manageable?
So this is one profitable direction for
inquiry,
and this is going to lead us
to the fabulous bell curve.
That is the subject of our next tableau.
A second direction for inquiry is
Chebyshev's inequality worked
beautifully for the binomial.
I wonder if it per chance
could work in other settings.
This turns out to be a very rich and
profitable direction for inquiry.
And this leads, among other things,
to new procedures and
algorithms for computing very complex,
very large problems.
This will be the subject
of our next mini lecture.

