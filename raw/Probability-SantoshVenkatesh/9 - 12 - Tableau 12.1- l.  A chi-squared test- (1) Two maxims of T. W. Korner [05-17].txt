So now we have come to it.
We are now ready to construct
a principled statistical test
of the bona fides of Burt's IQ data.
What is the key principle?
We've seen this before.
The idea is to compare the actual data
with the results of a given
Duncan probability experiment.
A thought experiment.
And ask are the data
representative of the experiment?
Are they aberrant with respect to
that conceived thought experiment?
We've seen this before in the analysis
of the hot hand phenomenon.
We've seen this in drug testing.
We've seen this in the context of poles.
It's the same idea for
a principled statistical test.
So we are going to compare Burt's
reported IQ data, for adults and
children, with what one would obtain,
this is a thought experiment, from
a random sample generated from a normal
distribution center at 100 with
a spread of 15 or a variance of 225.
To remind you of the setting,
here is this projected random sample.
A sample of size n drawn
from the distribution,
with the sample put into eight IQ bins.
And the number of elements in each IQ
bin will then be binomially distributed
with parameters n and a coin bias
probability p, which depends upon the bin.
P1, p2, p3, to p8.
Okay, now we're armed.
We now know the principle.
We know the underlying statistics and
the probability distributions.
And now all that remains is the execution.
Well, there's still a step to be made.
What is our key observation again?
Remember that each of the values Sn1
through Sn8, a generic value Snk for
bin k, will take values via the,
the law of large numbers.
Near the expectation n times p of k.
And, if we view this properly,
by centering and in the proper scale,
then these values s and
k will behave normally.
Like a bell curve, around the expectation.
This is the key idea.
And so, what is our statistical test?
Well, we want to compare
the values Snk with n times pk.
What should such a test do?
Well, if we only have one bin, it is easy.
This is exactly the context of poles,
and we know exactly how to do this test.
But now we have eight bins.
How do we read this riddle?
What do we do now?
Do we do them one at a time for
each of the bins?
That is surely possible.
But we're going to run into a difficulty,
a subtle difficulty here.
And that is, bear in mind,
that every time we do an estimation,
we incur a statistical error and
have to suffer a certain confidence.
The confidence albeit could be high,
say 94% or 99%.
But, when you do multiple tests, each of
them at a relatively high confidence,
collectively, the confidence leaks and we
have much lower confidence in the whole.
At least one of them
might turn out to be bad.
So, doing them one at a time is
the reasonable thing to try to do, but
has a subtle reason warning against it.
So we need a new test.
I'm going to go back to two
maxims suggested by T.W.
Korner, which seem very appropriate for
the setting.
First, whatever the test we develop,
it should be easy to apply.
Computational Complexity Reduction
is surely to be wished.
We are following along with
the dictates of William of Ockham.
You remember?
The 13th century
Franciscan philosopher and
monk, who opined that we should
find simple explanations for
data whenever possible,
eschew complexity if it were not needed.
Of course, we recognize that this is
the cornerstone of all modern science.
So we want a computationally simple test.
Two, we want the test to be fair.
It should fairly treat all the data
in fair and just proportion.
So, thus for example, a test which
only tests the last IQ bin and
ignores the others would be unfair.
It weights everything
down to one component.
We want to check all the IQ
bins fairly and evenly.

