We now turn to the question
of quantifying,
just how good our poll really is.
So recall this is our basic question.
We start with a random sample of
the newly trials from an underlying
population whose buyers is p.
We form the accumulated sum S sub n, and
look at the relative frequency of
successes, S sub n divided by n.
And the question is, how good is this
really, as an approximation for p?
Now the listener will well
understand that we are dealing
with a chance-governed process.
And in a chance-governed process,
errors are going to be endemic.
We are not going to be able to,
in general, get exact answers.
We are going to have to provide
approximate answers and there's going to
be a certain margin for error, an error
tolerance that we have to suppose.
In addition, the fact that it is
a chance game means that repeated
performances of this experiment will give,
in general, different answers.
And, therefore,
we cannot give absolute certainty
that something is going to happen.
We can only provide
a probabilistic guarantee.
In other words, if we say that
there's a certain amount of error,
we can have no more than a certain amount
of confidence in our pronouncement, and
this is because of
the very nature of chance.
And tying all of this together
are the underlying data themselves.
Our pronouncements are only going
to be as good as the data, and
in particular, the amount of data.
So let us suppose now that
the data have proper providences.
They in fact, are arise as a random
sample implicitly that there
is independence of the trials and
that an underlying population is
being fairly and uniformly sampled.
Once this is the case,
then our performance guarantees
are tied entirely with the amount of
data that we have, the sample size, n.
The error, the confidence, and
the sample size are three parameters, and
they are intricately
connected in this process.
They form a dance where
one influences the other.
Of course we now have ample reason to
believe that this kind of empirical
estimate of the underline
proportion p is good.
We have good reasons to
believe that there are,
that this kind of estimate is principled.
It is likely to give us good results,
but we want more.
We want a quantitative prescription
as to exactly how good this is.
Now the listener should gird
up her notational loins,
because at this point, we're going to have
to inevitably delve into some abstraction.
We're going to need notation,
and we're going to have to understand how
the notation fits into the problem domain.
All right?
So let's build this up by first starting
to look at a simple, concrete example.
So, let us consider a setting where we
are sampling from an underlying population
where a proportion p of 60% of the
population is of a certain type, say one.
And the remaining proportion q,
or 40%, is of type zero.
Let us say we sample randomly
from this population.
In other words, we have random sample and
let's say we look at a sample of size ten.
So here's an example,
x1 through x10 are the result of
Bernoulli trials whose
success probability is 0.6.
But bear in mind we don't know 0.6.
This is something that we
would dearly love to know.
'Kay.
But we don't know it.
It's a fixed number.
It is going to be revealed to
us gradually through the sample.
So, I generated a particular
sequence using a random number
generator on my computer,
setting the bias to be 0.6.
And here's the particular sequence
I came up with, a one, two zeroes,
three ones and four zeroes.
The number of accumulated successes in
our notation S with a subscript 10,
in this case, is clearly 4.
There are four ones in this process.
And of course we realize, we know,
that the number of accumulated successes,
when there are 10 trials
with success probability 0.6
is governed by a binomial distribution
with exactly these parameters.
10 trials, success probability, 0.6.
The specific form of distribution
is known to us, right?
And in our complex notation, we introduce
the letter b for the binomial, and
parameters 10 for the number of trials,
0.6 for the success probability, and
the actual number of successes k runs
between 0 and a maximum of 10 there.
And these are given by the binomial
probabilities, and you've seen,
now by now,
what looks like a very familiar curve.
In our experiment,
we've got exactly four successes and
we've highlighted that on the figure.
And this occurs for
the case of 10 trials where
the probability very slightly
in excess of about 10%.
Okay, now we are going to
use S10 divided by 10.
As an estimate of p, which is 0.6.
In our example,
S10 over 10 is 4 divided by 10, or 0.4.
Our estimate makes a 20% error.
The error is 0.2.
And this leads to a more general question.
Let's again keep the parameters,
p equal to 0.6 fixed,
this is fixed but
a priority are known to us.
Let's fix the number of
trials at n equal to 10.
And let's fix,
let's say the amount of area that we're
willing to tolerate as let us say 10%.
So of course in our example, we are making
an error larger than what we can tolerate.
We are making a 20% error for this data.
But here now is a general question.
If many people perform this experiment,
in other words, a large number of people,
each of them generate ten trials
according to this Bernoulli probability,
what is the expected number
of these individuals?
Whose estimate of p is 10 over 10,
differs from p by no more than 10%?
Formally, what is the probability that
the relative frequency of successes,
S10 over 10,
differs from the true probability,
p equal to 0.6, by at most, 10%?
But let's promptly set this up, right?
Now, at this point, we're going to
actually have to do a real computation.
Right.
The basic question says, I want my
relative frequencies of successes,
S10 over 10,
to differ from p by no more than 10%.
In other words,
I want S10 over 10 to be no smaller
than 0.6 minus 0.1 and
no larger than 0.6 plus 0.1.
And so writing it down mathematically,
we are looking at the probability that
a relative frequency of successes
lies between 0.5 on the one hand,
and 0.7 on the other.
Now we should promptly now figure
out what the event at hand is.
'Kay.
Let's clear the denominator by
multiplying throughout by 10 and
rewrite this as saying this is
the probability that the accumulated
number of successes in 10
trials of tossing a coin with
success probability 0.6 is between 5and 7.
In other words, the accumulated
number of successes is 5, or 6, or 7.
And, of course, in your figure,
these correspond to three settings and
with three particular
binomial probabilities.
Our event then is comprised of
the three integers 5, 6, and 7.
And by additivity,
the probability of the event is just the
sum of the three binomial probabilities.
Of course the notation is
looking cumbrous and difficult.
But bear in mind here that the number
of trials, n equal to 10 is fixed.
The success probability,
p equal to 0.6 is also fixed.
And what's varying here is the number
of successes, 5, 6, and 7.
And all that is done is to
write this expression down and
now all is over bother shouting.
All we've got to do is plug in these
binomial probabilities into expression for
the binomial and simply act them.
Here's a simple numerical
estimate remaining.
You could plug this into your,
a local scientific calculator.
Plug it into your computer if you
have a little software program.
Or you can simply fire up your
favorite internet browser,
and in the search domain,
type in binomial calculator.
And you'll find a variety of free binomial
calculators that are available for
you on the internet.
Plug in the parameters and
you'll find these numbers.
Add them up and find that the desired
probability is about 67%.
There is about a two in three
chance that your relative
frequency of successes is within
10% of the true but unknown answer.
The point however,
principal estimate, do we not, right?
Now, two parameters have
come into play here.
The first, the amount we
deviate from the true answer.
This is the error.
And the particular error tolerance
we specified here is 10% or 0.1.
The second factor that arises
here is the probability
that we land within this error
tolerance of the true answer.
This is a confidence in our estimate.
So we can roughly say that our
estimate gives us no more than a 10%
with a confidence of about 67%.
Now this is a simple numerical
example that's not very hard.
But of course, you know,
we can't simply compute numerical
examples again and again.
It doesn't tell us very much
about an underlying structure.
We'd like to intuit, figure out what is
the underlying analytical structure so
we can make predictions from this.
So we want to very
quickly generalize this.
Let's do this in two stages.
First, let us keep
the success probability,
the proportion of the underlying
population, fixed at 0.6.
And now let us ask the following question.
What happens as the sample
size n increases?
How does the confidence vary as
it increase the sample size?
Let's take stock, and
take a look at it again.
So here's our basic problem.
So, now we have a sample size n, and
we're looking at a relative frequency of
successes in that sample, Sn divided by n.
Of course, Sn now is governed by
a binomial distribution corresponding to
end trials of acquiring with success
probability, 0.6 and of course we have
these binomial probabilities which
are now very familiar to us.
We've already seen the case
when n is equal to 10.
In this case,
the chance of making a 10% error is
given by the sum of the three
binomial probabilities for
our number of successes equal to 5,
6 or 7.
What if we increase the number
of trials to say n equal to 50?
Well, then we get a binomial distribution
corresponding to 50 tosses of a coin
with success probability 0.6.
It is going to be centered
at 0.6 times 50, all right?
In other words,
it's going to be centered at 30.
If the relative frequency of
successes in the sample S50
divided by 50 is to be within
10% of the true answer,
then the relative frequency
must lie between 0.5 and 0.7.
0.5 times 50 is 25.
0.7 times 50 is 35.
And so this says a cumulative
number of successes in
50 trials should be between 25 and 35.
And now we simply add up
the binomial probabilities.
Bob's your uncle.
It's as simple as that.
But let's go one step further.
What if we increase
the number of trials to 100?
Well it's exactly the same game now.
We now have a binomial distribution
corresponding to 100 tosses of the coin.
We want the relative frequency between
0.5 and 0.7, 0.5 times 100 is 50.
0.7 times 100 is 70, which means
a cumulative number of successes has
to be between 50 and 70, and we simply
add up the binomial probabilities.
Of course doing this by hand is
very soon going to become tedious.
And of course we want to have recourse
to a scientific calculator or
a program on the computer which
calculates binomial probabilities or
just do it over the internet.
Type in binomial calculator and just say,
compute these probabilities for
these parameters.
And if you do this,
you will find these numbers.
The probability is about 67%,
two in three, when n is 10.
It is about 89% when n is 50.
It's about 97% when n is 100.
Or you have to get it in address.
The chance that your estimate
will make more than a 10% error,
your chance that your
estimate will be about 0.7 or
below 0.5 is about one in
three when in this state.
It's only about 11%, when n is 50.
And it's a really small 3%, when n is 100.
It is clear, that as n increases,
the concentration phenomenon
is working in our favor.
And we'll very quickly concentrating
probabilities around the expected value.
A relatively small sample is giving us
very good estimates, most of the time.
Now, let's quickly abstract
the basic principles.

