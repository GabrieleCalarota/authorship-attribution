We have established now that the binomial
distribution has unimodal character.
The binomial probabilities increase up to
an integer in the vicinity of n times p,
and the decrease thereafter.
Let us turn now to what is visible,
what is clear in the figure.
That there is a centering tendency,
a center around which
the binomial probabilities
are roughly evenly distributed.
Consider the setting
where we have 10 trials,
10 tosses of a coin whose
success probability is 0.6.
The binomial probabilities
are as shown in the figure.
It is clear that there
is a unimodal character.
And the maximum value,
the largest probability occurs
exactly at n times p in this
instance at 10 times 0.6, or 6.
Now, to capture the idea of center,
we turn to
an idea common from physics,
the notion of a centre of mass.
To put this in a proper context, it
might help if we simply flip the figure.
Imagine that at each of
the values 0 through 10,
we've got a mass attached, whose weight
is given by the binomial probabilities.
Now if we have a setting like this,
we have a distribution of masses.
Imagine temporarily that all
the masses were the same.
Then we have a setting
like this approximately,
where we have a uniform
distribution of mass.
Now, think of a see-saw.
Where would the point of balance be?
Where would the fulcrum be?
That is the centre of mass.
And in a situation where
the mass is evenly distributed,
the center would manifestly be
right in the center, in the middle.
But if the mass were unevenly distributed,
as is the case
in your figure, then one has a setting,
perhaps like this,
where mass is larger at
one end than at the other.
Now in this case, we would not have
equilibrium right in the middle,
but in fact the centre of mass,
where both sides are balanced,
shades towards the larger masses.
Now this is exactly the kind of
setting we have in front of us.
We now want a mathematical
prescription which capsi,
captures this idea of this
probabilistic centre of mass.
Common convention refers to this
by the Greek letter mu, and
we write it down exactly as we
would in physics, as follows.
To the point 0,
we attach a mass whose weight is b of 0.
To the point 1,
we attach a mass whose weight is b of 1.
To the point 2,
we attach a mass whose weight is b of 2.
To the point k, in general, we attach
a mass of weight b of k, and so on.
This weighted sum then
gives us very naturally
the probabilistic analogue of
the physical idea of a centre of mass.
Now, it is comforting to write a long
sum like this down because it makes
transparent exactly what
each element contributes.
But when one does
an analysis it is convenient
to compact this using summation notation.
And write it simply in the form
as a summation over an index k,
which varies in this case between 0 and
n, of quantities of
the type k times b of k.
The connection with the physical centre
of mass should be transparent here.
We call this the expectation of
the accumulated successes, S sub n.
And we denote it by E of Sn.
E of course stands for expectation.
Sn is the accumulated number of
successes in our chance experiment.
And this is also referred to sometimes
as the mean or the expected value.
And of course, one recognizes
in this just a generalization,
the analog of an arithmetic average.
Now, all these terms are useful,
they add color to the picture.
But, especially for a neophyte,
someone starting out of the gate,
it would be advisable to think of this
object by its formal mathematical
connotation, by referring to it
as a mathematical expectation.
Meaning therefore,
that there is an underlying,
very specific mathematical
formulation which captures this.
And this, in this case of course,
it is a weighted sum.
In many cases and practice,
when there is an ambiguity,
by long force of tradition we
will simply call this just mu.
But when there are more that
one chance experiments at play,
then we will refer to it
particularly as E of Sn,
the expectant value,
the expectation of Sn.
So now we've got a formulation.
We've now got a mathematical model for
a center of mass.
Our next question then is,
well, what does it evaluate to?
Of course,
we could compute this numerically.
If you tell me n is 10 and
p is 0.6, we could just simply
write down an arithmetic sum.
Evaluate the values,
add them up and here's an answer.
But of course this is
not very satisfactory,
it doesn't give us a feeling for
the problem.
It doesn't tell us how this
expectation depends upon a number or
trials n and how it depends
upon a success probability p.
So what we are looking for now, what
we are casting about for, is a simple,
compact, mathematical simplification of
the sum, which makes transparent for us,
the rules played by the number of trials
n, and the success probability p.

