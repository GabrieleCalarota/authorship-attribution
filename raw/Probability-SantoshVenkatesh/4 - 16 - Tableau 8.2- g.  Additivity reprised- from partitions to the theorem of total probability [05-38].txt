Let us take stock.
Beginning with the definition
of conditional probability,
we can now fold in a, as a very
simple application of additivity,
a principle which allows us to
partition a problem into two pieces.
And so in an abstract setting,
we can find the probability of any
desired effect by chaining together
two conditional probabilities.
Now, at its abstraction, it's just
an equation, a simple one albeit.
But, as we've already seen,
the applications can be subtle.
The devil, of course, is in the details.
It resides in how one finds
the right partition for a problem.
What was at the heart of
this simple observation?
A key principle.
The division of the sample space of
possibilities, into two disjoint pieces,
a partition of the sample space into two
components, A, and a complement of A.
This understanding all
today shows the way to
an abstract generalization,
which is useful in its own right.
So again, we began by partitioning
a sample space into two pieces,
and then figuring out how that
partitions and event of interest.
If, now, one moves beyond
a partition into two elements, and
thinks about a partition into, say,
a finite number, n, of events.
A1, A2, A3 and so forth.
What we now obtain is a finite partition
of the sample space into n sets.
Recall that a partition means
that a constituent sets A1,
A2 and so forth are pairwise disjoint.
They are mutually exclusive.
And together,
they comprise the entire space.
Formally, mathematically, we'll say,
Ai into section Aj is empty.
For any parenthesis i and j, and
the union of the Ai's must
be the entire sample space.
And the moment we come up with such
a partition, we say that a fortiori,
we have a partition of
the event of interest itself.
And we can decompose
an event of interest H,
the target event,
into a group of disjoint pieces.
That portion of H, which is in A1,
stitched together with that
portion of H and A2, and so forth.
Formally, of course, we have taken
a union of a bunch of intersections.
The key point here, of course,
is because the original sets
A1 through An are disjoint, so
are the pieces H intersection A1,
H intersection A2 and so forth.
Additivity arrives to the rescue promptly.
And we can know decompose the probability
of H as a sum of individual probabilities.
Additivity is the whole is
equal to the sum of the parts.
The moment where you have such
a decomposition, we say that on the right,
we have a system of
intersectional probabilities, and
we can promptly chain them together using
our definition of conditional probability
to come up with a generalization of
the basic principle of additivity.
It's abstract but we'll see examples and
applications in a moment.
Before you do that,
one more quick extension.
The pictures suggest a partition
of an underlying sample space into
a finite number of sets, say, n, number.
But the argument is clear.
That it will hold untrammeled
even if the partition contains
a countably infinite number of sets.
And if that's the case, then on the right,
we'll have an infinite sum chained
together of conditional probabilities.
This generic principle of additivity
in this particular context
is following chronograph
seminal work of 1933 called,
rather granduously,
the theorem of total probability.
Informally put, we start with
a partition of a sample space
into a collection of sets,
a collection of events, A1, A2, A3.
These events have to be pairwise disjoint
and the union has to be the entire space.
We now look at a target event, H.
We decompose H into its pieces in each of
the Aj's, stitch everything together, and
the result is a principle of additivity in
the context of conditional probability.
The theorem of total probability.
The summation sign there,
just says that, add over all indices j,
as Aj ranges over all
the events in the partition.
This collection could be finite,
could be countably infinite.
Typically, we'll start index j at one,
two, three, and proceed from there.
Occasionally, it'll be convenient
to start the count at zero.
And still,
more occasionally, it'll be convenient
to let j run through all the integers.

