So, to summarize Tableau 8, Part 2,
additivity and
the theorem of total probability.
In the simplest setting,
additivity in conditional probability
manifests itself as a chain.
And it can break up
a target event H based upon
a partition of base inside information,
either in event A or its complement.
This turns out to be
an extraordinarily profitable way
of thinking about breaking up a problem.
Total probability is an extension of,
of this additive idea.
To partitions involving
more than two elements.
To a partition involving a finite number,
or
possibly even a countably
infinite number of events.
And in which case,
we now have a chain formulation for
the probability of a target of a date
in terms of conditional probabilities
given various input events if you like,
AJ.
A corollary to this theme
was our ability now,
once we have this formulation
of total probability, of
reversing the direction of conditioning,
figuring out a posteriori probabilities.
Probabilities after
the fact via Bayes's rule.
As I mentioned earlier,
a beginning student should
probably not try to memorize
Bayse's Rule in itself.
But understand how it
arises by a systematic
application of the idea of total
probability, of additivity.
This concludes our discussion
of conditional probability for
the time being.
Our next lecture, Tableau 9,
will deal with, perhaps,
the fundamental aspect of
the theory of chance, which reskews
it from being nearly a fragrant by water
of a gentle abstract theory of measure.
And the concept, the idea I'm referring to
is the understanding of the idea
of statistical independence.
This will then be our focus on Tableau 9

