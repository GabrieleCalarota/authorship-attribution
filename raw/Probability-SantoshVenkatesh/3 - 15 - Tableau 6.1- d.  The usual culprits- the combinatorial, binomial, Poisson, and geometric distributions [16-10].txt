Having built up a right understanding of
how to construct a probability measure in
a discrete space, we should
promptly try out some examples for
mass functions to build up intuition.
So let us start with what I
shall call the usual culprits.
These are distributions of mass functions
which crop up repeatedly again and
again in theory and in practice.
So let us begin with the simplest of them
all, the combinatorial distribution.
We start with an integer parameter n fixed
but arbitrary, three, five, ten, 100,
1 million, what have you.
The sample space corresponds to
the integers from 1 through n.
For example,
if n was 6 this could be the sample space
corresponding to the throw of a dime.
We consider the following
allocation of atomic
probabilities to the sample
points 1 through n.
Allocate to each p(k), for k running from
one through n, equal value 1 over n.
Now, when I posit that this is a,
an honest mass function,
than I am saying in fact that the axioms
of probability are in fact satisfied.
Are they?
Which very quickly test this out.
The first axiom of Positivity.
Is the mass function positive?
Of course, this is completely trite.
The number 1 over n is a positive number.
For each of the values, 1 through n.
So, p(k) is manifestly positive.
What about normalisation?
Well, normalisation is equally trite.
In this case, if I sum the values of p(k),
k runs from 1 to n.
So I'm adding the values p(1) to p(2),
all the way up through p(n).
But each of these is
equal to the same value,
1 over n, and
there are n terms in the sum.
Therefore you get n times 1 over n or
1 and
normalisation is again tritely satisfied.
This is by far the simplest probability
measure in a discrete space and
this was the focus of
classical probability.
Probability in antiquity.
The games of chance, urn models, all rely
upon this at some fundamental level.
In settings like this,
it is natural to ask,
is it possible to write the probabilities
of events in a nice and compact way?
And indeed one can.
So, suppose A is any event.
An event in the space is then a subset
of the numbers from 1 through n.
Well then the probability of
A is obtained via additivity,
simply as the sum of
the atomic probabilities.
But all the atoms have
equal probability 1 over n.
And therefore the probability of A is
given simply by the number of elements in
a divided by n.
The cardinality of a divided by n, or
in more colloquial language,
the number of outcomes favorable for
a divided by the total
number of possible outcomes.
Here's another example.
Slightly more complex this time.
This leads to the august
binomial distribution.
It's a little more complex.
Now there are two parameters in play.
First, a positive integer parameter, n.
And second, a positive real parameter.
That is traditionally called lowercase p.
It's a number between 0 and 1.
In such settings it is inevitable
to write q to be 1 minus p,
so q again is another real number.
Also between 0 and 1, and
p plus q takes value exactly 1.
The sample space here is a set of
integers from 0 all the way up to n.
The sample space consists
of n plus 1 points.
Now, I have co-opted the letter little p
here as a parameter of this distribution.
So I don't want to use p of k for
the mass functions.
Let's use b(k).
B standing for binomial for
the mass function.
Now.
In a movable form,
this is a mass function or
distribution which has got two parameters.
Two fixed constants.
Fixed but arbitrary.
N and p.
So sometimes in a slightly
more verbal notation,
we would write b with a subscript n.
The argument k tells you
which outcome is in play.
And a semicolon,
followed by the second parameter p.
And the reason for the choice
of such an involved nomenclature
will be come apparent as we go along.
But allocate to the outcome,
the atomic probability
given by n choose k times p to the power
k times q to the power n minus k,
k now runs through the integer values 0,
1, 2, 3 up til n.
These are putative atomic
probabilities for an experiment.
Are these honest, are these legitimate?
Naturally enough, we want to verify
positivity and normalisation.
Let's talk of positivity first.
Well, again this is trite.
The expression b sub n of k and
p is involved binomial coefficient
which by definition is non-negative.
And involves puddles of positive
quantities, and therefore,
it is strictly positive.
And therefore, these atomic
probabilities are all positive.
For each of the values k
running from 0 through n.
Positivity is tritely satisfied.
What about normalisation?
Pause for a moment.
Write down a normalisation sum.
And see if we can verify indeed that
these form an honest mass function.
When you're ready, start the lecture.
Okay.
So now we're looking for normalisation.
Let rest try write down
a normalisation sum.
You're going to sum all our possibilities
for k, k now runs between integers from 0
to n, and then you write down
these binomial probabilities.
Did you recognize a binomial sum in there,
did you recognize the binomial
theorem coming into play?
That sum, is exactly
the binomial expansion of
p plus q to the power n.
If you don't remember this,
you should go back, take a look at
the summary in tableau 2 part 2.
Now remember, q was defined to
be 1 minus b and, therefore,
p plus q is identically 1 and 1 to the
power n is manifestly 1 and, therefore,
normalisation is satisfied.
Perhaps, no, not quite so trite this time.
We needed a binomial identity, but
nonetheless, we have an honest
masked function in hand.
This distribution turns
out to be the first
of the three fundamental
distributions in all the probability.
We will see that it makes an appearance
when we talk about polls.
When you talk about accumulated
successes in repeated trials in
gambling games it arises
naturally in statistical tests.
It's a fundamental distribution.
For our purposes today, all that matters
is it is a function of k which is honest.
It is a mass function.
It is positive.
It adds to one.
To formulate probabilities, I'm going
to have to add atomic probabilities.
As the combinatorial case,
I could hope to find simple closed-form
compact descriptions of
events probabilities.
Sadly, we don't have such simple forms
available to us, in settings like this.
So we'll have to have re-course
to either analytical estimates.
All simple numerical
probability computations
starting by adding up various
elements of a binomial sum.
The origins of the name,
the binomial distribution, are now clear.
It's because of the binomial coefficient
that appears in the definition
of this object.
The next of the major distributions
goes back to the year 1837.
It was discovered by Senior Poisson and
in his honor it is called
The Poisson distribution.
This is a single parameter distribution.
Traditionally the parameter
is called lambda.
Lambda is strictly a positive parameter.
The sampled space here is
all non-negative integers.
Tradition compels us sometimes to write
this as Z with the superscript plus.
To denote the collection
of non-negative integers.
The probabilities we want to assign
to atoms, say to the number k,
k being some non-negative integer,
we allocation probability p(k).
And sometimes to make explicit that
there's an underlying parameter fixed but
arbitrary positive constant in play we
write it as p(k) semi colon lambda.
We define this with
a particular exponential form.
E to the power minus lambda times lambda
to the power k over k factorial and
k here runs through all the atoms.
K runs through all the integers,
0, 1, 2, 3, 4, and so forth.
Now this looks like a more complex form.
Now we have not just a finite
number of atomic probabilities, but
an infinite number of them,
albeit a countable infinity.
Naturally, we want to check positivity,
and
again we see that this is trite, the
exponential function is always positive,
powers of a positive number are positive,
the factorial is positive, and
therefore the Poisson
probabilities are positive.
And therefore, positivity is trite.
What about normalisation?
Pause for a moment.
Write down your normalisation sum,
and see if you recognize the sum,
something from elementary calculus,
perhaps an atavistic memory might stir.
Pause the lecture and
when you are ready, we start.
So let's write down the sum
of the Poisson probabilities.
Here the index k runs through
all the non-negative integers.
The e to the power minus
lambda doesn't depend upon k.
It can come out of the sum.
And we had a sum of lambda to the power k,
divided by k factorial.
Did you discover, did you recover,
the exponential function?
The expression of the sum is exactly
the exponential series for
e to the power lambda.
And therefore, you get e to the power of
minus lambda, times e to the power lambda.
By the rule of exponents, this is exactly
one, and Normalisation is verified.
Excellent.
This distribution is a second
of the three fundamental
distributions in all of probability.
And we'll see examples.
It arises in the context rare events,
hurricanes, collapses,
failures of machinery.
It arises in arrival processes, in queues.
And we shall see several rich applications
involving distribution as we move along.
The last of our elementary examples is
what's called The geometric distribution.
This is a single parameter distribution.
It is determined by
a positive real parameter.
Again, sadly called little p.
Which takes a value between 0 and 1.
Again, in this context,
we'll write q to mean 1-p and
again q is also a number between 0 and 1.
The sample space for
this experiment, I won't specify what
the experiment is, but I shall eventually.
Is the collection of nonnegative integers.
0, 1, 2, 3, 4, 5, ad infinitum.
Now again, I've co-opted little p for
a parameter of the distribution,
it's a fixed constant.
And therefore I need to
give the mass function,
the distribution a different name.
Let me call it w(k).
The w(k) is the atomic probability
associated with the sample
point the integer k.
Again, If it becomes important
to bear in mind the underlying
parameter for this experiment.
This quantity which is fixed.
But arbitrary that we'll write a w(k;p).
Now w(k) is defined to be
q to the power k times p,
k running over all the non-negative
integers, 0, 1, 2, 3, and so forth.
And now the reason for
the name becomes apparent.
What I've given you is a geometrically
declining set of probabilities
as k increases.
Therefore, this is called
a geometric distribution.
Of course, we should verify positivity and
normalisation positivity.
And we realize immediately that q to
the power of k is always positive,
p is always positive.
And therefore, positivity is trite.
These geometric probabilities are strictly
positive for every non-negative integers.
What about normalisation?
Again, part of lecture.
Write down a normalisation sum and
see if it triggers some atavistic memory
from the calculus which can allow you
to write down the answer in one line.
When you're ready, restart the lecture.
Good, let's write down
the normalisation sum.
So we have a sum over all
the mass atomic probabilities.
Sum over the mass function as k varies.
Remember, p is a fixed but
arbitrary parameter.
Now in the expression q to the k times p.
Q does not depend on k,
therefore can come out of the sum.
And so we have to evaluate a sum of k,
of q to the power of k, and
you might recognize since q
is a number between 0 and 1,
that we have a geometric series
coming into play, the sum of powers
of q is a reciprocal of 1 minus q, and
therefore we get p divided by 1 minus Q.
Recall that 1 minus q is just another name
for p, and therefore we get normalisation.
So in fact we have a,
an honest mass function.
This mass function,
this distribution arises.
In models of waiting times,
in models of run lengths when we have
sequences of some event happening
before something else happens.
And again, this is one of
the fundamental distributions.

