Here is an experiment.
Take our favorite coin and
toss the coin repeatedly
until two successive
outcomes are the same,
until two successive tosses are the same.
We could ask a question about
such a conceptual experiment.
What is the chance that by the time you
terminate your succession of tosses,
that you'll have tossed the coin four or
more times?
Now, here's an experiment
you can actually do at home.
Pause for a moment, try it out.
See, without doing any analysis,
whether you can come to an understanding
of what the chances might be.
Pause for a moment,
think about the problem.
Come back when you're ready.
Okay, you're back.
Now, our first step in this
process is to try to come up
with a mathematical description of
the possible outcomes of this experiment.
Now, let's, before we write anything down,
imagine what could happen.
Well, the experiment is clearly going
to take more than one toss of the coin,
because you need two successive
tosses to have the same result.
The smallest number of tosses
you could imagine would be two.
And if you were to terminate after two
tosses, then it's manifestly clear
that both tosses have to be heads or
both tosses have to be tails.
Excellent, we've got a starting point.
We've identified two of
the possible outcomes.
Heads and heads, or tails and tails.
What if you don't throw heads and
heads or tails and tails?
That means you must have had
an alternating sequence of heads and
tails, either a head and
a tail or a tail and a head.
And if that is the case,
you would toss the coin again.
Mark how there's a possibility that
you could repeat what you'd thrown on
a second throw.
If that is the case,
then you will stop on the third trial.
Now a picture is emerging.
So the third trial, you could have a head,
a tail, and a tail, and you stop.
Or a tail, and a head,
and a head, and you stop.
If at the end of the third trial,
you haven't repeated a coin toss,
you will toss it a fourth time.
Now the picture is becoming apparent, and
a notation is becoming clear as well.
So for example,
we could start writing out the possible
outcomes of experiment as follows.
Head head, tail tail, tail head,
head head, tail tail, head tail,
head head, tail head, tail tail and so on.
The dots at the end of the screen
tell us that this experiment
could in principle go on indefinitely.
There is no fixed bound
where the experiment ends.
You cannot guarantee that
the experiment will be done and dusted
after 100 tosses of the coin or
after 1,000 tosses of the coin or
after a million or a billion or
any number whatsoever.
In principle,
albeit with an incredibly small chance,
the experiment could go on forever.
Now, for it to go on forever,
it's clear what must happen.
You must start out with
an alternating sequence, and
that alternating sequence
should persist indefinitely.
Surely we can conceive of this happening.
Though of course
the chances should be very,
very, very, very small that it'd go
beyond even more than a few tosses.
Okay.
This is
an example of a gedankenexperiment.
You notice that this experiment
is never actually done,
no one has ever tossed this game and
seen all possible outcomes.
There's an infinity of possible outcomes.
But it can certainly be
conceived of being done.
And we can identify in principle
the family of outcomes,
of course without listing all of them,
but we describe it compactly.
Things like, start with a head,
go with an alternating series, and
then conclude with two of the same.
Oh, this is nice.
Start with a tail, go with an alternating
series, and conclude with two of the same.
How long is a series?
Any fixed integer.
Now we have a way of describing it,
visualizing it,
even though of course we have no earthly
hope of writing down all possibilities.
There's an infinity of them.
This is so far so good.
It gets a little trickier to
ascribe an idea of chance.
So, let me throw out some numbers
at you and see if I can defend
numbers as measures of chance for
each of these outcomes.
I claim that the numbers I've
given you in the second row of
the table are actually appropriate for
the problem at hand.
Now, how would we go about
justifying something like this?
Well, an argument might go as follows.
Let's start by looking at those trials
where the experiment
concludes after two tosses.
If that is the case, you must have
either thrown two heads, or two tails.
Now, of all possibilities for
two tosses, well, how many are there?
Well, there are exactly
four possibilities.
Heads heads, heads tails,
tails heads, and tails tails.
There are four possibilities,
and naturally enough,
because we think our coin is fair,
we imagine that these four possibilities
for two tosses are equally likely.
If that is the case,
then the chance of seeing a head head
in two tosses should be one in four.
And likewise,
by an entirely similar argument,
the chance of seeing a tail tail in two
tosses should again be one in four.
Suppose the experiment doesn't
stop after two tosses, but
moves it on to three tosses, how many
possibilities for three tosses are there?
Now there are 8, or better,
2 to the power of 3 possibilities.
2 times 2 times 2 for
each of the three tosses.
What are they?
Well, we can very lexicographically write
down the possibilities if we wished.
And you might want to do that.
Head head head, head head tail,
head tail head,
head tail tail, tail head head,
tail head tail,
tail tail head, and tail tail tail.
There are eight possibilities.
And again, we have no reason to believe
that any one of them is any more likely
than any of the others.
We assign to each of them equal chance.
Out of those eight possibilities,
the chance of getting for example a tail
head head, well, that's one out of the
eight equally likely possibilities, and
therefore you assign a chance of
one in eight to tails heads heads.
And likewise a chance of one in eight for
head tail tail.
And we proceed apace.
If you look at four tosses, there are 2
to the power 4 or 16 possibilities.
I will not weary you by telling you what
they are, though you might want to write
them down just to make sure that
your intuition is now solid.
Of those 16 possibilities,
each of the two possibilities where you
terminate is one of 16, and
therefore you assign a one in 16 chance.
And now you see that these chances
are decreasing like 1 in a power of 2.
Here now is a measure of chance for
the elementary outcomes
in an experiment which has a countable
infinity of possible outcomes.
Now, you might look at this and say this,
well, gee, this is all very specious and
handwaving, but how do I know that
the chance you assign to, let's say,
a termination in 50 tosses, in other
words, 1 in 2 to the power 50, is right?
Doesn't, what if you're wrong?
The short answer to that,
of course, is that the outcomes
which require 50 tosses have
not been fairly tested.
Let's say, for example,
you look at 50 tosses or even, say,
60 tosses, the chance of
an experiment requiring 60 or
more tosses is, in this model,
1 in 2 to the 60.
Now, 2 to the 60 is
a mind-bogglingly large number.
It's about 10 to the power 18.
It's about the lifetime of
the universe since the Big Bang.
Even if we tossed a coin every,
once every second for the entire lifetime
of the observed universe, we would not
have exhausted those possibilities.
So we could legitimately say that
I'm giving you a number, and
disprove it if you can.
It surely has not been fairly tested.
Of course, these numbers
are getting fantastically small.
The second part of the justification
will be very simply that in the range
of termination with a modest number of
tosses, let's say five or six or seven,
the numbers we put down fit wonderfully
well with observed experimentation.
And therefore we have no reason to
believe that our logic is unsound, and so
we extrapolate on to an infinite sequence.
Okay, with this for justification,
we now have an experiment with a countable
infinity of possible chance outcomes,
and we have in a systematic,
principled manner assigned
certainly unequal probabilities,
chances to each of these outcomes.
Now let's look at what
the event at hand is.
That your experiment required you to
toss the coin four or more times.
Recall, our experience has told
us that events may be identified
with subpopulations,
aggregates of possible outcomes.
What are the outcomes here that
are appropriate to this event?
Well, clearly enough, all those sequences
which terminate in either two heads or
two tails, which start with four or
more end members in the sequence, things
which are identified in the green band
stretching out infinitely to your right.
And we can can capture an entire
infinite green band by a simple compact
notation, let's say greater than or
equal to four tosses, and
that succinct description
captures an entire infinity.
And now we want to figure out what is the
natural measure of chance for this event,
and we appeal immediately to
the fundamental principle of additivity.
The whole is equal to the sum
of the individual parts.
The probability of the event
that you need four or
more tosses is the probability
of the event that you needed
precisely the sequence,
heads tails heads heads or
the sequence tails heads tails tails,
or, and so on.
And we now decompose this event into
an infinite number of shards or
pieces, and
we simply add the probabilities.
Bingo.
Pause and write this down.
We've decomposed the event greater than or
equal to four tosses into
a sum of individual probabilities, but
now we've extended our finite intuition
to a countable infinite intuition.
But the logic appears clear.
This appears to be completely
unexceptionable, we believe it,
we write it down, appealing to
a global principle of additivity.
And now, since we've got numbers
we've assigned to these raw chances,
we very simply plug in, and
I've grouped these numbers pairwise,
two at a time,
because by looking at our table above,
we see that each of these
probabilities appears in pairs.
There are two 1 in 16ths, two 1 in 32s,
two 1 in 64, and so forth.
Factor out a 2, and
then add up the remaining probabilities.
You have noticed that each of the terms
inside their own brackets has got a factor
of 1 in 16 that is common to it.
Naturally enough, as long experience has
taught us, we factor out the term 1 in 16.
And what remains inside the brackets
is a sum of reciprocal powers of 2.
Oh, we are very familiar with this.
This is what, in a first calculus class,
we call a geometric series.
And the geometric series identity
tells us that a geometric series with
parameter one half sums to
the reciprocal of 1 minus a half.
You do the arithmetic and
you find the chance of needing four or
more tosses in this experiment
is now one in four.
Did your guess come close to this?
Go back and think about what you
did to come up with a guess.
And now look at the analysis and
see what went wrong or what went right.

