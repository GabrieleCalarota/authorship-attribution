If the student casts her eye over
our derivation of the law of large
numbers in the case of the binomial and
the setting of poles.
She might feel that the analysis,
the usage of a square and
Chebyshev artifice had
a somewhat accidental character.
A serendipitous feel to it.
Chebyshev discovered inequality in 1854.
And there are few results
the history of mathematics and
science which have continued to be so
profoundly useful.
So this segment then will allow
the student to get a feel for
why the inequality is so ubiquitous.
It is not specialized purely to
the binomial or the Poisson, but
is in fact a very general statement
about chance occurrences.
Let us consider Chebyshev in
the context of a continuous setting.
This underlying sample
space is the real line.
We have a continuum of possible point
to mark our chance-driven experiment.
The outcome of the experiment,
the sample point,
is a random variable denoted uppercase X.
It comes equipped with a density and
mass function per unit length, p of X.
And let us assume that this
mass-density has an expectation mu and
a variance sigma squared.
What is the probability that, for
a given deviation tau,
some positive number tau.
That x deviates from its center,
from mu, by tau or more.
So we should, pause and take stock.
Figure out what that blank space is.
The measure, and the events.
Let's take these in stride.
First, our outcomes are sample points,
take values of the real line.
The probability measure is induced
a density function p of x,
which has to be non-negative.
That's to satisfy the positive, the axiom.
And it has to be properly
normalized to have unit area.
So on the screen you seen an example for
generic density function.
Now we understand both the sample space
and the probability measure in question.
What is the effect of interest for us?
So as always lets use upper case A to
denote the in event of the interest.
And we recall that an event in this
setting is a subset of the sample
space and
therefore must be a subset of real values.
What real values triggered
occurrence of this event.
Well all real values little x
such that x differs from mu
in absolute value by the deviation tau or
more.
So that simple inequality,
the absolute value of x minus mu being
bigger than tau describes all values x.
Which fall within the province
of this particular event.
Now a figure's worth a thousand
words as they say, so
let's very quickly try to hone in
graphically as to what the set means.
For the figure at hand, for the density
at hand we have a two humped density,
a Bactrian camel if you will.
And we can see that the central
mass should lie somewhere
between the two humps.
Possibly shaded slightly towards the
broader, bigger hump, and indeed it is.
Say here is where the central mass mu
where this particular property law is.
And starting from mu you move
away by tau in either direction,
to mu plus tau and mu minus tau.
This region from mu minus tau
to mu plus tau is a region
of closeness to mu, allowed by our event.
What are we interested in?
The region to the right of mu plus tau and
to the left of mu minus tau.
And so exactly, the subset of
points A is described exactly
by those two regions, those two subsets.
Now, this completely describes
the event at hand of course,
but this is not quite
a visceral formulation.
And corking an eye at
an analysis of the binomial,
we try to massage this into
a slightly more convenient form.
Take the inequality again, absolute
value of x minus mu exceeding tau.
If we divide both sides
by the positive tau,
then we get an equivalent inequality
which says that the ratio
of the absolute value of x minus
mu to tau should be bigger than 1.
Right?
And therefore, our event A describe as
the collection of points x
satisfying this new inequality.
Now recall that
a number which is bigger than 1 has
a square which is also bigger than 1.
And in fact, for every number bigger
than 1, its square is bigger than 1,
and if its square is bigger than 1,
the number must be bigger than 1.
So, this is equal to saying that
a square of an inequality is also true,
in other words,
A comprises all those points x.
For which the square of x minus mu
divided by tau squared exceeds 1.
This you will recall is
the squaring artifice that
Chebyshev introduced in 1854.
We'll now come to a right
understanding of the sample space,
the probability measure and
the event at hand.
It has now come time to execute and
evaluate a probability.
So, let's start with this.
Recall we're dealing with
a real line sample space and
the event of interest deals with
a deviation from a mean value, right?
The region x for which x minus mu
squared over tau squared exceeds 1.
We now have to compute a probability and
of
course the probability of the setting and
the continuum is an area under a curve.
It's going to be an integral.
So you want to look at the area
which is in that particular region.
Now let us execute.
We are interested in the probability,
reading it out in words,
that X deviates from mu in
absolute value by tau or more.
This, of course,
is given by an integral, right?
A limiting sum.
It's going to be the area under
the curve indicated on the figure.
And formally, algebraically, in calculus,
we'll write this as an integral
over the set A of the density function,
p of x dx.
This is the area under the curve
of p of x over the set A.
Now, let's rewrite this ever so
slightly differently.
What we mean by this is the same
as saying that in the region
of interest,
the points x contribute a weight of 1 now.
Right?
1 times p of x is of course p of x.
And outside the region of interest, points
x contribute a weight of zero, nothing.
Well of course, I've done no,
no damage to the, the identity, right?
It is multiplying p of x by 1,
just gives me p of x.
So what?
But here's where the Chebyshev
idea comes to the fore.
Notice that in the region of integration
or all the X's that comprise of set A.
In that region, the square of x minus mu
divided by tau squared is bigger than 1.
And therefore,
if I replace the red 1 by x minus mu,
the whole squared, divided by tau squared,
I'd have increased the integrant.
And therefore,
increase the value of the integral.
Okay, now we are getting somewhere.
Why is this useful?
Because squares are mathematically
very amiable to analysis.
You'll see why in a moment.
But a little more housekeeping first.
We now have an integral of a square,
weighted according to
a non-negative mass function.
And this integral is over a region,
a subset of the real line.
But the integrand is a square and
a non-negative mass density function.
The integrand is non-negative
everywhere on the real line.
And so, if I take the integral and
expand its domain from
the set A to the entire real line,
I'm only adding more positive values.
I can only make the integral bigger.
And suddenly now I've got
an integral over the whole line
of a square weighted according
to a density function.
We're home free.
Let's observe that in the denominator
the deviation squared tau squared
is a constant.
It does not depend upon x,
the variable of integration and
can therefore come out of the integral.
So they pull it out,
we've got now a reworked integral.
Have we gone any further forward?
Have we got ferrata?
Pause for a moment and look,
long and concentratedly,
at that integral on the right.
Does it look familiar?
It should.
It looks like an expected squared
deviation from our center, does it not?
Indeed it is, in fact.
It is exactly the variance
of the random variable, x.
And therefore,
the probability that x deviates
from mu by more than tau is bounded
by an expression involving the variance or
the expected squared spread
divided the deviation squared.
Let's quickly summarize our discovery.
This is Chebyshev's
resplendent inequality.
Few inequalities have had such
an impact over such a period of time.
Now, what is this telling
us as a following?
If the variance is small
then the probability
of a deviation from mu is small.
That is very intuitive, remember,
variance tells us something about the
expected squared spread around the center.
And you're saying,
well if expected squared spread is small.
Then the variable tends to be
concentrated near the center.
It is unlikely to have a large
peregrination moving away from the center.
This is very intuitive, very reasonable.
It also suggests to us that the proper
unit of distance from the center
should be in terms of the square
root of the variance,
the spread, the standard deviation, sigma.
Here's a concrete question.
What if we look at,
three standard deviations from the center?
What if tau is three times sigma?
Well they just put a tau equal three
times sigma into Chebyshev's inequality.
And we observe that
the probability that you deviate
by three standard deviations or more
from the center is no more than 1 in 9.
Approximately 10%.
Of course,
now this is a fairly precise statement.
But to put it in context,
we should put in a snoga, slogan in place.
And without any further ado,
here's a slogan.
What we've discovered is
the probability that at chance
variable on the continuum
deviates from its expected value,
whatever it is, by more than three
standard deviations, is small.
It's no more than about 10%.
In actuality tends to be
much smaller than that.
Because Chebyshev's inequality for
all its virtues does not pretend
to be particularly sharp, right?
If you tighten the result you'll
find that the bound is excessively
generous in many cases.
And therefore, in the vast majority
of instances that a student will
encounter chance, she will find that you
have concentration around the expectation.
The center of mass of the experiment
within three standard
deviations with very high probability.
Now this is already telling us something
about the intuitive base of probability.
It now validates for
us 's ideas of center, the expectation.
As well as his idea of spread,
sigma, the standard deviation.
Both of these now have been put in
a formal footing, and connected gloriously
with our intuitive idea of what center and
spread should mean.
But Chebyshev's inequality is to
have much deeper ramifications.
It leads to the law of large numbers.
And we shall see this next.

