Abraham de Moivre discovered
the effect of the bell curve,
the normal, that we have walked
our way through pictorially.
He did this in 1733, in the 50
years after the dawn of calculus.
You must understand that, at this time,
there were no fancy displays
that he could look to.
No simple computations on a desktop
computer to churn through numerically
these distributions.
So, the investigations inevitably had to
be done by hand, abstractly, analytically.
It is all the more remarkable
that such a beautiful and
elegant realization came to the fore.
De Moivre wrote about the nominal
approximation to the binomial in 1733.
De Moivre worked with a fair coin,
p is equal to one-half.
It took a full 80 years before
the idea was generalized and
the next step was taken.
And this was taken by Pierre-Simon,
marquis de Laplace, and in 1812,
he showed that this
realization of the Moivre
extended seamlessly,
even if the coin was bent.
In other words, even if the success
probability, p, was not one-half.
And this led to this epochal theorem
that today we call the result
of de Moivre and Laplace.
So, let's start with a setting.
So, our basic setting is that
of repeated independent trials,
and to begin we are going to
deal with the tosses of a coin
whose success probability is some
value p between zero and one.
So other words X1, X2,
X3, Xn, and so forth,
constitute the Bernoulli trials
with success probability p.
We're naturally interested in
the accumulated number of success S of n.
So, for every n, S of n is a sum of the Xs
from 1 through n and we know that for
each n, Sn is now governed by a binomial
distribution with two parameters,
the number of trials, n, and
the success probability of the coin, p.
Our experience with the fair coin
suggests that we should promptly,
to view the Sns holistically across n,
we should center the distribution
at the expectation, and
scale it so
that we're looking at a unit scale.
In other words, a dimensionless problem.
Accordingly, we form, in the proper scale,
a new variable for each n.
Sn star centered at the origin by
subtracting from Sn its expected value,
and scaled to have unit spread
by dividing this by the standard
deviation of Sn, or in other words,
the square root of the variance.
We recall that the binomial Sn
has got expectation n times p.
And it has a variance n times p,
times 1 minus p, and so
the expression simplifies
into this fraction.
Okay.
The theorem of de Moivre and Laplace
deals with this normalized variable.
You will admit justly that
all we have done is properly
centered the distribution and
viewed it in the proper scale so
that we could view all of
them on the same framework.
The de Moivre-Laplace theorem
now says the following,
pick any values a and b, a is less than b.
We'd like to ask, what is the probability
that the discrete variable Sn star,
for a given n, lies between a and b?
In other words, we want to
accumulate the mass function for
Sn star in the range from a to b.
That's what additivity tells us.
The approximation by the bell curve
says that the sum of those probabilities
may be well approximated by
the area under a bell curve.
And the area being from a to b.
So, that integral notation just stands for
area, and
if you're not familiar
with integral calculus,
just think of it as an area under
a given function, a given curve.
We could relate this to
the distribution function as follows.
The area under the bell curve from a to b
can be obtained first by looking at
the entire area to the left of b.
Of course, that's what we call
the distribution function of the normal,
evaluated at b.
And then take away from thhe entire area,
all the area from a to the left.
And therefore, we get a simple form for
this in terms of
the distribution function.
Big phi of b minus big phi of a,
the de Moivre-Laplace
theorem says that the probability
that a properly centered and
scaled binomial lies between
any two fixed values,
is given approximately by the area
under the bell curve from a to b.
And that this approximation gets better
and better and better as n becomes large.
Now, there are a couple of
comments we need to make on this.
First, I have not given you
a proof of this result.
And at some level that is unavoidable.
This is the first time we're going to
encounter a result which is going to
require more than purely elementary and
easy mechanisms.
There are not many proofs of this result,
and
I can't say that any of them is
completely elementary or easy.
At least as can be shown in
a first year and a college class.
Right.
So, to get a full understanding of
the subtlety and nuance of this result,
we are going to need more
mathematics in our armory,
and I shall not dive into those waters,
okay?
We'll take our pictorial
evidence as compelling and
take this result at face value.
A second question that might
come across a student's mind is,
well, this is not completely
satisfactory perhaps because on
the left is a probability
I would like to evaluate.
A probability involving the accumulated
successes and the tosses of a coin.
A probability involving
a binomial distribution.
On the right, I've got an asymptotic
statement which says that as the number of
trials becomes large,
then we get a behavior of a certain kind,
a limiting behavior.
A question naturally then is, well,
how large does it have to be
before this approximation is good?
Or in the variation of the same theme,
for a given n,
if we approximate a binomial
probability by this
area under a normal curve,
how much error do we make?
Perfectly legitimate questions, and
important, and again let me form,
to solve, by an empirical observation
which can be supported by some theory.
The asymptotics in these settings
kick in very, very, very fast.
A typical rule of thumb is to
say that if n is around 30,
you already get very good approximations,
typically.
It also depends upon
how skewed the coin is.
The closer the coin is to being fair,
the quicker the approximation kicks in.
The more skewed the coin is,
the smaller p gets, or the larger p gets,
the more skewed the underlying
binomial distribution and
the longer it takes for
the nominal approximation to kick in.
But even in extreme cases, generally,
the approximation kicks in very,
very fast, by typically at around 100,
we should see very good fits to
the underlying probabilities.
Another rule of thumb is to say,
well, if n times p or
n times q is bigger than five,
then we should get good approximations,
and indeed we do.
It's sort of delving deeper into
the quality of approximation,
which inevitably is going to get
us into an algebraic morass.
Let us be satisfied with a slogan.
Okay.
To wit, that binomial probabilities,
viewed in the proper scale and
properly centered,
are governed approximately by
the area under a bell curve,
even for modest values of n.

