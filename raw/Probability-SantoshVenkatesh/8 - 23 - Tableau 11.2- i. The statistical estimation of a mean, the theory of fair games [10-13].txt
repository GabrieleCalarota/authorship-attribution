The law of large numbers is
perhaps the most intuitive
of the fundamental limit laws
in the theory of chance.
And part of the reason for
its appeal and for
its intuition is because it is a law that
we are most familiar with in examples.
Applications where the law of large
numbers is at work are rife around us.
So let's take stock of a few
of the common examples.
We begin by considering a game,
the basic setting.
And this time let me borrow from
the terminology of statistics.
We start with a random
sample X1 through Xn.
What this means is that we have repeated
independent trials with the variables all
conforming to a common underlying
distribution with some common underlying
expectation, say mu, and, say, some common
underlying variance, say, sigma squared.
We promptly form the partial
sum X1 plus X2 through Xn,
what we call S sub n, of the sequence.
At its heart, the law of large numbers
says something about the ratio of Sn to n.
Now, the moment we divide the sum by n,
what we're doing is
forming an arithmetic average
of the variables X1 through Xn.
In ordinary language, we are looking
at the mean of that sequence.
And since that sequence connotes a random
sample, we call Sn over n a sample mean.
At its heart, the law of large
numbers says something about
the deviation of a sample mean
from its expected value mu.
The probability of even a small deviation
from mu goes to zero as n becomes large.
But it is always useful,
when we have a clear statement like this,
to re-translate this in ordinary language,
because, of course,
it adds a little color to the picture,
and makes it a little more vivid.
What the weak law is saying, in essence,
is that if the sample size
n becomes suitably large,
then the sample mean is
approximately equal to the true
expectation or mean value, mu.
What this picture tells us is
that the weak law gives us
a principled estimate of
mu when it is unknown.
The sample mean is approximately
the expectation mu.
And if it is unknown, we now have
a principled process for estimating it.
Generate a random sample according
to the underlying chance experiment,
form the sample mean, and
that should be approximately mu.
With a certain error and, of course,
with a certain confidence.
If this sounds familiar,
it is because it is.
This is exactly the setting we saw when we
estimated population
parameters from polls, right?
So what the weak law is doing is
giving us a statistical estimate
of a mean value,
polls being our prototypical example.
In this case, the underlying chance
experiment is a Bernoulli trial.
You pick a member of a population.
And your X's are ones and zeros.
What is expectation that is unknown,
it's the population parameter p,
the subpopulation of the population in,
say, category one.
And the sample mean gives us a good
estimate of it, as we've seen.
Other examples, right.
A new car has come out, and it promises
a certain mileage, let's say an electric
car, and you get a certain mileage
before a recharge is needed.
Of course, when one purchases the car and
one drives it, the mileage
one gets depends a great deal upon
all kinds of external circumstances,
the nature of the driver,
the quality of the driver,
the traffic circumstances, maybe
the weather, maybe the age of the car.
All of these are parameters
which will affect the actual
mileage one gets in any given run.
In other words,
the mileage one gets is a chance variable.
Well, what is it, on average?
What is the expectation?
Well, this is a priori unknown.
How does the car company
give us such an estimate?
Well, the weak law tells us how
they might go about doing it.
Generate a random sample of runs,
take the sample mean and
that should give us a good estimate
of an underlying expectation.
This kind of setting can be applied to all
kinds of consumer properties that one is
interested in.
Another example is things like
calculations of lifetime.
If one goes and purchases,
let's say, an electric bulb,
it will typically come with some
kind of statement saying that its
life expectancy is so
many hours of continuous use.
How does one get such a life expectancy?
Of course, the usage depends,
again, on many factors.
Environmental factors, humidity, whether
the current is fluctuating, and so on.
Plus of course, natural variability in
the filaments and so on inside the bulb.
In other words, the lifetime of
the bulb is a chance variable.
How does one estimate its lifetime?
Form a random sample by taking many
bulbs and taking the sample mean and
averaging out the lifetimes.
And Bob's your uncle.
The same kind of time to failure is
applied to things of more portent.
For example, how long does an aeroplane
have in its operational lifetime?
In other words,
how long before metal fatigue develops and
the airplane is in danger of crashing.
And again, one in principle does
exhaustive tests of metal fatigue,
forms independent samples, forms a sample
mean, and again, we have an estimate.
Likewise, the time to failure of a bridge,
of a pylon, of a road or a building or
anything whatever, what have you.
All right, still other examples.
How does one form
actuarial tables of risk?
For example,
the life expectancy of individuals.
And again,
this is a chance-driven process.
If one forms a sample mean, we get an
accurate estimate of the underlying life
expectancy, expectation, provided
of course a sample is large enough.
The law of large numbers also gives a firm
foundation for the theory of fair games.
And the prototypical
example here is gambling.
Right.
Imagine one goes to a casino.
And one wants to play repeatedly
in some game of chance.
One pays a fee to partake of the game.
And when one plays the game, then one
wins or loses a certain amount of money.
The expected losses or
gains is governed by a parameter, mu.
Now, what should the casino charge for
you to play the game if it is known
that the expected winnings are mu?
Naturally enough, we'd say that the charge
for a fair game should be mu per trial.
If our expected winning is mu,
then you should be charged mu so
that your net expected gain is zero.
And that would be a fair game.
It's not biased or
skewed in one direction or the other.
The weak law gives us
a firm foundation for
belief that this actually is
a principled and good strategy.
One should be a little cautious in how one
interprets the weak law in the setting,
all right?
It is saying formally a very
precise mathematical statement,
that the probability that in absolute
value, the sample mean deviates from mu
by more than a tiny epsilon is small.
It does not say that you
are guaranteed to not win or not lose.
Now, the devil in here is in the details,
right?
And it is true that we've hit this
with a fairly blunt hammer in that
we normalized it by the factor n.
And over long number of trials, lots
of details are hidden in that large n.
This'll take us a little too far afield,
so I shall not get into this, but
it is in principle possible to construct a
fair game where one pays a fair entry fee
of mu each time we play the game,
and constructed in such a way that
one is essentially guaranteed to
lose lots of money, all right?
So one should always take a mathematical
pronouncement with some caution.
Remember, the mathematics,
the terminology, the definitions, and
the results, when applied to the real
world, are models of the real world.
To the extent the model captures
the essential features of the phenomenon
in consideration,
we will get accurate and good results.
To the extent that the definitions
miss a key feature, to that extent,
then the applications become suspect.
I will leave it to the student
to think of other applications
of the weak law of large numbers.
The next time she opens a newspaper or
listens to a news report, and
she sees a number thrown out, perhaps
she might be tempted to ask, oh, I see.
Here's where the weak law comes in.
And this will be a useful and
instructive way of thinking about
the weak law as you go forward.

