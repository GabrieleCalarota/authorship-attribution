Let us begin, discussion of
the first segment of this lecture.
By a review of the notions of
centrality that we have seen
in the laws of large numbers,
and in the centrality material.
The idea here is that
these capture typicality.
What other setting?
We start by considering a random sample.
Independent selection buys fray
from some underlying distribution.
Repeated independent trials.
So, let X1, X2,
X3 be a sequence of independent trials.
X represents an exemplar of this sequence.
They all have a common probability law,
a common distribution.
And let us say, for definiteness,
that the underlying probability
law has got an expectation mu and
a variance sigma squared.
We immediately form partial
sums of the sequence.
So let s sub n as usual,
represent the sum of the first n axis.
A standardized version of a sum is
obtained by centering it at zero and
making the spread unit.
And this leads to a standardized or
normalized variable.
Sn* which centers
Sn by subtracting its expectation n
times mu from it, and scales it to unit.
Makes it dimensionless if you will,
by dividing
by the standard deviation of Sn which
is the square root of n times sigma.
So, let's begin.
What can we say about these sums and
these standardized sums?
The law of large numbers, which is
the heart of the theory of probability,
says that the relative frequency,
Sn over n,
the sample mean, is asymptotically
concentrated at its expected value mu.
What does this mean?
Well we can dress it up by talking
about deviations from the center.
And in fact the strong law of
large numbers of [INAUDIBLE] says,
that the probability that Sn over n,
the sample mean,
deviates from it's expected value,
by epsilon or more, goes to zero.
Another way of saying this is that
viewed as a sequence of sample means,
S1 over one, S2 over two,
S3 over three, Sn over n.
These sequences converge with
probability one to their expected value.
So, here's a powerful
notion of concentration.
The centrality material adds
flesh to these bare bones and
says in the region around the center,
you have a normal distribution,
a bell curve emerging.
And so a formal statement is that,
a suitably standardized,
normalized version of Sn,
Sn* is asymptotically, for
larger values of n, normally distributed.
And of course what this means
is that the probability
that Sn* takes values in any interval,
is asymptotically governed by the area
under the bell curve over that interval.
So these are the two
fundamental limit laws.
We shall now tend to two subtle
applications of these laws.

