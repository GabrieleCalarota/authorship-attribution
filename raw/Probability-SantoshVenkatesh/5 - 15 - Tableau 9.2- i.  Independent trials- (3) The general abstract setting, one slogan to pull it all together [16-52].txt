We help manage to put our
finger on the key idea
of independence when we
have independent trials.
Events which depend upon
separate trials are independent,
provided the trials are independent,
in the sense
that the conjoined probability
measure is obtained via a product.
In other words, we have a product measure.
This is at the heart of this particular
understanding of independence.
The reason why coin tosses
successively are independent.
The mathematical reason why successive
tosses of a dice are considered
independent, not in ordinary language, but
in a formal and deep mathematical sense.
We should promptly codify all of this in
a general setting, of course, our notation
will inescapably get more cumbrous, but
the results are well worth the effort.
We are going to be able to get,
in a particular setting, independence
essentially for free, and this turns
out to be a key and subtle idea.
So let us do this without further ado.
So, we are going to look at
a generalization where we have
a succession of trials, and let's start
by looking at a finite number of trials.
How many?
Let us say a chance experiment,
where there are N individual
chance-based trials.
The trials need not be the result
of a particular device.
In fact, we could use different
objects for different trials.
For example, a dice once,
a coin next, cards third.
We would like to have a mechanism
which allows us to be flexible.
And accordingly, we think of the results
of each of these trials and identify them
by Gothic letters A and
identify the trial number via a subscript.
So Gothic A subscript one tells us
the possible results of Trial one.
Gothic A2, trial two.
Gothic A N, the results of trial N.
In full generality, identify
the possible results of trial one.
First, identifying the trial
number as a subscript one,
followed by the particular result,
say, the K X result and
identified as an element, A one K.
So, A one K, as K ranges over
various synthesis represents
the collection of possible results of
trial one, we call this Gothic A one.
Similarly Gothic A two and Gothic A n.
Comprised of elements A,
sub n, trial number k,
identifying which outcome gives you
the collection of results with trial n.
Each of these trials is chance driven.
It's a discrete setting, and therefore,
we ascribe an atomic measure
to the singleton, A1K.
The atom in the first trial
of obtaining the Kth element.
We allocate a probability,
a chance P subscript one for the trial.
And now, we move the identifier of
which outcome to the argument K.
So P1K is a mass in trial one
allocated to the atom A one K.
K now result of one or all possibilities.
Of course,
the P one Ks have to be non-negative.
And as a sum over K,
P one K has to add to one.
Do the same thing for A two, A three, A n.
For trial N, the space of
possible outcomes is Gothic A n.
The mass function we allocate to
each possible outcome, each atom, A,
trial N, outcome K is the probability P,
subscript n for
the trial, argument K identifying
which outcome we are looking at.
We now have a system
of independent trials.
Each trial has got an alphabet
of possibilities and
a mass function attached to it.
These mass functions individually have to
be non-negative, they have to add to one.
Non-negative, adds to one.
Non-negative, adds to one.
We now stitch the compound
experiment together by looking
all these trials on mass.
The compound experiment is specified
by specifying the result of each trial,
naturally enough.
We identify the results of each
trial as an ordered interval.
The first element of the [INAUDIBLE]
A1, K1.
Represents the K1 at element of trial one.
The second element, A2, K2,
represents the K2 at the element of
trial two, the last element, AN,
K sub n represents the K sub
n in that element of trial A.
The notation is inescapably more compress.
But we have many balls in the air, and
we need notation which keeps
track of which is which, alright?
We already defined a particular
element of trial one, K1,
a particular element of trial, two K two,
a particular element of trial, n, K n.
And together,
this forms an ordered n-tuple,
which tells you what a sample point
of a conjoined experiment is.
As K1 ranges a row of possibilities,
K2 ranges a row of possibilities,
Kn ranges a row of possibilities.
You allow your sample point
your N topple to range over
all possibilities in
the space of entrance.
This then is a conjoined sample space.
This is obtained as a cartesian
product of [INAUDIBLE] K1,
with [INAUDIBLE] K2, with [INAUDIBLE] Kn.
The sample space now is a product space.
And naturally, we now need a, measure.
And the measure we attach, and this
where the independent trials comes in.
Is to each atom, each will attach a mass,
a probability, which is a product
of the individual masses.
So the probability attached
to the n-tuple A1 K1, A2 K2,
An Kn is a product of P1 K1, P2 K2, Pn Kn.
The verification that this is a bona-fide
probability measure is trite.
It's clearly a product
of positive numbers.
It's positive.
It adds to one because the sum
of all K one K two through K n
separates it into a product of
sums over K one K two through K N,
just as we had for the case of two trials.
And therefore, each of those sums is one,
and the product of one is one, and
therefore it's properly normalized.
This gives us the setting of
the air compound experiment
where you have a product space, and
now a product measure, which is given
to us via the independent trials.
We now need events which
are determined in terms of each trial.
So, let us pick the trial for
definiteness.
The trial has results which are captured
in the alphabet Gothic A, sub j.
Each of those results has an atomic mass
captured in the values B sub j of K.
K running all possibilities.
Identify in the space of possibilities for
Gothic a j,
K sub j,
which represent elements of the Kth
of the Jth trial,
which can trigger your limit.
Identify the event A j with the occurrence
of those possibilities in the Jth trial.
Formally then,
A sub j clearly has an event.
It has to be a subset of a sample space.
Therefore, it's a collection of n-tuple.
But this collection of
n-tuple has a Jth member,
an element which is from
a defined set of possibilities.
It's in a restricted range of
possibilities, hard elements for
the Jth tiral.
And the remaining elements
are completely unfettered.
Again, bordering strongly
from the infusion we have.
For the case of two trials, we will say
a subject is determined by the Jth trial.
All you've done is
specify a set of indices,
[INAUDIBLE] K sub j, for the Jth trial.
And the outcomes of any of those
outcomes triggers event AJ.
The remaining trials are irrelevant,
as far as AJ is concerned.
Do this for J running from 1 through N.
We now have a collection
of N independent events.
A1, determined by the first trial.
A2 by the second trial, and so on.
A n determined by the nth trial.
Any events so constructed,
where each of the basic constituent events
is determined by one underlying trial
alone, separate from the others.
Any such collection of independents is,
without the necessity of any calculation,
independent.
Why?
For the same reasons we saw for
the case of two trials.
The probabilities separate and multiply.
There you go.
In this simplest case, this is a setting.
That is so familiar to us and
intuitive for us.
A specialization of this gives us coin
tosses, throws of a dice, and so on.
And this is perhaps the single
most important sub case
of the general edifice, right?
And it's worth focusing on.
This is the case of
a repeated independent trial.
In other words, I'm flipping the same,
same coin, again, and again, and again.
The coin has the same alphabet,
let's say Gothic A for every trial.
And we're throwing the same dice,
again, and again, and again.
The die has the same alphabet,
gothic A, for each trial.
In other words.
All the alphabets.
Gothic A1 through gothic A and,
are all the same.
They're some alphabet gothic A.
We can now dispense with the subscripts
because you just have one common alphabet.
Let's just call the possible
outcomes of a trial little A sub k.
And K runs over a finite, or
possibly a countably infinite range.
Okay.
Now, the trials are repeated and
independent.
They are repeated with the sense that
the chance conditions don't change.
Or therefore, for each possible outcome
in a trial, we allocate a mass.
And that mass for say, the K, the outcome
AK is, let's say, a value p of k.
The p of k's have to be non-negative.
Have to add to one.
Positivity and normalization.
We now have a chance
experiment trial by trial.
The statistical conditions for
the trial have remained the same.
But of course,
the trials are independent, in the sense,
that when you stretch together
a conjoined experiment,
now omega is a cartesian product of
Gothic A with Gothic A, n times.
And naturally enough, we'll write
a product with it's self n times,
as a power, Gothic A to the power n.
It just represent a cross a,
cross a, cross a, n times.
What is this?
It's a collection of elements of N
elements and N-tuple from our alphabet.
Sampling is with replacement.
Oh, this is beautiful.
So AK one, AK two, AK three, AK n.
The results of what happens
in each of the trials
when you are drawing from the same
alphabet, again and again and again.
Sampling with replacement.
The product space is now simple,
it's clean.
The system of n-tuples we
saw this repeatedly, for
example, in three tosses of a coin.
The product space was
a system of three heads or
three tails, or
a triple of heads and tails.
Each element of triple is drawn
from the same alphabet, h and t.
The location of the element in
the interval tells you which
trial we're dealing with.
And that's it.
Very clean and simple notation.
What does a probability
measure we attach here?
Well, the mass function
attached to any n-tuple
is simply a product of the individual
masses of the elements of the n-tuple.
So in other words, if you're looking
at the end tubal, k1, k2, through kn,
the probability of ak1-akn is p of k1,
the mass attached to a k1, times p of k2,
the mass attached to a k2,
and so on, times p of kn,
the mass attached to a kn.
Very, very clean and simple structure.
In this setting, any event A1,
depending only on the first outcome.
Any event A2,
depending only on the second outcome.
Any event AN depending
only on the NA outcome.
These constitute a family
of independent events.
This is now a specialization,
but it's a useful and
practically important
specialization of a gentle setting.
A generalization in
a different direction will
consider not just a finite number
of independent trials, but
possibly uncountably infinite
number of independent trials.
We've seen examples of this type before.
Imagine for instance,
the repeated indefinite toss of a coin.
Of course, this is a Gedanken experiment,
a thought experiment.
One couldn't possibly toss a coin
indefinitely an infinite number of times.
But one could imagine this.
This now results in a setting
where the compound experiment
is a very rich product space.
A space of entire infinite
sequences of trials.
All that we've done here will more or
less go through the one place where we
get a technical complication is, and how
does one specify a probability measure?
We can't simply multiply probabilities
because if you multiply an infinite number
of them, you get zero.
We saw that this technical complication
was handled by more spaces.
We won't take the time to make
that particular detour now, but
we will abstract out the key elements
of what we've discovered now, for
both finite and countably infinite spaces.
And we've captured in a slogan.
In the case of a finite or
even countably infinite number
of repeated independent trials,
the compound experiment or
the product space, to which is attached
a product measure by applying individual
atomic properties in such settings,
events determined by distinct
trials are independent.
What this means is that in such settings,
that formidable list of conditions
required for independence.
[INAUDIBLE] probabilities, all three-way
probabilities, all quadruple prob-,
all of those conditions
are immediately and tritely satisfied.
This turns out to be wonderfully rich and
subtle.
And when you work on this next by
looking at first a couple of examples
from settings which are much more rich and
interesting than
the more banal tried simple
examples,we looked at in literature.
Well, take one, for example,
from this fear of gambling.
And we'll pick one from genetics.
And with those under our belt,
we'll now have come to a proper and right
understanding of what this subtle and
deep and central idea of independence is.
With an idea of independence under
our belt, we will now be properly
situated to ask what happens
when independence is lost.
First by small amounts,
then by large amounts.
In understanding of independence,
confers upon us eventually a deeper and
richer understanding of settings where
we do not have independence, so we show.
Go on next to two rich examples of this
particular framework we have put together.

