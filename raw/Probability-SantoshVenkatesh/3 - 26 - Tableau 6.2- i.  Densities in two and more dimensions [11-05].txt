With the structure of one-dimensional
densities under our belt,
we should promptly grasp some low hanging
fruit waiting there for us to pick up.
Okay?
The structure extends naturally and
inevitably to two and more dimensions.
And let's see how this works.
Following this sage advice of
Hungarian mathematician George Polya,
we should begin with the simplest version
of the problem for which we do not yet
have an answer.
And naturally the simplest step up
from one dimension is two dimensions.
So what would comprise
a density in two dimensions?
Now naturally enough, borrowing from
what we know from one dimension,
we will insist upon two properties.
Positivity on the one hand and
normalisation on the other.
We will think of a two dimensional
function where the variables,
naturally and inevitably we call x and y.
We'll reuse notation p for
now a two dimensional density function.
And therefore a two
dimensional density function,
p of xy satisfies two properties.
It has to be non-negative everywhere, and
the integral over the plane of
this function should be unit.
Of course there's an underlying chance
experiment we're appealing to, right?
Where does the probability come in?
Well the function p of two variables
stands in the rule of a probability
density in two dimensions,
where its units are of
probability mass per unit
area at the point x, y.
And where does the area come in?
Well through the terms dx dy.
This correspond to a rectangle,
an infinitism of rectangle of sides dx and
dy and
dx dy use the area of the rectangle.
And therefore all we've said
in the integrand is that
when you multiply mass over area by area,
you get mass.
Stitch together the masses using
integral as summation per Leibniz and
we get the probability mass.
There you go.
Now, there's of course
an underlying chance experiment and
we should crystallize our
understanding by focusing on it and
making sure we understand in just
proportion how all these objects fit in.
So, let's go back.
So we have a density in two dimensions
satisfying positivity and normalization.
What is the underlying sample space?
Well, naturally we have two variables.
And we're thinking of
a composite experiment
where the space is a Euclidean plane.
We could interpret it geometrically
as two coordinate axes and
the region described by them,
and algebraically,
in the notation which has become
traditional for the plane, R2.
So in this context,
a sample space omega is R2.
And R2 means, algebraically,
all pairs of points x,
y where x and
y range over all real values.
Each performance of this experiment
gives us a composite experiment
where you have two trials which
produce values, let us say x and y.
So a sample point in this experiment
is a pair of real values, x and y.
Tradition compels us to use uppercase
letters at the end of the alphabet
to describe these possibilities for
the trials.
The pair of values X, Y then
constitute a pair of random variables.
And such a pair in this
composite experiment
describes a sample point
of the experiment.
What are the events in this experiment?
Well remember, events always
are subsets of sample points.
What are natural subsets of sample points?
Well they are regions of the plane, and
the simplest, basic regions we could
imagine building upon the intuition
of intervals in one dimension
will be a pair of intervals, and a pair of
intervals gives us naturally a rectangle.
And therefore, the basic events, basic
regions, are rectangles in two dimensions.
We can describe it of course
geometrically, or algebraically,
as a Cartesian product of two intervals
which gives all pairs of points x,
y where the first variable x
lies in the first interval,
the second variable y lies
in the second interval.
Now we want to go ahead and
assign a probability to this.
How do we go about doing this?
Well very simply accumulating
mass by integration.
In notation, the probability
that a pair of random variables
comprising the composite experiment x, y,
the probability that this lies
in a rectangle is the integral,
the accumulated mass of your two
dimensional density over that rectangle.
There you go.
Simple.
What about more general events.
Well a generic event in two
dimensions is some region, or
possibly some composite collection
of regions in two dimensional space.
Algebraically your region A is
a subset of the Euclidean plane R2.
How do we compute its probability?
Well, we could imagine tiling this plane
with a lot of itsy bitsy tiny rectangles.
And then simply adding up the masses
of each rectangle via additivity.
But of course, we'll want to make the
rectangles tinier and tinier and tinier.
And in the limit, the sum becomes an
integral and what we have, the probability
that a random pair lies in
a designated region A is
that integral over A of your two
dimensional density function.
It is how all the intuition
just carries forth.
Having got two dimensions under our belt,
it is now clear what the general
structure should be for many dimensions.
Right?
The terminology and
the notation is going to now, inelectably,
get a little more complex.
But that should not concern us,
the basic ideas are clear.
What should describe a density in say
some generic number n dimensions?
A density n dimensions is
a function of n real variables.
Which satisfies A positivity,
and B normalization.
The integral of this
function should be unit.
Of course there's an underlying
chance experiment,
where does the probability come in?
This density function
connotes a mass density.
But now it's a density in N dimensions.
In one dimension you have
units of mass per unit length.
In two dimensions you have
dimensions of mass per unit area.
In three dimensions naturally you
have units of mass per unit volume.
And in any dimensions we will
generalize this concept and
say this is representing a mass per unit
n-dimensional volume at a given
n-dimensional point x1 through xn.
Where does the volume come in?
It comes in from the product of
all those infinitesimal terms.
Think of this as an n-dimensional
parallelepiped whose sides are dx1,
dx2, dx3, dx4, dx5 through dxn.
You multiply all of these values out, you
get the volume of that parallelepiped and
that is your infini, infinitesimal volume.
Mass per unit volume times
volume gives you mass and
you're just stitching masses together,
they are added to d, and there you go.
What is the underlying sample space here?
The underlining sample space is
a Euclidean n-dimensional sample space.
And again tradition compels us
to write level maker, Rn for
the n-dimension Euclidean space.
Of course the notation
looks a little complex for
someone who hasn't seen it before,
but it describes a very simple object.
This connotes a composite experiment,
where you have n trials giving
you values x1 through xn.
These values x1 through xn range over
all possibilities for real values,
and that gives you an n-dimensional space.
Tradition again compels us to
write the sample points for
this experiment using uppercase letters,
generically X,
but now we have to describe which
trial gives rise to which outcome,
and naturally we'll use subscripts
to capture this complexity.
X1 denotes the chance
outcome of the first trial,
X2 of the second trial, Xn,
the chance outcome of the nth trial.
And together we have
a composite experiment
comprised of n chance outcomes,
X1 through Xn.
Naturally enough this describes
in the parlance of algebra,
a vector, and therefore the sample
points in this composite experiment
are what we call random vectors.
It is frequently useful
to compact notation and
write simply vectors in bold font to
represent an n-tuple of real numbers.
Okay, so now we have a right understanding
of what underlying sample space is and
what the outcomes are, these idealized
outcomes of this composite experiment.
What are the events?
Well naturally events are regions
in this Euclidean space.
So any even A has to be a subset of Rn,
and
I've given you a fanciful region just to
show you a region could be anything you
like over which one can accumulate
probabilities in some reasonable way.
How do we compute probabilities?
Well, probabilities are via
additivity identified with integrals.
The probability that a random vector,
the outcome of this composite
idealized experiment,
lies in a designated region A, is
simply the integral of your
n-dimensional density over that region.
And there we have it.
So starting from a process of
segueing from discrete experiments to
a continuous experimental limit, we get a
one-dimensional random variable, and from
there very quickly, we build up composite
experiments in two dimensions and more.
All of these ultimately are probability
spaces, characterized in the continuum
by these functions which distribute
mass according to a density.
And the density, generically,
has got units of mass per unit volume.

