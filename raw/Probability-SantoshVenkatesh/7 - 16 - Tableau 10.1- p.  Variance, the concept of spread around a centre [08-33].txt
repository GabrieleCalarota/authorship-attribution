So thus far we have validated our
observations that the binomial
distribution has a unimodal character.
The probabilities increase and
then decrease.
The point of maximum probability
corresponding to a vicinity of n times p.
The center of mass of the distribution
is exactly n times p.
And the probabilities are distributed
fairly evenly around that center of mass.
Our next question, then, is,
how are the probabilities dispersed
around the center of mass?
Are they highly dispersed.
Or are they more concentrated?
So let's begin by taking
a look at a familiar picture.
Consider the setting where the number
of trials is fixed at 100,
and consider the effect of
varying the bias probability p.
Say we have three figures on our picture,
corresponding to p = 0.3,
and p = 0.5, [INAUDIBLE], and p = 0.8.
It's clear, visually, that
the distribution probabilities are most
dispersed where the coin is fair that
the probabilities get more concentrated,
albeit slightly,
on the scale when p moves away from 0.5.
A little introspection will convince
us that this is entirely reasonable.
After all, if one has a fair coin,
we have a maximally unpredictable outcome.
The moment that you put
a bias into the coin,
the outcome becomes more predictable,
and therefore,
that should suggest that a probability
distribution should be most unpredictable,
in other words, should be most spread out
and not dispersed when the coin is fair.
And, of course,
we see this in the picture.
Now fixing n has fixed one
parameter of varied p.
The other possibility is to fix
the success probability and vary n.
So let us suppose that the success
probability is fixed at 0.7.
Here is a picture.
Corresponding to ten trials and
we get something very much
like what we've seen before.
The maximum of this distribution
arises exactly at n times p
which is the expected value and
happens at ten times 0.7 or at seven.
If we increase the number of trials while
keeping the success probability fixed.
Say we consider 50 trials.
Now, we've got the experiment
where the number of successes can
be anything between 0 and 50.
The maximum probability outcome
corresponds to the setting where we have
exactly n times p, or
15 times 0.7, or 35 successes.
And now, we have a spread around there.
If we increase n yet again, and
look at n is equal to 100 trials,
now, the largest probability corresponds
to the expected value of 100 times 0.7 or
70, the probabilities
falling on either side.
Now, it's very clear, when one looks at
these pictures, that the probabilities
appear to be getting increasingly
clustered around the center.
It'll be convenient to view
this in a common scale.
After all, when n is 10, then our X
axis is running from 0 through 10.
When n is 50, we get from zero to 50.
N is 100, zero through 100.
To put things in a common scale,
divide k, the number of successes, by n.
Call that x.
So as k runs from zero to n,
k over n or x runs from zero to one.
Now, we can plot everything on a common
scale from 0 to 1, as a function of' x.
In other words,
if we replace k by n times x,
then in each of the 3 cases,
we can allow x to run between 0 and
1, and obtain the various binomial curves.
It's very clear that as n increases that
the probabilities are getting more and
more and more concentrated
around the central point.
Of course, the probability is
also decreasing in height, but
that is inescapable
because as we increase n,
there are more and
more points with positive mass.
And they all have to add
to one by normalization.
But the main idea that there
is a shrinking of a support of
the distribution or the effective support
of a distribution seems inescapable.
Now, to quantify this,
we need to have a clear and
simple mathematical way of quantifying
this notion of spread around the center.
Let us turn to this next.
As before, physics and
physical intuition gives us
a very good starting point for
constructing a measure of spread.
The picture on your screen shows you
a familiar object, a binomial distribution
corresponding to 10 tosses of a coin
who's x's probability is 0.6.
More gingerly, we know what the binomial
probabilities look like, and now, we also
know that the expected value of the
distribution occurs exactly at n times p.
In this case,
with n being 10 and p being 0.6,
they expect a value that
coincides exactly at the 0.6.
Now, suppose one does the experiment.
We toss a coin ten times and
we count the number of successes.
Suppose we get eight successes.
It's clear in the picture that
the probability of eight successes is
not negligible, it's significant.
How far is eight from the center six?
Well, clearly, eight minus six are two.
Now, it makes little matter whether we are
to the right of six or to the left of six.
All that matters is how far
we are from the center.
That gives us an idea of
distance from the center.
So, a 0.4 is just as far from
6 as 8 is on the other side.
And, therefore, we are more
interested in the absolute value
of the distance from the center than
in the distance itself with a sign.
It would be more convenient
in such settings
to consider the square of the distance.
And if we did that,
we find an object which is on physical
grounds very, very, very familiar.
The square of the distance of zero from
six is 0 minus x the whole squared.
Of course, 0 successes occur with
a very small probability, v of 0.
The square of the distance 1 from 6 is 1
minus 6 the whole squared or 5 squared.
Of course,
one success occurs in the setting, again,
with a very small probability, b of 1.
But as k increases, 2, 3, 4,
the binomial probability increases and
is spread,
k minus the center, the whole
squared becomes not at all unlikely.
We simply accumulate the weighted sum of
squared distances from the expected value.
This is called a Variance of the Binomial,
the Variance of the Accumulated Number
of Successes, and formally,
it's written as 0 minus np the whole
squared times the binomial probability
of 0 plus 0 minus np the whole squared
times the binomial probability of 1.
The generic term is if 1 has k successes
the square deviation from the center
is k minus np the whole squared.
You multiply that by b of k, the weight
attached to that likelihood, and
sum over all possibilities.
We have weighted sum.
The students might feel this is
vaguely familiar, and indeed it is.
In physics,
this corresponds exactly to the concept of
a moment of inertia of a physical system.
And now we have this in hand,
we should promptly compact the notation.
And putting in place a summation
notation says, allow k to run over all
possibilities, in this case over all
the integers from zero through n, and for
each such value of k,
take the squared distance from mp,
k minus mp the whole squared.
Weighted according to the probability
of observing, K successes and add.
And now, we have a mathematical, precise,
clean concept of the idea of
spread around the center.
This is called a Variance, and if we want
to identify the underlying experiment,
we'll write it as VAR for variance of
the accumulated number of successes as n.
Tradition compels us when there
are no ambiguous settings
to call this the Greek
letter sigma squared.
Of course, sigma squared has become
part of our common consciousness.
It's become part of our
common understanding
its referred to frequently in news.
We here about sigma,
2 sigma deviations, 3 sigma deviations.
The concept arises in this idea of
a probabilistic moment of inertia.
Now, when it sends that as we look at
the variance, we're looking at a squared
deviation, or more precisely an expected
squared deviation from the center of Mars.
If one wants the spread itself,
naturally enough,
one should look at the square
root of this quantity.
This is called the Standard Deviation.
It's, of course, represented by sigma, and
it is defined to be the positive
square root of the values.
Of course, given an n and given a p,
one could readily compute such a sum.
Of course, such a numerator
calculation has it's own limitations.
In particular, it gives us no feeling for
the dependents of the spread
on the parameters n and p.
So, to do this, we're going to need
to do some more algebraic spade work.
So let's turn to this next.
The computer variance, keep in mind,
that we have to look at, is sum of squared
distances, squared deviations from
the mean value, from the expectation.
To do this, let's go back and
take a look at a generic term in the sum.
Sigma squared is the sum of k minus np,
the whole squared,
weighted by the binomial probabilities.
It looks messy.
It looks ugly.
But, to begin with,
let's isolate the role of k.
Take a look at the whole
squared inside the sum.
Well, that looks familiar.
Well, that is of the form of
a minus b the whole squared,
which, as a very early middle
school class has taught us
is given by the beautiful formula
a squared minus two ab plus b squared.
Identify a with k, b with np,
and expand out the square.
At the moment we do that,
we've got an expanded square
weighted by binomial probabilities,
summed over all possibilities.
One of the great virtues
of addition is that
the sum of a bunch of objects can
be done in any order whatsoever.
This is commonly called
the Associative Property of Addition.
In this case, let's group the sums
together by first grouping the sum
over the k squared term, then grouping
the sum over the 2 n p times k term,
and then finally grouping the sum
over the n squared p squared tab.
And if we do this,
we now have 3 separate sums.
Now, it's going to begin to look
moderately complex, moderately heavy, but
it's going to simplify in a hurry.
Let's begin by looking at
the first sum on the right,
the sum of k squared times b n of k.
We've already done in
our sample exercise,.
A computation of what k squared
times b n of k is, as we say.
And therefore,
simply replacing k squared b n of k
by our identity involving binomial
probabilities of n minus 2 and binomial
probabilities of n minus 1 tosses.
And summing all k gives us a morbidly
complex looking expression.
But we immediately realize that
the binomial probabilities over b n minus
2 add to 1, the binomial probabilities
over b n minus 1 add to 1.
And so,
the entire sum of k squared times b n of
k p sums into a very simple expression.
We have a closed form answer, n times
n minus 1 p squared plus n times p.
Let's turn to the second sum.
Now, we have a sum of k times b,
n of k p, but.
We have a basic binomial identity.
Replace k times b n of k p by
a summing for v b n minus 1.
The sum of the b n minus 1 probability
is again adds to one binomialization and
then does some.
We observe as now just n p times one,
or n p, of course,
we realize, we've just retraced our
path in computing the expected value.
This sum is exactly the expectation of
the accumulated number of successes.
And finally, the last sum.
And this is the simplest of all.
We're summing over all the binomial
probabilities corresponding to
n tosses of a coin, and that's sum
manifestly by normalization is one.
Now each of the three terms has
been simplified at closed form.
Now, there's little that
remains bother shouting.
All that remains now is assembling
algebraically the various expressions
and simplifying.
So let's begin.
The variance is first a term corresponding
to the summation of k squared b n of k,
then a term corresponding to
the sum of k times b n of k,
then finally a term corresponding
to the sum of b n of k.
And now we've got an expression like this.
And a little algebraic massaging sees
that the term n squared p squared
offers several places in the expression.
You end up having a total of n
squared p squared minus 2 n squared p
squared plus n squared p squared and
those terms vanish,
leaving behind a simpler expression,
minus mp squared plus n times p.
But, of course, now we feel irresistibly
compelled to simplify it further by
factoring out the quantity n times p,
leaving behind n times p times 1 minus p.
One more step makes it
completely transparent.
We realize that 1 minus p is
exactly the failure probability q.
And therefore, the variance becomes n p q.
Again, one could hardly ask for
a simpler resolution of what
looked like a very complex sum.
And so to summarize.
If one has a binomial experiment, well,
if one accumulates successes over n tosses
of a coin whose success probability is p,
then the expected squared
spread around the expected value, around
the mean, around the center of mass.
In other words, the variance of this
distribution is exactly n times p times q.
We should very quickly consolidate
by taking a look at a picture.
So let's go back to the setting
where p was 0.7 and we vary n.
Now we say that the concentration
of the distribution
of the spread around the center
is now quantitatively clear.
Variance is n times p times q.
The standard deviation or the true measure
of the spread, is the square root of
the variance, and that one behaves
like a square root of n times p q.
In other words, if n is is 10, then
the spread is around square root of n,
around plus or minus 2 or
3 around the center.
Now the reds you start at 7 and
you look at plus 2, plus or
minus 2 around 7 and you get most
of the mass of the distribution.
If n were 50, then the square root
of n is about 7, and therefore,
the spread of the distribution
should be around plus or
minus 7 units around the center of 35.
And again, it's very clear that his
is exactly where the spread is.
And finally, when it's 100,
then the square root of 100 is 10.
We expect most of the spread to be
within 10 units around the center of 70.
The whole point of the exercise is this,
as n increases, the spread,
the center of mass inreluctantly
moves to the right.
But the relative spread only
increases like the square root of n.
So, if n is 100, then the center is
at 70 for a coin with a bias of .7.
It's at 50 for a coin with bias .5.
But the spread is about plus or
minus 10 in both cases.
If n were 10,000, then the spread
around the center is only about 100.
So, yes, of course, the spread is
increasing as it increases, but
proportionally, much, much, much,
much less than where the center is.
We should capture this
promptly in a slogan.
The binomial distribution is
increasingly concentrated
around its expected value as n increases.

