Let us now take a look
at our definition NU and
extract a different flavor,
a different color from the definition.
It's a variation on that theme,
this time taking into account
conditional information.
Recall that the idea of conditioning
effectively reduces a sample space
to the space governed by the event
described in the aside information.
No new concepts are needed to describe
independence with regard to this new,
foreshortened, smaller sample space.
So without any further ado, here is now
a definition of conditional independence.
So we'll say that two events A and B in
some probability space are conditionally
independent given an event C of
some positive probability if
indeed a rule of products emerges for
conditional probabilities.
The probability of A intersection B
given that C has occurred is equal
to the probability of A given C
times the probability of B given C.
Again, let me reemphasize that no
new ground has been covered here.
This is exactly the same old idea of
independence as a rule of products.
In a conditional setting, we're taking
into account a fact that conditioning
on the occurrence of some site
information C, some event C,
effectively has reduced
the sample space to C.
And then all event probabilities
are now reflected into C.
Independence in that context,
again, is just a rule of products.
Now, let me immediately
give you an example
of a common setting where this kind of
conditional independence arises naturally.
Again, we go back to coins.
So, the setting is as follows.
Imagine that now we have two coins,
a coin, say coin one,
really creatively called,
whose success probability,
the probability of a head, is one in six.
And a second coin, creatively coin two,
whose success probability,
the probability of a head, is five in six.
The first coin is
weighted away from heads.
The second coin is weighted towards
heads in these proportions.
We select one of these
two coins at random.
We're equally likely to select one coin or
the other.
Having got the coin, we pick it,
put away the other coin, and
toss the chosen coin twice.
[NOISE] Okay, what is the probability
space that is appropriate for
this problem?
As always,
we'll begin with the sample space.
The sample space, recall,
to identify what it, this is,
we're going to have to track down all
sources of uncertainty in the experiment.
Let's do this systematically.
First, there is an uncertain
selection of a coin.
There's a coin-choosing mechanism.
Good.
Having specified a coin,
there are two tosses.
So naturally enough, if I specify
a triple, the selection of the coin,
the result of the first toss,
the result of the second toss, I'll have
completely eliminated all uncertainty
in what happened in this experiment.
The sample space, therefore, is a space
of triples specified by a coin selection,
coin one or coin two, followed by
the results of two tosses, heads or
tails, heads or tails.
Excellent.
Let's put some events
of interest together.
So, let's start with some very banal and
simple events.
Say A is the event that the first toss
of the chosen coin results in a head.
Let B be the event that the second
toss of the chosen coin
also results in a head.
Now, we want to say something about A and
B, right?
But the structure of
the problem makes clear that
an ancillary event might be useful.
And which ancillary event?
Naturally, the specification of the coin.
Let C be the event that the coin
chosen is the first coin, coin one.
Of course,
the complement of C is the event that
the coin chosen is the second coin.
Now, what is an appropriate
probability measure for this problem?
Now, I'm going to posit for you that there
is indeed a natural probability measure.
And that measure says that conditioned
on the choice of the coin,
if I chose the first coin,
then the chance of getting a head on
that toss of that coin is one in six.
And this chance is unaffected
by the number of tosses.
Mathematically, we want to put all this
verbation to a succinct encapsulation.
And what we're really saying is
the probability of A given C,
the probability of a head given that
coin one is chosen, is one in six.
And this the same as the probability of
a head given that coin one is chosen on
the second toss of the coin.
So far, so good.
How do you put the two tosses together?
Now, the moment we think about flipping
a specified coin again and again,
we want to model that these
successive tosses are independent.
Okay, I'm using the word loosely now.
But what is a formal structure?
This means that the result of each toss
don't affect the results of other tosses.
Formally, this means there
must be a rule of products.
And what we're saying then is the chance
of getting a head successively
on two tosses of this chosen coin one
must be covered by a rule of products.
And therefore, the probability of
A intersection B, of getting two heads
in a row given that coin one is being
tossed, must be, if there is any
justice in this world, the product
of the corresponding probabilities.
But each of the probabilities
is one in six.
And therefore, the chance of two heads in
a row for coin one should be one in 36.
Observe that in the specification
of this problem,
it is implicit and
natural that the events A and
B are conditionally independent
given the choice of coin.
Of course,
you verify that given C complement again
results in independence for A and B.
In this case, I'm tossing the other coin.
And the chance is now going to become
5 in 6 times 5 in 6, or 25 over 36.
Well, so here's a natural setting where
conditional independence arises naturally.
Before we move on,
we should pause and say, well,
what can you say unconditionally
about the events A and B?
At the moment I start thinking about this,
you realize, well, gee,
we have a bit of a problem here.
What is the probability of the event A,
that the first toss results in a head?
But A says nothing about the chosen coin,
so it could be that I tossed coin one and
I got a head, or it could be that I
tossed coin two and I got a head.
Which of these two cases?
Well, this is not specified a priori,
but in, implicitly here is a natural
partition for the problem.
The event C partitions the space
into two pieces, C and C complement.
And therefore, total probability,
additivity of probability measure,
allows us to stitch together a chain for
the probability of A, in what will be
the conditional probability of a heads
given the particular choice of the coin.
But here we know all the parameters.
The probability of
A given C is one in six.
The probability of choosing
coin one is one in two.
The probability of A given
the complement of C is five in six.
The probability of
choosing coin 2 is 1 in 2.
And the fractions assemble, collapse,
and give you back one in two.
Pause for a moment and
say, did I expect this?
And some introspection might convince
you that in fact this was to be expected
because of such beautiful symmetry,
chance one in six and five in six,
chance five in six and one in six, that on
average I should get exactly the midpoint.
Nice and beautiful.
What about the event B?
Well, a little thought will show you
that the analysis is exactly the same.
And therefore, the chance that you
get a head of the second toss of
whichever the chosen coin is by total
probability is again one in two.
What about the probability
of A intersection B,
that the first and
the second tosses result in heads?
Again, we are naturally going to condition
upon whether we chose coin one or
coin two, and
total probability gives us an identity,
the probability of A intersection B
conditioned on whether coin one was
first chosen or coin two was first chosen.
And now we simply write
out these probabilities.
Remember that A and B are conditionally
independent given C or given C complement.
Given C, A and
B have a chance of one in 36.
Given C complement, A and B have
a chance of 5 squared over 6 squared,
or 25 over 36, as a ballot.
Put it all together and
we've got a simple identity.
Simplify it, massage the numbers,
and we find we get 13 over 36.
Now, the actual numerical value
makes little matter here.
The question at heart,
if you're going to ask is this, are A and
B independent, unconditionally?
Well, naturally we need to
check a rule of products.
Is it true that a rule of products P of
A to section B is equal to the probability
of A times the probability of B?
Well, the probability of A times
the probability of B is one-half times
one-half, which is a quarter.
And that is manifestly
not equal to 13 over 36.
And therefore, the probability of
A intersection B is not equal to
the unconditional probability of A times
the unconditional probability of B.
We conclude that A and
B are not independent unconditionally.
There's a slogan we need
to extract from this, okay?
And what is the slogan?
That independence can affect things in
unexpected ways, and that in particular,
conditional independence does not imply
unconditional independence or vice versa.

