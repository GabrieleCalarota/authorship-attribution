Here's the setting.
Remember the Sk's are all the ky's
intersection probabilities summed.
k running from 1 through n.
The inclusion, exclusion principle tells
us I can get the probability of a union,
a disjunction, by adding and
subtracting these terms one at a time.
Now I'm going to remind you of an old
principle and
this is going to engender for
us a completely new and
very useful vantage point.
So to do this, let me introduce
just a smidgen of notation.
Remember that in the context of sieves,
I talked about the events A1,
A2, through An as bad events.
Of course this is just to
lend color to our problem,
which would otherwise be drab and
algebraic.
So, bad events occur if
any of the As occur.
Let's introduce some notation to
capture the occurrence of a bad event.
So, for an index j,
running from 1 through n,
let x sub j be a variable,
which takes a value 1,
if the bad event Aj occurs,
and is 0 otherwise.
In other words, the value of Xj is a flag,
is an indicator for
whether the bad event Aj occurred.
This is promising because the moment
I give you nomenclature and
notation like this,
that should tug at maybe a faint
atavistic thread in your memory.
Where have I seen things like this before?
Of course, Xj is a chance variable.
It depends upon the performance
of the underlying experiment,
and whether the chance outcome is in Aj or
not.
But Xj at heart is a coin toss.
It's a 1, 0 variable.
Xj is a Bernoulli trial.
Now what's complicating matters here is
that as you let j run through the indices
1 through n, then the values X1, X2,
X3, through Xn, the indicators for
whether the bad events 1 through n
occurred are not necessarily independent.
If they were we'd have the familiar
setting of repeated tosses of a coin.
Now we have tosses of a coin but the
tosses could depend on previous tosses.
Dependency is baked into the,
into the cake here.
But nonetheless,
our experience of 20 coin tosses says,
well, it is very profitable to
look at the accumulated sum,
the accumulated number of
successes of these coin tosses.
Let's do so without any further ado.
Suppose we add the indicators
X1 through Xn.
I, I would love to give this sum a name,
S, but S is already spoken for.
We used Sk for these sums of these
conjunction probabilities, so
let's introduce a new notation.
Let's call this partial sum
of X1 through Xn, M sub n.
This stands for
the accumulated sum of these indicators.
In words, Mn tells us how many
of the bad events occurred when
the experiment was performed.
Excellent, now we've just recast
the problem in a new light.
A little smidgen of extra notation,
if you will humor me.
We're going to simplify all our equations
and make them look very, very symmetric.
Sk was defined at the top of your
screen as a sum of conjunction
probabilities, k at a time.
k ran from 1 through n.
It'll be awfully convenient if we
introduce some notation for the ks,
k equal to 0, and here it is.
As a matter of definition,
let's define S sub 0 to be 1.
Now, with this in hand, let's go and
take a look at our
inclusion-exclusion principle.
And now let's recast it
in a very general light.
So, the inclusion-exclusion principle,
the probability of the union,
tells us about the probability
of anything bad happening.
The probability of nothing bad happening
is clearly the obverse of the complement
of this.
And therefore,
the probability of the intersection of
the complements of all these events.
The probability that no bad event occurs,
that only good events will occur,
is 1 minus the probability
that any bad event occurs.
Of course, this is simply additivity.
We partition the space into two pieces,
a bad union, and
its complement, a good intersection.
For the probability of the union,
the baby version of
the inclusion-exclusion principle tells us
we've got an alternating sum,
an S1 minus S2 plus S3 and so on.
Now, in front, I've got a 1 there.
Well, we can identify that with
S0 per our definition, and
then I subtract my alternating sum,
S1, S2, S3.
So, what have I done?
I started with S0.
Then, I have to subtract S1,
and now the sign of S2 flips.
And it becomes adding S2, and
then the sign of S3 flips and
becomes subtracting S3, and so on down.
And the last term becomes minus
1 to the power nSn, and so
now we've got a fairly elegant,
at least easy to remember,
formulation for
the probability that no bad event occurs.
It's written as S0 minus S1 plus S2 and
so on.
With S0, just by definition as 1, but
the probability of no bad event occuring.
This could be added and
defined with the probability, in terms,
offer a accumulated sum of indicators.
This is the probability that
the accumulated sum of indicators is 0,
no bad event occurs.
In other words, what we've computed is
the probability of an atom for M sub n.
Where is said the probability
that M sub n is 0,
that no bad events occur is given by
this alternating sum starting at 1.
S0 is 1, minus S1 plus S2 and so forth.
And this opens a door for
a generalization which is quite powerful.
It is unlooked for, remember, originally
we were just talking about unions,
conjunctions and, and disjunctions.
But now we realize at the heart
of it is also a counting element.
We are counting how many bad events occur.
Of course,
it'll be ideal if none occurred.
But in the context of rare events, we
should always draw the distinction clearly
between implausible and impossible.
It is implausible that a rare event
occurs, but it is not impossible.
Then the question is,
how many of these bad events occurred?
We are now prepared.
Our inclusion-exclusion principle, the
baby version tells us in the simplest form
that the probability of no bad events
occurring is given by an alternating sum.
It should come as no great surprise that
in general now, we've got an alternating
form for the probability of a certain
number of bad events occurring.
What is the probability that
exactly k bad events occur?
It doesn't matter which k,
but some k occurred.
What is the probability of that?
Again, we are going to have an alternating
formulation involving these Ss.
Remember, what are the Ss?
They represented sums of
conjunction probabilities.
If we expand out such a sum, for example,
on the right, what is the very first term?
Well, j is 0.
If j is 0 then we have k choose k
times Sk with a positive sign in
front because we have
minus 1 to the power of 0.
What if j is 1?
Then you have minus 1 to the power 1,
which gives you a negative sign.
And then you have k plus 1 choose k,
times S of k plus 1.
The sum of all intersection
probabilities k plus 1 at the time.
Next term, j equal to 2,
while minus 1 to the power 2 is plus 1.
Then you have k plus 2 choose k and
then you have Sk plus 2,
the sum of all k plus 2y's intersection
probabilities and so on down.
Okay, this is now the industrial strength
version of
the inclusion-exclusion theorem.
And I have not made clear yet
that whether or
how this connects to this
idea of a metaphorical sieve.
Right now we are building background,
but we can see some tenuous connections.
The sieve dealt with
probabilities of unions or,
equivalently, the probability of
the intersections of complements.
The sums of these indicators,
these bad indicators,
tell us something about how many
of these rare or bad events occur.
And so in this inclusion-exclusion
principle seems
a strategy, for
creating a metaphorical probability
sieve to try to sieve out the bad
events from the ensemble.
So let's begin first by providing
an elementary proof of this principle.
And to do so,
there are several approaches.
But to do so,
I'll pick one which appeals to me,
by appealing to a principle from magic.

