Wars are a sorry commentary
on the human condition and
they've occurred repeatedly through time.
In 1944, Louis Richardson decided
to look at an ensemble of wars over
the second half of the millennium,
over a period of about 432 years.
And he catalog the wars that
had happened in that time and
found there were over 299 wars.
A question that immediately arise is,
is the following.
How does one categorize wars when they
range from small conflicts to large
continent enveloping disasters?
And so, this goes to the crux
of the modeling itself.
One way to do it is to
take large conflicts and
break them up into a secession
of smaller key conflicts, and
this is the approach that
Richardson has taken.
For example if you look
at the First World War,
he treats it as five different conflicts,
and similarly with other large wars,
the 30 years war, the Second World War of
course, the Napoleonic wars and so on.
And, with such a break point in mind,
he finds 299 wars distributed
over a period of 432 years.
By now,
the methodology is becoming very clear.
We don't anticipate too
many wars per year.
Wars are rare occurrences.
There are many opportunities for wars,
but they happen rarely, one hopes.
And therefore, a Landa here,
the frequency of wars in a given year,
is 299 out of 432, or about 0.7 or 0.69.
Roughly speaking, we anticipate,
based on this data,
about two wars in every three years.
And if we now try to find a Poisson
approximation to the observed data,
we can find a quite remarkable fit.
There is a philosophical point
here that might well be worth
spending a little bit of time on.
And, it deals with this.
What is the work of the modeling?
How does one reconcile these statistical
models of these sociological phenomena
with common understanding of how
this phenomena were engendered?
So very frequently for,
in the case of wars, for example.
History tends to point at one
individual or one circumstance or
the other as being
the preceptor of such a war.
So, for example, the Archduke Ferdinand
was assassinated, and
therefore that caused such and such war.
The probabilistic view takes a much
more ensemble averaged view of this.
There is no individual
sparking laws in this context.
The analysis, the models suggest that
there's just an ensemble of chance to and
possibilities, and some things happen.
Wars happen.
Bad things happen to good people, rarely.
But they do happen.
And here's a description of the frequency,
and what we should anticipate.
All we can say now, based upon the
observed data and the models to the data,
is that the fittest quite remarkably good.
Of course, this kind of statistical view
of these chance driven phenomena does not
take into account individuals, particular
happenings, accidents of history.
The view here is approximately, well,
I can't tell you if Archduke Ferdinand
was going to be assassinated.
But somebody,
somewhere, will be assassinated at
the rate of about two every three years.
And this may indeed be the proper way
to think about these mathematical
models of these large-scale
sociological phenomena.
What we're doing is getting to
the heart of a statistical trend,
a probabilistic trend and
eliminating from the discourse
all the things which add color to it,
and which perhaps
may give a spurious feeling of
particularity to a particular phenomenon.

