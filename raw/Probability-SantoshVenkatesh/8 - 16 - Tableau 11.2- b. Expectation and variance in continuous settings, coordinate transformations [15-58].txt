The idea of a probabilistic
center of mass expectation and
a probabilistic moment of inertia variance
carries over smoothly from the discreet
domain to the continuous domain.
Naturally enough, all that the student
will have to keep in mind is that
the chance variables have gone from
discrete to continuous, and so
probability masses gives way naturally
to probability mass densities,
and sums segue naturally into integrals.
And if she keeps these basic ideas in mind
then she will find the arguments carry
through without essential change.
So without further ado, let's begin by
considering a continuous sample space.
The sample space here is the real line.
We have a continuum of possibilities.
Recall that in such settings,
we call sample points, these idealized
outcomes of experiment, random variables,
an evocative terminology which
suggests that we have a change experiment,
which is going to spit out a real number.
And that real number naturally
we'll call a random variable.
Again traditionally we'll denote
a generic such random variable by
an uppercase letter X.
So this represents
a chance-driven real number.
The question of course is,
well what is the probability
measure underlying this experiment?
And in the continuum
this is given to us by
the specification of a density function.
Recall that a probability
density function,
p of x, represents a probability
mass per unit length at a point x.
Since it represents a mass, it must
satisfy the basic axiomatic principles
we've begun, wait for
probability measure, in particular,
p of x has to be a non-negative function
and it has got to be properly normalized.
The area under the curve must be unit.
Now, with this to begin
consider the weighted sum,
let's call this mu,
of taking a real number,
x, weighting it according to
the mass that your random variable,
uppercase X, places near little x.
And simply summing, or
of course, now in the continuum,
the sum segues into an integral
over all possibilities for X.
What have we done here?
We've formed a weighted integral of
possible values over chance experiment.
And we've had a name for this.
Of course, this is exactly what
we called the expectation of X.
Of course, here's an expression, for
the expectation of X in the continuum.
A beginning student should
feel free to argue by analogy.
Remember, in discrete settings,
we had probability mass functions which
put probability mass at individual points.
Averaging out the values that
the variable takes according
to the mass function
gives us an expectation.
When one moves to the continuum,
mass functions give way naturally
to mass densities,
which are units of mass per unit length.
You have a continuum of possibilities.
And sums now segue
naturally into integrals.
An integral is nothing but
a limiting stylized sum.
And therefore we have it completely
in analogy with what we've done
in discrete settings for
the binomial and the Poisson variables.
Here now is an expression for
the expectation of
a continuous random variable.
This term's for
a probabilistic center of mass.
Now, before I move on there is a small
caveat that we have to put in place.
The integral we have here is formally
an improper Riemann integral.
Its limits are at minus infinity and
plus infinity.
And if the student thinks
back to her calculus class,
she might recall that
there is an inconvenient
possibility that integrals of
this form do not converge.
All right.
So, we have to take cognizance
of such a possibility.
We say that this expression mu is
the expectation of X, also written E of X,
provided that integral is
absolutely convergent.
Okay, now with that caveat in mind,
let us think about ideas of spread.
As always let's call sigma
squared the integrated
squared deviations of
x from its center mu.
x minus mu tells you the,
the distance of x from mu,
positive or negative, squaring it gives
us a squared distance of x from mu,
weight it in accordance to the mass
near the point x, p of x dx.
And integrate over all possibilities for
x and
you've got an expected squared
deviation from the center.
Of course this is what we call
the variance of a random variable.
Recall that variance just connotes
a probabilistic moment of inertia.
And now there you have it, completely
by analogy the discrete setting,
sums segue naturally to integrals and
otherwise nothing much changes.
With just a little caveat in the back of
our minds that these things actually have
to converge for
us to say something sensible about them.
And so now we have these ideas of
a probabilistic center of mass and
a probabilistic moment of inertia.
We call them the expectation and
the variance.
Now before we move on, let us look at
a small variation on this theme, okay.
When we start with an experiment,
we have a coordinate random variable,
say uppercase X.
Now it is frequently the case that we
start with such a chance observation and
then we process it.
We pass this chance observation
through a transformation, a function.
And in this way,
we obtain new chance variables.
So let's consider again the setting, where
we have a coordinate random variable,
uppercase X, which is equipped with
a probability law, a density, p of x.
Now suppose we consider a transformation,
where we take X,
pass it through a given function f,
and spit out a value Y, of course,
you're very familiar with such a notation
from your elementary calculus classes.
What is f?
Well, at the point of this time being,
I'm going to keep it very,
very general and abstract.
f could be any generic, reasonable
function that you could think of.
For example,
you could think of Y as X squared.
It's a quadratic function.
Or Y is E to the power of X,
an exponential function.
Generically, any reasonable function.
Here's a transformation at hand.
Suppose we look at this
transformed variable and
form the following weighted sum, or
more precisely, a weighted integral.
As X wages above all possibilities,
look at the value of the function
f of x at the point x.
Weight it in accordance
with a mass that your
coordinate random variable places near x,
px, dx.
Some overall possibilities
integrate overall possibilities and
Bob's your uncle.
There you've got a value m.
Again with the provider, the caveat,
provided that integral converges.
And if the integral converges then we
have nothing to say on the matter.
Now what does it remind you of?
Of course it reminds
you of an expectation.
What it is an expectation of?
As X ranges of all possibilities,
you're looking at the expected value,
the center of mass for f of x.
Naturally, we'll call this
the expectation of f of X.
This is now standing in place
of our usual formulation for
expectation for Y.
In fact,
this is exactly the expectation for Y.
Of course you observe that if you want
to try to write an expectation for
Y, then our expression involved
in integral over Y values,
weighted according to the mass density for
Y.
All we have done to obtain this
alternative expression is do what is very
familiar.
We'll run this integral
to a change of variable,
replaced Y by f of X and
there you have it.
You have an alternate expression for
the expectation of Y.
This turns out to be very useful.
Because this tells us that in order
to find the probabilistic central
mass of Y, the expectation of Y,
it is not necessary to first figure out
what the probability measure of Y is.
That is induced via a potentially
complicated process through p of x and f.
Now, to find the central mass,
it will suffice to simply integrate
over all X possibilities.
Weighting the values, f of x.
Of course, we can probably do something
very similar with the variance, right?
Let's call s squared
the weighted integral,
where we look at the values,
f of x, at a given point x.
Ask, how far is it from the center m?
Square it.
Now we've got the squared
distance from the center.
Weight it in accordance
with a mass density near x.
Integrate overall possibilities,
and what do you have?
Naturally, the variance, the probabilistic
normative inertia of f of x.
In other words, we've got another
expression for the variance of Y.
Now, this is going to be useful for
us shortly.
But let me take it one step further
because we have some low-hanging
fruit here, and
this is going to be easy to grasp.
What if we have a coordinate
transformation in more than one dimension,
in two or more dimensions?
This is now going to lead us into
a function of two or more variables.
And I should warn you that notation is
going to ineluctably get more complex.
We have now many balls in the air,
because there are going to be many trials.
And we're going to have to keep track off
which ball is which, which trial is which.
Notation is going to get more complex.
But, with that, as a provider,
the essential logic of the situation
is not going to change.
And if you keep your eye firmly
on the logic of the situation,
then all these expressions will
become completely transparent.
So to begin,
suppose we have an experiment,
which comprises a system
of n probabilistic trials.
The outcomes of these trials, let's
call naturally X1, X2, X3 through Xn.
X1 through Xn represent random variables,
each from its own chance experiment.
The composite experiment gives us
an idealized outcome, a sample point,
which is now a random vector,
an integral of individual possibilities.
This random vector lives in
the Euclidian n-dimensional space.
If you feel more comfortable with
something concrete, think of NS2.
Are we dealing with the Euclidean plane?
If NS3, we're dealing with
ordinary three-dimensional space.
And, more generally,
for higher values of n,
we are dealing with algebraically
in terms of an ordered n-tuple.
Of course,
this is a chance-driven experiment.
Each individual trial results in
an apriory unpredictable outcome.
So the composite outcome is
going to be unpredictable.
It's governed by a probability law.
They're dealing in a continuous space.
The law then is going to be
covered by a mass density.
Now is a function of n variables,
p of x1 through xn.
What does this connote?
It connotes the probability mass,
the unit and
dimensional volume at
the point x1 through xn.
Of course, you would recall that
a probability density of this
nature has to be positive and
has to integrate to 1.
These are the positivity and
normalization axioms.
Now with this to start,
consider a transformation.
Think of these as coordinate variables or
observed variables.
A transformation massages these variables,
and gives rise to a new variable Y,
which is a function of X1 through Xn,
think of your favorite function of n
variables and imagine that as your f.
Y of course, now is a new chance variable.
It inherits its chance law, its quality
measure from the underlying measure from
X1 through Xn, which is governed
by the density p of x1 through xn.
Suppose we were interested in
the expectation of the variance of Y,
we'll do exactly what we
did in one dimension.
Look at a point in n dimensions,
x1 through xn.
The function value, the Y value, at
that point is f of X1 through Xn, weight
it according to the mass that a random
vector places at and near that region.
Integrate over all possibilities by
allowing x1 through xn to vary over
a whole space.
The result gives you a value m,
with the provisor assuming
that these integrals converge.
And of course this is
naturally what you'll
call the expectation
of f of X1 through Xn.
This is now another expression, for
the probablistic center of mass of Y,
the expectation of Y.
We have a very similar expression for
the variance.
Write s squared now, again, for the
integral of the function value at a point.
Look at how far it is from m.
Square it.
That is the squared deviation of
the Y value from the center m.
Weight it according to
the polarity mass density.
Integrate over all possibilities.
And there you have it, the variance of Y.
And we'll write this more evocatively
as a variant of f of X1 through Xn.
For the reader who is a little
unsure about densities,
she should go back and
refer to tableau six part two,
where we flesh out these
concepts in some more detail.
With these under her belt, what she's
got now is a way of characterizing
in continuous spaces, these ideas of
a probabilistic center of mass and
a probabilistic variance,
the expectation and the variance.

