So what are the three
elements we have in in hand?
First, there is an inescapable
error we are going to make.
Well, what do we call the error?
We could make it one 10%, 5%.
Well, naturally, now if we wanted
to use an algebraic notation for
it, let us call the error after
the formidable Augustin-Louis Cauchy
who introduced his notation in 1821.
Let's call it epsilon, the greek letter.
Of course the epsilon is standing for
e for error.
What about the confidence?
Well, we can only say that we'll make
an error no more than this
with a certain confidence.
Let us say we call
the confidence one minus delta.
Now what do I mean by this?
Well, if I want a confidence of, say,
90%, then I want delta to be 10%.
If I want a confidence of 95%,
I want delta to be 5%.
So, here for example, are samples
of what epsilon and delta could be.
So epsilon's 0.03 says that I
make an error of no more than 3%.
A confidence of 95% says
that delta is 0.05.
So this pronouncement will say
that I'd like to say that I make
an error of no more than 3%
with a confidence of 95%,
or, in other words,
I make an error or exceeding 3%,
the chance of that happening
is no more than 5%.
Here are other possibilities for
epsilon and delta.
For example, an error of 1%,
epsilon is 0.01.
A confidence of 99%, delta is 0.01.
Or, yet again, an error of 5% and
perhaps a confidence of 90%,
epsilon is 0.05, delta is 0.1.
It would be wise not to make an absolute
pronouncement on what epsilon and
delta should be ahead of the fact.
Different settings need different error
tolerances and different confidences.
For example,
a setting involving, let's say,
the number of misplaced
letters by the postal service
requires a certain error of tolerance and
a certain confidence, right?
Perhaps one cannot get away
with misplacing more than,
let's say, 1 in 100 or
1 in 1000 letters, all right?
Otherwise there might well be a revolt
on the streets, but if the setting is,
let's say, that of space exploration,
then an error of 1 in 100 or
1 in 1000 could be catastrophic.
The moment one lets a spacecraft
go it is beyond our reach and
we have a very small tolerance for error,
and so in those cases an epsilon might
need to be a very tiny, maybe one part in
a million or even one part in a billion.
So the moral of the story is that we
should not restrict what epsilon and
delta should be a priori.
We should let the problem inform
us how much error is tolerable and
what confidence is desired.
The third element of course in this
program is the amount of data available,
the sample size n, and, of course, these
three quantities are ineluctably linked,
and so here is a million dollar question.
If one is given, let us say,
a maximum error tolerance,
epsilon, and a desired confidence
that the estimate is within that error
tolerance of say, one minus delta.
For definiteness if the reader wishes,
she can think of epsilon as say,
3% and delta as 5%, so
we're talking about a,
in no more than a 3% error with
a confidence of at least 95%.
What size of sample will guarantee
these error and confidence parameters?
All right,
let's set it up now mathematically
because now we want to make quantifiable
progress at estimating this.
Recall what do we want.
For the desired sample size, n, we want
the relative frequency of successes,
Sn divided by n to be; on the one hand,
no smaller than p minus epsilon and
no larger than p plus epsilon.
For definiteness if p is 0.6 and
epsilon is,
say, 10%, 0.1, then I want my
relative frequency to be no smaller
than 0.6 minus 0.1, or 0.5 and
no larger than 0.6 plus 0.1 or 0.7.
So with this in hand here
is our desired relation.
The probability the rate of frequency of
successes lies between p minus epsilon and
p plus epsilon should
exceed one minus delta.
If so, what this is saying is this,
if this kind of guarantee can be provided,
let's say epsilon is 3% for definiteness,
let's say that one minus delta is 95%,
delta is 5%.
If we can proffer such a guarantee; then
we are saying that if we do the random
experiment and we generate what was na's
and generate the sample of that size and
generate the re, the relative
frequency of successes, Sn over n,
then this relative
frequency of successes will
be within 3% of the underlying p,
which is not known to us, but it'll
estimate p by no more than a 3% error,
and this'll happen, on average,
at least 95% of the time.
Very roughly speaking,
if 100 people did polls,
each of them reported a relative
frequency of successes, on average
95 of them will get answers within
plus and minus 3% of the right answer,
and of course, unfortunately,
sadly, 5% of this group,
5 of these 100, on average,
will get answers out in left field.
They have no idea where
the answers could be.
They could be very bad,
very vague, very small.
We'll have no confidence
whatsoever in those answers.
Of course we'd like to know ahead of
time whether a particular sample is good
or bad.
Sadly, we cannot proffer
any absolute guarantees.
We live in a chance driven world.
Bad things happen to good people.
Life happens, and so
all we can say is the following,
that if a random experiment is performed
then a large percentage of them,
say 95% of such experiments,
will give good answers, but
we have to live with the sad reality that
our particular experiment might be bad.
But this happens rarely and
we can take solace in that.
Now, this is an adequate
statement of what we want, but
let's try to compact it mathematically.
What this is saying is that Sn over n,
the relative frequency of successes,
differs from p by no more than epsilon,
either on the negative or on the positive
side, and therefore by the expedient
of taking absolute values we can simply
this expression and write it down as
the probability that the absolute value of
the difference of the relative frequency
of successes from the underlying
probability p is no more than epsilon.
Or yet again, if the relative
frequencies of successes differs
from p by no more than epsilon
with probability one minus delta
then the probability of the obvious event,
the compliment,
should not exceed delta, and so
this is equal to saying that
the deviation of the relative frequency
of successes, Sn over n from p,
an absolute value, exceeds epsilon
with a probability no more than delta.
This has got, of course,
many moving parts here.
There are epsilons and
there are deltas and there are ns,
but at least we have a compact
expression to look at.
Now, let's take a look at this and
see what we can do with this.
So here is our desiderat.
We are given, a priori,
an error tolerance epsilon.
We are told that our estimate should
lie within that error tolerance with
a confidence of at least one minus delta.
Here is a simple mathematical
condition to check, and
now it is clear that in that
equation is buried a relationship
between the error, the confidence,
and the sample size.
As I mentioned earlier,
the error, the confidence, and
the sample size form an intricate
waltz in this process.
They're intimately connected and
the heart of the connection is
a mathematical relation which
is buried in that relation.
This is what we would like to extract.

