Now that we have computed
within the model of Poisson,
the probability of convicting an accused.
Our next question and the fundamental
question, of course is what
are the chances of an erroneous
conviction or an erroneous acquittal?
So let's start first with the probability
of an erroneous conviction.
What does this mean?
Well, let's transcribe this in words.
It means an individual is innocent and
is convicted.
And therefore, what we're looking for
is to write down
the probability of innocence.
Conditioned upon
the occurrence of the event
that the individual is convicted.
Oh, an atavistic target memory
might suggest that this kind of
conditioning probability
argument is familiar.
This is related via Bayes's rule to
a change in direction of conditioning.
And the moment we write it in this form,
it all falls into place.
Their denominator is the probability
that the individual is connected and
we have just by dint of some
effort by using additivity,
computed this in closed form in
the expression on the upper right.
In the numerator is the term,
which corresponds to conviction
if the individual is innocent.
And this is exactly
the second of the two terms,
which is comprises the sum for
the probability of conviction.
And so, in one fell swoop,
we can simply write down the answer.
The second term for innocence divided
by the probability of conviction.
The listener should take a moment to
write it down and think about it and
make sure she has absorbed it.
The expression looks algebraically
formidable to be sure, but
the logic by this point should
be completely transparent.
And as a test of your understanding,
it would be a good
idea to ask whether you can
now retrace the steps and
write down an expression for
the probability of an erroneous acquittal.
Pause the lecture, take a moment and
see if you can just follow through and
see what they answer might be.
Restart the lecture,
of course, when you're ready.
So by an entirely similar process,
an erroneous acquittal
means that the individual is guilty,
but has been acquitted.
And so we're asking for the probability
of guilt, conditioned upon acquittal.
This is an A posteriori probability,
a probability after the fact.
And following exactly the same line
of argument, we write it through and
we find an expression very similar.
In fact, it's a mirror image
of the expression above.
At this point,
now we've got the two fundamental drivers
of errors in the jury verdict process.
That the probability of an erroneous
conviction on one hand and
the probability of an erroneous
acquittal on the other.
Of course,
the balance between these is going to
depend upon societal sensibilities.
What at a given time, in a given society
is felt to be more important of the two?
But clearly, we would like
both of these to be small, but
their relative proportions,
as I've mentioned it depends.
But there's still a block here.
How in these expressions,
does one actually compute them?
We do not know theta and P a brewery.
And how do we actually determine theta and
p?
The theta, the probability of
guilt is a parameter of the model.
We don't know this A priori.
P, the probability that a juror
of a particular veneer,
a particular group of
potential jurors makes errors.
Hopefully, theta is bigger than half,
at least in a tranquil and stable society.
Hopefully, P is less than half.
Hopefully, much less than one-half
in a well chosen jury system.
But how does one actually
determine these parameters?
Now to understand this,
we now need some more theta and
so let's go back to Poisson's data.
In the years 1825 to 1830,
the outcomes of jury trials were available
and this is an abbreviated list.
The original list
actually had more detail.
It had additional information on
that whether the accused was for
a crime against a person or for
a crime against property and so on, but
here is a compacted list.
It shows the number of accused in
the years between 1825 to 1830,
when jury trials comprised 12
individuals with a majority of at
least seven being needed to convict.
And here is the number
of convictions per year.
Using this as, as data,
we can go back to the models and
try to estimate the parameters theta at P.
An objection immediately arises,
because you realize we have
binomial probabilities.
So we have powers of P and that's going
to make these calculations difficult.
Poisson found ingenious ways
of simplifying calculations.
And found out and he, as he noted,
there was a remarkable stability
across these years in terms of
conviction rates in terms of,
in terms of the probabilities of guilt.
And Poisson estimated from these data,
that theta was about 0.64.
That about two-thirds of
individuals who were accused were
actually guilty,
while P was about one-fourth.
The probability that a juror
makes mistakes was one in four,
three out of four times the juror
makes correct decisions.
So we have now,
at least in principal a way forward.
We've got a formal model.
If one has data,
then one can go back to the data and
estimate parameters of the model and then
we can use the model to test new data.
This sounds promising and good.
Of course, I've left out details here.
In those years in France between
1825 to 1830, if a jury had
a majority of seven for conviction, then
the case actually went to a higher court.
Which would again examine the data and
then come out with a decision and
possibly overturn the original decision.
Poisson actually cleverly used
such cases to provide secondary
checks on his estimates.
Following 1830, between the years 1831,
1833, France moved to a 12
member jury system where now it,
a majority of 8 was needed to convict.
And of course, there were data for
that as well and
Poisson analyzed that data as well.
All of this suggests there is
a principled methodology here,
where one can setup the probabilities
of error in jury verdicts.
The question now, of course is
what is the proper size of jury?
And what super majority does one need?
What is optimal?
And so to do this, we want to
promptly generalize what we have.
So here are the expressions
we've computed.
The probability of erroneous conviction
when you have a jury of size 12
with the majority of 7 needed for
conviction and
the probability of erroneous acquittal.
Suppose now, we abstract the problem.
And we say,
suppose we have a jury of n individuals.
Why n?
Because it is inconceivable that I use
anything else for an algebraic variable.
And suppose we need a majority,
let's say, m individuals to convict.
In our example from France from
1825 to 1830, n was 12 and m was 7.
How many of these expressions change?
Well, all we need to do is go in and
strike out sundry
references to 12 by n and
sundry references to 5 or
12 minus 7 by n minus n.
And that's all there is to it,
Bob's your uncle.
And so,
our first expression becomes an apparently
more complicated expression,
involving n and m.
Our second expression again,
replacing 12 by n and
5 by m minus n becomes this.
And there you have it,
we now have a formal model.
Now testing such models is difficult and
perhaps,
it's a sad commentary on the rate
at which society progresses.
That in late 20th century America,
the amount of data,
the type of data available was
not really very different from
what was available in
nineteenth century France.
So, in the intermediate
period of a 150 years,
we still have essentially the same kind
of system and the same kind of data.
Very little further progress
actually has been made.
So perhaps, there's a little commentary
on the evolution of society in time.
And we can use this as a starting
point to analyze jury systems and
jury verdicts, but
our purposes here are slightly different.
For our purposes,
this problem of jury selection and
the validity of jury verdicts,
the justice of jury verdicts was
useful in establishing a context.
The story was nice and
colorful to be sure.
It allowed us to really examine the
binomial distribution in a meaty context
and to resurrect ideas from independence
and conditional probability and
which allowed us to get a clean
review of these principles.
Let us leave this for the time being.
Move on, because at heart
here is now a new question.
The binomial probabilities in, ineluctable
get folded into these expressions.
How does one actually
evaluate these probabilities?
How does one evaluate such sums,
even if one knows the parameters?
This leads or lead to a discovery
Poisson made in 1837 and
that is the subject of
our next mini lecture and
the subject of this tableau as a whole.

