A curious discovery of Poisson.
Now recall that jury verdict probabilities
were given in terms of the binomial
distribution.
And there was this question about how
one computes these probabilities.
You should bear in mind the time,
it was early in the 19th century.
And to compute a binomial probability
by hand was going to be moderately
unpleasant.
So there was a computational question.
Of course, these questions are moot today
because even the simplest of our handheld
devices can chew up calculations
like this in a trice.
But in the 19th century,
calculation, computation,
was actually really a serious question.
Poisson went ahead and
analyzed his expressions using
the law of errors, what we today
call the central limit theorem.
And this will be our concluding tableau.
But along the way,
he noted as a curiosity.
A curious little approximation to
the binomial which could in principle
be used to simplify the calculation
of these binomial probabilities.
This was good to have unexpected and
subtle effects.
So, let's start with a setting.
Remember this is about a jury model.
And p represented the probability
that a juror made an error.
Naturally, we hope that
p is a small number.
In the context of coin tossing then
we are thinking about tossing a coin
which is very heavily
weighted towards failure.
A failure here means a correct decision.
All right.
Slightly twisted language,
I know, but there it is.
So if p is small then we'll
require very many tosses of this
coin before one sees a success,
a head, an error.
This is a province of rare events, so
naturally we're now thinking about
a setting where p is,
is small positive number.
And n is sufficiently large.
In this domain, we recall
the expected number of successes for
the binomial was n times p.
The number of trials times
the success probability.
If p is small and n is large then n
times p could be a moderate number.
Let us call that by
the Greek letter lambda.
So, the setting is the following.
I want to evaluate binomial probabilities
where p is small, n is large,
and the product n times p, or lambda, is
a moderate number for varying successes.
So let us fix k.
Okay.
Zero, one, two, three, four and so on.
Fix that and
think of a large n and a small p.
Now the binomial probabilities have
got two parts to their expression.
One, a binomial coefficient and
then a power of 1 minus p.
Let's take them individually.
First, the binomial
coefficient times a power of p.
Now, what is the binomial coefficient?
Well, n choose k is n to the k following
over k factorial, or writing it out
n times n minus 1 times n minus
2 through n minus k plus 1.
In other words, start at n and
go through k terms each one
less than the previous term.
The whole divided by k factorial and
then multiplied by p to the power k.
Each of the terms in the numerator
of the fraction is very close to n.
Remember, n is a big number,
k is some fixed number.
So when you take away a small
fixed number from a large number,
you're still going to get
essentially a large number.
Each of those terms is essentially
n let reinforce it by pulling out
a factor n from each of the terms
in the product and the numerator.
So you have n,
then if you pull out n from n minus 1,
you get n times 1 minus 1 over n,
then you pull out n from n minus 2.
You get n times 1 minus 2 over n.
And so on down,
you pull out n from the last turn,
you get n times 1 minus k minus 1 over n.
So, how many n's have you pulled out?
Well there are a total of k n's in the
numerator, k terms in the numerator, and
each of them has contributed one term n,
and so we get n to the power k.
Combine that with p to the power k and
write it as simply n times p to the power
k, and now we've got an expression.
N times p, the mean or the expected
value of the binomial to the power k,
over k factorial, times a bunch of
correction terms in wrong brackets.
But here is the point.
Each of the correction terms defers
to 1 by a very small amount,
in a reciprocal of n.
Remember, n is large.
And therefore,
each of those k terms is very close to 1.
You multiply them out, you get something
else, which is very close to 1.
And since n times p,
we're calling lambda, the entire
product evolves into something like
lambda to the power k over k factorial.
So here now,
the entire binomial coefficient is
simplified into a much simpler form.
A power of lambda divided by k factorial.
We are half way there.
Now let's take a look at what
remains in the expression for
the binomial probability.
1 minus p to the power n minus k.
The rule of exponents tells us that
when you have x to the power a plus b,
we can write it as x to the power
a times x to the power b.
Identify x with 1 minus p, a with n and
b with minus k, and
we've got a rule of products.
In the first term on the right,
1 minus p to the power n.
Recall that p is simply lambda
divided by n, or lambda is n times p.
And therefore,
rewrite it as 1 minus lambda over n
to the power n,
times 1 minus p to the power minus k.
Now, the second of the two terms,
remember p is very tiny, so
1 minus p is very close to 1.
And it, taking a number very close
to 1 and taking a power of it,
you would get something very close to 1.
1 minus p to the power of minus k
is approximately 1, for small p.
What about the first term?
Oh, it might be a faint atavistic
tug in your memory banks,
perhaps from your first calculus class or
your pre-calculus class.
1 minus lambda over n to the power n.
And the lambda is a moderate number.
N is a large number.
This expression is an approximation for
e to the power minus lambda.
In fact, we could use it as a definition
of the exponential function.
And therefore, we get e to the power
minus lambda times 1 approximately, or
simply e to the power minus lambda.
Everything is simplified gloriously.
The binomial probability of k successes in
n trials with a success probability of p,
is approximately given
by lambda to the power k
over k factorial times a to
the power minus lambda.
All right, a new student,
a neophyte, might well
be pardoned for being skeptical about
this, this strange algebraic maneuver.
So, what have we accomplished?
Well I've got a, a messy thing, and
I replace it by some other messy thing.
But something important
has been accomplished.
Take a look at new,
at the binomial probability on the left.
It, for a given k,
is determined by two parameters n and p.
And each of the terms is not easy
to compute, especially by hand.
And choose case complicated.
It takes effort.
And you have various powers of p and
1 minus p.
On the right,
we have a single parameter lambda.
Our entire expression is approximately
determine by a single parameter.
And this is going to
have deep consequences.
We should promptly rush it and
put in some nomenclature,
some terminology to capture this.
On the right,
I've got an atomic mass attached to k.
A probability that sums design
chance experiment takes value k.
It is completely determined by one
real parameter, a positive number,
lambda, which is fixed once
the parameters n and p are known.
Let's call this the Poisson probability.
Po for Poisson, of k emerging in a chance
experiment with a hidden parameter lambda.
Now this all looks very
esoteric to be sure.
But, as a first question, you say well,
maybe Poisson had to do
something with this.
But surely today,
we have access to computing devices.
What do we care?
We can just threw in a binomial
probabilities into the machine and
outcomes and answer.
The answer is going to
be at least two-fold.
And the first part of it is
simply the following realization.
An expression like this already gives us
a lot of information that we
were not privy too earlier.
What this tells us is that
a variety of binomial expressions,
a variety of binomial experiments
with differing n's and p's will
give rise to similar probabilities as long
as the product n times p is a constant.
So, a whole slew of binomial expressions,
all give rise to similar chance outcomes.
That is an important realization.
The second question that might arise at
this point is, well, we've waved our hands
around and done this algebraic
approximations, how good are they?
We could approach this
in a couple of ways.
One is we could try to make a formal
limit out of this and try to
provide explicit expressions for the error
we make in such an approximation.
And that can be done, but we won't bother.
We'll simply appeal to
a numerical evidence
to see that this actually
gives us convincing results.
So here is a test of the quality
of the approximation of Poisson.
Start of the binomial probabilities
corresponding to two parameters n and p.
Let the number of successes,
k, run from 0 through, say, 4.
Allow n to range of
suitably large numbers.
In this case, I picked 20,
60 and 100 as my large numbers.
Allow p to range from 0.15, 0.05 and 0.03.
15%, 5% and 3% in three cases,
respectively.
The numbers are chosen that n
times p is a fixed value 3.
This is my lambda.
So in each of the three cases,
n is 20, p is 0.15.
N is 60, p is 0.05,
n is 100 and p is 0.03.
We have n times p equal
to lambda equal to 3.
So, n is large, p is small,
lambda is a moderate number.
The Poisson probabilities that you called
Po of k, parametrized by lambda and
given by lambda to the k over k
factorial times e to the minus lambda.
And here's a table of values.
And even cursory examination shows that
the approximations are getting quite
remarkably good, even for
these relatively small values of n.
We could, at this point,
create a test of a hypothesis.
But we'll not bother.
At a high level, all we will need is
to say, if we are in the province of
rare events where the coin toss has
got a small probability of success, p.
Then if there are enough trials,
if the coin's tossed enough times,
then the binomial probabilities
are approximately governed
by Poisson probabilities.
The approximation is quite remarkably
good, even for small values of n.

