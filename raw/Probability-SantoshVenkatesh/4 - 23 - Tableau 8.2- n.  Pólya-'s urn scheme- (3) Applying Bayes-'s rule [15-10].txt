This is the setting.
At this point,
we have now come to a right understanding
of the premises of the problem, right?
Notice we put in a fair
amount of work here, right?
Just to understand what
the problem is saying.
And we should always do this
before we launch directly
into an attack on the problem.
I know that long tradition and
custom through your early school years
has brought in this,
this instinct to just dive into a problem.
But any worthwhile problem
needs introspection,
needs thought,
needs time to understand what is going on.
And this will well repay you.
At this point, we've understood how to
write down the A Priori probabilities.
We learned to write down the forward
conditional probabilities, but
the question at hand is can I reverse
the direction of conditioning.
But for now we're required to do this.
Let's move directly onto this.
Let's start with a simple question.
Given that the second ball drawn is red,
so
you're looking at the second level of
the tree, and you're looking at the red
transitions to your left,
given that one of those happened.
What are the chances that a first
transition falls to the left?
Okay.
Let's build it up one step at a time.
Our first question, what are the chances
that the first two balls drawn are red?
Well, from a formal mathematical
point of view, the event
at hand are R1 and R2 and we want the
conjunction of the intersection of them.
We write down an expression like this and
we can just read this out from the table.
But what have we used here?
We used the chaining principle from
the definition of conditional probability,
which expresses the desired
probability of R1 intersection R2,
in terms of the A Priori probabilities
on the transition of a forward
conditional probabilities.
What can we say about the probability
that the second ball is red, hm?
Formally, we want the probability
of the event R2, and
we can write down an expression as you
see emerging on the right of your screen.
Write it down, pause, absorb it.
What is the guiding principle for
an identity like this?
Do you recognize the use
of total probability.
We broke up the sample space,
into two pieces.
R1, and the complement of R1,
or B1, this was a natural partition,
which is given to us by
the structure of the problem.
Given the partition,
we now have a formal expression.
But, in this expression we can read
out all the relevant probabilities
by looking at the vertices,
the a priori probabilities, and
the transitions, and there you go.
It's as easy as that, we've now got
the sum of two fractions and now all that
remains is to massage it using ordinary
algebra, collect the denominators.
Pool the numerator terms, factor them,
catalogue common factors, and
you've got a beautiful,
simple, elegant expression.
The probability the second ball is red,
is r over r plus s.
Now, we've put in some work to get here,
and we're tempted to say,
oh we're tired,
we want to take a break, but.
We should always look back
when we get an answer.
Does it illuminate the problem?
Can I verify it by direct methods?
Is it intuitive?
Now this expression r over
r plus s looks familiar.
Have I seen this before?
Oh yes indeed we have.
It is exactly the probability of
drawing a red ball on the first draw.
Hm, so the chance of drawing a red ball on
the first or the second draw is the same.
If it's surprising,
you should pause and think about this.
Perhaps then you might conjecture the
chance of drawing a red ball at the third
trial is also the same.
Try it.
See if you can verify this.
See if you can then come back and
say, I get it.
This is why it should work.
This is what I mean by an answer
eliminating a problem.
Showing you unexpected crannies and
nooks that you might not have anticipated
at first blush, when you first saw it.
Let's move on.
The question at hand is
a conditional probability.
What is the chance the first
ball drawn is red,
given that the second ball drawn is red?
Well, let's write it down mathematically.
From the point of view of a definition,
we can write down the conditional
probability as a ratio.
But the whole point of the exercise
is the object in the denominator
can be expanded out
using total probability.
In other words,
we've just used Bayes's rule.
And now we've got a simple ratio
fractions, cancel our terms, and
we've got again, a simple expression.
R plus a divided by r plus s plus a.
Again you should ask, hm,
does this say anything about the problem?
And staring at this fraction,
it becomes clear as an easy
exercise in clearing denominators.
I'll show you that our answer is larger
than the fraction of r over r plus s,
which is exactly the probability of
drawing a red ball on the first draw.
And so our expression tells us that
the conditional probability of drawing
a red ball on the first draw given that
a red ball was drawn on the second
draw is larger than the probability of
drawing a red ball in the first case.
Hm, I wonder if this is reasonable.
Do you see any connections to
anything that we've already seen?
Somehow the conditional
probability has increased.
If on introspection you said,
that looks a little bit like
LaPlace's Law of Succession.
Repeated occurrences of something
predispose an increased chance
of something having happened.
And now I understand the connection.
I say, oh yes, of course, if you
tell me that a red ball was drawn on
the second draw,
it is intuitive that there's a heightened
chance of having a larger population
of red balls after the first draw.
Okay, now this is very intuitive.
But of course, our actual expressions
adds precision to these verbal bones.
It adds flesh to this.
It tells us precisely, quantifies
exactly how much the odds have improved.
Now let's move on to the original
question we started with and
this of course is going to require
a little more elaborate calculation.
But now we've got the structure of
the calculation under our belts.
We understand exactly what's going on.
So here's our new problem.
Given that the second and
third balls drawn are red,
what is the chance the first
ball drawn is red?
Now come again, you should keep the tree
in front of you, because it is going to
allow you to write down just visually
the kinds of expressions you need.
Let's first express the probability
at hand mathematically.
We are interested in the conditional
probability of the event R1
given the joint occurrence of R2 and R3.
The definition of conditional probability
allows it, us to write it down as a ratio.
And now we say that if we are all primed
for total probability and Bayes's rule.
Let's work through the steps.
First the numerator.
What is the joint probability of
drawing three red balls in a row?
Immediately, the structure
of the transitions suggest
that we apply a chain rule.
The probability of R3 given R1 and R2
times the probability of R2 given
R1 times the probability of R1.
Notice this decomposition is in
the forward direction because
that is what is known to me.
That is what is given to us.
And of course, once we have this,
we just read things off from the edges of
the tree and write down an expression.
And now, all that remains is to simplify
it, pool terms, and you are done.
What about the denominator for the
conditional probability we started with?
Well, things are set up for
total probability.
The natural partition is based upon
what happened in the first step.
Remember Lao Tzu and the Chinese proverb,
a long journey starts with a single step.
Well what happened with the first step?
You either had a red ball or a black ball.
That naturally partitions the space and
so total probability allows us
to write down the probability
of interest that the second and
third balls drawn are red,
in terms of what the first draw was.
And now we simply read out
the probabilities again.
And there we go.
Right.
So we first start with a chain rule for
each of the terms on the right.
Again, by conditioning on
what the first draw was.
Now simply read out, the various
probabilities in the edges of your tree
and write down what look
like complicated fractions.
But of course, now we can
simplify this very, very quickly.
Pool the denominator, collect terms
in numerator, cancel common terms in
numerator and denominator, and ultimately
obtain a fairly simply expression.
We are now all equipped to go
back to our starting question.
The conditional probability, the first
ball drawn is red given that the next two
balls drawn are red is now
given by a ratio of fractions.
Simply cancel terms.
Pool it and obtain a very simple and
elegant expression.
There is divinity that shapes our ends
when you get answers so beautiful, so
simple, so elegant.
The conditional probability of
interest to us is r plus 2a divided by
r plus s plus 2a.
We should pause now and
take a look at what we already know.
We observe that this fraction is strictly
larger than r plus a over r plus s plus a,
which in turn is strictly larger
than r over r plus s, and
this is done by simply clearing
the denominator and comparing terms.
And so we've obtained again
a reinforcement of this Laplacian idea.
The conditional probability of
a red ball in the first draw,
given the next two balls drawn are red, is
larger than the conditional probability of
a red ball,
given that the next ball drawn is red,
which is larger than the probability of
drawing a red ball in the first case.
The more red balls that are drawn
later on, gives you more and
more reason to believe that you have
a larger red population to start with.
This was now the moral of the story.
Of course, this example is rich.
And it can see all kinds of connections to
it as a first step in a very simple model
of, for example, the spread of contagion.
Our purposes here were to
illustrate how in a, an example of
a fair amount of structure,
with a fair amount of richness
there is ample scope for the application
of total probability of additivity.
And reversing the direction of
conditioning, Bayes's rule.
Before we move on to conclude this lecture
and the segment, we should pause and
extract a little more information
about the setting at large.
Can we extend it profitably
in other directions?
Now if you pause and think about the
problem, a represents a number of arrivals
ineluctably, a has to be some kind of
positive integer, perhaps one, two, three,
ten, something like that.
But, if the student will go back and
take a look at the analysis,
she will find that, nothing really
precludes a being a negative number.
So here is a question.
What if the quantity a is a negative
integer in Polya's urn scheme?
Suppose for definiteness, a is minus 1.
Hm, what does this equivalent to?
Well in the urn scheme, this says that
you start initially with r red balls and
s black balls, you reach in,
draw a ball, and look at it.
Okay?
The original model said,
well, you put it back in and
put in a more balls of that color.
But of a's minus one,
what you've effectively done is,
you reduce the red population, if a red
ball was drawn initially, by one unit.
If a black ball was drawn initially,
you would reduce the black population,
by one unit.
In other words,
this situation with a being minus one,
is equivalent to sampling
without replacement.
More generally, if a is a negative number,
which is let's say the negative of
the greatest common divisor
between the integers r and s.
Then at each step you keep taking
out a balls of a given color and
then sooner or later,
the urn will be empty.
Okay.
Now the student should go back and
ask, are any of the calculations
that we have done compromised?
Do the calculations generally work?
How far does the tree extend?
Is it now an infinite tree or
is it a finite tree?
Do the conditional probability
inequalities that we derived,
do these still hold.
What other application domains could
this kind of extension fit into?
Well these are all profitable
directions for inquiry, and
I'll let you think about them.

