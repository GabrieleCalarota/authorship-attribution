>> Let's take stock of what we've done.
Some magic apparently has been performed
here and let's try to demystify it.
This ensemble of ideas, as I've told you,
is called the Monte Carlo method because
of it's chance driven overtones.
And there are some immediate,
obvious positive points for
it, some pros for the method.
Now what are the various pros?
First, a relatively small
sample of points in the cube
will give rise to accurate estimates of
the underlying integral of a function.
So, a million points give us an error of
no more than 1% with a confidence of 99%.
But something remarkable
has also happened.
Nowhere in our estimates does
the dimensionality, d, appear.
In fact, the sample size estimate
needed for our Monte Carlo sum
does in no way, depend upon the underline
dimensionality of the problem.
A million points will give you
an estimate with a 1% error,
so will a million points
in 200 dimensions.
Or a thousand dimensions, or
a hundred million dimensions.
This is beautiful.
And if the listener thinks back she
will say we saw something like this.
This is exactly what we saw
in the context of polls.
Of course, doing polls to estimate
population sentiment seems, on
the surface, very different from computing
a numerical integral, does it not?
But the underlying chance mechanism by
which we approach the problem is exactly
the same, and just as polls don't depend
upon the size of underlying population.
So too, the Monte Carlo methods
don't care about the underlying
dimensionality of the problem.
These are wonderful attributes for
an approach to computation,
especially in what appeared to be
a completely intractable setting.
But, there are some caveats.
So here are some cons of the method.
First, a method very quickly gives rise
to good estimates of j, the interval.
But the method falters and
breaks down if the area of tolerance
you need is exceedingly tight.
If you tell me I want an answer with as,
which is within one millionth of a part of
J, then you're going to need to sample,
something like ten to the power 14 times.
Well the number's getting large now, now
we're getting to be a significant burden.
And if you want an even tighter tolerance,
things blow up on us because
the scaling is like one
over epsilon squared.
And the smaller epsilon gets,
the faster that blows up.
So, the method is, is very good in
getting you a quick and dirty answer,
maybe even better than dirty,
a quick and reasonably accurate answer,
that is going to break if you demand
exceedingly tight precisions.
Again you might be reminded of
the same kind of phenomenon
we encountered in polls.
If you're looking at polls and
we're looking at a political setting and
we want to try to separate two candidates.
If they are very close to each other,
polls start breaking down.
And this is exactly the same phenomenon.
Now the listener might
have another objection.
Now she might say, like,
look, this is okay,
but if you're going to tell me I
can't get to, smaller than 1% to
her without things breaking on me, but
I need more, I need better position.
What can I do?
Well the short answer to that is,
you know, life happens, okay?
We can't always get everything we want.
But let me borrow or
paraphrase a, a bromide of T.W.
Kearn from Oxford who
pointed out that in any case,
a principled and reasonable
answer in a small number of steps
is to be preferred to no answer at all and
an infinite number of steps.
With a million steps, I get an answer
with an error of no more than 1% and
a confidence of 99%.
That's surely better than no
answer at all in trying to compute
a sum over ten to the power 95 terms.
That's hopeless, right?
So, here's a practical reason why, this
is, at least gives us a way forward and
gives us principal answers.
There is another corner however, and
that is this computational method
is intrinsically chance driven.
If I do a Monte Carlo computation today,
I sample a million points,
add them up, divide by a million and
say here's your answer.
And if I do this computation tomorrow,
I'm going to get different answers.
This is troubling.
Something deep and
fundamental has changed.
And to put this in context, you know,
bear in mind that computation through
the middle part of the 20th century
historically meant two things.
When we computed, we computed
precisely and we computed certainly.
Computation connoted precision and
certainty.
Two plus two was always four and
will always remain four.
Now, this was the idea of computation from
pre-history all the way through
the middle of the 20th century.
With the advent of these probabilistic
methods, philosophically,
something fundamental had changed.
Computation using these methods
now connoted both approximation.
We are giving up getting exact answers.
And it connoted also a lack of certainty.
It's not only that I'm getting
an approximate answer,
in fact any numerical integration is
going to give you an approximate answer.
But, even worse, there's a possibility
that my answer is completely and
totally and irretrievably wrong.
But we take some comfort in
the following understanding.
If a hundred people do this
Monte Carlo simulation, on average,
99 of them are going to get good
answers within 1% of the true answer.
One of them, sadly, on average,
is going to be out in left field.
And very sadly,
he won't know that his answer is wrong.
There's no way of knowing a priori
that your sample was aberrant.
But that's life in the 20th and
21st centuries, right?
Following Kearn's bromide once more,
keep in mind that in any case
a principled answer in a small number of
steps, with some probabilistic guarantees,
is always better than no answer at
all in an infinite number of steps.
Following its discovery, in the 1950s,
the Monte Carlo method has taken
over the computational landscape.
Now, its a common garden tool
in the hands of professionals,
scientists, in all kinds of areas.
Climate change engineers use
it to do climate modeling.
So the engineers of all stripes in
fluid mechanics, in electro magnetism.
Physicists use it.
Chemists use it.
Biologists use it.
Mathematicians use it.
Sociologists use it.
An auditor who has to check
the transactions of a large company
over the course of a year faces a large
number of files of transactions.
How does he handle a very large
number in a short period of time?
He samples randomly.
Again we are using Monte Carlo methods.
These tools now are embedded in all
our common computation devices,
in our mobile phones,
in our computers, over the internet.
The Monte Carlo method
has become ubiquitous.
At it's heart,
all we're doing is the same phenomenon
that we are appealing to,
that work repose.
The engine fueling the Monte Carlo
method is this wonderful,
magical law of large numbers.
Now, I could go on to give you
more beguiling examples, but
we shouldn't be greedy.
So we are going to stop with
our applications at this point,
conclude this segment and
we'll come back when we finish up with all
the required lectures with some optional
dessert items where you'll see some more
hopefully intriguing beautiful examples.

