So thus far, we've come to a right
understanding of the sample space
underlining the pro, Poisson process.
We've seen that the sample space
is described by a sequence of
repeated independent trials
of these inter-arrival times.
But, of course, a natural and
more immediate question of interest
is the actual times of arrivals.
So, let's turn to these next and,
as always,
let us use uppercase S to represent a sum.
And therefore,
let S1 be the time of the first arrival.
Naturally, S1 is X1.
Let S2 be the time of the second arrival.
Naturally, S2 is a sum of X1 and X2.
S3 is the time of the third arrival.
It's X1 plus X2 plus X3.
And likewise,
Sn is the time of the nth arrival.
And we can represent Sn as a sum of the
inter-arrival times X1, X2, X3 through Xn.
In these settings,
especially with time going to your right,
it is natural to think of Sn as
the waiting time til the nth arrival.
One can imagine somebody waiting for
individuals to show up,
impatiently tapping your legs waiting
until all the people show up.
Right.
So Sn represents a waiting time.
So underlining a Poisson process is really
a characterization of
waiting time distributions.
Now, these waiting times
are much more complex in terms of
their chance description.
For one, these are continuous chance
variables, they're not discrete in nature.
How does one go about
characterizing probabilities and
probability measures involving
these continuous variables?
The listener might take a cue
from how we did the analysis for
the first waiting time.
So you might want to pause the lecture,
think about it for a bit and
see where they can write down
a way of formulating an attack
on say, the waiting time Sn and
its distribution.
Restart the lecture when you're ready.
Okay, let us take a leaf out of our
analysis of a first arrival time.
'Kay.
So here is a key observation.
When does the waiting time for
the nth arrival exceed a given value, t?
Well, if the nth arrival occurs
after a certain point in time t,
then it is manifestly the case that
the number of arrivals from 0 to
t has to be either 0 or 1,
or 2, up through n minus 1.
We cannot have more than n minus
1 arrivals between 0 and t.
If the nth arrival is occurring after t.
Now this is precisely the kind of
observation we made for the first arrival.
And now let's very quickly write
down the relevant probabilities.
So fix an arbitrary positive value t.
The probability that the waiting time for
the nth arrival exceeds t,
then is exactly the probability that
the number of arrivals, n of t,
up til t, is no more than n minus 1.
Of course, this then corresponds to
a collection of integer values for
the number of arrivals and the collection
is 0, 1, 2 up till n minus 1.
The probability of that collection by
additivity is that somewhat
the individual atomic probabilities.
And therefore,
by one quick appeal to additivity,
we say that this is a sum k
running from 0 to n minus 1 of
the Poisson probabilities,
p of k with the parameter of our t.
Of course, we now just simply right
down the Poisson formulation,
e to the power minus alpha t does
not depend upon k to the sum.
It can be pulled out of the sum, and we've
now got a simple, closed-form expression
for the distribution of the waiting
time for the nth arrival.
This is called the gamma distribution.
To be sure, it's, it is a slightly
more complex form than the exponential
distribution we saw before.
But, nonetheless,
it is simple enough in its own right.
And it is just as in the case
of the exponential variable.
A complete characterization
of the probability measure
associated with the nth waiting time.
And therefore, we have a simple slogan.
The arrival times,
the waiting times of the Poisson process
are governed by gamma distributions.

