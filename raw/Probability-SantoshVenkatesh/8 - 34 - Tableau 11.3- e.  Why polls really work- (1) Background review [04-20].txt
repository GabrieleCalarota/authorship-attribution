Armed with this mysterious
approximation using a bell curve,
let us return to our application of polls.
We have seen why polls
have a principal justification
via the law of large numbers.
Our difficulty then was not that it
didn't seem to work, but that the sample
estimates were just a little
uncomfortably too large for our tastes.
So let's go back and take a look
at it again through a new prism,
through normal approximation,
and see where it gets us.
So let's review very quickly the model for
a poll.
We have an underlying population,
which has got two subpopulations
in proportions say, p and q,
where q is, of course, 1 minus p.
p and q are fixed but unknown.
We'll want to estimate p, and
we do this through the mechanism
of repeated independent
trials where we sample,
at random, with replacement
from the underlying population.
This engenders a sequence of Bernoulli
trials, X1, X2, X3 through letter say Xn,
where each of these takes value 1 with a
probability p, and 0 with a probability q.
We form the accumulated
successes by adding these Xs.
Development has led us to
conclude that it is reasonable
to estimate the fixed but
unknown p by the known but
chance varying sample
mean Sn divided by n.
Of course,
the moment we make an estimate of a fixed
quantity by a chance varying element,
then we make errors, and we can only make
pronouncements with some confidence.
We've seen how the error in the estimate,
the confidence in our pronouncement, and
the sample size all interact beautifully.
Chebyshev's inequality tells us that
a probability that our sample mean
deviates from the underlying probability
p in absolute value by epsilon or
more is no more than the reciprocal
of 4 times n epsilon squared.
And we'd like those to be no more
than some given small delta.
We want a confidence of 1 minus delta that
we don't make an error as big as this.
And this leads us immediately to a very,
very simple proscription.
If n exceeds the reciprocal
of 4 epsilon squared delta,
then we can guarantee an estimate
error of no more than epsilon
with a confidence of at
least 1 minus delta.
And we saw that immediately,
numerically it gives rise to numbers
like those you see on the screen.
In particular,
if we demand an error of no more than 3%
in our estimate of p and
we want a confidence of 95%,
then the sample size that is
requisite is about 5,500.
Now, that doesn't sound very much,
especially when one is dealing with large
populations of millions or even billions.
But getting an estimate of even 5,000
individuals is laborious,
time consuming, expensive.
We realize that this estimate is
sufficient, but might not be necessary.
We might be able to trim
some fat from this estimate.
And the suggestion is, arises
because of the inequality of Chebyshev,
which is like a blunt instrument.
We are taking a cleaver to cutting butter.
In the process of making
an estimate in upper bound,
the estimate was perhaps
a bit too generous, and
therefore, the sample size estimates
then also become a little bit too large.
Let us see where normal approximation
leaves us with this desiderata.

