We've seen how the repeated toss of
a coin engendered, surprisingly,
an outcome which was equivalent
to a point on the unit interval.
And from this, we can think of
a conceptual Gedankenexperiment
which gives rise to a continuum of values.
And so these engender chance
experiments in continuous spaces.
We can think of these experiments
as limiting sequences of discrete
experiments, but as always,
it is going to be mathematically and
conceptually more convenient to
deal directly with a continuum
rather than with a tedious process
of approximation as a limit.
So, what are these continuous spaces?
To begin,
a chance experiment whose idealized
outcome takes a real value is said to
take values in a continuous space.
The outcomes of such experiments
are called random variables, and
they are usually denoted by uppercase
letters at the end of the alphabet, R,
S, T, and so forth, and of course,
X is a generic representation for
an idealized sample point
in such an experiment.
If one considers a composite experiment
where now one has a sequence of trials,
the outcome of the first trial is X1,
that of the second trial is X2, and
the last trial is X z, what we have is
an ordered sequence of random variables,
and this gives rise,
engenders a random vector
as a sample point in
an n-dimensional space.
So far so good.
So now we have an idea of
what the sample spaces are.
What are the basic events?
Well, events naturally
are subsets of the sample space.
What kinds of subsets are natural?
Well, surely, in one dimension, an
interval is a natural object to look at.
In two dimensions, a rectangle perhaps.
In three or more dimensions,
parallelepipeds.
Individual points now lose their
significance as mass carriers.
There are just too many
of them in the continuum.
So the idea of masses now
segues from points on
to intervals, rectangles,
parallelepipeds, and so on.
These objects, intervals and so on,
are now the mass carriers
in continuum experiments.
To get probabilities, we're going to
have to stitch these together properly.
Okay, so, and that comes up next.
What constitutes a probability measure?
In the discrete domain,
we put individual mass on elements.
When we move the continuum,
we segue from masses to mass densities.
So, what a density?
It is a function which has to
satisfy two axiomatic principles.
One of positivity.
The function has to be
non-negative everywhere.
It does not make sense to talk
about a negative probability.
And two, it is properly normalized, so
that the entire space has got probability
one, and we do this by saying that the
integral of the density has to be unit.
So positivity says in one or
more dimensions the function
is non-negative everywhere.
And normalization says that
the integral of the function,
the area under the curve in one dimension,
must be unit.
And in more than one dimension,
the integral of the function must be unit.
How does one compute a probability for
a generic region?
Well, a generic region is some integrable
region in our space, call it A.
That is my event of interest.
To compute the probability of A, we simply
accumulate the density over the region in
the limit, and of course,
that gives us an integral.
So in one dimension,
we have a one-dimensional integral over
the region of the density function.
And that gives you the probability
the chance outcome,
the random variable,
takes a value in that region.
And of course, in more than one dimension
we have an n-dimensional integral.
Let us take stock.
We have now come to a rather rich
understanding of probability spaces,
at least in simple settings and
how to construct them, all right?
In the discrete end,
we simply put together a mass function,
which attached masses in accordance with
the axiomatic principles of positivity and
normalization.
But when you move to the continuum,
the mass functions give way naturally to
a mass density in terms of mass per
unit length, mass per unit area,
mass per unit volume.
And all the principles carry through.
The sum segues naturally into
a stylized limiting sum,
or an integral, and
everything goes through.
Now, with this under our belt,
we've got a fairly rich understanding
of basic elementary probability spaces.
Where do we go from here?
Well, there's still some more
foundational work to be done.
So our next step in the process is going
to be really, an understanding of how
side information in a chance experiment
affects underlying probabilities.
This is the province of what we are going
to call conditional probabilities.
But before we launch into that,
there is a tempting avenue,
a byway which I find impossible to ignore.
And so we're going to do a small
digression in one lecture in tableau
seven.
Given that we understand now what a model
for a formal probability space is,
how does that actually
reflect itself in reality?
And we'll start with a very,
very simple and beguiling question.
The toss of a coin is a very natural,
chance driven experiment.
But when you think about it,
a coin is a physical object.
Why is there anything at
all chance driven about it?
And what is there that
allows us to suitably
model deterministic physical
phenomena by these random processes?
So this is a dangerous bend, digression.
Not that it's particularly hard.
It is not.
It is actually going to be very
elementary in terms of the mathematical
background you will need.
But it is in fact a digression from our
main theme, in that we want to talk
a little bit about the impact
of chance on life.
And so with this digression, well,
we'll come back to conditional
probability following it.

