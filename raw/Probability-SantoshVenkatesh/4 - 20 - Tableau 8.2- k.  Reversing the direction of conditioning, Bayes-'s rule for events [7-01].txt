The idea of total probability allows us to
partition problems into manageable sizes.
The particular problem of interest,
of course,
depends upon the nature of the problem.
A consequence of total probability
is our ability to reverse
the direction of conditioning.
Some context first.
So let's start with some
abstract probability space,
some sample space Omega.
And imagine that we have
a partition of Omega into
a finite of countably infinite
number of events A sub j.
We will think of the probabilities
of these events A sub j,
as the a priori probabilities
of a chance experiment.
These are the probabilities
that one starts with before
the experiment is actually performed,
before we see anything.
Probabilities before the fact.
We imagine, as we've seen repeatedly
in our examples, that the probability
measure is implicitly specified via
certain conditional probabilities.
A conditional probability
of some target event H
given some prior outside information,
A sub j.
We will think of these, in a natural
sense, as we will see shortly,
as forward or
transition conditional probabilities.
From a purely mathematical point of view,
we think of this as
starting with an A sub j,
and then proceeding to an h.
The question now is what happens,
what can we say about the conditional
probability in reverse?
What is the probability
of an A sub k given an H?
Now, now the setting is
something like this.
It might be fruitful to imagine that
the A sub js represent a partition,
an input to some probabilistic system.
For example, imagine mobile telephony.
Imagine digital communications.
Where the A sub js could represent symbols
that are transmitted, one after the other.
For example, the A sub js could represent
the letters of the English alphabet,
a, b, c, d, up through z.
The a priori probabilities
reflect the intrinsic or
natural frequency of these
letters in that alphabet.
For example, the letter e in the English
alphabet occurs about 13% of the time,
an excessively large proportion of
the time, in ordinary conversation,
in written language.
The letter Zed, on the other hand,
appears very infrequently,
about 0.07% of the time.
So these represent
the a priori probabilities.
We'll assume they're known.
We can infer them from the problem.
Imagine again that they're
transmitting a letter.
We receive some letter but
the transmission medium is noisy.
It is imperfect.
So we receive a letter, let's say H.
I don't necessarily mean
the English letter H, but
an image H representing some
symbol that is obtained.
And the transition from
a transmitted symbol
H A to a received symbol H
has a certain probability.
A conditional probability of H given H A.
We will very naturally in terms of this
diagram think of this as a forward
conditional probability, or
if you're thinking of it dynamically,
a transition probability.
These we assume are known to us,
are given to us.
For example, from an understanding
of the Physics of the problem or
the Biology of the problem.
Or perhaps purely on mathematical
modeling considerations.
The question at hand is this.
Again, keep in mind,
let's say a communication example.
A symbol has been transmitted.
A symbol has been received.
Once the experiment is done,
once the transmission has been done,
the receiver has a symbol at hand.
And her burning question is,
what symbol actually was transmitted?
In other words, she wants to reverse
the direction of the problem.
She says,
given that a symbol H is in hand,
what are the chances that a particular
symbol, Ak, was transmitted?
If you can compute these reversed
probabilities, these are naturally called
the A Posteriori Probabilities,
probabilities after the fact.
Then in principle we could figure
out what is the most likely symbol
which was originally transmitted given
that a particular symbol was received.
Okay.
Now how does one go about
doing something like this?
Well naturally,
we're given the A priori probabilities,
we are given the transition probabilities.
Our natural instinct is to partition
the sample space according
to the events A sub j in our example,
the symbols that were transmitted.
And then total probability allows us
to write down the probability of obt,
obtaining any of the received
symbols simply by addition.
So far, so good.
This is now readily computable,
given the information we haven't had.
Our question is,
how do we reverse the quest.
Well by the definition of
conditional probability,
the probability of an event Ak
given H is obtained as a ratio.
The joint probability of these two events
divided by the probability of H, but
we can decompose the probability of H
in terms of the A priori probabilities,
in terms of the forward
conditional probabilities.
And then we now get an expression which
looks like a chaining formulation,
and you'll notice that on the right the
only terms that appear are the A priori
probabilities and, the forward conditional
probabilities that are known to us.
We now have an explicit mechanism for
reversing the direction of conditioning.
This formulation is called
Bayes's Rule for Events.
Now, at a high level,
a student would be well advised to
not memorize a formula, but
understand how it was stitched together.
At work is simply
the principle of additivity,
the principle of total probability.
An experience will then allow her to
stitch reversed probabilities together.

