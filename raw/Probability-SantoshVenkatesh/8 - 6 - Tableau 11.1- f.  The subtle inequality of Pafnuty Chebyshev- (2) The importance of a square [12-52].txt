So let me just extract that last equation.
What have we got from
a formal point of view?
If we, if we cut away all the, the color,
the context,
we've got a mathematical identity.
If you sum,
k running from 0 through n off a square
weighted by a binomial probability,
you get n times p times q.
Remember q is 1 minus p.
This is our starting point.
So what did we want to estimate?
Well, we want to estimate
a particular probability.
And what is the probability at hand?
It is the probability that
something bad happens.
What is bad in this context?
We are given an error tolerance
epsilon that we do not wish to exceed.
And we want to know, what is the
probability that the relative frequency of
success is, S n over n,
our estimate for p.
Differs from the true fixed but
sadly unknown p by more than epsilon.
Let's promptly write it
down in terms of an event.
What is the event?
Well, let's clear the denominator
by multiplying all terms by n.
And so, this becomes the probability.
That an absolute value,
the accumulated number of successes,
sn defers from np by more than n epsilon.
This is a bad thing.
How do we go about computing it?
Well let,
I think a picture will help us, right?
So let's draw a picture of
a typical binomial distribution.
The region around np, the center,
up to n Epsilon to the left and n Epsilon
to the right, is the good region.
That's the region where my estimate
is within Epsilon of the true answer.
The region to the left of that and
to the right of that shown red
in the figure is a bad region.
All the integers k which land in those
regions give rise to bad results,
so our event is all integers k to
the left of the green region and
to the right of the green region.
All right.
How do we compute this probability?
Well, we simply add all the binomial
probabilities in that effected region.
And here's some notation.
This notation looks cumbrous and
complex, right?
We have a subscript which looks messy and
deep and
this is not the way we usually write sums.
Well what does this mean?
This means that we're going to be
adding binomial probabilities.
And what kinds of binomial probabilities?
Remember n,
the number of trials, is fixed.
P the sexist probability
is fixed albeit unknown.
And so what we are going to do is
add over a number of successes, k.
Over what range?
Well, the figure tells us in
what looks like the red region.
And what is the red region?
All those integers k for
which k differs from np
in absolute value by more than n epsilon.
And if we break it down into the two
individual pieces, this means that either
k is smaller than np minus n epsilon or
that k is larger than np plus n epsilon.
Oh, here's where a neophyte or
even a person with experience
can spend a lot of time.
Writing out these binomial probabilities,
staring at them, attempting to
massage them, and hoping against hope
that something good will happen.
Sadly, such efforts are doomed to failure.
That sum is intractable in the sense that
there is no simple closed form for it.
We're going to have to
take a different tack.
Here is a seemingly innocolous idea
which doesn't look very promising.
Of course, there's going to
be a method to the madness.
The binomial probabilities of the sum.
If multiplied by 1,
well it changes nothing.
Binomial probability times 1 is just
the self same binomial probability.
Nothing has happened, has it?
But here is the key idea of Checkosky.
If the term one inside the summation
is replaced by a number bigger than 1.
Then all we'll have done is
increase the value of the sum.
Mm, I wonder if this could be useful.
Well for it to be useful we're going to
have to replace 1 by something creative,
something which gives us
analytical simplifications.
Let us take a look at what the range
of values k is over which I'm summing.
Remember I am summing
over those integers k for
which in absolute value
k np exceeds n epsilon.
If I divide both sides by n epsilon.
Then this is completely equal to say that
the ratio of the absolute value of k
minus np to n epsilon is bigger than 1.
So we've got a sum over all integers k for
which that inequality is true.
Or yet another way of looking at it,
if I have a number,
say X, which is bigger than 1.
Then x times x is certainly
also bigger than 1.
x squared is bigger than 1.
Think of x as a fraction, absolute value
of k minus np divided by an epsilon.
The square of this number
then is also bigger than 1.
And so equivalently our summation is over
all integers k for
which k minus np the whole squared,
divided by n squared epsilon
squared is bigger than 1.
Every term in that sum
satisfies this inequality.
If I replace 1 by k minus np that whole
squared by m squared epsilon squared for
k in that range, then I can only
increase the value of the sum.
And your immediate reaction is the man
has gone stark, staring bonkers.
He started with a small expression,
and now he's made it bigger and worse.
How is this in any way useful?
Bear with me, we're going to make things
a little worse before we make it better.
Okay.
I've now got an ugly expression,
admittedly true, okay?
Over a range, the red range
schematically in your figure.
Right, for the particular choices
n equal to 10 and p equal to 0.6.
Now, the terms I'm adding
are clearly all positive, right.
I've got a whole square,
which is a positive number and
binomial probabilities which by
definition are also positive.
So I'm adding positive quantities.
If I add still more positive quantities,
I can only make the sum bigger.
All right?
I can keep making the inequality
go the right direction.
Well which terms can I add?
Well what about all the missing terms?
Those terms near np
which I've thrown away.
If I include those terms schematically
shown in the green area,
then I've added all values k from 0 to n.
And then I'll get an even larger number.
And again, your reaction is,
this is getting crazier and crazier.
How is this actually
going to lead anywhere?
But we're there.
Take a look at the denominator.
We have n squared epsilon squared.
Well, that doesn't depend upon k.
Right, so we can pull it out of the sum.
So let's say we pull it out of the sum.
The moment we do that,
we've got 1 over n squared epsilon squared
times the sum of a square
times a binomial probability.
Does this is any way look familiar?
Does this tickle an atavistic memory?
Of course it does.
This is exactly the calculation we did for
the variants.
And if we look at the, at the identity,
which we had discovered by dint of so
much effort in tableau ten part one,
we find that that that entire
sum is just n times p times q.
And, and probably,
things have simplified enormously.
Cancel an n in the numerator and
the denominator.
Recall that q is 1 minus p.
And we've got an elegant,
simple, unexpected formulation.
The probability that the relative
frequency of successes deviates from p by
as small an amount as epsilon or
more, is no more
than p times 1 minus p
divided by n epsilon squared.
We should take a deep breath and
savor this.
We have no right to expect something
as elegant and so, so simple and
so beautiful.
This is magnificent.
So go ahead, applaud yourself
because this is wonderful.
All right.
Now admittedly,
this is not an exact answer.
This is true.
We, we now have an upper
bound on the probability.
That's okay.
And let's see what we can do with this,
okay.
One other concern might
strike the listener.
We're trying to estimate
the chance that a sample is bad.
What does bad mean?
That the relative frequency of successes
deviates from p by as little as epsilon.
It's nice to get an explicit bound on this
probability, but the bound depends upon p.
And remember, p is unknown.
So what uses that?
I've got a bound in terms
of an unknown parameter.
That doesn't help me,
we don't seem to have made any progress.
Well not to hasty because
it's a bit of an expression,
we should see if it's going
to lead somewhere else.
Take a look at p times 1 minus p.
As p varies from 0 through 1,
with what you're given is a function of p.
And we can actually plot what p times
1 minus p would look like as p varies.
And if we did that, we find that we
have this beautiful inverted cup for
the graph of the function.
Of course, a listener who has done
some calculus will realize that
really I've got p times 1
minus p is a quadratic in p.
So I'm going to get a parabolic shape and
indeed I do.
Now, an examination of this
shows that the largest value for
this function occurs when p is one half.
And at p equal to one half,
the function takes its maximum value of
one half times one half or one quarter.
Of course, this is evident in the graph
but if a listener really wishes to,
she could write down a function
f of p as p times 1 minus p.
Take its derivative,
set the derivative equal to 0.
They're the usual maximization
process in the calculus.
And rediscover what you've seen in one
fell swoop by just drawing a picture.
So here is a lesson.
The numerator is not known to us,
a priori.
It is in terms of an unknown p.
But whatever it is, the numerator
is no more than one quarter for
any value of p, and
therefore we've got an even simpler part.
Oh, this is magnificent.
Let us take stock for just one moment
before we really savor this result.
Notice on the left hand side,
we have a probability of a deviation of
a relative frequency from the unknown p.
On the right hand side,
I've got an expression involving only n.
The sample size, and
the desired error tolerance epsilon.
p has vanished.
Poof!
It's gone.
We have a result which is uniform in p.
Wow, this is transcendent and magnificent.
Now let us savor this.

