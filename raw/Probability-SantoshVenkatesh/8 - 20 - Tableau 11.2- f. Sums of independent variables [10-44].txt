A consideration of a sum of two variables
gives us almost all we need for
sums of many variables.
But there is one technical point.
It is as well to take stock of it,
it'll help or to reinforce and
settle our ideas of independence.
And then we can move on with
a general formulation under our belt.
So accordingly let us start with a setting
of repeated, independent trials.
And we are going to think of independent
variables with a common density.
So let's say x is a chance variable,
a generic chance variable.
And let's say it's got a mass
density function p of x.
And let us suppose that x has
got an expectation mu and
a variance sigma squared.
We consider X1, X2, X3, and so
forth, a potentially infinite sequence
of such coordinate variables.
To be independent realizations of
variables with the same density as x.
In other words, we have a setting
of repeated independent trials.
Now in this setting what is
going to be of interest to us
is when we form partial
sums of the sequence.
So the idea here is instead of looking
at a single chance experiment where
we add a certain number of variables.
We look at them in sequence.
We look at an infinite collection
of such chance experiments
by looking at various partial sums.
In our usual notation now,
let S1 be just X1.
S2 is a sum of the first two coordinate
variables, S3 is a sum of the first
three coordinate variables and Sn is a sum
of the first n coordinate variables.
N, of course, is a generic number and
we're going to start thinking about n as
it becomes larger and larger and larger.
Now in this setting a simple observation
smooths our path and
allows us to connect a general setting
to the setting of the sum of two
variables that we've already seen.
Accordingly lets start with S2.
We recognize that X1 is just S1.
So we can rewrite S2 as S1 plus X2.
Similarly, S3 is the sum of X1, X2 and X3.
But X1 plus X2 is S2.
And therefore, S3 is S2 plus X3.
And now we see immediate pattern.
So for the nth of these partial sums,
Sn is Sn minus 1 plus Xn.
Let us focus on the last
line on the screen.
Sn is a sum of Sn minus 1 and Xn.
A key and beautiful observation
may be made at this point.
To whit, that Sn minus 1,
the sum of the first n
minus 1 coordinate variables,
is independent of Xn.
The listener might feel that
this seems obvious after all
Sn minus one depends on the first n
minus one coordinate variables only.
And the first n minus one
coordinate variables X1 through
Xn minus one are individually,
severally, and
collectively independent of
the nth coordinate variable, Xn.
And certainly that is true.
How do we go about verifying this?
Well, the pattern follow
exactly the procedure
we took to verify that X1 through
Xn were individually independent.
And here we go.
So to verify independents, we have
a variable Sn minus 1 and a variable Xn.
We are going to ask the joint probability,
the probability of the intersection
of two events, that Sn minus 1
takes values in a certain range and
Xn takes value in another range.
So, think of A as some
sub-collection of real values,
maybe an interval or
a collection of intervals.
B as some other interval perhaps or
some other collection of intervals.
And if you're asking for
the joint probability of the event that Sn
minus 1 lies in A and Xn lies in B.
How do we go about doing this?
A quick sanity check.
What is the underlying sample space?
Well, the space we're dealing with,
as far as X1 through Xn are concerned,
is n dimensionally Euclidian space.
So we have an n dimensionally
Euclidian space,
we're going to identify
the event of interest in it.
What is the event of interest.
Well, that portion of the space where
the first n minus 1 coordinates adds up
to a number which lies in a region A and
the last coordinate lies in a region B.
We simply add up or in our case now,
integrate over that region.
So accordingly the notation is
looking a little formidable, but
the logic now is impeccable.
We're cutting up a region
of n dimensional space
described by the following relationships.
The first n minus 1 coordinate
variables must add up to a number
which lies in a region A.
And the last coordinate
variable must lie in B.
And what are we adding up, integrating?
Naturally, the joint density
of the n coordinate variables.
And here is where independence
comes to play a key role.
Because the variables are independent,
the joint density is a product
of the individual densities.
But each of the individual densities is
described by exactly the same function.
They all have the same distribution.
They're all governed
by the same density P.
And so you have p of x1,
which tells you the density of the first
coordinate variable at the point x1,
multiplying p of x2,
the density of a second coordinate
variable at the point x2, and so on down.
And there you have it.
Now once you've got
an integrand which peels off so
beautifully into a product
of individual pieces.
Then naturally enough we can
separate the intervals and
do the integration iteratively.
And we break it up into two pieces,
a segment where we are doing an integral
over the first n minus
1 coordinate variables.
Over what region?
Well, exactly the region where
the sum of those variables lies an A,
and then the second integral
where we have Xn lying in B.
Now observe that the last integral is
accumulating the probability over
the region B for the innate variable?
Or, in a more familiar notation,
this stands for
the probability that the innate
coordinate variable takes values in B.
Now that's nice and simple.
What about the big ugly multiple
integral mess in front?
Well if we look at it carefully it says,
this describes an event
involving a first n minus 1 coordinate
variables where their sum lies in A.
Or, in other words, it describes
a probability that Sn minus 1 lies in A.
And there you have it.
The probability of the intersection
of the two events is a product
of the two probabilities.
NS and minus 1 and
XM therefore are independent.
With this key observation under our belt,
now let's go ahead and
see what happens to expectations and
variances for the partial sums.
So let's build up a story once step at a,
at a time.
Starting at and equal to 1.
Well, S1 is X1, so this is trivial.
The expectation of S1
is that the same of X1.
It's mu.
The variance of S1 is sigma squared.
The first non trivial case is S2.
But S2 is S1 plus X2.
S1 is independent of X2.
Both these variables have to
got mean our expectation mu and
both of them have got
variance sigma squared.
And therefore the expectation of S2
by our additivity result is twice mu.
The variance of S2.
But additivity because the summands
are independent is twice sigma squared.
Oh, and now it's easy to build it up.
S3 will add S2 to X3.
And if we keep doing this.
Sn.
We'll add Sn minus 1 to Xn.
Again, now we have two independent
variables which are being added.
And therefore, by inductive process,
it's now clear that the expectation
of Sn is n times mu.
And the variance of Sn is
n times sigma squared.
Beautiful.
Addidivity has managed to wrap up and
you know, put in a very
compact package both the expectation and
the variance of these partial sums.
The density of the partial sums is
a much more complex affair, no matter.
The expectation and
a variance leverage additivity and
record these beautiful
compact expressions.
Now, this story is already
telling us a little bit more.
The expectation of Sn is n times mu.
Or another way of saying this is,
the expectation of Sn divided by n is mu.
And our experience with
the binomial suggests,
therefore, that we might
anticipate that Sn divided by n
concentrates near mu is a good
estimate of mu, and indeed it does.
And this is going to be the content
of the law of large numbers.

