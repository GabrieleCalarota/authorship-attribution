In our abstraction of the polling
process we are now come
upon the following question.
We have a sequence of Bernoulli trials,
X1 through Xn,
which represent random samples
from an underlying environment.
We form the accumulated
number of successes S sub n.
And we anticipate the fraction,
S sub n over n,
may perhaps, if we're good and lucky and
say our prayers at night, represent
the underlying unknown
population proportion p.
Now, to begin to make progress in
analyzing whether this in fact
is the case, we should start
by understanding the nature
of the chance variable S sub n,
the accumulated number of successes.
It is wise in such a setting to follow
the Hungarian mathematician George Polya's
sage advice that we should begin with
the simplest version of the problem
that we don't quite understand.
Accordingly, let us consider
the following setting.
Let us begin by looking at a setting
where we have a sample of size three.
In other words,
we look at three Bernoulli trials,
three coin flips whose outcomes are X1,
X2, and X3.
The coin has got a bias p towards success,
but p is fixed.
But at the back of our mind we keep in
mind that in the polling application,
p is not known a priori.
Now we have a chance experiment,
three flips of a coin.
What is the sample space?
Well, by now we are quite familiar with
how to write down a sample space in such
a setting.
To completely articulate the sample space,
we're going to have to write down
all possible strings of length
three of zeroes and ones.
There are eight possible strings
arranged from left to right
in order of first through third.
What are the probabilities we want
to assign to each of these atoms?
Well here's where independent
selection comes into play.
Chances multiply under
independent selection.
We actually have a product space here,
a space of repeated independent trials.
The success probability is p,
the failure probability is q.
So, the chance of updating
three failures in a row,
three zeros,
is q times q times q or q cubed.
The chance of updating two
failures to begin with,
followed by a success is q
times q times p or q squared p.
The chance of obtaining a failure,
a success, and then a failure,
is q times p times q, or
q squared p, and so on.
Of course we are very familiar
now with this kind of setting.
We promptly take this raw data and
assemble it to form the accumulated
successes in our sequence
X1 plus X2 plus X3,
what we've called S sub 3.
And of course S3 is now
a new chance variable.
It inherits certain atomic probabilities.
What is the chance that S3,
the accumulated sum of three tosses, is k?
Now, hitherto we've used p of k for
the atomic function, but
p is overloaded here.
We are using p for
the success probability of a coin.
So let's introduce some new notation.
Let's call this b, for Bernoulli, of k.
So b of k represents the probability that
the accumulated sum of three
Bernoulli trials takes value k.
We now have a new chance experiment.
We've got a new sample space
engendered by these sums.
What are the possible values for S3?
Well this is easy enough.
Since S3's the sum of three 0s and 1s,
the outcome can only be 0, 1, 2, or 3.
And therefore we can think of this
metaphorically as a new chance experiment
where we're throwing a four-sided object,
and the object has got faces 0,
1, 2, and 3 with certain weights or
atomic probabilities.
Our job now is to figure out what
these atomic probabilities, b0,
b1, b2, and b3 are.
Let's take, for example,
the case where the sum is 1.
How can the sum of three 0s and 1s be 1?
Well, it's very clear that by
an inspection of the possibilities,
this can only happen if there
are precisely two failures and
one success, two 0s and
a 1 somewhere in the sequence.
And inspection shows that
this can only be the case if
the outcome of the original
experiment is 0,0,1,
0,1,0, or 1,0,0.
What is the probability we
want to associate then with
the accumulated sum being 1?
Well, naturally enough,
we add the various atomic probabilities.
But we notice that each of these atomic
probabilities is exactly the same.
Two failures and a success, in any order,
has got probability q squared p,
and therefore the ensemble
possibility is thrice q squared p.
Right, and you now put this all together.
So b1 in our setting
is thrice q squared p.
What about b0?
Well that corresponds to S3 being 0.
This can only happen if all three of
the trials result in failures, and
therefore the probability is q
times q times q, or q cubed.
And so on.
And we see how we can
build these probabilities
by consideration of
the individual sequences and
careful reassembly of sequences
which belong together.
Well, this idea generalizes very,
very quickly.
And so, without further ado,
let's consider the general setting.
So, let us begin now with
a sample of a generic size n.
X1 through Xn represent
Bernoulli trials by tossing your
coin with success probability p.
Let S sub n represent the accumulated
successes in these n trials.
Our question is to figure
out what the distribution or
the atomic mass function there is that
we should associate with S sub n.
Now let's consider a particular sequence,
in this case let's say we're
looking at a sequence of 12 trials.
I've col, color coded as before,
0 by blue, 1 by orange.
And we've got a particular sequence,
12 trials,
of which exactly 4 are 1, 4 are orange.
What is the probability,
the atomic probability
in the space of these Bernoulli
trials of observing such a sequence?
Of course here's where independent
selection comes in and
simplifies matter, matters for us.
Probabilities simply multiply.
And so, reading from left to right,
I'll multiply probabilities q
times p times p times 4 q's times p
times 2 more q's then a p then a q.
Or in other words,
there are exactly 4 p's in the product,
and 12 minus 4, or 8 q's in the product.
And so, the probability of observing this
particular sequence of 4 successes in
12 trials is p to the power 4
times q to the power 12 minus 4.
And now we come to, to the key idea.
What is the chance of observing some
sequence with exactly 4 successes?
Now a little introspection tells us,
well the picture is going to be a lot
like the one we have on the screen.
With the proviso, the 4 orange balls
could migrate and move around, but
at the end of the day we're going to have
12 balls of which precisely 4 are orange.
Every sequence, every atom that
contributes to the event that there
are exactly 4 successes will have
probability p to the power 4,
times q to the power 12 minus 4.
And so the question now is,
how many such sequences can we construct?
This now is a purely
combinatorial problem, and
of course, we are very familiar with this.
And, if the listener thinks back to
the preview lectures on combinatorics,
in particular Tableau 3 Part 2,
we realize oh,
this is given by a binomial coefficient.
The number of ways in which we can select
4 positions out of 12 is 12 choose 4.
And therefore the probability
of observing exactly 4
successes in 12 trials is 12 choose 4,
times p to the power of 4,
times q to the power 12 minus 4.
And now we've understoo,
understood the basic principle.
What if we want to observe k successes?
Well our first line will be replaced by
a particular sequence with k successes.
And of course, n minus k failures
will have probability p to the power
k times q to the power n minus k.
What is the probability of observing
exactly k successes in those n trials?
Well, those k successes must be
distributed somewhere in n positions.
There are n choose k ways of selecting
them, and therefore the probability at
hand is n choose k times p to the power
k times q to the power n minus k.
Simple.
Now, a little notational grease will make
our steps forward a little bit easier.
Of course, these probabilities
are exactly the probabilities,
the atomic probabilities for
the accumulated successes.
These are what we called b of k.
But we should keep at the back of our
mind that k is the free variable here,
counting the number of successes.
But we are parametrized by
two additional variables
which are the backbone of the problem.
n, the number of trials, and p,
the success probability of the coin.
So these two parameters n and p
parametrize an underlying probability law.
As k varies between 0, 1, 2,
3 up til n, the number of
successes in n trials can only
be an integer between 0 and n.
Now let's very quickly
summarize our discovery.
We have discovered the august
binomial distribution.
We started with variables
corresponding to independent sampling
from an underlying distribution, in
this case the simplest of distributions,
a Bernoulli trial, a coin flip.
X1 through Xn are Bernoulli trials.
The underlying sample space is strings
of zeroes and ones of length n.
Of course, independent selection
makes the description easy.
S sub n is the accumulated
number of successes.
We will say it has
the binomial distribution
because of the appearance of the binomial
coefficient in its formulation.
And this distribution,
this atomic probability law is
parametrized by two parameters.
The number of trials n, and
the success probability p.
So k is your free variable,
k is going to run between 0 and n.
And for every choice of n and
every choice of p, we've got a new
binomial distribution, right?
The word distribution
here is a surrogate for
the atomic mass function for
this particular problem.
What we've discovered is a new chance
variable, the accumulated successes,
contrived, put together from these
old variables X1 through Xn.
At this stage,
before we proceed, we should
consider whether we've done
our calculations correctly.
After all, the b of k,
as k varies between 0 and
n, is purportedly at atomic mass function.
And of course this engenders a probability
measure on the underlying space,
which now I can think of as just
the numbers from 0 through n there.
Is this in fact a bona fide mass function?
Have we done our calculations correctly?
Of course we could say, look, clearly
we have done our calculations properly,
in which case,
this must be the right answer.
And therefore, the bk's must be bona fide,
which means that they must be positive and
they must add to 1.
But of course it is always wise
to double check our calculations.
Let us begin by checking positivity.
But of course this is trite.
The binomial coefficients are positive
by definition, and p and q are positive,
and therefore the binomial probabilities
bn of k and p are going to be positive.
So positivity is trite.
What about normalization?
Well, we'd require these
probabilities to add to 1.
Now let's check.
The sum of these binomial probabilities
gives you an expression, a sum.
But, a little introspection
tells us that looks complex but
it's actually very familiar.
The sum in the middle is
exactly the binomial expansion
of p plus q to the power n.
But recall that q is 1 minus p, or
in other words, p plus q is exactly 1.
And therefore 1 to the power
of n is manifestly 1, and
therefore indeed the sum of the binomial
probabilities adds to 1, as it must.
The listener will recall that we've
gone through this calculation in
a less intuitive setting.
In Tableau 6 Part 1, when we were
looking at examples of mass functions,
and we just pulled this one out of a hat,
and it worked out.
But now we understand
where the setting is.
This arises anytime we consider
sequences of coin tosses,
and we consider polling applications.
Now we've got a bona fide distribution
at hand, a bona fide atomic mass
function on this space of accumulated
successes 0, 1, 2, 3 up til n.
Before we proceed further, it is always
wise to see, examine the structure,
to see whether we can glean
some more information from it.
In the wise words of the Hungarian
mathematician George Polya once again,
Polya recommends that we draw a picture.
And so let us draw a picture
with out further ado.
Let us focus on the case where
n is 100 for definiteness.
So we pick a suitably large number, which
is not so small that it is trivial, and
plot the binomial probabilities for
different values of p.
So in your figure you now see three cases,
p being 0.3,
p being 0.5, that is a fair coin,
and p being 0.8.
And you can see the binomial probabilities
already have got certain nice symmetries,
certain clean characteristics
which are tempting to analyze and
which suggest that there are simplicities
waiting for us to discover.
So for example it seems clear that
the binomial probabilities are unimodal,
they increase smoothly and
they decrease smoothly.
It seems clear that
the binomial probabilities
march to the right as p increases.
A little introspection will convince
us that this is in fact to be expected.
Though of course, intuition by
itself would not tell us the precise
nature of how quantitatively
these probabilities behave.
The picture suggests also that there
is a centering phenomenon going on.
When the coin is fair, p is 0.5,
we find that the maximum value center
of these atomic mass functions and
distribution, arises at the halfway point,
100 divided by 2.
When p is 0.3,
the center appears to be one-third,
sorry, 0.3 of 100, or at about 30.
And when p is 0.8, the center appears
to be at about 0.8 times 100, or at 80.
Accidental?
Certainly not.
But of course, this is going to
require some more discovery.
Something else can be gleaned.
As p moves away from 0.5,
the curves get narrower.
In other words, there appears to be a
maximal uncertainty when the coin is fair.
And as p moves towards 0 on one side, or 1
on the other, the curves get narrower and
narrower, which means there is more and
more certainty.
Again, very intuitive, because when it,
when a coin is very heavily biased
towards 1, you would expect a lot of 1s.
You'd expect a big accumulated sum.
And similarly when a coin
is very biased towards 0.
But again, the picture tells us, there's
not just a quality to understanding,
but it suggests that there's
a quantitative description
lying in wait to be discovered.
But we are getting ahead of ourselves.
Right, this gives us enough
to understand the nature
of the distribution of accumulated sums.
We should pause now and
promptly try to settle our intuition
by looking at simple examples.
So this comes up next.

