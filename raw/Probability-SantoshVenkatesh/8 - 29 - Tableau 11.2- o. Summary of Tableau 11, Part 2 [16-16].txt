Summary of tableau 11.
The fabulous limit laws, part two.
The law of large numbers in continuous
spaces, computation a' la Monte Carlo.
When we discovered the law of large
numbers in the setting of the binomic
through the art of Chebyshev's inequality
it was already clear in the analysis
that the mode of thinking was
going to be wildly extensible.
So the purpose of this lecture was to
illustrate some of the features that
emerge almost naturally
from that kind of analysis.
So, to begin we're going
to have to extend and
generalize the background we started with.
So, we first start with building up
a background lexicon that we can utilize.
I chose to specialize the presentation
to a continuous setting just to
illustrate how things they grace smoothly
from the discreet to the continue.
Though as we've seen through the progress
of this tableau, the analysis and
the results extend much more widely.
But accordingly,
let's begin with a continuous setting and
the keys here are the continuous analogs
of the ideas of expectation and variance.
So to begin,
consider a chance variable, x,
which is governed by a probability law
conferred by a mass density function,
p of x.
Recall, p of x connotes, in one dimension,
a probability mass for
unit length at the point x.
The expectation of X now
is defined as a continuous
analog of a probabilistic center of mass.
Of course, we will call this mu, or
write it more formally as E of X.
It is now just a weighted
integral of values
weighted according to
the mass density function.
Now the moment we go
down to infinite domains,
there are caveats we have to
keep in the back of our mind.
What are written down on the screen is
an improper integral with infinite limits.
And we know, from basic calculus,
that such integrals might
occasionally fail to exist.
So, the caveat is if
the expectation exists, sadly there
are variables and there are densities for
which the expectation does not exist.
But in our setting we
have focused resolutely
on variables where there
is an expectation.
And indeed,
where there is an ideal variance as well.
The variance connotes the probabilistic
version of a moment of inertia.
It captures the expected
squared deviations of
the values of a chance variable around
its center, around its expectation.
So we look at the deviation,
square it, weight it according to do
the density function and integrate out.
Now even if an expectation exists,
there's a small possibility that the
integral might diverge to plus infinity.
Again, we focus resolutely on settings
where we have expectation and
we have a variance.
Now with this in hand, the variance,
as we have seen in the discrete cases,
connotes an expected squared
spread around the center.
So the spread itself should be measured
by the square root of a variance, or
the standard deviation.
This is actually captured
beautifully by the elementary but
wonderful inequality of Pafnuty Chebyshev.
So, what does Chebyshev tell us?
Chebyshev asks for the chance that
a chance variable deviates from its
expectation in absolute value by
more than a designated amount.
Let us day designated by
the Greek letter tau.
Chebyshev's inequality
gives us a bound for
this probability in terms of the variance.
So the probability that X deviates from mu
in absolute value by more than a deviation
tau is bounded about by the ratio of the
variance to the square of the deviation.
Now, this is elementary.
You should not run away
with the idea that this is
a very tight and very accurate inequality.
The inequality is rather crude.
It's like taking a very blunt knife to a,
soft situation.
You're going to get a crude cut.
Its potency lies in the fact that it's so
elegant and so simple.
It requires so few constraints.
And its largest impacts are seen
when things are independent.
And to capture this, we focus on settings
where we have sums of variables.
Additivity plays a large role
right through the entire theory.
And to begin we observe that expectation
is essentially a glorified sum,
a limiting sum.
Expectation is always additive.
If we add independence to the mix
then we find remarkably that
variance is also additive.
So, the expectation of a sum
is a sum of expectations.
The variance of a sum of independent
summands is a sum of variances.
This is the background we need.
The tools which are going to fuel
the res gestae, the key results to come.
So what are the results that emerge
from this kind of background?
We immediately start with
the idea of a random sample.
Now what is a random sample?
Recall a random sample constitutes
independent selections from
some underlying distribution.
Let's say generic chance variable is
called x, it's got some probability law.
And at this stage, the law could be
continuous or it could be discrete.
It makes little matter.
We're going to draw by independent
sampling, a sequence X1,
X2, X3, Xn, and so forth.
Independently, from the same law,
the same distribution, as a generic X.
Let us say that there's
underlying distribution has got
common expectation view and
a common variance sigma squared.
We'll promptly form the partial
sums of this independent sample.
So, let S1 be X1, S2 is X1 plus X2,
S3 is X1 plus X2 plus X3, and so forth.
Sn representing the nth partial sum.
Now, observe then the variables in the sum
are all independent copies of each other,
statistically speaking.
They're all drawn from
the same distribution and
therefore they all have the same
expectation and the same variance.
Additivity of expectation
immediately allows us to write down
that expectation of Sn is n times
the common expectation view.
And because these summands
are assumed to be independent,
the variance of Sn is now n times
the common variance sigma squared.
It's a very potent and
powerful observation.
You see, because recall that
spread is captured by the square
root of the variance,
the square root of a variance now behaves
like the square root of n times sigma.
Assuredly the expectation is
moving linearly in n and for
large n is going very far out.
And assuredly the spread around
the center is increasing with the number
of steps while the spread is
increasing much slower than the spread,
the expectation actually is moving, right.
In 100 steps,
the spread is only a factor of 10.
In 10,000 steps,
the spread is only a factor 100.
This potent observation,
coupled with Chebyshev's inequality,
gives us the law of large numbers.
We call it the weak law of large
numbers because, as I pointed out,
there are stronger versions of this which
can say things about the entire sequence.
But let us focus on the weak law
of large numbers, for the nuance.
And what does this say?
This tells us that the ratio of Sn to n,
in other words,
the sample mean,
deviates from its expected value mu,
an absolute value,
by as small an amount as epsilon.
What is epsilon?
My favorite tiny number,
10 to the minus 23.01.
What you will.
Fixed, but arbitrary.
The probability of such
a deviation is bounded about
by the ratio of the variance
to n times epsilon squared.
For any variance, and
any epsilon however small,
as n becomes larger and larger and larger
the bound on the right goes off to 0.
And therefore, in short,
the phenomenon that we are describing
here is that the sample mean, Sn over n
is increasing concentrated around
its center, its expectation, mu.
You know, you should pause to appreciate
the wonder that's emerging here.
We are dealing with a chance experiment.
The variables X1, X2, X3 are fluctuating
randomly, uncontrollably.
And we're just adding.
And somehow the sum of a bunch
of randomly varying entities
is behaving, properly normalized by n,
as if it were a constant.
It's as if all the uncertainty
has been washed away and
we're getting a almost
deterministic answer.
This, as we've seen, has very potent and
powerful consequences.
We've seen a variety of applications and
it list is a smorgasbord of possibilities,
it's a small set of the various
things we can actually do.
The earliest example,
we saw was that of polls and
this was what motivated our
entrance into this cannon.
We see that we can estimate
proportions of subpopulations
in a large population very
efficiently via a small sample.
The same idea is tweaked very
slightly to create a double sample.
Can be utilized to check
the efficacy of drugs and
this is at the heart of drug testing.
We can use these models more
generally to estimate expectations or
means of underlying processes
that are a priori unknown.
For example, what is the mileage
that you might expect in a new car?
What is the rate of imperfection
a semiconductor chip manufacturer
might expect in their chips?
What is the rate of production
in a shift at a factory?
All of these are presumably
governed by chance laws for
which we do not a priory ahead of time.
Know the expectation.
But if we observe these phenomena over
a period of time and average them,
then the law of large numbers is going to
tell us that the sample mean is going to
give us a very good estimate of
the true underlying expectation.
These ideas are utilized in quality
testing, in the time to failure.
How long does a computer have?
What is its lifetime?
How long will a light bulb last?
How long before an airplane
becomes unsafe?
What is the life expectancy of a bridge?
These are all questions
of time to failure.
If you think of turn and shake of
death as failure then we can utilize
these ideas to design actuarial tables for
life expectancy, for risk.
All of these based upon
the laws of large numbers.
Going back in time, the earliest examples,
where people intuitive a large number
of numbers was in the theory of games.
And of course,
the law of law of large numbers now feeds
in directly into a model,
a theory of fair games.
More subtle and sophisticated applications
arise when we move into what appeared
to be deterministic settings where we have
to add a large group of quantities for
you to integrate known functions.
And there, the complexity of the problem,
as we've seen,
can blow up very quickly with dimension
to the point where it becomes
fundamentally intractable because of
a limitation in the resources available.
Remarkably, in the middle of the 20th
century the discovery of Fermi,
and alarm, shows us that we can
replace deterministic computation
by the strange and wonderful idea,
a probabilistic computation.
This ensemble suite of replications
that we call Monte Carlo methods.
Remarkable.
These methods now have become
a fundamental in the 21st Century.
They permeate essentially every science.
The laws of large numbers have
applications in other areas, for
example, in the design of
optimal stock portfolios.
How does one allocate resources
in a diversified manner so
as to maximize our returns?
This is a small,
a very small set, of the possible
applications of the law of large numbers.
What I hope to have done is to
have stimulated your curiosity,
to explore more widely.
Now with this under our belt,
let us go back in our next
tableau to the question of pose.
The issue at hand there was the law
of large numbers gives us large
guarantees as to why polls work,
why drug testing works.
But we concluded tableau 11,
part one with the observation that,
encouraging as these results were,
the crude application
of Chebyshev's inequality
gave us population of sample
estimates which were still
much too large for comfort.
The question now is,
can we revise downwards our requirements,
our estimates so
that we can come up with estimates which
are practically feasible and appealing?
This investigation will take us
in a beautiful new direction, and
like a flower unfolding we'll find
the bell curve emerging from these ruins.
This is going to be the subject
of our next tableau.

