We have now come to a right understanding
of how to fold inside information
about the occurrence of an event
into our probabilistic framework.
Before we summarize, we should take
a look at the definition again and
examine it to see whether we can
draw any more conclusions from it.
So recall that a conditional probability
of a target of a day given another event B
is given by a simple ratio.
Colloquially, we think of this expression
to represent the proportional chance of
A within B.
Ineluctably to the definition is folded
in the idea of a joint occurrence.
A conjunction of two events.
That portion of A that resides inside B.
Suppose we change perspective and
focus on the intersection and
let us see then what comes out of this.
This is an illustration of how
sometimes what looks like drab
mathematical manipulation can sometimes
lead to surprising observation,
intuition, and results.
So, let's begin with the definition
of conditional probability and
clear the denominator by multiplying left
and right hand side by the probability B.
We now get an identity for the probability
of an intersection of a conjunction.
Now let's rewrite what you've got.
Here's a good place to pick
up your pencil and paper and
write the equations down to make sure
you internalize what we are doing.
So, we're clear denominators.
And identify the probably of
an intersection of two events on
the left-hand side as a product of
two probabilities on the right.
A conditional probability
of one given the other,
times the probability
of the ancillary event.
Of course, we could equally well have
reversed the roles of A and B here,
because intersections are commutative.
You could do them in any order.
So, we could over to the right-hand
side just as well as the probability
of B given that A occurred times
the probability that A occurred.
Okay, but no matter, for our purposes
what you've now got is an expression for
an intersection probability.
Aha.
I wonder if we can milk this and
get other expressions for
more complicated intersections.
The next step might be to consider
the intersection of three events.
Well let's consider what happens.
Suppose we have events A, B and
C in the probability space.
I want to say something about
a probability of A intersection B
intersection C.
If we group two of the events.
Say, B and C together.
Well, then we have a composite
event B intersection C.
If the rule of B is now taken
over by B intersection C,
we simply apply the definition
of conditional probability and
write this down as,
the probability of A given
this ancillary event of B intersection C,
times the probability of B intersection C.
Aha, but wait a minute!
On the right now I've got the probability
of the intersection of two events.
And the first line tells me I can write
that in terms of conditional probabilities
and at the end I now have a chain
of conditional probabilities.
The probability of A given B and C.
Times the probability of B given
C times the probability of C.
And interestingly,
we can write down the probability
of an intersection of three events as
a chain of conditional probabilities.
This is sometimes called the
multiplication rule but chaining seems to
be more a property, the chaining
conditional probabilities together.
Of course the moment you've done three,
now there's no reason why we
can't do more than three.
So suppose we now have n events,
A1, A2, through An and
we are interested in
the conjunction of all of them.
In the intersection of all of them.
We proceed just as we did for
the case of three events.
We first grouped the last
n minus 1 events together.
Actually or any n minus 1 events together.
Conditional occurrence
of that composite event.
And now we've got a probability
of n minus 1 of them.
Oh, but that's easy.
Condition again on n minus 2 of them.
And going down the line we get
a chain of conditional probabilities,
A1 given the occurrence of A2 through An,
A2 given the occurrence of A3 through An,
and so on.
An minus 1 given the occurrence of An, and
then you are left with nothing
except the occurrence of An.
And there you are.
We've now got a chain of conditional
probabilities which captures the essence
of a conjunction probability.
Of course the last equation
is a long out form.
And it's taking you awhile
to write this down.
Okay.
We can encapsulate this in
a compact mathematical terminology
using a product operation.
And I can write this whole thing down.
Meaning exactly what we've written here,
in this compact form.
The probability of an intersection of
an events A1 through An can be returned
as a product of conditional probabilities
where the index J runs from 1 through n.
And we're interested in
the product probability of AJ
given the events AJ plus 1 through An.
The last conditional probability here
is just the probability of An itself.
Okay?
And so here's a compact way of
writing down this chaining formula, but
I hope you agree with me that
the chaining formula is very mnemonic.
It's very easy to remember the condition
on the last group and you get the first
one, then condition on the next to
last group and get the second one.
And you keep going down until
you've exhausted all the things
you want to conditional.
Now this is just mathematical
manipulation, and you might wonder,
well is this any use whatsoever?
Okay so you've got a formula
it looks a little clumsy,
it looks awkward, but
is it any use whatsoever.
The answer depends upon
the problem at hand.
In most of the applications we have seen
so far, I've given you settings where
it is simple to directly deduce what
the probability measure is, for example.
By writing down atomic probabilities.
In instances like that,
it is simple to deduce directly from
the statement of the problem,
conjunction probabilities.
And once you've got
conjunction probabilities,
you can then compute conditional
probabilities by the usual ratio.
But there are settings and there are
applications where it'll transpire that it
is more natural to either
define the probability measure
implicitly in terms of
conditional probabilities.
Or settings where even when the
probability measure is specified directly,
it is easier to compute to
the conditional probabilities first
before we compute
the conjunctional probabilities.
I'll give you one example
of the latter instance now.
And I.

