So let's go back to an old familiar,
a dice problem.
So here's the setting.
Imagine that one has a pair of dice,
one die with five red faces and
one white face.
Imagine that this one has a white face but
perhaps towards me so you don't see it.
And one white die, one die with
five white faces and one red face.
Imagine the red face is concealed from me.
We have two dice.
A die is chosen at random.
Now, what does that mean?
I'll imagine that you have a fair coin,
and you toss it.
And if it comes up heads,
you select the die which has five
red faces and one white face.
If it comes up tails, select the die which
has five white faces and one red face.
Good.
Once you've selected a die, you throw it
twice and observe the face that shows up.
The question at hand.
What is the chance that for
the chosen die,
you're not told which it is, that given
that the first throw shows a red,
that the second throw
also shows a red face?
Pause and think about this for a minute.
See what your intuition tells you.
Of course, we are going to put
together a formal framework for
an analysis of this elementary problem.
Now the problem setting that I have
given you can morph into other
mathematically equivalent but
formally different settings.
For example, we could replace the two
dice by an urn and ball problem.
Imagine one has two urns.
In one urn there are five red balls and
one white ball, and
in the other urn, the second urn, there
are five white balls and one red ball.
An urn is selected at random, and
then balls are drawn from the selected
urn, sampling with replacement.
Given that the first ball drawn is red,
what are the chances that
the second ball drawn is red?
We could replace the urn and ball
metaphor, for example, by a coin metaphor.
Imagine one has two coins, one is
biased 5 to 6 in favor of successes,
the other is biased 1 to
6 in favor of successes.
A coin is selected at random and
tossed twice.
Given that a success
occurs in the first trial,
what are the chances of
a success on the second trial?
Settings like this inform
a variety of problems,
and we'll see application domains
in epidemiology, for instance,
which draw upon such simple urn or
dice models.
But to come back to our problem.
So imagine again we have our two dice.
We've selected one of them at random and
we've thrown it twice.
What are the chances that the second
throw results in a red face, given that
the first throw has given you a red face?
Well, naturally enough,
we should begin by constructing
a sample space for the problem.
Now recall that one constructs
a sample space by identifying all
chance-driven elements and
then encapsulating our understanding of
all that is uncertain in the experiment,
in some simple and evocative notation.
Now, in our setting, there are three
elements that are chance driven.
First, the selection of the die.
Second, the outcome of the first throw.
Third, the result of the second throw.
And so to completely specify a,
an idealized outcome for
this experiment, one has to
specify a triple of possibilities.
The selection of the die, one or two, and
the results of each of the two throws.
Let's say that we call such a triple,
identify it by three letters,
x, y1 and y2.
Of course, you'll notice there's
a semicolon after the x.
This is purely for visual purposes.
There's good notation and
there's bad notation.
The best kind of notation is evocative.
It clarifies and brings to the fore
the key elements of the problem.
Poor notational choices obscure and
confuse.
Now in our setting we have three elements,
and to visually distinguish the different
flavors of chance in the three elements,
we utilize different letters.
x for the coin toss which
selects the die and y for
the face of the die that results.
We use a semicolon in addition to separate
these chance elements to visually tell us,
there are two different pieces of chance
going on in this conjoined experiment.
Now the sample space is the space
of all such triples where x
takes one of two values, 1 or
2, representing the two dice.
And y1 and
y2 take values either red or white,
and we abbreviate it, of course,
naturally, by r and w.
What is, or what are the events
that are of interest to us?
Now, on consideration,
it becomes clear that for
our problem there are two
events of interest.
An event that the first throw of
the randomly selected die is red.
Let's call this event R1,
the subscript 1 connoting the first throw.
R of course connoting red.
R1, then, is a subset of
the space of idealized outcomes.
It's a subset,
a subcollection of triples, x, y1 and y2,
with the property that y1 is red and
x and y2 are unfettered.
Similarly, a second event of natural
interest in this problem is R2,
the event that the second throw
of the die results in a red face.
What is R2?
It is that subcollection of sample points,
that subset of triples x, y1, y2,
where the third element, y2,
is red and x and y1 are unfettered.
They can be anything.
Now before we go into a, launch into
an analysis, into a calculation,
we should pause and
think about the problem.
Is there a way of partitioning
it in a reasonable way?
And the notation makes clear that there
is indeed a natural partition for
this problem.
Is there an ancillary
event of interest to us?
Naturally enough, on consideration,
we realize that the entire
game seems to hinge upon which
of the two dice is selected.
And therefore, we introduce a third event,
an ancillary event of
interest in this problem.
Let's call it A.
This event is going to encapsulate
side information about the problem.
Let A be the event that
the first die is chosen,
let's say the die with five red faces.
And let A complement,
therefore, represent the event
that the second die is chosen, that is,
the die with five white faces.
We are now equipped to do a computation.
We have identified two target
events of interest, R1 and
R2, and potential events which
can function as ancillary
events conveying side information,
the events A and the complement of A.
What now is the probability measure?
Now, there are several ways of approaching
this problem, but perhaps the most natural
is to specify the probability
measure implicitly via
certain natural probabilities we would
like to associate with certain events.
So let's begin with
the selection of the die.
Random selection means one is
going to flip a fair coin,
and if it comes up heads pick die 1 and
if it comes up tails pick die 2.
Naturally enough, I want to allocate
to both possibilities equal chance.
And therefore the probability measure,
whatever it is,
should allocate to A and
A complement both probability one-half.
Very well.
How do you proceed?
Now once a die is selected, now there's
no ambiguity about which die is selected.
I picked up die 1 with five red faces and
one white face.
Now the chance experiment is very,
very clear.
For this die, the chance of throwing
a red face is naturally 5 in 6.
There are five red faces,
one white face, and therefore,
the chance of throwing a red face, given
that I have selected this die, is 5 in 6.
And therefore,
the chance of throwing a white face,
the event A complement, given that
I've selected this die, is 1 in 6.
Now what are the chances of
throwing two reds in succession?
Well, there are five possibilities for
the first red face, five for
the second red face, and therefore,
by the elementary principle of counting,
5 times 5 or 25 possibilities for
selecting the pair of red faces.
Out of how many possibilities?
6 times 6, or 36 possibilities for
the arrangements of the two faces.
And therefore, the chance of
throwing two red faces in a row for
die 1, given that die 1 is selected,
is now 25 of 36.
And by a, an identical calculation,
the chance of throwing two red faces,
given that die 2 is selected.
Well, if die 2 is selected,
there's only one red face, and
there's only one way of
throwing the same face again.
And therefore, the chance of
throwing two red faces in a row,
given that die 2 is selected,
is now 1 in 36.
Of course, we can build other conditional
probabilities along these lines.
Now with this in hand, we are in business.
We've got at least implicitly
a probability measure at hand.
And now, we want to compute,
say, the probability,
the chance that the first throw of
the chosen die results in a red face.
Pause for a minute, and
see without doing any calculations,
what your instinct tells you.
Now, when you're ready,
how do we partition the problem?
The event R1 stands in
the role of the target event
H in our definition of
conditional probability.
The events A and A complement stand
in the role of ancillary side
information which conditions
the occurrence of H.
And by the additivity principle,
we simply write down the additivity of
the property for conditional probabilities
and we find we have a very,
very simple expression.
Compute it and
we find there's a 50% chance that
the first throw results in a red face.
Introspection says that's very natural.
You know, I have two possibilities,
and they're so symmetric.
Five red, one white.
Five white, one red.
Pick one at random.
There should be a 1 in 2
chance of getting a red.
And indeed there is.
Now let's take a slightly
more complex question.
What are the chances that the first two
throws both result in a red face for
the chosen die?
I've not told you what the die is.
What does instinct tell you?
Well, raw intuition might say look.
Throwing the die once is as if I had a die
with three red faces and
three white faces.
I threw it once and the chance of
getting a red is 3 in 6, or 1 in 2.
If I were to throw it twice,
the chance of getting
a red should be then 9 in 36,
or 1 in 4, 25%.
Now let us see what our
calculations give us.
Now we have a more complex event,
a conjoint event.
I want the first throw to
result in a red face and
the second throw to result in a red face.
We are looking at the intersection
of the two events, R1 and R2.
Now R1 intersection R2 stands in
the role of H in our definition
of conditional probability, and
we partition the space again by A and
A complement depending
on which die is chosen.
We write down the definition
of conditional probability.
Pause.
Write it out.
Make sure you absorb this.
Write down now numerically what we
have in our implicit definition
of probability measure.
Work your way through the fractions and
we find that the chance of seeing
two red faces in a row is about 36%.
Significantly more than the 25%
I might have naively anticipated
in an equivalent experiment, which turns
out not to be equivalent after all.
This suggest that there are pitfalls here.
There are pitfalls for the unwary.
That we should be cautious in
setting up the experiment.
And this points again
to the salutary value
of setting up the sample space carefully,
setting up the events carefully,
setting up the probability
measure carefully.
We now have two probabilities, the
probability for R1 and the probability for
the intersection of R1 and R2, and they're
all primed for a conditional probability.
Write down the definition
of conditional probability
as the proportional chance of R2 and R1.
Write down what you observed.
Work our way through and find,
the chance of two of a second red,
given that the first throw was originally
red, is quite alarmingly large.
It's about 72%.
That sounds remarkable,
given that on any given throw
the chance of observing red is 50%.
This sounds strange and wonderful.
How did the chance jump from 50% to 72%?
Conditioning does affect raw probabilities
in frequently unanticipated ways.
Of course, once we have an answer,
we can look back at the problem and
figure out what went wrong
with our raw intuition.
On consideration, we realize that when
a red face is seen, since a red face is so
much more likely if the first die is
thrown than if the second die is thrown,
the occurrence of a red
face suggests that there's
a likelihood that the first
die is the die being thrown.
And, of course, if it is the first
die that is being thrown,
the chance of another red face
is quite high, and therefore,
our raw probability is increased
via this conditioning.
Now this is a salutary example.
It's elementary.
There's nothing deep about it,
but it serves as
a useful vehicle to reinforce
the basic definition.
And the key principle of additivity
melded with the definition of
conditional probability via chaining.
Stare at this example, work through
the probabilities again carefully.
Settle the concepts.
These ideas are going to
become repeatedly useful.

