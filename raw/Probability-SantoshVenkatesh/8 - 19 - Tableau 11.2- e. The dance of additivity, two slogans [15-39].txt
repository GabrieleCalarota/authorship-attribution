Additivity plays of subtle role
right through the theory of chance.
And it is going to be manifested here in
a result on additivity of expectation and
variance.
So let us begin, begin with
the simplest setting of two variables.
And to keep ideas clear,
let's focus on a setting where we
have independent chance experiments.
So we consider two independent
trials resulting in
continuous chance variables X and Y.
X is governed by some probability law,
a one-dimensional density,
say p 1, with expectation mu 1 and
a variance sigma 1 squared.
Y likewise is governed by
some one-dimensional density,
say p 2, with expectation mu 2 and
variance sigma 2 squared.
The underlying composite experiment
here is engendered by a product
space of pairs X,Y.
And the underlying product
measure is specified as a product
of the individual one-dimensional
density functions.
Okay, so here we have it.
Now before we launch into a categorization
of expectations, it'll be wise to bear in
mind the very basic change of
variable theorem for expectations.
So consider a transformation.
Think of X and Y as your coordinate
variables, and a measurement,
a transformation which takes X and
Y into a function, f of X and Y.
So now we have a new chance variable,
f of X and Y.
How do we compute its expectation?
Well, a simple way to go about
it is to look at the values
of f of X,Y at a given point X,Y,
weighted according to the mass density
of the pair at the point p of x,y.
In this case, of course, we have assumed
that p of x,y factors into a product
of two individual
one-dimensional functions.
And integrate out over all possibilities,
some are in the limit
of all possibilities.
And Bob's your uncle, you've got
an expression for the expectation.
How about the variance?
Well, we can do it exactly in
the same kind of argument.
We start at a given point X,Y,
at the value of f of X,Y.
Consider its distance from
the expected value E of f of X,Y.
Now that difference squared gives us
a squared distance from the expectation,
weighted according to
the mass density of the pair.
In this case, the mass density of the pair
is a product of one-dimensional functions.
And integrate out over all possibilities.
So this is a very simple
change of variable theorem.
Now let's apply it to the very
simple function where we have a sum.
And so suppose f of X,Y is X plus Y.
What can we now say about its expectation?
Well, by the change of variable theorem,
the expected value of the sum can be
written as take the values x plus y,
weighted according to the mass density,
integrate our all possible possibilities
for X and Y, and there you have it.
Now the fact that the function we
are dealing with has got such a simple
additive form here has got
an immediate consequence.
In the integrand, we have a sum, X plus Y.
But integration is additive, right?
Of course, this is just a manifestation
of the associative property of addition.
Namely, to wit,
that addition can be done in any order.
And so if we isolate the term x and
then the term y,
we can put together the contributions
of each and add them up.
And so we can write down this double
integral as a double integral over x times
a density function plus a double integral
of y times the same density function.
And now we've got on the right-hand
side two double integrals, and
we evaluate them naturally by iteration,
one integral at a time.
So now we notice that the integrands
are separable into functions of x
times functions of y, and therefore,
the integrals separate as well.
And so the first double integral gives
rise to a product of two one-dimensional
integrals.
The second double integral gives
rise to an analogous product
of two one-dimensional integrals, and
I've color-coded them for visibility.
Now take a look at each of those single
integrals on the right-hand side.
Do you recognize them?
Pause, think about it, and then proceed.
Now observe that the first integral,
colored in green,
is just the expectation of
the first chance variable, big X.
The second integral, color coded in red,
is just the area under
the density function p 2.
By normalization,
that area has to be unit.
The third of the integrals is the area
under the density function p 1.
By normalization of that measure,
that integral is unit.
And finally, the last one-dimensional
integral, color coded in green,
is just the expectation of Y.
And we have this beautiful
additivity property.
The expected value of X plus Y
is the expectation of X
plus the expectation of Y.
Now this actually already suffices for
our purposes.
But there's a deeper additivity property
just lurking under the analysis, and
it would be a shame if
we didn't pull it out.
Okay.
What if the variables
were not independent?
Well, in that case, we can't use the joint
density as a product of densities.
We are to go back to
a joint density p of X,Y.
So let's write it down
in terms of p of X,Y.
Again, in the integrand,
we have a sum X plus Y and
the double integral separates
into a sum of two integrals.
And the change of variable theorem tells
us that the first double integral on
the right is just expectation of X.
The second double integral on the right
is just the expectation of Y.
And so this property,
this additive property of expectation,
does not even demand independence.
The expectation of a sum is a sum of
expectations, even if the underlying
variables are dependent, and for
surely if the variables are independent.
But independence is going to be
needed when we turn to variances.
And we do those next.
So let's start with our observation.
If we start with two coordinate variables,
X and Y, which are independent, and
we formed their sum,
we have discovered in particular
the expectation of a sum
is a sum of expectations.
What can we say about
the variance of a sum?
Let's promptly appeal to the change
of variable theorem again.
Identify f of x,y with x plus y.
The variance of X plus Y is
obtained by allowing X and
Y to vary over the entire
coordinate plane.
And at each point X,Y,
look at the sum X plus Y.
Compare it with
the expectation of the sum,
which we've discovered is
the sum of expectations.
Square it.
Now we got a squared discrepancy.
Weight it according to the joint density,
which,
in our case now, because of independence,
is a product of densities.
And integrate out.
There you go.
Now, this looks complex admittedly.
But let's see if we can do a little
algebraic massaging to simplify it.
Inside the square brackets
in the integrand,
we fe, find two terms, an x plus y and
a sum of expectations.
Let's group them by variable.
So associate the expectation
of the chance variable big X
with the coordinate variable little x.
Associate the expectation of the chance
variable big Y with the coordinate
variable little y.
All right, and I can rearrange this sum,
and now I'll put things inside round
brackets and we have, instead of square
brackets, two terms in round brackets.
And we're going to square the sum.
Well, the square of a sum,
we know, by the simplest application
of the binomial theorem.
a plus b squared is a squared
plus 2 a b plus b squared.
Identify a with x minus
expectation of big X.
Identify b with y minus
the expectation of big Y.
And now we've got three terms
that the square of the,
the term in the square
brackets expands it.
Integration, again, is additive.
An integral of a sum of three
terms is a sum of three integrals.
Let's write each of these down,
one at a time.
So first, we have an integral
involving the term little a squared.
And then we have an integral involving
the term 2 times little a times little b.
And then we have an integral
involving the term b squared.
And now we're expanded
out into three terms.
All right, so our expression is getting
algebraically more cumbrous, right?
What is of the essence here is that
we have three terms on the right.
Let's give them names, uppercase A,
B and C, respectively.
And so our task now is to evaluate A,
B and C.
Now let's turn to this and
see what we have.
Let's start with A.
A is the integral of a square.
The integrand has devolved into
a product of a function of x and
a function of y, and
therefore we iterate the integrals and
write this down as
a product of two integrals.
One over x, one over y.
Excellent.
Now, take a good hard look at
the two terms on the right.
Do they strike a chord?
Well, they should.
The first term is just the variance of X.
The second term is the area
under density function.
By normalization, it is 1.
And therefore,
the entire term, A, the first term on
the right, was just the variance of x.
Let's promptly turn our attention
to the term we called C.
Again, we have an integral.
The integrand devolves into
a product of functions of x and y.
We'll write it as two
one-dimensional integrals.
And now, the analysis carries through
in exactly the same way as for
A, with the roles of x and y interchanged.
And therefore, without further ado,
we see that C is identified
with the variance of Y.
Now, and that leaves the more
cumbrous term in the middle, B.
The factor 2 comes out of
the integral as a constant.
And again, the integrand is a product
of a function of X and a function of Y.
The integrand separates.
And so the double integral
separates into a product
of two one-dimensional integrals.
One over x, one over y.
Now let's take a look at, say,
the first of these integrals over x.
Right?
A good hard
look at the integrand shows
the integrand is a difference.
But integration is additive.
The integral of a difference
is a difference of integrals.
And so we promptly write this down
as the integral of x times p 1,
from which you subtract out the integral
of an expectation times p 1.
But the expectation of X is a constant,
say mu 1.
It can come out of the integral, and
now we have a simple expression.
The first of the integrals on the left
is exactly the expectation of X.
The second integral is
unit by normalization.
And therefore, we have the difference of
the expectation of X with itself, or 0.
The first integral is 0, and for
the same reason the second integral is 0.
A product of 0 and 0 is 0.
And so, without further ado,
that big complex term in the middle, B,
vanishes quietly.
Like the snows of yesteryear,
it disappears and
does not trouble us any further.
All right, let's put all of this together.
What do we have?
The variance of a sum has got
three contributions, A, B and C.
A was the variance of X, B was 0,
C was the variance of Y, and so
we've discovered that when
the variables are independent,
then the variance of the sum
is a sum of the variances.
Now the additivity of expectation
is at some level to be expected.
After all, expectation is an average.
Average is a sum.
A limiting sum is an integral.
And sums can be done in any order.
And therefore, expectation should
be additive, and, in fact, it is.
The fact that the variables
are dependent or
not is irrelevant to
the additivity of expectation.
But for variance,
we need something a bit stronger.
Variance involves a square, and
a square is intrinsically not linear.
To recover linearity from a square,
we are going to need more structure.
And that structure is
provided by independence.
The curious listener might want to go
back and ask, can I relax independence?
Indeed, you can.
But it's clear that something
along those lines is needed.
So I shall not dig into it
any further at this point.
We have more than enough for our purposes.
Let's summarize our
discoveries in two slogans.
The first of the slogans for expectation,
expectation is additive,
with no qualifications.
This turns out to be a fact of
surpassing importance in the theory and
the applications.
In particular,
expectation is certainly additive
when the variables are independent.
And the second slogan,
variance is remarkably also additive, but
with the proviso that the underlying
variables are, in fact, independent.
Now with these two tendrils
of additivity in hand,
we are now equipped to launch
into what should we show as
an equality says about a sum
of independent variables.
This comes next.

