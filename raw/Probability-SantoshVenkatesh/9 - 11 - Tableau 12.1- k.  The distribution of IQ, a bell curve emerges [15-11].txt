It is now time to actually do an analysis,
and we're going to find
that a bell curve is going to emerge
majestically from these ruins, right?
Now, keep in mind
William Feller's exhortation,
even strong faith must be
fortified by a statistical test.
So we're going to try to come
up with a principled mechanism
to be able to analyze
data like that presented.
So let's go back.
Here are Burt's two tables on
intelligence from his 1961 paper.
The first table, the IQs of adults.
The second table,
the IQs of their children.
What is the principled path forward?
Now, there are many kinds of tests one
could imagine doing in this setting.
But let's focus on one where we look at
the totals in the last row of each table.
The totals tell us something
about the number of individuals
tested in various IQ bins from,
say, 50 all the way up through 140.
Yeah.
The data themselves are scattered
all over the place.
But a cursory look at the last
rows shows that There is a quite
beautiful coherence of these numbers.
The laws of large numbers appear to
be working beautifully in our favor.
So, let me now show you a histogram
plot of the last two rows,
the totals for the IQs for
the adults and for their children.
Now, in the interest of
explaining the mechanism for you,
remember we're dealing with a nominal
size of 1,000, though perhaps it's
actually really 40,000, but
it's been standardized to 1,000.
And what I'm going to do and
show you in this histogram
table is to collect all the IQ
bins to the left of 70,
pool them together, and
collect all the IQ bins
to the right of 130 and
pool them together.
The motivation, of course, is that as
you go down into very small IQs or
very large IQs,
that we have very few representatives.
And it is well known that the very small
probability occurrences have much,
much worst confidence.
But of course, this is in
the interest of full disclosure, and
the listener should judge for herself
whether this is a reasonable procedure.
T.W. Colonel has provided a very
readable account of this entire saga,
has pointed out that one of
the reviewers of his book said that
this was a subjective statistical fudge.
But of course, the reader should make
up her own mind as to whether this was
a reasonable procedure.
So what I've now done is collect
the totals in the IQs for adults and
children for you in various bins.
The bins are IQs to the left of 70,
and then 70 to 80,
80 to 90, 90 to 100, and so
on, 120 to 130, and finally,
a last or eighth bin from 130
onwards all accumulate together.
Now look at the picture.
This is very persuasive.
The data themselves for adults and
children were spread out.
There were certain things you could gleam
from just the table of data itself.
But the executive summary, the totals,
show a quite remarkable coherence.
These numbers seem to march
beautifully in lockstep.
And we also see the emergence
of a bell curve.
It looks like this kind of data are
beautifully explicable by a bell curve,
a normal distribution.
Remember, the data was standardized,
centered at,
at 100 with a spread or
a standard deviation of 15.
So, to understand or compare it with a
statistical posited generation mechanism,
which is normal, we start with
a hypothetical normal distribution,
a normal density whose
mean is exactly 100,
whose standard deviation is 15,
or equivalently,
whose variance is 15 squared or 225.
Here is a posited normal
distribution of IQs.
Many biological processes have been
posited to have normal approximations,
in part because presumably of an idea
that these processes, like height,
weight, intelligence and other things,
perhaps could be thought of as
being aggregates of very large
numbers of independent perturbations.
And then we wave our hands briskly and
make a vague appeal to our
central limit theorem, and
voila, a normal density emerges.
Howsoever it may be,
here is a presupposed distribution
which is provided as
a potential explanation for
the mechanism by which
the data were generated.
Now, what are the data?
Well, we are sampling at
random from this distribution.
In other words, we create a random sample.
Of what size?
Well, naturally n, though here,
the nominal n we have in
mind is n is a 1,000.
But bear in mind that
the actual numbers reported
there's some uncertainty in that score.
So this could actually be 40 times 1,000,
or 40,000, or some number in between.
So, any some generic number,
nominally 1,000 in our application.
We're going to generate a random sample
honestly, transparently by independent
sampling from this bell curve
with the mean 100, variance 225.
Now, once you have a sample like that,
what are we going to do with it?
Well, they are thinking of IQs here as
being measured on a continuum scale,
but of course, when we report it,
we bin the IQs into bins,
traditionally in bins of size ten.
So let's break up the IQ scale into bins,
accumulate all the bins to the left of 70,
and systematically put the other
bins together, ten at a time.
And now, we have eight bins of IQ.
Now, here's what we do.
Look at each element of the sample,
X1, X2 through Xn.
Each of these values purportedly
represents an IQ of a generic individual.
That IQ is going to fall into
one of these eight bins.
Accumulate the number of elements in
the sample, which were in the first bin.
These were all the individuals in
the sample whose IQ was less than a 70.
Accumulate all the elements in
the sample which fell in the second bin.
These were the individuals in the sample
whose IQs fell between 70 and
80, 80 and 90, and so forth.
In other words, we binned the sample.
Of course, if when you do this, right,
when you talk about accumulated numbers,
we, our natural notation
is to use S sub n.
Our notation is going to get a little
cumbrous because now we have not one
identity to take care of, for
example, the number of successes,
we are actually eight bins.
Let's keep n as a subscript,
S for the number, and
let's use as a superscript an identifier,
1, 2, 3,
up till 8, which targets the bin
that we're talking about.
So, Sn1 represents the number
of outcomes in that random
sample of size n which fell in bin one,
IQ is less than 70.
Sn2, the number of outcomes in that random
sample which fell into the second bin,
in IQs between 70 and 80, and so on down.
With Sn8, all the outcomes in the
accumulated bins from 130 on to the right.
Okay, modestly more complex notation,
but not grievously so, I hope, right?
S sub n, we're familiar with.
It's an accumulated sum, a total.
And the superscript tells us
which bin we are talking about.
So Snk represents the number
of elements in the sample
which fell into the kth IQ bin.
Let's promptly generate these numbers, and
I'm going to show you a particular sample
that I generated with n equal to 1,000.
So we have the adult IQ numbers,
the children's IQ numbers next to them.
And flanking them as a third column,
a third bar,
the number of elements in
a random sample of 1,000 from
a normal density with mean 100 and
variance 225,
the number in each of the eight bins,
Sn1, Sn2, through Sn8.
Of course,
the sum of these Sn's is exactly 1,000.
There are 1,000 elements in the sample
distributed in these numbers
in the eight bins.
Again, I look at this.
It shows that there is a quite remarkable
consistency between these numbers that I
actually obtained in one particular
sample that I generated, and
the numbers provided by
Burt's in these tables.
And they all look like there's a very
good potential for a normal fit.
The bar to the right is
from a normal distribution.
The other two bars are potentially from
a projected normal distribution, but
that is what we're testing.
If anything, the data I generated appear
to be a little bit more variable.
The data that Burt provided appears
to be much more convincing, but
they are all very close.
And they all seem to have a reasonable,
normal fit.
So, here's a question now.
To do an analysis,
in the counts of the number
of elements in the sample in each bin,
of course,
these are charts driven,
how are they distributed?
So now, let's take stock again.
Let's go back to our picture.
Doing that, we are dealing with
a hypothetical normal distribution of
IQs with a mean 100, variance 225, or
equivalently, standard deviation is 15.
We are going to generate a random sample
by independent sampling from this
distribution, and we are going to put
the sample into eight bins of IQ.
What is the chance that a given element
in the sample falls into the first bin?
What is the chance it
falls into the second bin?
What is the chance it falls
into the third bin, and so on?
Well, the probability that
a number drawn at random from
this normal density falls in a given
bin is, by our usual procedure,
the area under the curve of
the density for that bin.
And now all you've got to do is
a simple numerical computation,
which any internet calculator will do for
you, for what these areas are.
And I'm going to give you
a tabulation of these areas.
So this is a numerical
estimate of the area under
the curve in each of these eight bins.
Remember, we have eight IQ bins.
Bin one is IQs less than 70,
bin eight is IQs bigger than 130.
The other bins are IQ bins of width ten.
A typical IQ bin, let's say, is j.
j runs from one through eight.
What are the chance that a random
sample member falls in bin j?
Naturally, let's call it little p sub j.
And the numbers little p sub j are clearly
symmetrical around the center.
And they increase from 0.0228,
truncated to
four decimal places,
all the way up to 0.2475,
and then decrease symmetrically
on the other side.
Now, [SOUND] with this in place,
what can we say about the distribution
of the number of elements in
each of these eight bins?
Now let's take a look at the fist bin.
You got a sample, and
a certain number fall into the first bin.
The chance of falling
into the first bin is p1.
We have repeated independent
trials in the sample.
And each sample member independently has a
chance, p1, of falling into the first bin.
The number that fall into the first bin,
naturally, is the accumulated
sum of these trials.
Think of a trial as having a success
if it falls in the first bin,
success probability is p1, failure if
it falls in any of the other bins.
The accumulated number of successes
in our old notation is Sn1.
And now we see that it must
be a binomial distribution
corresponding to n trials with
a success probability of p1.
Likewise, how many fall
into the second bin?
Well, each trial has got a chance
p2 of falling into the second bin.
There are n independent trials.
Sn2 is binomially distributed
with parameters n and
probability of success or bias, p2.
And likewise,
all the other Snks fall into place.
Each of these is binomially distributed
corresponding to n trials with
varying success probabilities based
upon the area under that normal curve.
Remember the marvelous theorem, Laplace's
theorem, a central limit theorem beckons.
A normal law beckons for these sums.
And the normal laws are going
to be centered where?
Well, Sn1 has expectation n times p1.
Sn2 has expectation n times p2, and so on.
Let's take a look once more
at the histogram of values,
this time with a fourth column,
the expected values for these numbers Snj.
The expected value of Sn1 is np1,
expected value of Sn2 is np2, and so on.
And again, we see a quite remarkable
concordance of these values.
The values reported are accumulating
near their expected values.
There seems to be a concentration,
a drift in that direction,
the law of large numbers,
the central limit theorem.
Our question now is,
well, how do these values
actually compare with what we expect?
Can we quantify this?
This will be the basis of
a principled statistical test.

