So where do we go from here?
The Latin phrase quo vadis asks
a question, where are you going?
As is frequently the case in mathematics,
in science,
in engineering,
when a first discovery has been made,
however humble, it can set in motion
a process of discovery which
unearths subtle deep consequences.
And so it was with Chebyshev's
wonderful inequality in 1854.
The inequality leads to
the law of large numbers.
Now a natural question then,
which preoccupied thinkers and
researchers was where
else can we apply it?
How can we generalize this
epiphany of Chebyshev?
So let's begin again, what is a setting
we have repeated independent trials
of variables which come from
some common distribution and
they have a common expectation that is,
say, mu.
There was also a requirement that
the variables had a common variance,
say sigma squared.
But it is a sad fact, but true,
that a variety of Chebyshev's experiments
in the physical world can be modeled by
variables whose variance is infinite.
Is it possible that we can take the
Chebyshev idea, the law of large numbers,
and extended it to situations where we
have variables with an infinite variance.
Now, a priori,
this looks like slightly stony ground.
It doesn't look promising.
Why?
An infinite variance connotes,
yes, you have a center here but
you could be infinitely far spread out.
And that creates complications,
does it not?
Let's see what will happen if you
retrace the path, form a sample mean,
and then let's ask how does
this sample mean behave.
And some 75 years after Chebyshev
discovered his result Khinchin
in 1929 showed that the large
of law numbers continues to be
in operation even if the individual
variables do not have variance.
In other words the probability
that an absolute value,
the sample mean deviates
from it's expected value,
by as small an amount as epsilon for
any fixed positive epsilon,
will go to zero when n
becomes large enough.
So here is a powerful generalization
of the law of large numbers.
Now, one of the casualties in our analysis
is going to be, sadly, the fact that
the rates at which we can compute these
probabilities now start to blow up.
But we got simple estimates for
sample size in terms of desired error and
desired confidence, but now if
the variables don't have variance, we
must anticipate that we're going to need
to put in a much larger amount of effort.
You're going to need a much
larger sample before
we get any kind of concentration near
where the underlying expectation is.
And so it is.
So here's one direction for
generalization.
I want to explore this further, but I do
want to expose you to the basic kind here.
And you will forgive me, I hope,
if I don't try to prove this result.
Because now we understand that the moment
we start to try to tweak a basic result
and try to get more and
more nuanced results out of it that there
is going to be a corresponding increase
in the technical dexterity we will need,
more mathematical background
that we will need.
Now you will have seen that in
the title of the law of large
numbers I've put in the word weak.
Okay, now the word weak then suggests
that there's a corresponding strong
to go with it.
Right, so the original inequality of
Chebyshev handled what we today call
the weak law of large numbers.
To see how we could strengthen
the result in another direction,
let's take a slightly
different vantage point.
So let's come back to
the context of tossing a coin.
Imagine that we have a coin and
we toss it repeatedly.
One, two, three, and so on.
Ad infinitum.
Now, I don't defy as always,
heads with a one, tails with a zero,
and now we form the accumulated sum,
the partial sums of the sequence.
Now imagine, this is a Gedanken
experiment, a thought experiment.
Imagine this coin has been
tossed infinitely often.
Well, we start by forming the run,
running sum.
After one trial, you just have
the first result divided by 1.
That's a sample mean after one trial.
After two trials,
you've got a new sample mean.
The sum of the first two
variables divided by 2.
And if you keep doing this after n trials,
you have the sample mean Sn divided by n.
And we keep doing this for
every value of n.
What we've got now is
a sequence of sample means.
Now, how does this sequence
of sample means behave?
What can we say about it?
Well so to begin, what we might want
to do is see if we can plot it and
see what might happen.
So I'm going to show for
you now a portion of a sample path between
about 1,500 trials and 4,500 trials.
Now the law of large limits tells
us that since the coin is fair
Sn over n should behave like
one half with high probability.
And indeed it does.
The base line for us with drawn
corresponds to Sn over n being 0.5.
And so
now we're measuring things above 0.5.
In the particular tosses I generated,
we find that the variance is near 0.5.
But they fluctuate, naturally,
because of course this is a,
a chance-driven experiment.
We notice that around
1,500 tosses the error,
the difference from 0.5 is within 10%,
is between 0.5 and 0.51.
That looks very good.
You have an area within
10% of the true amsal.
But, an unfortunate thing happens
as the number of trials increases.
Around about 2,000 trials
the error floats briefly
about 10%, and
then it drops below 10% again, but
then, at about 2,500 trials or so,
it jumps up, and now it's above 10%.
Eventually, it comes back down below 10%,
but then goes back above 10% again.
Okay, this, you're true you're
not very far from point five, but
you seem to repeatedly coming closer and
moving farther away.
So, here is a question.
Wouldn't the chances that,
if you look at one particular sequence.
In other words, somebody's collecting
data and repeatedly tosses a coin and
sees what happens to his sample means.
What are the chances that the sample
mean will repeatedly move away by
more than 10% from 0.5?
Of course 10% is just
an arbitrary number we picked.
Pick a tolerance epsilon.
This is your favorite small number.
In my classes at Penn, I'll ask my
students, what of the small number?
Then most of them say, I don't know.
I'll tell you.
My favorite small number is
10 to the power minus 23.
But, pick your own.
Make it fixed but arbitrary.
The key is you want to make it as
small as you want, but not zero.
Pick your epsilon.
What are the chances that a given trail,
a given sample part,
deviates from point five by epsilon or
more?
Repeatedly, again, and again, and
again, and again, infinitely often.
So let's take stock again.
I'm still dealing now with a sequence
of repeated independent trials,
where the variables have an expectation
but not necessarily a finite variance.
We found the sample means and now,
we track the sequence of sample means.
We know that for a large enough
number of trials, on average
the typical sample mean is going to be
close to the underlying expectation mu.
This is the law of large
numbers of Khinchin.
But, is it the case that
as you follow one trail
that yes on average you're good but,
that you periodically become not good.
You become bad.
You move by more than
epsilon away from the center.
Four short years after Khinchin proved his
result, Kolmogorov attacked his quest.
So, the question here asked was this.
Is it possible that if you look
at one trail, one sample path,
do the events of the sample mean deviate
from the center by more than epsilon,
can happen infinitely often.
Kai Lai Chung, the probabilist, has come
up with an evocative short notation for
infinitely often.
Call it IO.
And so the question is,
is it possible for this probability of
an infinite number of deviations from ep,
but more than epsilon to occur.
And Kolmogorov showed that indeed
it was going to not be possible.
The probability that on
essentially all sample parts,
that you deviate from mu by more than
epsilon infinitely often is zero.
Another way of saying this is that almost
all sample parts will have the property
that you deviate from mu by more than
epsilon no more than
a finite number of times.
Or, yet again, this says that for
almost all sequences
we have the property that the sequence
will sooner or later get close to mu.
And once it gets close to mu,
it never leaves.
It always stays close to mu.
This is a powerful generalization of
the weak law of of large numbers.
And naturally enough, we call this
the strong law of large numbers.
This is a weak law on steroids.
Its got muscles.
Okay.
From a practical point of view,
the strong law is what tells
us why pools really work.
Imagine that one is sampling from
a population to estimate something.
For example,
which candidate is winning an election.
And the data being reported
progressively in time.
So after one day we've got so
much data and that is reported.
But in another six hours even
more data has been collected so
the sample mean is adjusted, increased,
but now n has become larger.
It will be very disquieting if we get
an estimate which we think is good,
but as we get more data,
we find the estimate changes.
It becomes bad.
At that point,
then we start worrying about
well how do I know the poll is
selected at a good point in time.
That will be very problematic.
What the strong law of large numbers
of Kolmogorov tells us is that,
we don't have to worry about it.
Because polls and in general sample means
will eventually get close to the
underlying true but unknown expectation.
And stay there.
Now these laws of large numbers are a
fundament in the theory of probability
and, as we've seen, they inform beautiful
glorious applications all around us.
At their heart, you know, if we take
away all the technical sophistication,
what they're saying is
something very intuitive.
An average will give us
answers close to the true probabilities,
as long as you average enough.
Very intuitive.
Very reasonable.
And in fact, if we go all the way
back to our first lecture this was at
the heart of the frequentist idea of
probability due to Robert Phronesis.
We had to discard that because
it fluctuated too much, but
this validates that
frequentist institution and
that is at its heart what
the law of large numbers does.
In the ensuing 80 years or, or so
after Kolmogorov, the law of large numbers
in both variants, weak and strong,
have been expanded in a myriad of ways.
And very rich, very deep, very nuanced,
very sophisticated results have emerged,
which show that there is concentration
in all kinds of random circumstances.
Of course,
to dig that deep our technical armor
is going to have to need to
be correspondingly larger.
So I will leave you with a story.
The law of large numbers is
a fundament in probability,
and it informs for us,
among other things, why polls work.
Why drug testing works, and
it also tells us how we can go about doing
apparently intractable computations.
We have more to come.

