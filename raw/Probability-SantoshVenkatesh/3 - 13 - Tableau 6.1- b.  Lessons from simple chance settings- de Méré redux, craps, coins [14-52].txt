Thus far, all the examples we
have looked at deal with chance
in what we may call discrete spaces,
or discrete settings.
The word discrete that
we use here stands for
finite or countably infinite.
If you cast your, your eyes back,
metaphorically speaking,
to the particular problems we've
looked at, you'll recognize that every
one of the problems has outcomes,
which one could enumerate.
The outcomes were either finite in number,
or could be enumerated sequentially
in principle,
were there world enough and time.
For example, by counting them out a la
the natural numbers or the integers.
Now, this is the setting that
we have looked at there, so far,
and therefore this provides
a useful test setting
in which we can test out our assumptions,
our axioms, and
see whether we can construct probability
measures in consonance with the axioms.
So, let us begin.
The first example we looked at dated back
to the awakening in 17th century Europe,
in the glittering salons running
from Paris through Vienna to Prague.
And, we posited as our first problem,
the problem posed by
Chevalier de Mere and here is one
of the two problems that he posed.
Suppose a die is thrown four times.
What are the chances that no ace is seen?
Now let's pause and
summarize our discoveries
in the light of the nomenclature,
the terminology that we built up.
First, what are the idealized
outcomes of this chart's experiment?
What is the sample space?
Now, it is natural in
this setting to identify
the outcomes of the die
throws one at a time.
The first throw, the second throw,
the third throw, the fourth throw.
In this sense, as far as
the collected outcomes are concerned,
the experiment is indistinguishable
from throwing four distinct dice,
let's say four different colored dice,
at the same time.
Time is not of the essence at this point.
Now, one does this.
Then we simply label the outcomes, outcome
by outcome for each of the four dice.
And we then end up as
an idealized outcome for
the overall experiment,
a fourtuple of integers.
Naturally we'll give integers names,
say k1 for the first die, k2 for
the second, k3 for the third,
and k4 for the fourth.
And this ordered fourtuple of integers,
where each integer runs between
one through six, form all possible
conceivable outcomes of this experiment.
Now, the natural description here is
in terms of fourtuples of integers.
But in principle, there are a finite
number of possible outcomes.
In fact,
exactly 6 to the power of 4 outcomes,
1,296 such fourtuples.
And we could, in principle, have just
labeled them one, two, three, four,
five, all the way up to 1,296.
Of course then, this would rely upon
us having a dictionary of translation,
where for each fourtuple,
we can assign a number.
A rose by any other name would
smell as sweet in this context.
It makes little matter what we call
an outcome, as long as we can identify it.
Now, having got this
collection of outcomes,
what are the events of interest here?
Well, in this case, the event is the
event, let us say A, that no ace is seen.
Identify the word ace in this context,
with throwing a six on, in a die.
Now if no ace is seen, then none of your
four die throws could result in an ace.
And therefore, A, corresponds to
the particular subset of fourtuples,
where none of the elements,
k1, k2, k3, or k4, can be 6.
In other words k1, k2, k3, and k4 have to
represent numbers from one through five.
There are exactly five
to the power of four,
or 625 different idealized outcomes
in this particular subset.
Finally, we come to the idea
of a measure of chance.
The probability measure
here builds in two stages.
First, we identify this experiment as
one of a generic class of ER models.
Where every idealized outcome or
sample point is equally likely.
What does it mean?
Well, there are 6 to the power 4 possible
sample points in the sample space.
1,296 sample points.
And therefore, to that event
consisting of a singleton sample point.
Now what does that mean?
It is a set consisting
of one outcome alone.
Let's say the outcome we pick consists
of the integers k1, k2, k3, and k4, k4.
Notice the nomenclature, the terminology.
We put curly brackets around the single
outcome to distinguish it from an outcome.
This is a set consisting of one outcome.
This kind of distinction seems
like just shuffling words around,
but it is important.
You see, probability measure can assign
values to sets, not to sample points.
And therefore, we identify the singleton
sample point consisting of one element.
And to each such singleton sample point,
we allocate a value 1 over 1296.
This is how we begin.
To borrow the language from physics,
we will think of singleton sample
points as atoms of this experiment.
Now, we've allocated equal
mass in this terminology,
in this language, to the atoms.
Now, we build up on this
by looking at the event A,
which consists of 625 different atoms.
And, allocating to A the sum of the atomic
probabilities for each of the atoms.
Of course, in this case it is very,
very, very simple.
Because, each of the atoms
has got the same probability,
1 over 6 to the power of 4, 1 over 1,296.
There are exactly 625
different elements in A,
and therefore the probability
of A is the cardinality of A,
625 divided by the cardinality of omega,
1,296.
Now, this is a setting of urn models
where all outcomes are equally likely.
How do we proceed if we
move on to a setting where
the outcomes are not equally likely?
Well we can set the first
throw in the game of craps.
Recall in craps,
we repeatedly throw a pair of dice and
in the first throw, we've thrown two dice,
and we simply sum the face values.
The question, what is the chance of
losing on that first throw in this game?
What is the sample space?
Well, as long we're considering
just the first throw,
we're summing the face
values of two dice and
therefore the numbers, that we were
to obtain are between 2 and 12.
The sample space consists
of exactly 11 points,
the integers 2 through 12, excellent,
what is the event of interest here?
Well the rules of craps say that
you lose on the first throw,
if you happen to throw a 2 or 3 or a 12.
This, then is the event A of interest.
It consists of that particular subset of
these three sample points 2, 3, and 12.
How does one assign a probability
of measure to this experiment?
Again, we begin one atom at a time.
Elementary considerations lead us
to assign to the atom consisting of
the set with just the element 2 in it.
The probability 1 in 36.
And likewise, for
the atom consisting of the element 12.
For 3 and 11,
we assign a chance of 2 in 36 for 4.
And 10, we assign a chance of 3 in 36,
and so forth.
Now we now have a system
of atomic probabilities.
How do we compute the probability of A?
Well, we identify that A is
comprised of three atoms, 2, 3 and
12, and we simply sum the atomic
probabilities to get the probability of A.
Now, how does one go beyond this?
Well, the next experiment we can suggest
was when we have not just a finite
number of outcomes, but
a denumerably infinite number of outcomes.
And, so
we returned to the tossing of a coin.
The experiment in context
was the following.
Toss a coin repeatedly until the same
face shows twice, then stop.
What are the chances that
in this experiment one
has to toss the coin four or more times?
Well, let us begin with a sample space.
Now, of course, this is an infinite
sample space, so we can't
enumerate all of them, but
we can deduce what the pattern is.
For example, a sample point could
consist of two heads, or two tails.
Or, as we've seen, a tail, and
two heads, or a head, and two tails.
Or a head, a tail, and two heads.
Or a tail, and a head,
and a two, and two tails.
And so on, and
now the pattern becomes clear.
A sample point has got
alternating heads and
tails, and
terminates with two of the same face.
Now before we go on, this looks like
a messy description of the sample space,
but we realize we could
actually enumerate it.
And in this case,
one possible enumeration is to identify,
to rename the sample point consisting
of two heads and call it just one now.
Rename the sample point consisting
of two tails, call it minus one.
A tail and head and a head, call it two,
minus two, three, minus three and
so forth.
And it becomes clear that if one takes
away the particular structured language,
the color of the coin tossing experiment.
That this is an instance where
the sample space may be identified
with a set of integers.
Of course, there's an assumption here that
one establishes a dictionary
of translation ahead of time.
So that if I say, the integer three,
then it is clear to all parties involved
that three stands for the outcome,
a head, a tail, followed by two heads.
But once the dictionary of
translation is established,
then the sample space is just
the collection of integers.
Now, in this space,
what is the event of interest?
The coin is tossed four or more times.
Well, let's write this event in terms
of our identification of the sample
space as a set of integers.
Well, looking at a table,
it's clear that four or
more tosses correspond precisely
to the integers, three or
minus three, four or minus four,
five or minus five, ad infinitum.
And therefore,
we can in a very compact sense,
write down the event A of interest,
that four or more tosses are required.
As a collection of integers K,
which an absolute value, is at least 3.
This is nice and simple.
At least, it's a compact description
of what was a rather verbose,
a colorful coin tossing
description of this problem.
But of course, this presumes that there is
an underlying dictionary of translation.
Finally, how does one allocate
a probability measure at this setting?
Now, this is clearly not
an urn model any more.
This is a more complex model.
But, we've argued from first principles,
that it is natural to
allocate to each atom,
probabilities which decay
exponentially in powers of 2.
Thus, the sample point 1 forming the atom
consisting of just 1 as a sample point,
has probability 1 in 4, as does the atom
consisting of,
just the outcome, two tails.
Two heads, two tails,
half labels k being 1 or minus 1.
And therefore, we can identify
with these particular atoms a,
an exponentially decreasing probability.
How one does this, right?
We have a systematic assign,
assignment of probabilities.
One-quarter and one-quarter, one-eighth
and one-eighth, one-sixteenth and
one-sixteenth, and so forth.
How does one evaluate a probability?
Well, A consists of all integers 3 or
larger.
To A, we assign the probability, which
is the sum of the atomic probabilities,
sum of these reciprocal powers of 2.
And when we do this we obtain,
as we saw here, the probability 1 in 4.
We should now use this to
come up with an idea of
how one builds chance models
in discreet settings.
This is next.

