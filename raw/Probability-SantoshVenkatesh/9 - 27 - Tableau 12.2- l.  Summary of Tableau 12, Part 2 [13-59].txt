Summary of Tableau 12, a potpourri of
titillating applications, Part two.
Rare events and
probability sieves: pondering existence,
[FOREIGN], and the coupon collector.
We begin this [INAUDIBLE] by going back
to a basic, apparently tried prob-,
probability, a probability measure
that I want to [INAUDIBLE].
And discovered that a new
perspective engenders subtlety and
nuance which was completely unsuspected.
Modern tenacity gave us
Boole's inequality and
out of Boole's inequality comes
this elegant idea of a sieve.
If I have a collection of events, A1
through An, which I'm going to think of as
bad events, if the sum of the event
probabilities is strictly less than 1.
Than can conclude that there
exists occurrences of good events.
We can deduce existence from a very,
very, very simple test.
We saw how in the context of a two
coloring of a ball it already gave us
non trivial information.
But admittedly, this is a crude sieve.
We discovered a much more delicate,
nuanced
sieve by turfing through
an inclusion-exclusion paradigm.
Now, the crux of this paradigm depends
upon these inclusion-exclusion sums.
And admittedly, they look formidable.
But when you stare at them long enough,
it becomes clear that they are actually
are saying something very reasonable and
intuitive.
And algebraically,
perhaps not really that complex.
Each of these inclusions,
inclusion sums are looking at
probabilities of intersections.
And this immediately suggests to
a natural province where this kind of,
of methodology is likely to be profitable.
Intersections probabilities
are easy to computer
when we have independence but perhaps
also when we have near independence.
And so can we find independence
running right through this.
So, if we have events for
which conjunction probabilities
are not difficult to compute, then we
can put together an inclusion-exclusion
frame work and of course at this level,
we've got an exact answer.
but, it's not very intuitive, is it?
It's an ugly, alternating sum.
It's not clear what it says to us.
Things become completely transparent when
there is actually no interdependence.
When the inclusion-exclusion sums behave
like the ratio of a power to a factoid.
Now where do the power come in, and
that suggests where the independence is
baked into the cake, or
the near independence.
If you have things which are vaguely,
nearly independent, then
the inclusion-exclusion sums are going
to behave like powers of the vectorials.
And the moment that happens,
we find the Poisson distribution emerging,
beautifully.
Out of the framework.
So now we say that this apparently
accidental discovery of Poisson
in 1837 was not so
much of an accident after all.
It arises out of these independent or
near independent trials.
And so now we have a brand new
perspective of where the Poisson emerges.
We promptly put this to good use.
We saw how we could use this
Poisson's sieve formulation
to look at the Poisson
approximation to the binomial.
We saw how Poisson's sieve that in
the problem in the wrong conch,
the matching problem of the model.
And we saw how this formulation worked
in the coupon collector's problem.
You would say well, these all look
like fairly modest applications,
with the Binomial perhaps excepted.
Do we have any read solid weighty
applications out of all of this?
Indeed we do.
Ok, so let me just sketch out
domains of where you see.
The Poisson sieve working beautifully,
gloriously.
In one setting,
a start of models of brains function and
among other things we could ask well look,
I remember individuals by how
their facial features look.
If I see a photograph of somebody
after some years, I might well,
feel oh wait there there's a tug in
my memory, I think I recognize them.
But, maybe there are some
errors in recall.
If we lucky we don't make too many errors.
At a high level, you think about
a functioning organism like this, or
at least a model of a functioning
organism like a brain.
As a computational device,
which is producing results,
where there are a few rare errors.
And the moment we have this we say
oh maybe there's a poisson paradigm
which is embedded in here.
And indeed there is right.
In suitable mortals we can argue that the
number of errors is roughly governed by
poisson distribution.
And again,
it comes out of the person save.
Another setting, if we take some
images very similar to all of us,
mobile telephony,
satellite communications.
Of course, a bane of modern life is
that fact that our mobile devices
sometimes drop our calls rudely,
unanticipatedly, unexpectedly.
Of course, if too many calls are dropped,
then we get angry and we get furious.
And we switch our service provider.
We find another company.
So there's an incentive for
all the telecommunications companies to
provide a high level of performance, so
that people don't get distressed and
angry with them.
And if you're seeing this,
with good luck and
good management it could well be the case
that relatively few calls are dropped.
Dropping a call is a rare event.
What categorizes the number of
these outages, these drops?
Whether it's for about film.
Oh it is for
an Internet router dropping packets.
Or if it's for satellite communication,
communicating images from Mars,
where some bits are scrambled in an image.
Well, with luck,
these errors are all rare,
and therefore can we anticipate
a forsworn paradigm coming into play?
And indeed it does Perhaps.
Even more modern application.
In the modern day, technology has
progressed to the point where we can
create large numbers
of very small sensors,
sensing all kinds of things
that we're interested in.
Things like climate,
pressure, temperature.
Of Newtons, and we can create
large numbers of these sensors
at very low cost, and so
this gives us now a new way
of trying to actually monitor
things in our environment.
Well, let's say that we're out,
we're worrying about the,
the consequences of
pollutants in farm lands.
Imagine, with a crop duster airplane,
flying over a large field where
there there is irrigation.
And you're dropping sensors
all over the field at random.
Each sensor checks for certain pollutants,
certain chemical contaminants.
Now the,
these sensors can collect information, and
then they send information out.
And if any of the sensors or a group of
them say there are pollutants that we know
we have an issue and
then we can go ahead and deal with it.
Active management will not be needed.
And of course this creates new issues.
You see because if you're going
to create very tiny sensors.
If the sensor detects anything,
there's got to be a mechanism
by which it can get the information out to
the individuals monitoring the situation.
How does a sensor get the information out?
Well, it can't simply say,
pack up my bag and walk down,
out to the edge of field and say,
oh by the way there was a problem there.
A sensor can't do that.
How does it do it?
Well the sensor does it by broadcasting
the information using telemetry,
electromagnetry.
In other words, they got how to equip
the sensors with antennas and batteries.
But the sensor is very tiny.
It is impractical to put
a large clunky battery on it.
So the boundaries are limited in power,
which means that range over which a sense
I can communicate is also limited.
All right, if that is the case,
how do you get the information out?
Well, perhaps be collaborating.
A sensor, me, I, detects something wrong.
And I sort of rich it because
I've got a very small voice,
just broadcast in my local vicinity.
Guys, guys, I've got a problem,
I've got a problem, I've got a problem.
And these mutterings are seen
by neighboring sensors.
They pick it up and pass it on.
And, if you keep doing this eventually
with luck, and good management, and
good network protocols,
you might find the,
the information emerging from the field to
some monitoring devices on the periphery.
I'm giving you very stylized way
of thinking about this problem.
And now we can see we can
maybe get the information out,
but this now begs a mathematical question.
If by a sort of [INAUDIBLE] are to
eventually be taken out of the field,
then somebody has to be able to hear me.
There's got to be a sensor, a person or
individual, an antenna
battery in my near vicinity.
If I throw in too few sensors,
then I'm going to lose that.
So, this says,
I need to analyze how many sensors
I need to throw out there, so
that, every sensor is connected to
a neighboring sensor, and that the entire
assemblage forms a connected
network of communicating entities.
This is a coverage problem.
In the classical theory
of advanced probability,
it's called a mosaic process, alright?
Imagine that each transfer can see in
the small region around it, alright?
It's something like somebody is painting
a room but the painter is very messy,
so he keeps dropping big
blobs of paint on the floor.
Hopefully none of the blobs is very big,
okay?
This is a small pilot antenna, right?
But if you threw enough blobs,
then the whole floor would get
covered by connected blobs.
Okay, here is a setting for
a mosaic process.
We are talking about when will it all be
connected so I can get information out?
The analytical question
here is going to be then.
What is the chance that
it will have senses,
that one sense is isolated if they have
a small weight as a communication.
This with luck is a right event and
if through enough senses then the chance
of somebody getting isolated is rare.
How many sensors are isolated.
How many blobs are stuck on their
own without connecting blobs?
Well it should be covered
by a Poisson paradigm.
We have a small number of errors and
they're rare.
And they are approximately
independent of each other.
Not quite.
There's a level of dependence,
but with luck it is weak.
And of course we can analyze this.
Well of course you understand that
analyzing settings like this is going to
require correspondingly more and
more formidable algebraic machinery, and
I will not do that here.
This was to give you a feel for
the vast scope of the applications.
Of these limit theorems
involving the Poisson
in a completely unexpected direction.
The central limit theorem
gave us guarantees for
typical evidence centrality.
The Poisson distribution tells
us things about rare events.
We should finish up our story with just
one big slogan to complete all of this and
here it is.
Rare events follow a Poisson law.
All good things must come to an end,
and so to this course.
We've covered a lot of
ground in eight weeks.
I hope I've been able to communicate
to you a theory which is rich,
which is luxurious, which is sumptuous.
It's built upon very
simple basic principles.
But these principles inform applications
which are exotic, which are beautiful,
which are potentials, which are rare.
And if I have given you that much,
I have given you something valuable, but
I shouldn't let you run away, simply
with a saying, well here is a formidable
mathematical apparatus and
some of it was very tedious and difficult.
Yeah, the applications are neat, but
boy I am studying how to apply
these was not easy at all.
It's beginning to feel like drudgery
when I put it in those terms.
Now, the discovery of beautiful
ideas like this is, at it's heart,
a very human endeavor, and
this endeavor carries a great
deal of satisfaction inside it.
At it's heart,
this endeavor is actually fun.
And so if you will indulge me in
a last concluding adieu good bye,
to you from me and all our chefs
who contributed to this enterprise.
I'll begin with a word
from James Clark Maxwell.
Exulting the value of
frivolity of fun in discovery.

