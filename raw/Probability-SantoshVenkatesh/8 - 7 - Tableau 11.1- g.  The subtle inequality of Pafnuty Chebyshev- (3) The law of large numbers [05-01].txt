So let's summarize the key element.
We are dealing with
a binomial distribution,
that is what the sample is going to
give us, when we accumulate successes.
We are interested in the probability,
of a bad event happening.
We're trying to characterize, what
fraction of samples are bad on average.
So we're asking, what's the probability,
the relative frequency of successes
differs from p by epsilon or more?
And Chebyshev tells us that for
any choice of p, and
any choice of epsilon,
this bad probability is bounded by
the reciprocal of 4n epsilon squared.
We could hardly ask for
a simpler, cleaner,
resolution of what appeared to be
a completely intractable problem.
And within this already,
we have the, at the heart of this,
already a resolution of our basic problem,
that of quantifying the efficacy of polls.
But before we do that,
before we jump ahead,
already the bound on the right
hand side is going to be useful.
Clearly you see a bound would not
be useful, if I had a bound for
a probability which is 1.
How is that helpful?
Every probability is less than 1.
Such a bound, an upper bound will
only be useful if it is small,
because then that tells us
the probability is forced to be small,
it's sandwiched between 0, the little
bound and whatever the upper bound is.
And if the upper bound is small,
then the probability is forced to be tiny.
Okay, so this is the key.
But if epsilon is small, then we see you
have epsilon squared in the denominator.
Right?
So if you want a smaller tolerance,
well 1 over epsilon squared
is going to be a big number,
no matter,
however small epsilon is, fix it.
That is your desired error tolerance,
say 1%.
Say half of 1%, it doesn't matter,
fix your epsilon.
The moment you fix it,
now choose a sample size big enough,
as n becomes bigger and bigger and
bigger, eventually, the quantity on
the right, becomes as small as you desire.
This is a qualification of what is
now called the Law of Large Numbers.
This is the fundamental, original,
limit law of probability.
The law which puts our intuition
on a firm theoretical basis.
This is why polls work.
The probability that the relative
frequency of success is sn over n
deviates from its expected value p,
which is fixed but unknown.
But even a tiny,
infinitesimal amount tends
to 0, as the sample size gets larger and
larger and larger.
Now, as I told you,
this is going to actually give
us a quantified prescription.
So Chebyshev's inequality does a little
bit more, it flushes out the rate,
at which these probabilities become small,
and that's going to give us a number.
But before I jump there,
I should tell you a little bit about
the value of such a bounding process.
You see the value of Chebyshev's
inequality is not that it is precise,
no it is not.
Okay.
You can get much better total results,
which are closer to
the desired probability.
The value lies in its extreme simplicity.
Few results in mathematics or
in the human experience have stayed so
relevant for so long.
This inequality has legs,
it has endured, and
the primary reason for its, its lasting
value is its essential simplicity.
Only experience will convince
the listener, that simple,
elegant result is like this,
where in one swoop,
one glance, we can capture
the essential features of the problem.
Such results are worth much,
much more than ponderous,
condition bound mathematical results
which take pages to describe.
This result is perhaps, the prime
example of the verity of simplicity.
The key idea of Chebyshev was in his
recognition of the importance of a square,
and out of that, almost magically comes
this beautiful and elegant result.
So our next step then,is to say, what does
this portend for the efficacy of a poem?
And, we will turn to this next.

