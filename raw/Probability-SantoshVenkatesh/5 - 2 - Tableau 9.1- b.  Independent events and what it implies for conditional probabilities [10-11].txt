What have we learned
from our simple examples?
That, a multiplication
table is at the heart
of the simple decomposition
of probabilities.
In other words,
there appears to be in certain
settings a natural rule of products.
Let us promptly generalize and
abstract this into a definition.
We will say, as a matter of definition,
that two events, A and B,
in a probability space are independent.
If the probably of A intersection B
is equal to the probability of
A times the probability of B.
You should bear in mind
that in definitions,
the definitions are always two ways,
right?
But we'll usually dispense with the AND,
ONLY,
IF portion of the definition,
it is implied.
It's part of a definition.
Here is now a mathematical
definition of independence.
We should keep in mind that this,
in principle,
abstracts out what we commonly understand
by independence in ordinary language.
But it says more.
It says things very precisely,
very concretely.
Whenever we talk about independence,
probabilistic independence,
though we usually drop the word
probabilistic in our setting.
Independence is always in a mathematic or
probabilistic context.
When we say independence, we mean
precisely the object on your screen.
Nothing more, nothing less.
Intuition or feelings for
independence in ordinary language
can all affect,
help understand the problem.
But ultimately, when push comes to shove,
that mathematical relation
is what we mean and
we understand by independence.
Of course, this relation did
not come out of a vacuum.
It arose out of simple
combinatorial settings.
Out of simple multiplication
tables of possibilities.
And we understand that in various
settings, this kind of relation emerges.
What we now do, in such a definition,
is take that observation and
expand it widely, greatly,
vastly to a general abstract setting,
where intuition may be albeit.
We'll see examples shortly, but for
the time being, let's go ahead and
say we have a very simple definition.
We hard-wire this into our brain.
This is what we mean by independence.
Let us explore what its consequences are.
To begin,
that definition looks vaguely familiar.
Why?
Well, it talks about
the probability of an intersection.
Where have I seen that before?
And a little thought says oh,
well we've seen that
in our definition of conditional
probabilities, here is a chaining rule.
We can write the probability of
the intersection of two events.
In terms of a chain involving
a conditional probability of
one event with the respect to the other.
Now, this is generically true, as long as,
of course, the event to be has got
positive probability and
the objects on the right are defined.
But now, we have another relationship for
an intersection probability.
But this requires more
structure in the events.
What if the events, A and
B, are independent?
Well, that says
the intersection probability
is given by a product of probabilities.
Now, by examining the right-hand side,
we can immediately deduce a relationship
of conditional probability.
But before we do that, let's pause
to observe that our chain rule of
conditional probability can be written
in the other direction just as well,
by conditioning on the event A.
And now, examining the objects on
the right-hand side, we can simply deduce
that if A and B are independent
events of positive probability,.
Then the conditional
probability of A given B
is just the probability of A itself.
And the conditional probability of B given
A is just the probability of B itself.
In other words, we are in a political
age and so we should want a slogan, so
here's our slogan.
Mathematical independence,
probabilistic independence,
captures a very intuitive idea.
That if two events, A and B,
are independent, then the occurrence
of one does not affect
the chance of the other.
This is very promising.
This is exactly what we'd
hoped that a definition of
independence would try to capture.
But of course,
now we have a formal mathematical
structure which captures this.
So again, independence means that a joint
probability is a product of probabilities.
Independence is a rule of products.
Before we move on to
applications of more heft.
We should examine the definition
a little bit more and
see if certain other attributes emerge.
So let's start with
a very simple question.
I've given you two events, A and B.
The moment I give you the events A and
B, other events are implicitly part
of the structure, for example,
the complements of these events.
If I tell you that A and
B are independent, well,
that means you have the rule of products.
Does it say anything about A and
B complement?
Now, instinctively, I want to say, that
if A and B are independent, then A and
B complement are independent, natural.
But of course, this instinct,
based upon our experiences.
The question is,
does our formal mathematical
definition capture that intuition?
And for this,
we can't just appeal to intuition,
we have to go back to first principles.
What is a structure we looked at?
Does a rule of products
emerge from that structure?
If it does,
then we can conclude independence.
If it does not, then sadly,
we will not be able to.
So, let's go back and
write down what we need.
We want to ask something
about the probability
of the intersection of these two
new events, A and B complement.
And I've given you an identity.
Do you recognize where it comes from?
Naturally, they're using additivity.
Think of an event A.
The events B and B complement partition
space, and therefore the chance of A can
be decomposed into the chance of A,
which is a part of B.
And the chance of that portion of
A which is a part of B complement.
This is additivity.
Move one of the probabilities
to the right, and
you've got the additivity in front of you.
This is very nice,
because on the right, we have
a term which represents a probability
of a conjunction of an intersection.
But if we know events A and B, we know
that that can be decomposed as a product.
This is what independence gives us.
Oh, this is lovely.
The moment that happens,
I've got on the right, various terms.
And naturally,
I factor out common factors, in this case,
the probability of A.
And then in round brackets in the right,
I get one minus the probability of B.
That's familiar.
I recognize that via additivity again,
as the probability of B complement.
However, B and B compliment together,
instead of the whole space.
Normalization tells you that the entire
space has got probability one, and
therefore we're caught by additivity,
an identity.
On the left, the probability of
the conjunction of A and B complement.
On the right, the product of
the corresponding probabilities.
By the definition of independence,
we conclude that A and
B complement are indeed independent.
But then what about other combinations?
What about A-complement and B-complement?
What about A-complement and B?
Let's pause before we do
any more analysis, okay?
What has our analysis told us?
If you start out two events, A and
B, then, and they're independent.
If you complement any one of them,
then the two are again independent.
So if you complement B, then A and
B complement independent.
Of course, we can run exactly the same
analysis if you complemented A,
which means A complemented
B are independent.
But if A and B complement are independent,
then if you complement one of them,
it didn't get independence.
So if you complement A, then A complement
and B complement are independent.
What we just discovered
is that one identity,
that one rule of products
on your upper left.
Right.
That implies all the other
rules of products for
various con, complement conjunctions.
And therefore,
any one of these independence relations
implies the truth of all the others.
Oh, this is very nice.
Elementary, I know.
But still, very nice.
This gives us confidence that
the mathematical structure is indeed
capturing the physical kind of reality
that I vaguely refer to as independent.
Here's the mathematical structure.
Now, we want to build upon this and
see what it says in applications
where independence is not nearly quite so
clear.
That is our next step.

