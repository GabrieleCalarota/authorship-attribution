Next application has a little more heft
to it, and the students will readily
be able to see applications to problems
of, of weight important to the real world.
The testing of new drugs, sera,
vaccines, is big business,
multi-billion dollar business.
And sadly, it's necessitated in part
because of the discovery of new pathogens,
which spread virulently and violently.
Here's a make believe toy example to try
to get the key ideas behind the setting.
Imagine that one has a virulently
spreading animal disease,
affecting fully one quarter
of the population, 25%.
And naturally, we are worried.
And so, there's a scramble to develop a
medication, a vaccine, a serum, a remedy.
Let's say that three particular sera,
creatively labeled A, B,
and C, are tested for
efficacy against this disease.
And, let us say that
the following data are reported.
So, serum A was tested on,
let's say, 10 cows.
10 were inoculated and
it was discovered on observation
that none of those cattle fell ill.
Serum B was tested on 17 cows, and after
a period of observation, 1 cow fell sick.
And serum C was tested on 23 cows,
2 fell sick and died.
Now, of course these numbers are much
too small for applications in practice.
We can't really get much confidence
with samples of this size.
But again, we are trying to settle
our ideas, settle concepts.
And once we have these
concepts fairly in place,
we'll be able to extend
them to realistic examples.
Our question is this,
is there any evidence that
any of these three sera are efficacious?
Of course, now,
they've given real data, and
we are in search of
a statistical principle.
Now students might feel that
this is vaguely familiar.
And of course, you'd be right.
This is very much like a setting we had
in the discussion of the hot hand
phenomenon in sports psychology.
And a very similar kind of test can
be constructed in this setting.
Here is the basis of a statistical test,
the birth of a principle.
Suppose we compare what happens
to an unvaccinated control
group of a size similar to that tested and
ask,
do we get statistically similar outcomes?
Or do we get different outcomes?
So, to crystallize matters,
suppose we are construing serum C.
The thought experiment,
the gedanken experiment,
that is being put forth is the following.
Consider 23 cows from the large
unvaccinated group of cows.
Observe them for a period of time.
If at the end of the observation period,
two or
fewer cows get sick then we have results
for the unvaccinated control group.
Which are similar or better than those
that were advertised by serum C.
If more than two cows get sick,
then the unvaccinated control
group has worse outcomes.
Our question then is, what are the chances
that the outcomes for the unvaccinated
control group in this thought experiment,
this gedanken experiment?
What are the chances that the outcomes are
as good as or better than those reported?
We now have the basis for
a principle test.
Let's put some nomenclatures and
terminology, some notation around this.
So, what is the model?
It's admittedly macabre.
The idea is that each cow in
an unvaccinated control group
falls sick or not,
falls sick with a chance of one in,
in four, a 25% chance.
Or avoids getting sick with
a chance of three in four.
In other words, a cow falling sick or
not in the general population
corresponds to the toss of
a fictional coin whose
success probability is 0.25.
Of course, this is a rather macabre and,
and cruel use of the word success,
because success here means
the cow falls sick and dies.
Now, suppose we consider
a control group of size n,
because in our three sera,
we have different sizes of test groups.
Say let's try to do this in general for
a group of size n.
What we're dealing with
is a sequence of n,
Bernoulli trials with
success probability 0.25.
We're looking at n cows and asking,
how many of those cows fall sick?
In other words, we are looking at
an accumulated number of successes,
illnesses in that group of n cows.
And of course now we know that S sub n,
the accumulated number of successes is,
is governed by a binomial
distribution with parameters and
the number of trials or
the number of cows in the group.
And success probability of p of 0.25,
macabre to be sure, connotes falling sick.
Okay, now we want to compare
the setting to what has been reported.
So, we're going to ask, what are the
chances that in this control group,
we got results as good as or
better than those advertised?
Let's introduce a little bit of notation.
Let upper case B of n and
k represent the probability that
in your control group of size n,
unvaccinated cows,
that k or fewer fell sick.
That the accumulated number of
successes is no more than k.
Of course by additivity, this is just the
sum of the binomial atomic probabilities.
If we add them all up, we get a B n of k.
Now, how does this work for
the three sera at hand?
For serum A, we are looking at a control
group, in our thought experiment,
of 10 unvaccinated cows.
We want none of them to fall sick.
For serum B,
we are looking at a control group,
in our metaphorical experiment,
of 17 unvaccinated cows.
And we are looking for
1 or fewer to 4 sick.
And for serum C,
we're looking at a group of 23 cows, and
we are asking what are the chances
that 2 or fewer fell sick.
Now this is a simple binomial calculation,
we just plug it into the equations.
And here come these answers.
Numerically, the chance of getting
results as good as that of serum
A in an unvaccinated control group of
size 10 is just over 5%, it's about 5.6%.
The chance of getting results comparable
to or better than that of serum B.
In an unvaccinated control group of
size 17, is just about 5%, 0.05.
And the chance of getting results as good
as advertised for serum C is about 0.049.
Now the high level.
What does students should
take away from this is that
in each of these three examples,
the chance of replicating or
bettering what is reported is about 5%,
it's not very high.
What does this tell us?
Now if we think back to how we
set up the whole time phenomenon.
When you look at illnesses in a control
group, we're asking, are these,
is there evidence that the control
group behaves statistically similar
to my vaccinated test group, or
is the statistical behavior different?
What our data tells us is that
in each of the three cases,
there's only a 5% chance that
the unvaccinated control group of
the right size gives you results as
good as better than that advertised.
The odds are not very high.
And that suggests that each of
the sera actually is doing something.
The smaller the chance of replicating
what would seen in a control group,
the more the evidence for
the functioning of the serum.
And this gives us a slogan.
The more unlikely it is for
claimed positive results for
a serum to be replicated in
a unvaccinated control group,
the greater the evidence for
the belief in the efficacy of the serum.
So on this basis, we believe that
all three sera are reasonable.
That they all appear to give us
evidence that they are functioning.
But our slogan gives us a bit more.
The smaller the chance
that we can replicate
what is observed in an unvaccinated
control group, the smaller the chance,
the more the evidence in
favor of the serum working.
Based on this argument, we would
say that serum C, which has a 0.049
chance of getting its results
replicated in the general population,
is superior to that,
there's more evidence in its favor.
Then serum A, which has got a 0.056 chance
of having its results replicated
in the general population.
Of course, these numbers are so
small that they don't
practically make any difference.
But the principle is telling us something
about how to implement strategy.
A student might look at this and say,
oh, that's all well and good, but this,
this is really a toy example.
You would not surely make decisions on
health care based upon testing 10 or
17 or 23 cars or people, assuredly not.
But of course now,
we're going to have to scale
this up to a suitable where we
have reliability in the sample.
Where we have statistical guarantees
that a sample does something well.
Here are a couple of questions.
One, how big should the sample
be before we get an adequate
guarantee that we don't get
a statistical fluke in our testing?
Of course this is really
a million dollar question.
We really do understand something
about the size of the sample and
how it's going to play into the game.
A second objection the student
might have is that the analysis
assumed that one in four cows were going
to get sick due to this new disease.
That 25% of the agents we're
going to get infected.
How does one know that?
Now in a large population,
it's not at all clear if we have
that kind of data available to us.
In fact, in general, we don't.
How does one get around this?
Well the answer to the second
question is by trying to infer
at least implicitly what p is,
by doing more statistical tests.
And the way this is done, for
example in testing of a new drug,
is to compare individuals were
given a drug with another group,
a controlled group of individuals who are
not given the drug but to make sure that
there is no bias no subtle
psychological factor coming into play.
You give both groups what
look like the same pills,
except that some of
the pills are the drug.
And some of the pills are sugar pills.
They're placebos.
And by doing such a testing, one can,
in principle, build upon this framework.
We will come back to this.

