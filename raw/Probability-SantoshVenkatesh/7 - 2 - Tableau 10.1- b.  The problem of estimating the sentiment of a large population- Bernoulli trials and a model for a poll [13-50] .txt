To proceed, we begin by abstracting
the underlying problem.
So imagine we have a large population.
And we abstract it by drawing let's say,
a box of orange urn, and
the elements of the population, people,
creatures, widgets, we abstract
away as balls of one of two colors.
So now we have a dichotomous
population in this case,
an urn with orange balls and blue balls.
The fundamental question is the following.
What fraction, say p,
of the balls in this urn are orange?
The moment I understand this,
I understand how the sub-populations are
distributed in the underlying population.
Now, how would one go
about estimating this?
Well, naturally enough, one way to do
this is to simply make an exhaustive
count of every single
ball into the population.
Count everybody in
the general population and
examine who is of type orange and
who is of type blue.
If one did this for the figure at hand
you'd find there are 360 balls in total,
of which 101 are orange.
But of course this gets rapidly weary, and
in a large population this becomes
unworkable on a routine basis, right?
For instance, in a political setting,
looking at the persuasions of every
individual in the country is essentially
what we call a general election.
But of course one can't run
a general election every year, or
even every other year.
So in such settings
we still wish to understand how
the sub-populations are split up.
In politics this becomes
important to understand what kinds
of legislation should be passed, which
is representative for the population.
In the environment, it becomes important
to understand the extent to which
we have an invasive species spreading
into a native domain, and so on.
It is natural now, if we can't count
everybody in the population, to query
what happens if we sample
randomly from the population?
So let's consider what happens
if we take a random selection X
from the population I've given you.
Now the element could be an orange ball.
Now what are the chances
one draws an orange ball if
one selects at random from
the whole population?
Well there are 360 balls, as I've
informed you, of which 101 are orange,
and therefore the chance,
the probability of selecting an orange
ball is 101 over 360 and so,
p is approximately 0.28.
The chance of selecting
a blue ball likewise
is 259 out of 360, and that's about 0.72.
Now what is key here is this.
The proportion p is fixed.
It's 0.28 but
it is a priori unknown to us.
And what we would like to do,
is what we would dearly love to do,
is to be able to estimate p accurately,
without having recourse to counting
everybody in the population.
Naturally from here,
we think about sampling
repeatedly and
independently from the population.
Let's abstract out the whole picture now.
You see it does not align population
with elements of two flavors,
orange and blue to begin with.
Well, we could think of this abstractly
as some collection of objects
that proportion p of which is orange, and
of proportion q,
1 minus p of which, is blue.
Now, names orange and
blue here, Republican Democrat,
python alligator, are just names.
As far as the estimation problem
is concerned, they are irrelevant.
We might as well simplify annotation,
and the way we compute it,
by calling the elements
of one type the number 1.
The elements of the other type,
the number 0.
Now, what does a random sample look like?
Well, a random sample could be either
of type 1, or it could be of type 0.
In other words, picking a random element
is equivalent to the toss
of a metaphorical coin.
[SOUND] Now, what's different
here is we're tossing a coin.
It has got a certain
bias towards success p.
But we do not a priori know what p is.
Our estimation problem is given
the coin to determine what p is.
Now, there's some nomenclature
which goes with this and
there's some history which goes with
this and we should take stock of this.
The first thorough examination
of sequences of coin tosses was
undertaken by Jacob Bernoulli.
The patriarch of the Bernoulli clan,
may perhaps the most famous
clan in mathematics, and
his work was published posthumously
seven years after he died in 1713.
And the work was called the Ars
Conjectandi, The Art of Conjecture,
and in it Bernoulli flushes
out the mysteries of
coin tossing to a hitherto
unimagined level.
And so from that point on we have
called coin tosses Bernoulli trials.
And so here is another name or a sequence
of names for the underlying experiment.
Selecting an individual at random from
the population when the population is
dichotomous corresponds to flipping
a metaphorical coin with a fixed but
unknown success probability, p.
We will call such a coin flip a Bernoulli
trial with success probability p.
We will denote it,
the outcome of this trial by a letter x.
x is a chance variable.
It's going to take one of two values 1 or
0, orange or blue.
With probabilities p and
1 minus p respectively and
from now on in this context we will
always write 1 minus p to be q.
Here now is a setting.
Sampling from an underlying population
is equivalent to the toss of a coin.
Now, how do we go about determining
what the buyers might have to pay?
Well naturally enough,
we sample repeatedly.
Okay, and so let's move on and
consider a setting where instead of
a single sample,
we sampled repeatedly from the population.
Of course,
the sampling is with replacement and
is independent and is bias-free.
What's bias-free?
Connote in this context
is that in principle,
I can pick any one in the population.
All elements being equally likely.
This of course makes sure that I'm
not selecting from one segment of
the population or some other
narrow region of the population.
We select from the whole population
because I want to say something
about the population as a whole.
Naturally enough, I'd like a sampling to
be with replacement, because otherwise I'm
diluting the population and the underlying
proportions will change from step to step.
To avoid this, we make sure
sampling is with replacement, and,
of course we want a sampling
to be independent.
So we now have the context that
we are very familiar with,
that of repeated independent trials.
And this is the simplest of those
settings, the repeated toss of a coin.
Now, if we toss a coin repeatedly,
in this case,
I've shown you what happens
if we toss it 12 times.
Then we get a sequence of outcomes,
1s and 0s, successes and
failures, oranges and blues.
Here's a particular listing of
a sequence of successes and
failures, 0 representing a failure,
1 representing a success.
Now for this particular sequence, we've
sampled from the population 12 times.
We've got a particular sequence
of elements in the population.
Of the sequence we are interested
in asking how many are of type 1?
How many are orange?
And a simple count shows there are exactly
four 1s in that sequence, out of how many?
Well, out of 12.
And therefore, the frequency of type 1
in the sequence I have drawn is 4 in 12 or
1 in 3.
Now you may recall that in the original
population that had started with
of balls in the urn,
there were 360 balls and 101 orange balls.
And therefore the proportion
of orange balls is about .28.
We now get one third in
this particular sample.
Now that's pretty close to 0.28.
I wonder if this is a more
general phenomenon.
So our fundamental
question is the following.
Are the subpopulation proportions in
the sample that we have selected,
representative of the subpopulation
proportions in the entire population?
Would this be the case?
Well, on the face of it,
this does not look very promising.
Imagine, if you are, if you like,
a large underlying population,
say of a country, divided into two groups.
That's a very large population,
maybe 100 million, 500 million, per,
perhaps 1 billion people.
A typical poll,
if you look at a newspaper,
will look at something like the order
of 1,000 individuals, if we are lucky.
Why should the proportions in 1,000
individuals have anything whatsoever
to say about a population which
is 1 million times larger,
a population of the size 1 billion?
This doesn't sound reasonable at all,
but perhaps we should examine
this a little bit further to see what
we can glean from this structure.
So, let's immediately abstract
away the underlying problem.
I'm going to start with
a dichotomous population,
one of the many types I've shown you,
or that you could imagine.
I'm going to abstract away the process
of random selection from the population
by the metaphorical toss of a coin.
In oth, in other words,
a Bernoulli trial with a coin,
whose bias p towards success, is fixed,
but sadly, unhappily, unknown.
We're going to toss this coin
repeatedly a finite number of times.
How many times?
Well, ineluctably of course n times,
what else would you call it?
We'd like to have some
generic number of trials and
then see what happens as
the number of trials increases.
So in other words,
we now have a sequence of coin flips
Bernoulli trials x1 through xn.
Each of them taking values, 1 and 0,
biased towards 1 with probability p,
biased towards 0 with probability q.
Of course, the coin flips are independent.
This constitutes the raw data of a poll.
Out of this, we abstract first,
the number of elements
in the poll in the sample of type 1.
Let's give it a name.
The number of elements of type 1
can be obtained by simply adding
the xs from left to right.
Of course, every time an x is a 0
it has no contribution to the sum.
And when an x is 1,
it gives you exactly 1 to the sum.
And therefore,
the sum of x1 through xn will be exactly
representative of the number of 1s,
or the number of successes.
We call this, naturally,
the accumulated number of successes
in a sequence of Bernoulli trials.
Let's give it a name, s for sum, and
use a subscript n to remind ourselves
that you're summing over n trials.
So s sub n represents the accumulated
successes in n tosses of a biased coin.
How does a basic question get reflected
into this mathematical substructure?
Well very simply.
The fractional 1s in the sample is sn,
divided by n.
In our example, we had n was 12, sn was 4.
And so the fraction was 4 divided by 12.
And our basic question is,
is Sn divided by n
a good approximation to the fixed but
unknown p?

