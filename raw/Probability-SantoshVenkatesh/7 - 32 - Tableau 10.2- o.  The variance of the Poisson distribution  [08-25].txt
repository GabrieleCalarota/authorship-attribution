Let us now turn to a consideration
of the spread of the distribution
around the center.
Empirical evidence suggests that as lambda
increases, the spread increases and,
naturally, we'd like to determine.
What the actual functional form of
the dependence on this parameter is.
So let's hold lambda as a fixed, but
arbitrary positive parameter and
we will hide lambda, the notation,
to keep our expressions more manageable.
The natural notion of spread
that we will look at is,
of course, what we've encountered
before in the case of the binomial.
The idea of a variance.
We think of the variance as
a probabilistic moment of inertia.
It's going to connote
the expected squared dispersion
of likely values of your chance
variable from the center.
How do you define this formally?
Mathematically the variance
is always connoted
var of the chance variable,
var of course transfer variance.
It is almost invariably
compacted in notation by using
the square of the lower case greek
letter sigma and we call the variance.
Sigma squared and is defined just as
in the case of the binomial as follows.
After variable takes value 0,
it's distance from the expectation is
0 minus lambda, the square of the distance
is 0 minus lambda, the whole squared.
And you're weighted according to the
probability p of 0 of taking the value 0.
If the variable takes the value 1,
it's squared dispersion from the center
is 1 minus lambda the whole squared,
weighted according to the probability
p of 1 of taking value 1.
And so on, and
they accumulate all the possibilities.
Now, an expression like
the one on the right is.
It makes very clear exactly
what the underlined process is,
but undeniably, it's a little awkward,
a little long, a little clumsy.
And we try to summarize this in
mathematical notation, by using,
of course, estimation sign.
And we simply say, this is the sum over
all relevant values k, in this case,
k is running from 0,
1, 2, 3, and so forth.
Of terms of the kind, k minus lambda,
the whole squared times p of k.
Of course,
the difficulty in further simplification
is in the form of the squared
term inside the summation.
So, let's take a good hard look at this.
So, now we've got an expression for
the variance of a Poisson variable.
It's a summation over k of k minus lambda,
the whole squared, times p of k.
How can we fair simplify this?
The listener should possibly take
a moment, pause the lecture, and see if
what we've learned by our manipulations
of similar objects in the bi,
the case of the binomial.
Are suggestive of what we might do here.
Restart the lecture out when you're ready.
Now, in this summation, we've got
a whole square, and that is of course,
very reminiscent.
Of the far often, the whole square of
the kind of a minus b the whole squared.
Where we identify a with k and
b with lambda.
And the key of course
is that a whole square,
we know from our very early
exposure to Algebra, is,
can be expanded in the form of
a squared minus 2ab plus b squared.
Of course, this is an elementary
form of the binomial theorem.
Now, lets go ahead and
expand the square of the others.
And if we did this,
we'd get the expression on the right.
Of course,
the expressions become a bit bigger.
We have, in fact, expanded other square,
but this going to lead to simplifications.
And the key here is to observe that
addition is an associative operation.
We can add numbers in any order
whatsoever and so, inevitably, we
take the expression on the right and group
similar terms together and form sums.
So, we group the terms of involving
k squared together, form a sum.
Group the terms involving k together,
form another sum.
We can pull 2 lambda out of
the sum because it's a constant.
And finally,
take the last lambda squared times p of k.
Form a suminal link for those tabs.
Again lambda comes out of the summation
because it is a constant.
Now we've got yet longer expression
on the right hand side but
we are making progress.
Because in each of the sum adds.
We recognize quantities for
which we have discovered identities.
Let's start with the first one.
The sum over k, of k squared times p of k.
We've discovered an identity for
k squared times p of k.
Let's promptly put it to good use.
Write it down.
This is involve expressions of the form
lambda square times p of k minus 2
plus lambda times p of k minus 1 and
sum of all possibilities.
But, the moment we do that, if you sum
all possibilities for k of the form
p of k minus 2, what we in effect do
is sum over all Poisson probabilities.
P of 0, p of 1, p of 2, p of 3,
and so forth and, of course,
that sum must be in fact
1 by normalization.
Likewise, a sum overall
k of p of k minus 1.
Again, sums are all Poisson probabilities
and, by normalization, that sum is
also 1 and therefore the entire expression
on the right default becomes very simple.
They just lambda squared plus lambda.
Now, let's look at the second term.
A summation of a k of k times p of k.
Now, this is very familiar.
We've done exactly this in
the computation of the expectation.
But, let's run through
the calculation again.
Replace k times p of k by lambda p of k
minus 1, pull out lambda from the sum,
observer your summing over all the Poisson
probabilities that must be 1, and
therefore, everything becomes just lambda.
And finally,
the last term is the simplest of all
because a sum of a k of p of k just
sums overall Poisson probabilities.
It is manifestly one bi-normalization and
we're done.
Pooling our expressions together, we get
the following result for the variants.
The first summation
involving k squared p of k
gives up the term lambda
squared plus lambda.
The second summation involving k
times p of k gives us the term
minus twice lambda times lambda.
And the third term involving
a summation of p of k by itself
gives us examine lambda squared times 1.
And an examination of the expression
righteous that lambda
squared fortuitously cancels.
You get lambda squared minus 2
lambda squared plus lambda squared.
And it goes away.
And so that all that remains is
a beautiful, elegant expression, Lambda.
Let's quickly summarize.
If x is a Poisson chance variable, then
it's variance is exactly
the Poisson parameter lambda.
At this point,
we've discovered all the basic
features of the Poisson distribution.
Our next three segments constitute
dangerous bend digressions,
dangerous bend topics which show
how the Poisson distribution
arises naturally in the context of
traffic congestion modeling queues.
These concentrations lead
naturally to the Poisson process.
Which is a fundamental
in these disciplines.
These topics are within our reach,
within our grasp, at this time.
But they do constitute a slight
digression from the main theme.
And they are technically more involved,
hence the, the dangerous bend designation.
Sample them as time and inclination allow.
If you wish to jump ahead then this
would be a good place to right
to the conclusion, to the summary
before you move on to the next table.

