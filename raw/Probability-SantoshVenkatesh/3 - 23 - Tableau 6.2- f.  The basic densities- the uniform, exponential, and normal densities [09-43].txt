So, for our purposes,
a probability experiment in the continuum
is categorized by a density function,
a function, p of x,
which connotes mass per unit length,
or mass density at the point x.
Of course, the density functions
have to be non-negative,
they connote probabilities after all, and
they have to have unit area,
this of course comes from additivity.
You stitch together all the pieces and
you make the pieces tinier and
tinier, in the limit you
should get exactly one.
What kinds of functions occur in practice?
So let's go back and look at the very
first one that we encountered through our
coin tossing experiment, and
this of course was in uniform density.
This density has a rectangular function.
So u of x, u for uniform is defined
formally all the whole line,
but it has support only
in the unit interval.
It takes value 1 in the unit interval,
and is 0 everywhere else.
It's manifestly non-negative everywhere.
The only question then is,
is it properly normalized?
And of course, we've seen this.
And to do a sanity check,
let's run the integral through.
The integral over the whole line of u
of x dx, well, the only portion that
contributes is the unit interval because
u of x is 0 outside the unit interval,
and so that leaves us an integral from 0
to 1, of 1 times dx, and of course that
just gives the length of the interval
from 0 to 1 or in other words, unit.
So in fact, the uniform density
is properly normalized.
This density,
which puts mass equally everywhere
is the natural continuous limiting
analogue of a combinatorial setting.
And I'm giving you for example
a genetic combinatorial distribution
on a set of possibilities where each
possibility has got the same probability.
In the limit, as you look at a crowd
of points getting closer and
closer together,
you end up with a uniform density,
which has got a constant mass
density in a certain wager.
The natural application domain for
things like this go all the way back
to Pierre-Simon, marquis de Laplace.
And Laplace thought of the uniform
density arising in any setting where
we had a continuum experiment where there
was random choice in some interval.
Now how do we go beyond this?
A natural example is
the exponential function.
The exponential density, and of used the,
the terminology g of x here and
that's because this density is
connected to a family of densities called
a gamma densities, that the student will
see in table in part two as he goes along,
so let's use g of x for it temporally.
It's just a name for this function.
It stands for the exponential function
in the positive half of the line.
So it's a decaying exponential, and
it's a 0 on the negative half of the line.
Again, it's manifestly
non-negative everywhere.
A look at the graph convinces you that,
yes indeed, it's, it's never negative.
And what about its area?
Well, the area under the curve
is a very elementary integral.
The integral of the function g of x over
the whole line will have contributions
only from the positive half line because
g of x is 0 on the negative half.
The integral from 0 to infinity
of e to the minus x dx
is an elementary exponential integral.
We run it through the mill, and
out comes exactly 1 minus 0 or 1.
The exponential density is
also properly normalized.
Should pause for a moment and ask,
have I seen a function like
this in the discrete domain?
Cast your mind back to
the upload six part one.
Did you encounter any of kind
function with this kind of
decaying exponential character?
If you came up with
the geometric distribution,
then indeed, it's exactly the discrete
analogue of this de, density.
And again the geo, geometric
distribution as you might recall has a,
an exponentially decaying character
over these discreet integer points.
Where does this density arise?
This puts more mass density
near 0, and very little mass density
way out for large values of x.
In other words,
the probability of taking a value
somewhere near 0 is relatively high.
The probability of getting a value,
a random variable, which takes on
a very large value, is very small.
This turns out to be the natural model for
randomness in settings,
where we have queues, where we
are waiting for something to happen.
The time between two arrivals at a queue,
this turns out to be exactly
the way one models these problems.
Still another example and
this one is not so
elementary but it is familiar to us.
Constant mention of the bell
curve in common parlance has
led us to common acquaintance
with this mysterious object.
The normal density has got a form
which is like an inverted bell.
The function is nice and
symmetric and the explicit form
is given by an exponential with
a decaying quadratic in the exponent, and
a normalizing factor,
square root of 2 pi, in the denominator.
By long tradition going back to Gauss,
we call this function phi of x.
Okay, now, it's clear that this function
is strictly positive everywhere.
Why?
Because the exponential function is
always positive.
So positivity is trite.
It's not so clear that it is
in fact properly normalized.
Now where does that mysterious factor,
square root of 2 pi come from?
The very moment you see pi, you start
thinking of a circle perhaps, but
there's no circle in evidence here.
So, is it in fact, properly normalized?
Indeed it is.
But, at this point, we've exhausted almost
all the probabilistic intuition for this,
and verifying that this function is in
fact properly normalized to unit area
becomes a calculus exercise,
albeit with a little twitch to the tail.
And so I will define that for a moment.
Where does this arise?
Where is the discrete analog?
And this is harder to say.
The student might want to pause for
a minute and say among the examples
we saw in the uploads six part one,
is there anything where the function
in the discreet domain looked
something like this bell curve?
If you came up with
the binomial distribution,
then indeed we found a discreet
analog of the bell curve.
It is not at all obvious that the binomial
distribution on the one hand has
a function of form which gives us
anything like this bell curve for
the normal density.
That it is not at all clear analytically,
but it turns out to be deep and powerful
process of analysis which goes
back three centuries and led
to some of the most potent and fundamental
results in the theory of chance.
We'll come back to this in
our concluding lecture.
That so for the time being,
the normal density arises
naturally in settings where there's
any kind of central tendency,
when things are being brought
into the center in some way.
Notice the normal density
puts more mass around 0 and
relatively little mass far away from 0,
yeah.
The bell curve turns out to be
ubiquitous in the theory of chance.
And, it is ubiquitous through
the agency of the mysterious
central limit theorem,
which very roughly put,
says that if you add enough
independent perturbations,
whatever that means,
the end result looks approximately normal,
looks approximately
distributed as if it were drawn
from a continuum experiment
with this character.
These are the basic densities,
the three fundamental densities, right?
There are many others of course, and
in specialized circumstances you'll
encounter different functional forms.
Generically we'll call them p of x.
They're all characterized
by the attributes,
that they have to be non-negative,
they constitute a mass density, and
that they're properly
normalized to have unit area.
How do you compute probabilities?
Figure out the subset of the real
line that constitutes the event under
consideration, and simply look at the area
under the relevant curve in that region.

