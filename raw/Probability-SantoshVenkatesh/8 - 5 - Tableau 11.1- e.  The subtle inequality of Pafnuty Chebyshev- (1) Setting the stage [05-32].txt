We are now ready to tackle the estimate
for the probability frontally.
Recall our basic question was this, if one
is given an error tolerance epsilon and
a confidence that is
desired of 1 minus delta.
What size of sample as
a function of the allowed
error epsilon and as a function of
the confidence parameter delta?
What size of sample will
get us estimates which
are within these error parameters with,
within the desired confidence?
When one looks at a mathematical
equation like this, well, at some level,
it is a formidable object.
And if one tries to deconstruct it,
you say, oh my God,
I don't see how to make any progress here.
Now, surely we can make progress if we're
informed what the error tolerance is,
if you're told, say, that epsilon is 3%.
And we're told what n is.
Then we can simply, for the probability
of the left, write down the event,
and then sum up
the binomial probabilities,
add numerically,
calculate what these probabilities are.
Of course,
this is already within our purview and
we've already seen examples of this.
But this is, at a deep level,
not very satisfactory.
What this doesn't allow us to do is intuit
what the relationship between n and
these parameters, epsilon and
delta, really is.
All we have is a bunch
of numerical estimates,
which gives us a feeling that maybe things
are okay, but we have no guarantees.
To get we actually are going to have
to try to, at least in principle,
analytically try to compute an som,
an expression for that probability.
A huge step forward was made by the great
Russian mathematician Pafnuty Chebyshev.
Chebyshev was born in 1821.
He lived a long life.
Died in 1894.
And in many ways, we can find Chebyhev's
fingerprints all over modern mathematics.
In many ways, you can think of Chebyshev
as a father of Russian modern mathematics.
Chebyshev made contributions
in a plethora of disciplines.
He made deep contributions
to the theory of numbers.
He made a, a fundamental contribution
to what is now called the prime number
theorem that was ultimately resolved
by Hadamard and de la Vall√©e Poussin.
He made deep contributions to the theory
of what is now called orthogonal
polynomials.
If a student is ever inspired to
take a course in Fourier analysis or
Fourier theory, she will encounter these
wonderful polynomials of Chebyshev.
And these, incidentally,
were inspired by Chebyshev's interest
in James Watt's steam engines.
And Chebyshev made deep and fundamental
contributions to the theory of chance.
His students are among the most noted
probabilists of the 20th century.
In 1867, he proposed a subtle and
seemingly trite looking inequality for
the kind of probability
we are concerned with.
And this inequality
turns out to be the key.
Now, to begin the story,
let me set it up by going back and
reviewing what the basic structure is.
So here's a quick review.
We're considering polls for definiteness,
so there's an underlying population,
which is a target.
We're going to sample elements
independently from the entire population.
And that sampling is going to be
uniformly with respect to the population,
and the sampling is going
to be with replacement.
In other words, we're going to have
a sequence of Bernoulli trials,
let's say a sample of size n.
We're going to form an accumulated
number of successes.
Of course, we call that S sub n.
And we now know that S sub n has
got a binomial distribution with
parameters n and p.
And now we know what the mass function for
S sub n is.
The atomic probabilities are given
by the binomial probabilities.
And we know exactly what the generic shape
of this distribution looks like now.
But we've done more, right?
In tableau ten, in part one,
we looked at the idea of a center of mass.
A probabilistic center of mass are what we
call the expectation of this distribution.
And we found that the expectation
has got a very simple form.
It is centered at the maximum probability,
n times p.
And we also computed a notion of spread
in a probabilistic moment of inertia,
or in other words, the variance.
The expected squared spread
of the values of the binomial
around its expected value is
given by n times p times q.
Now the last is all we will need to
slung on through Chebyhev's inequality.

