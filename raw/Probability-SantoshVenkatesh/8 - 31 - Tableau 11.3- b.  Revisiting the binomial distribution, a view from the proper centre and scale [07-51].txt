We had seen a bell-like curve
make a rather mysterious appearance
when we looked at the binomial.
And naturally to get a just
understanding of where
this phenomenon comes up we should return
to that most elementary of settings.
The toss of a coin.
So let us go back and consider
the repeated tosses of a fair coin.
We're considering a situation where
we have symmetric Bernoulli trials,
in other words, fair coin flips.
As always,
we will denote the successes by ones,
failures by zeros, and we have
an independent sequence of ones and
zeros that we'll label x1,
x2, x3, xn and so forth.
In our notation, these can note
symmetric Bernoulli trials or
Bernoulli trials with success
probability p equal to one-half.
Naturally, we form accumulated sums and
once you form the accumulated successes
over end trials we get what we call
s subscript n or
simply sn as a sum of the first nxs.
Sn denotes a number of successes and
n tosses off a fair coin.
Of course, we know now that this
governed by a binomial distribution
with two parameters, n, the number trials,
and the success probability,
which in this case is one-half.
We have a discrete setting and
of course it's completely
categorized by a mass function.
The mass function attaches to
atoms the atomic probabilities
that we called b sub n of k.
With a parameter little p,
which in this case it gave us one half.
And said the binomial
probabilities simplify in form.
They are the form n choose k
times 2 to the power minus n,.
And k runs through the possibilities for
the number of success, 0,
1, 2, 3, up till n.
We know that this distribution
has a center of mass and
expectation exactly at n over 2.
And a variance at, as n over 4.
By now, we have a fairly good
understanding of this most
basic of objects, the binomial.
The question at hand is,
how does this distribution behave
as n marches on and
becomes larger and larger and larger?
And so here are a couple of pictures.
Say, we look at a sequence of values,
n is ten,
25, 50, and 100.
And, as you can see,
the distribution marches on to
the right getting increasingly
spread out and more congested.
And already certain interesting features
are beginning to become apparent.
But one of the things we will promptly
need to do is to find some mechanism for
comparing all these
different distributions for
different values of n all
under the same framework.
So, let's take a look
at this picture again.
And this time realize that
what characterizes each of
these distributions is its expected value,
which is growing with n as n over two and
a variance.
Yes, assuredly as n increases
the distribution is getting more and
more and more spread out.
But the spread,
as we've seen, is small compared to
how far the expectation is moving.
A natural way to characterize this is,
first, to center
the distribution at its center.
In other words, at n over 2.
And look at it on a proper scale.
In other words,
by dividing the standard deviation.
So, let's introduce a new variable.
For every value n, let Sn star denote Sn,
centered to the origin by subtracting
its expected value and then
normalized to a unit's spread by
dividing by its standard deviation.
What we've really done is taken
the accumulated successes and
made them dimensionless.
Centered and dimensionless.
Of course, we realize that here
the expectation of Sn is exactly n over 2.
And the variance is n over 4.
And so our expression becomes
Sn minus n over 2 divided by
in the denominator one-half
of the square root of n.
Here is a new chance variable, and
as it increases we get
a sequence of chance variables.
Of course, each of these
variables is inherited from Sn,
and therefore inherits
the binomial distribution,
except that it's going to be
recentered and rescaled so
the axis is going to be scaled till we
have at least it spread to the unit.
What does the mass function of this
standardized scaled variable look like?
Now Sn star, a properly centered and
scaled of version Sn is
itself a discreet variable.
It is now centered at 0 so
the value Sn star takes
are scattered around the origin.
The spread is normalized units so
each of the Sn stars is spread
essentially the same range.
At least most of the probability
lies within say two or
three units of the origin.
But it is still a discrete variable and
it inherits it's mass
function entirely from Sn.
In other words, each of the curves
you see is going to get rescaled and
moved to form the mass function for
Sn star.
Let's take a quick look at it.
So starting on the screen from the upper
left, clockwise, all the way down,
you see n equal to 10, n equal to 25,
n equal to 50 and n equal to 100.
These recentered, rescaled mass functions.
Does anything leap out at you?
Indeed, it does.
These functions all look
remarkably similar.
Taking into account,
of course, the fact that,
as n increases, Sn star has got more and
more points, places where it takes values.
But the mass function,
if interpolated through,
is beginning to look more and
more like a bell curve.
Now to be sure there is a vertical
normalization to be taken account of.
And this arises because as it
increases there are more and
more points where Sn and
therefore Sn star takes values.
And therefore the probability mass
is getting more and more spread out.
And therefore the heights
the probabilities are decreasing.
But properly normalized,
it does appear that
the curves are looking
increasingly bell-like.
And this leads to a powerful and
potent and
speculative idea that it
just may be possible to
compute binomial probabilities
approximately by areas under a bell curve.

