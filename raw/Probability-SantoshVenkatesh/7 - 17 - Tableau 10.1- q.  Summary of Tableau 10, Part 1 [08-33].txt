Let us summarize what we
have discovered so far.
Tableau 10, From polls to bombs.
Part 1, deals with the polls
portion of the segment and
introduces the binomial distribution.
The basic element, the building block,
off our entire classes.
Was to form a model the toss of a coin.
And for
this we go back to Jacob Bernoulli and
his publication of
Ars Conjectandi in 1713.
The first time where a formal
understanding of a model for
coining tossing became clear.
As an important and
fundamental building block in
the understanding of chance processes.
So accordingly, a Bernoulli trial
corresponds to a formal coin toss.
The abstract outcome,
let's say x, is a number 0 or 1,
1 connoting a head or success,
0 a failure or a tail.
We allow the coin to be biased, but
some say success probability p,
and the corresponding failure
probability q, equal to 1 minus p.
Of course, a humble coin toss by
itself is not very interesting.
We now build upon this, and through
the medium of repeated independent trials.
To construct a model of a pole,
or of a random sample.
So, accordingly, suppose X1,
X2, X3 through Xn,
constitute repeated Bernoulli trials.
Repeated independent trials,
corresponding to the tosses of a coin,
whose success probability is p.
At this point the sample space and
indeed the probability splace,
space is, is transparent.
The sample space is a sequence of n,
0s and 1s.
To which we attach atomic measure obtained
by multiplying probabilities of 0s and 1s.
That's the product measure.
We immediately, from this rich sample
space, abstract out a key element.
A chance driven process of
particular interest to us.
The accumulated successes in these
n tosses is denoted by s sub n.
And of course, is now just a sum
of the values X1 through Xn.
At this point,
we have created a new chance process from
the underlying chance
process of repeated trials.
This new process gives you a value,
an idealized outcome S sub n.
Which takes values in a new sample
space if you will of integers 0, 1, 2,
3, up till n.
Now naturally enough,
we are interested in what
is the probability measure
that is inherited by this new chance
variable, the accumulated successes.
And this is indeed.
Is exactly the binomial
distribution with two parameters,
a number of trials in, and
the success probability peak.
With those held fixed,
the number of successes ranges over
a sample space from 0 through n.
With corresponding atomic
mass probabilities,
the atomic mass function is given by
exactly the binomial distribution.
And our exploration shows
us that we understand
now what the distribution looks like.
It got a unimodel character.
It increases to a maximum value
in the vicinity of n times p.
It has a center of mass,
an expectation which is exactly n times p.
And it has a variance, and
expected squared deviation from the center
which is exactly n times p times q, or
equivalently as standard deviation sigma,
which is a square root of npq.
In other words, as n increases.
The expected center moves proportional
to n, but the expected spread
moves only proportional to the square
root of n around the center.
You get increasing concentration
around the center.
So we could've done this from a purely
abstract point of view, just.
Imagine this metaphorical experiment,
do the algebraic calculations, but
of course, this is dry and boring.
What makes this come alive is
the rich panoply of applications
of this beautiful simple underlying model.
The applications we have seen,
span every large range.
From cards, to earned problems,
to the throw of a die,
of course a toss of a coin, to more
interesting and complex settings, such as,
medical testing of serum and vaccines and
the process by which drugs are approved.
To understanding,
estimating some populations of large
populations via small samples.
We are now in,
in the province of statistical testing.
And in this province, we now have
this range panoply of applications.
Who would have thought that the humble
coin toss had this richness within it?
Only experience will
convince a student that
the simple coin toss carries within it
an inexhaustible source of inspiration.
And these applications are a small shop
window of some of the consequences and
directions that one can go into
when one explores coin tossing.
For our purposes now, what we've done is,
obtained a distributional form for
an accumulated number of successes and
found its applicability in
a wide range of applications.
Quo vadis?
Where does one go from here?
Well, we now have an explicit mathematical
form, we have some understanding of it.
We have some, a feeling for
it's application domain.
But if one tends to something
like poles we have,
via Fisher's maximum likelihood principle,
a principled procedure for
making guesses about sized of
underlying sub-populations.
But what we don't yet
have is any guarantee about
the quality of this procedure.
Yes we now have some reason to believe
that this is a good way to do it, but
how good is it really?
How expensive is it, and
what are the chances of error?
Of course these questions are still
waiting and looming for us.
We shall return to these questions and
these will be our concluding lectures
in this course, and
that will take us into the marvelous,
remarkable world,
the fabulous limit laws, but
before we do that I want to
take one small digression.
And this connects us to a curious,
mysterious
identity in approximation discovered
by Sim√©on Denis Poisson in 1837.
And that discovery,
connected to the binomial,
a rich new direction,
leading to the province of rare events,
arrival processes, and
knitting it all together.
The second of the fundamental
distributions in chance,
the Pearson distribution.
So, we shall come back in Tableau ten,
Part two.
With the Poisson distribution, and
we'll see its applications in
a plethora of beguiling areas,
including bombs and cues.
Once we've done that,
we shall come back to the binomial.
And return to the question of how
good our pulling mechanism really is?

