Now, let's abstract our key ideas.
Now, what have we done so far?
We've looked at five different chance
experiments in very common place settings,
throwing dice, balls and urns, and coins.
And abstracted three critical features.
This, then, should be at
the heart of a theory of chance.
Again, think of geometry as a metaphor.
We start with an idea of geometry
principles by experimentation,
by building.
But eventually,
we cut loose the trappings of reality and
abstract out clean,
mathematical, sanitized elements,
lines, planes, arcs, circles, points,
from these physical entities.
But we're cutting loose
the vestiges of reality,
we allow ourselves to focus on
the logical heart of the problem.
In very much the same spirit, we're going
to try to extract from these examples,
the key ideas.
Which will then allow us to
abstract away the reality, and
look at a mathematical structure.
Once we have such a structure,
we can examine its properties, its logical
consequences, and its predictions,
just as we do in Euclidean geometry,
by proving theorems once we have
a basic axiomatic set up in place.
And finally, to close the loop, we will
then see where these results fit in,
in applications in a diverse
range of settings.
So this is our game plan but
we are jumping ahead.
Immediately, what have we done so far?
Let's look at the principles
we've had to define.
So this is a summary,
chance in commonplace settings.
Chance experiments
are governed by outcomes.
These outcomes are chance driven,
they are not [INAUDIBLE] predictable.
And, we begin by looking at outcomes,
which are constrained to be in
a finite collection of outcomes.
This was a natural setting of
communitorial probabilities.
And then, we moved on to look at settings
where we have more than a finite
collection, in fact, potentially
infinite collections of outcomes.
The first question in a chance
experiment for you should always be,
can I identify the collection of
feasible outcomes of the experiment?
Either explicitly,
by writing them all down.
Of course you can only do this if this
is finite, or implicitly, where you see
a pattern and using that pattern, you can
completely describe a generic outcome.
That is always the first question.
What are the conceptual outcomes of
the underlying chance experiment?
With this in hand, the next question is,
what are the events of interest to me?
And if you go back and
look at our examples, every event is
identifiable by a subcollection
of the possible outcomes,
by an aggregate of the possible outcomes.
Your next question then should be, can I
model my chance problem mathematically,
compactly, identify the outcomes.
And then identify the set.
The subcollection of those outcomes which
trigger the events of interest to me.
This is always the next
step in the program.
The final step of the program is,
once I've identified the subset
of interest to me, what is the chance or
the probability I want to ascribe to it?
And here, we will inevitably
resort to this wonderful,
transcendent principle of additivity.
Success in solving problems and
coming up with chance,
estimates which are accurate,
is good devolve critically again and
again upon our ability to take
a raw problem, an event and
partition it, break it up,
into pieces which are logically cohesive.
Of course I cannot tell you in advance
how to break up a generic event.
That is going to be what
the problem has to tell us.
But each problem will inform us
of the way to think about it.
And once we identify a clean partition,
we result to additivity,
and put the whole together by
simply adding up the pieces.
This is the starting point for the
development of a rich and fecund theory.
And this was the part that
Andrey Kolmogorov followed in 1933.
Where he laid down an axiomatic basis for
the science of probability.
This will be the content
of our next lecture.

