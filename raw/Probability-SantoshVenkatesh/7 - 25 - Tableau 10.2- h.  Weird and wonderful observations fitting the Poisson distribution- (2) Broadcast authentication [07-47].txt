Our next example is in
broadcast authentication.
I should set the stage by
telling you about the setting.
Computer communications came
of age in the late 1970s.
And originally called
the ARPANET the network was
designed to communicate between
computers across a few institutions.
Everything was well and good as long as
you just had naive academics well meaning
to be sure who were communicating with
each other issues of transfer transparent.
But of course the moment a larger
community gets involved security and
fast becomes more difficult to verify.
The early issues in the 70s were two-fold.
How could one guarantee the integrity
of one's communication and
how could one guarantee the
confidentiality of one's communication?
In other words,
how does one ensure that a third
party does not spoof one
pretend to be somebody else?
In other words, the integrity of
the communication is maintained.
And how does one ensure that a third party
does not eavesdrop on
a private conversation?
Now, these issues are solved by
a beautiful number theoretic methods
which provided cryptographic protection.
So today if you get at a website and
you go to a secure website to make for
example a financial transaction and
you see the words https.
Appearing on a browser that
will signal to you that
your transaction is being
protected from prying eyes.
And that an, a nefarious individual can't
insert themselves into the loop and
pretend to be you.
So this is a measure of
cryptographic protection,
what it does is it provides
guarantees of confidentiality and
integrity of information, at a cost.
The cost is in terms of
the computations that have to be
performed by the receiver to verify
the bonafides of the transmitter.
Things went good through
the 80s until the communication
explosion really exploded with
the development of the internet in 1994.
And at this stage a new security
threat became apparent.
And we discovered to our
chagrin that our protocols in
our systems was not really well
equipped to handle this new threat.
This threat is called a denial
of service threat to the system.
An attacker and a various individual
finds they are cryptographically blocked
from spoofing or from eavesdropping.
So they're cryptographically blocked
from affecting the integrity or
the confidentiality of the communication.
But they decide that no matter
they will simply overwhelm a server
by sending a flood of messages.
We're just spurious which
the server requires time and
effort to verify and discard.
But if you send enough the server runs out
of computational resources and crashes.
And examples of denial of service
attacks have been in the news over
the last decade.
Major corporations even countries
have been attacked by third parties.
This is a difficult strategic and
security threat to handle
partly because our infrastructure was
not designed with these threats in mind.
Fortunately, there are avenues.
Steps one can take to
mitigate such threats and
a key here is to understand that
the needs of an attacker and
the needs of a legitimate client
are fundamentally different.
The attacker is looking
to take down a server,
a country, a large corporation,
a government.
For his attack to work
the computers which receive
the information must process
a large chunk of what he sends.
If one can discard a large
chunk of what he sends
then you end up defining the attack and
so here's a very simple strategy.
In a randomized way, there's card
packets that arrive at your door and
keep let's say,
only a small fraction p of those packets.
If you did this for
example is p is point one
then you accept only one in ten
packets randomly selected,.
And so
the attacker can't gain the system and
in one fell swoop you discard
90% of the attack packets.
If you can handle 10% of the attacker's
attack then you're in business, right?
Your competitional load
is now under control.
So here is a very simple way of
reducing the intensity of an attack.
Simply discard stuff,
of course this comes at a cost because
you discard not just spurious attack
packets but legitimate client packets.
Now what do the clients need?
The client has got a signed packet,
cryptographically signed
which authenticates him.
He needs you to examine one packet.
The attacker needs you to examine
essentially all his packets.
Here's a very simple client strategy,
send a few copies of your packet.
Each copy is examined with
say a chance of one in ten or
more generally probability p.
But if we change just a few packets,
the chance of at least one being
examined goes up very, very quickly.
So for example, if p is 0.1,
90% of the packets are discarded.
Then if n is 25 you just repeat
your one packet 25 times remember
the opposition has sent a billion packets
you've sent your one packet 25 times.
90% of your oppositions attack
has been eroded and thrown away.
But of your 25 packets,
the chance that at least one is examined
by the receiver to establish
a bonafides is now about 92%.
And if you send just 40 packets
your chance of getting at least
one packet through jumps to 98%.
Now how does one do the analysis?
Well, each packet gets through or
not for you with a small probability.
It's a rare event,
n is moderate here but 40, 50, 60.
You're governed by a Poi,
Poisson system approximately.
The probability that
no packet gets through
is the Poisson mass probability
with k equal to zero.
The probability that at least
one packet is accepted,
by activities one minus the probability
that none are accepted.
And there you go here's a very very
simple mechanism which can be readily
implemented.
This was suggested by Gunter,
Khanna, Tan, and Venkatesh in 2004.

