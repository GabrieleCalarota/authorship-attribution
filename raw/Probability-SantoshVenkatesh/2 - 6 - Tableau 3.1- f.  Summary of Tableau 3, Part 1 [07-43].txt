Okay.
Let's summarize and
see what the key ideas are that we have
extracted from these three simple
combinatorial experiments.
So, here's a summary.
Chance in commonplace settings
where sampling is from finite sets.
And in such settings, the idea of balls
and urns give us a potent metaphor.
So, we're beginning with
a combinatorial setting.
Well, what is a combinatorial setting?
Implicit here is the idea that
all chances are equally likely.
All outcomes are equally likely.
This is the heart of random sampling.
We have a chance experiment,
which has outcomes
which range across a finite
collection of possible outcomes.
This finite number could be very large,
but it is bounded.
It is a finite set,
sometimes called a universal set, and
in this context invariably represented
by the uppercase Greek letter omega.
So, uppercase omega always represents for
us the space of all possibilities for
the chance experiment at hand.
It is a set or collection of all possible
conceivable outcomes of the experiment.
Now, what is important here is not that
we've actually done the experiment.
But it is important that we be
able to conceive of doing it.
And this is the heart of the modelling,
a conceptual experiment,
a thought experiment, or in the German,
a gedanken experiment, okay.
And omega then represents all possible
conceivable outcomes in such a setting.
The next step is to identify
events of interest.
And an event is always
represented by an aggregate or
a subcollection of the possible out exper,
outcomes in the experiment.
From a notational point of view,
it will be good to give events
simple small compact names.
And by reasons of tradition,
we will call events by uppercase
letters at the low end of the alphabet.
So for example, uppercase A or B or C or
D could all represent events of interest
in a particular chance setting.
And finally,
we're interested in a probability measure.
Our probability measure then has
to ascribe a notion of chance.
Very vaguely,
we'll keep in mind that the idea of chance
means that if we do the experiment many,
many times, then a certain event
occurs a certain fraction of the time.
This is the idea we want
to ascribe to chance.
Of course, we want to formalize this.
Some colloquial notation is useful
in describing these, this setting.
So for example, colloquially, we will
say that an outcome is favorable for
an event if it is in the aggregate of
outcomes which comprises that event.
And so colloquially, we'll imagine
that if we do an experiment and
an outcome in an event occurs,
an outcome favorable for
the event occurs,
then we will say that event occurs.
And so, now our idea of chance in
a combinatorial setting is very clear.
The chance or the probability we want
to ascribe an event in notation,
a bold font upper case P for
probability with an argument,
the argument being the event of interest
to us, a collection of outcomes.
In a combinatorial setting where
the outcomes are equally likely,
the probability of the event is
very simply the number of outcomes
which are favorable to the event at hand.
In other words,
the size of that set divided by
the total number of possibilities.
Now, and in notation, we can capture
all of this very cleanly and simply.
P of A, the probability of
the event A in words is the number
of outcomes favorable for A divided by
the total number of possible outcomes.
Even more compactly,
we can say this is the cardinality of
A divided by the cardinality of omega.
Card or cardinality represents
the size of the underlying set.
Now, this already gives
us a starting point.
Chance experiments are characterised
at this level by outcomes,
events and probability measure.
Now, our setting so far has
considered a simplistic view where we
have a finite number of possible outcomes,
and everything is equally likely.
In other words,
we have a combinatorial setting.
Our next step is to
relax these constraints,
move away from the straitjacket.
What if chances are not equally likely?
What if a die is loaded?
What if the coin is unfair?
And we're one step further beyond that and
at least envisage settings where
we have chance experiments,
where the number of possible
outcomes is not finite anymore.
The number of possible
outcomes is infinite.
Now, of course, the moment you have
an infinite number of outcomes,
they cannot all become equally likely.
How should one handle this?
Is there any worth in handling this?
And so, we want to build
a theory which allows us to
expand into real life settings,
into problems of heft and importance.
And so, we'll want to consider this next.
Now, before we do this,
I will want to take a detour through
a tableau where the ball and
urn models we are looking at,
which implicitly kept in
mind distinguished balls and
distinguished urns are relaxed.
What if instead of having
two distinguished balls,
a red die and a white die?
What if we had two balls
which were indistinguishable?
What now?
How does the game change?
I will illustrate this in a setting
of interest in its own right,
in a setting of statistical mechanics and
in the turmoil of the early part
of the 20th century, when quantum
mechanics was taking center stage.
And the story is interesting in its own
right and gives us actually results
which seem to fit real models of
real particles very, very well.
This is a dangerous bend section, and
you should glance at it advisedly.
If you're interested, if you're curious,
by all means look at the next tableau.
If not, it can jump ahead to the following
tableau, where we relax the idea
of chance in combinatorial settings
to take in the idea of chance
where the probabilities are unequal and
the space is possibly infinite.

