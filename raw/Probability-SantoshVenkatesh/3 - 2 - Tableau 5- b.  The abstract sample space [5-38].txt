So let us without further ado start
with the first element in the setting.
The abstract sample space corresponds to
the performance of a chance experiment.
Now we should either be able
to perform such an experiment.
Or, at least,
conceive of it being performed.
This was asked then in
a gedanken chance experiment.
A thought experiment.
Something that can be conceived of.
And the moment one performs such
an experiment, one gets outcomes.
The idealized outcomes of
this chance experiment
are called the sample
points of the experiment.
The idea being that if from
a purely visceral point of view.
One is performing an experiment like
throwing a die, tossing a coin.
And the moment you do this we
are inheriting, acquiring an outcome,
an idealized outcome by sampling randomly
from the possibilities for the outcomes.
And hence we call the possible outcomes
in this formal setting, sample points.
And together the collection of
all conceivable sample points
forms a rich collection of elements,
a set of elements.
This set we'll then call the sample space.
The notion that identifying the family
of events precisely and carefully.
Surprisingly it was relatively new,
the idea is due to Robert Fawn Mesis, and
only came to fruition around the end of
the first quarter of the 20th century.
But around that time it became clear,
we have to codify exactly what
the objects are under discussion.
The awakening of probabilistic
interest in the 17th Century during
the Renaissance and thereafter,
saw results emerging and
a theory in nexus slowly emerging
from the foam as it were.
But the theory was not well formed what
we had obtained through the 17th to 18th,
and even through the 19th centuries,
with a gradual quickening of results,
as is the case in much of science.
We find a scatter shot set of results
without any formal fabric
knitting everything together.
What is the worth of a formal theory?
Well the ultimate rule of the formal
theory is in the richness of
the predictions it provides.
And a verifiability of those
predictions in common place settings.
In this of course,
the theory of probability has been wildly,
surpassingly, successful.
So, here's where we begin.
Identifying, codifying following formizas.
The collection of all idealized outcomes,
the sample points of the experiment.
Sum notation will lubricate the wheels,
so let's without further ado.
Talk about idealized sample points of
experiment, generic sample points,
typical sample points, and identify them
by the lower case Greek letter, omega.
Now omega now represents a generic,
typical if you wish, outcome of
an idealized experiment, a sample point.
The collection of all possible,
conceivable sample points
forms this potentially very
rich space of possibilities.
This is called the sample space and
we inevitably use
the upper case Greek letter
omega to represent this.
Now this is standard traditional
notation over the last century, but
we will not adhere slavishly to
this kind of notational convention.
If for example our experiment turns out
integers, then enter of little omega.
We will inevitably represent an integer
by a letter from the latin alphabet
in the middle portion of the alphabet,
i, j, k, l, m, or n typically.
And, the collection of integers,
is represented by tradition,
by an uppercase Zed.
Likewise we happen to be
dealing with a real line.
Inevitably we describe the outcomes,
the sample points.
We denote them by lower case letters
at the end of the latin alphabet.
Something like x is your
prototypical such letter,
sometimes y or zed,
occasionally r, s, t, u or v.
And the collection of real
numbers in the continuum,
the real line is denoted
by an upper case R.
But to proceed when we want to talk in
abstract we think of the sample space
as represented by an upper
case greek omega.
Now so much for the sample space.
You should think about the experiments
we've done in tableau three and
identify for yourself what the sample
space was in each of those settings.

