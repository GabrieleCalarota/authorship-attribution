We should investigate the
inclusion-exclusion formula by looking at
specific cases to build intuition for
words likely to be used for.
Before we launch in to calculations,
it would be wise to step back and
take a look at it from a vantage point,
a philosophical vantage point as it were.
To see where this is positioned and
where it is likely to be used for.
On the left of the inclusion-exclusion
formula what I've
got is the probability of a sum.
What a computing is the distribution
of the number of occurrences of
certain events.
So far so good.
That looks like it might well
be useful in many contexts.
What about the right-hand side?
That's where we have a formidable looking,
alternating sum.
The terms of the sum on the other hand,
involve intersection probabilities.
And so, immediately from a philosophical
point of view, this suggests to us,
that where inclusion and exclusion
is most likely to be profitable is
where the events are independent or
perhaps, mildly dependent.
In other words, the degree of
dependence is not too strong.
If that is the case, then we may
be in with a shout at computing
these intersection probabilities.
Because you recall that
independence is a rule of products.
Intersection probabilities devolve
beautifully when there's independence.
So this suggests that we should start
by looking at independent cases first.
Following the prescription of George
Boole, we're going to look at the simplest
of these cases to see if we can
understand where this leads us.
And as we shall see,
very shortly, an old friend is going
to resurface from these concentrations.
So, accordingly, let's start by
specializing in event to family.
A1 through An in some
abstract probability space.
Let's specialize it to the case,
where the events now are independent.
Let's make further simplifications here.
And assume further that not only
are these events independent,
but they all have the same mass.
They all have the same probability.
Inevitably, we'll call this probability,
little p.
Now what does inclusion-exclusion
tell us in such a setting?
Now before we dive into
an analysis of the formulation,
let's take a look at the generic
terms on the right of the equations.
They're going to sum in
an alternating fashion for
a given value k, the terms Sk,
Sk plus 1, Sk plus 2 and so forth.
The generic term on the right
is of the form Sk plus j.
Now what does the inclusion-exclusion
sum Sk plus j plus portend?
Well, the formulation is
admittedly formidable.
But in words,
it's quite simple to describe.
Pick any k plus j of
the events A1 through An.
Look at the intersection.
Compute the probability and
sum such probabilities
over all choices of k plus j
events from the collection.
Okay.
Now where does this lead us?
The summands on the right deal with
the probability of an intersection.
Independence is a rule of products.
And probability of an intersection
in this case is going
to be exactly the product of
the individual probabilities.
Now there are k plus j
events in that intersection.
Each of them by our assumption,
as got probability p.
Independence says, we're going to
multiply p by itself, k plus j times.
In other words, we're going to
accumulate further probability,
precisely p to the power k plus j.
Once you've done that,
every probability in that sum is
equal to p to the power k plus j.
How many terms are there in the sum?
Well, we have n events to start with and
are selecting all possibilities for
k plus j.
In other words, there are exactly n
choose k plus j terms in the sum and
each of the summands contributes exactly.
P to the power k plus j and there you go.
And the case where things are independent,
the inclusion-exclusion sums
simplify magically, beautifully.
The proper vantage points frequently adds
a magical clarity to an otherwise dim and
murky business.
Now let's dive into the formulation.
So what do we have?
We've got various
inclusion-exclusion sums,
which have got a particularly
simple binomial character.
What are we trying to do with these?
Well, we're going to look at
the indicators for the various events.
X1 is the indicator for a 1,
X2 for a 2, Xj for a generic Aj.
We are going to look at
the sums of these indicators.
X1 plus X2 through Xn is going to tell
us how many of the events occurred,
how that is precisely the domain of
the inclusion exclusion theorem.
The theorem tells us the probability
that precisely k of a events A1
through An occurred is given by
an alternating sum starting at Sk and
then we, well, we need Sk plus 1 and
then Sk plus 2 and so forth.
Each of the terms, Sk plus j on the right
has got a simple binomial formulation.
Let's promptly write it down.
And now, inside the sum,
I've got something,
which looks a little forbidding initially.
Because I've got a product of
two binomial coefficients, but
that should look familiar.
We've seen something like
this not very long ago.
In fact, in the last video lecture, we saw
precisely a combinatorial identity for
a product of a binomial
coefficient of this form,
when l was some generic
number bigger than k.
And if we select l to be n exactly,
then we can just write
down what we discovered in
our combinatorial identity.
Writing this product of binomial
coefficients down as a different
format of binomial coefficients, this
time with j appearing only in one place.
Let's plug it in.
And now, I'm going to do a little bit of,
let's call it housekeeping.
I'm going to rearrange them and
color code them.
First, observe, I've taken a product
of two binomial coefficients and
replaced it by my combinatorial identity.
Next, I've taken p to
the power k plus j and
rewritten it as p to the power
k times p to the power j.
And finally, I've color coded things,
blue and green.
The objective being that I want to
isolate terms that do not depend upon j,
pull them out of the sum.
These are the terms in blue.
And identify precisely
those things in the sum,
which are going to devolve
upon the choice of, of j.
Now the terms in blue, n choose k times p
to the power of k, don't depend upon j.
They can be pulled right out of the sum.
Let's promptly do this.
The terms in green inside
the sum depend on j.
And of course,
they cannot be pulled out of the sum.
But there are the form, minus 1 to
a power j times p to the power j.
In other words,
we have both terms to power j.
Let's combine them and
write it simply as minus p to the power j.
Now take a good look at that
sum on the right-hand side.
Does this look familiar?
Of course, it does.
This is another instance of
the generic binomial theorem.
If you remember, 1 plus x to the power
m can be written as a sum of
binomial coefficients times powers of x.
If we identify n in this
case with n minus k and
we identify x now with minus p,
everything is ripe for
an automatic application
of this binomial theorem.
Now let's properly do this.
We get n choose k times
p to the par k times
1 plus x replaced by minus
p is 1 plus minus p,
the whole to the power n minus k.
Now does this look familiar?
Assuredly, it does.
This is something we've spent a lot
of time playing with over the last
couple of weeks.
And you'll say uh-huh.
This is a binomial distribution.
This is exactly the binomial
probability attached
to the number of successes,
k in n tosses of a coin.
Whose success probability is p.
1 minus p,
of course is a failure probability.
We've, we've rediscovered the binomial
distribution, magically, beautifully.
What you've just discovered is that
the number of occurrences of the events,
A1 through An.
In other words,
the sum of indicators X1 through
Xn is governed by a binomial
distribution corresponding
to n tosses of a coin,
whose success probability is p.
This is lovely.
It is always gratifying to how old
acquaintances come back to us.
In retrospect, was it surprising?
When you think about it, it says,
what does this remind us of?
Of course, of repeated,
independent trials.
If we imagine that each of
the events A1 through An,
represented success or failure
outcomes for each of n coin tosses,
n distinct independent trials,
then this is precisely the setting.
And now of course,
it is not at all surprising that we've
discovered the binomial distribution.
What we computed in a number of events
that occurred is precisely the number
of successes in tosses of a coin and
so the binomial has emerged.
Now this is a lot of work to get
to the binomial distribution and
assuredly our earlier approaches were
much faster and much more to the point.
They got us to the binomial
distribution quickly and efficiently.
Was there any worth in this exercise?
Of course, there was.
The fact that the binomial distribution
in marriages when you have independence
in these events A1 through AN is
gratifying and is suggestive that this
line of thought can now be extended
beyond purely combinatorial arguments.
On that when he looked a binomial,
we developed the distribution from
purely combinatorial principles.
This now tell us that if we go beyond
the provence of independent coin tosses,
if we go into a setting where there is
of dependencies, perhaps not excessive.
That this formulation is going
to lead us to answers beyond
the binomial to be sure, but
perhaps not too far from it.
And in particular,
this suggests that quantities that emerge
from the binomial might well hold
on a much larger scope and scale.
And in particular,
beautiful approximation to the binomial
distribution in the context of
rare events that he discovered in
1837 when he was talking about
jurisprudence in France.
This might well be something that
emerges and holds true more generically.
At any rate,
this is well worth investigating.
This is up next.

