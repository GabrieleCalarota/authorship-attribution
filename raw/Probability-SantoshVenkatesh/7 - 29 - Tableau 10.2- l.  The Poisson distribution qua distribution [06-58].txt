We have seen how Poisson's approximation
to the binomial was inspired
in the context of rare events,
where the success probability was
small in a sequence of coin tosses.
This gives rise to an approximation,
a function, a mass function of
an integer parameter k,
indexed by a parameter lambda.
Now, this, you know, more formal,
elaborate notation is what we called in
the previous slides, p o of k lambda.
But now that we are dealing in
the context purely of the Poisson,
and, without the parameter p to confuse
as a possible success probability,
we'll dispense with a more
elaborate notation.
And, write simply, going back, reverting
to our notation from mass functions.
P of k or p of k and
Lambda to denote a particular mass
function of the Poisson kind.
Now, well, originally, this,
these approximations or
rows as I mentioned in the context of rare
events, as we saw in the later examples,
the, these approximations
arise naturally as models or
phenomena where there are arrivals
in time at random instance.
Distributions of random points in space.
Think for a example of other
distributions of wars in time.
Think for example of the distribution
of bombs landing on London.
And it became clear over
a fairly long period of time that
the Poisson approximation was going
to take on a life on its own.
It was going to become an important
distribution in its own right.
And this leads us now to
considering the Poisson,
particularly in the context
of arrival processes,
where you have arrivals in time, a random
point in time, giving rise to an ensemble.
So the picture we have in
mind is something like this.
Imagine that time is flowing to the right,
and
that you have arrivals of people,
packets, cars
arriving at random points in time and
filling up, let's say, the time lapses.
Consider if you will a unit of time.
Let's say, one hour, or
one minute, or ten minutes.
Fix that unit of time.
A natural question then becomes, how many
arrivals are there in this unit of time?
And this is precisely where the Poisson
distribution comes into play.
So, let's start.
A Poisson variable, then think of a chance
experiment which produces a value and
a nonnegative integer value
which has an implicit sample
space as an unbounded collection
of nonnegative integers.
Of course, naturally, the question is what
is the probability measure of this space?
And clearly, we have a discreet space and
the natural measure is given by a mass
function which assigns to atoms,
in this case, the atoms of the
distribution, are the isolated integers k.
And to each integer k, we want to asso,
associate a probability.
This is precisely what we called
the Poisson probability p of k and lambda.
Underlying this experiment is a fixed
underlying positive parameter lambda,
which if you will the rate at which
arrivals are happening in this experiment.
So, formally, we will say the outcome of
this chance experiment is the Poisson
variable, with parameter lambda,
it's mass function,
hiding the lambda for the time being,
is P of k, given by the Poisson form
e to the par minus lambda, times
lambda to the par k over k factorial.
Now, naturally, we should verify
that this indeed is a proper,
a bonafide mass function.
But of course, we've already done this.
We've seen this in topic 6.1 as
an example of a mass function.
What do we have to verify?
Well, first positivity, but this is trite.
The exponential function
is always positive.
And second normalization but
this, as we've seen,
arises naturally as a consequence
of the exponential series.
The sum of lambda to the k over k
factorial is exactly e to the power lambda
and therefore,
the series is properly normalized to unit.
We now have a bonafide
distribution on our hands.
This is the Poisson distribution
with parameter lambda.
The context, the overlap of the binomial
is the setting where we have rare events,
such as probabilities are small.
In that regime, the Poisson and
the binomial overlap.
But the Poisson stands in its own right
to capture settings where you have
arrivals, arrival processes, queues.
Given such a function, it is natural for
us to examine it closely and
discern its key properties.
And naturally, the first thing it'll
want to do is visualize this function.
So, let's begin.
So, we start with
the Poisson probabilities,
here is your distribution.
And on your screen are displays
of the Poisson probabilities for
three particular choices
of the parameter lambda.
Lambda being 2.5,
7.3, and 12.4.
Of course, you understand that,
by virtue of proper normalization,
all of these probabilities for
a given lambda must add to 1.
Certain features are broadly,
immediately visible and superficially,
they are similar to the kind of features
we've seen for the binomial distribution.
For one, for any choice of lambda,
the distribution appears to be unimodal.
It increases and then decreases.
The distribution appears to have a center,
an expectation,
a probabilistic center of mass, centered
at or very close to the value of lambda.
The distribution is spread out around the
center of mass, around the expectation,
and the spread seems to increase as
the parameter lambda, increases.
And finally, there is a bell curve
which looks like an approximation
to these probabilities and
of course none of these are accidental and
we will turn to examining each
of these properties next.

