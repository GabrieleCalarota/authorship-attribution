To begin, we want to compute
a probability that a sample mean deviates
from a fixed but unknown
probability P by more than epsilon.
This is a bad event.
Okay, if this happens,
then our estimate is considered to be bad.
Epsilon is our tolerance,
whatever it is, 1%, 3%.
What have you.
Let's sort of massage this relationship
into a form where we can utilize
the result that we know.
First, let's clear the denominator
by dividing by n to write
this Sn minus n times p over n,
an absolute value deviates from epsilon.
Now, the numerator on the left
is beautifully centered.
Sn minus np is now centered
at 0 because we realize
that Sn has expectation n times p.
We would now like to scale
it in the proper scale.
The proper scale goes as
the standard deviation of Sn.
Would we recall is the square
root of n times p times q.
Accordingly, let's rearrange the terms by
making the denominator square root of npq.
And to make things work out, I'll have
to take the square root of n from
the denominator, and
move it into the numerator on the right.
And divide a new, the right-hand side
by the square root of pq, and now,
I've got an identity.
All right, this looks promising.
Inside the absolute value on the right,
I've got exactly my properly centered and
properly scaled version of Sn,
what we called Sn star.
And so our,
our equation now becomes a probability
that in absolute value Sn
star exceeds a given quantity
which depends upon our tolerance epsilon,
it depends upon the sample size n,
and it depends upon the unknown
probability p and q, which is 1 minus p.
Let's simplify our
notation a little bit and
take this entire ratio on right and
give it a name.
Let's call it say, z.
So z stands for epsilon,
times the square root of n
divided by the square root of pq.
And so our desired probability
can be written very compactly
as the probability that an absolute value,
Sn star exceeds z.
Now, let's take stock and
figure out what we have done so far.
Z is something which is governed
by the air of tolerance,
the sample size, and
also the unknown probability p.
No matter, z is some fixed number.
An absolute value Sn star
exceeding z means that Sn star
is either to the right of z or
Sn star is to the left of minus z.
And how do we compute a probability?
Well, we simply sum the atomic
masses in that range,
sum all the atomic masses of Sn star to
the right of z and to the left of minus z.
And that should give us the probability,
Bob's your uncle.
Of course, an explicit computationalist
probability is intractable.
The binomial is not
friendly in that regard.
We can't hope to get a closed
form answer for this.
But the normal approximation of the and
Laplace tells us that this
probability can be approximated
well if n is even modestly large
by the area under a bell curve.
Normal approximation beckons.
So, we start with a bell curve.
This is the function we
called little v of x.
If we wanted, we could write down
an explicit functional form,
though that is not requisite here.
It's really e to the minus
x squared over 2,
over the normalization, which is
the reciprocal of the square root of 2 pi.
But no matter,
this is what the curve looks like.
And all we have to do is
identify the point z and minus z.
Look at the area to the left of minus z,
the area to the right of plus z,
and add them up.
And that gives us an estimate for
the probability of the right.
This is beautiful,
we can do some simplifications here.
When we observe that by
the symmetry of this function,
the area to the left of minus z is exactly
equal to the area to the right of plus z.
And therefore the entire probability
is approximated by twice the area
to the right of z under the normal curve.
Now what do we need from this.
Well in a bell, we are trying to
estimate the probability of a bad event.
That is that our estimate, sadly,
differs in absolute value from the thing
I'm trying to estimate, the population
propulsion, P, by epsilon or more.
However tiny the epsilon that
is the desired tolerance,
I want to be inside that epsilon.
If this ever distriggered then something
bad has happened that is in other words
i'd like to have a high confidence
that something bad doesn't happen.
In other words I would like
the probability of something bad happening
to be small.
How small?
Well of course, no more than our
given confidence parameter, delta.
Now delta is broken up into two pieces.
A portion where Sn star
is larger than z and
a portion where Sn star
is smaller than minus z.
By asymmetry they are equal in size,
at least in a normal approximation,
and therefore we simply apportion,
equal confidence.
Delta over 2 and
delta over 2 to the both sides.
Together, they add up to delta.
Oh, very good.
At this point, we simply say that our
operating principle is as follows.
Remember, how did we select z?
Z depends in a complicated way on epsilon,
the error tolerance on the sample size n,
and on the underlying fixed but
unknown probability p.
If we select z to be that unique number.
Such that the area to the right of z,
under the normal curve, is delta over 2.
Now, let's see what happens.
If we select z to be 0,
the area to the right is exactly half the
area to the normal curve, which is unit.
As z increases,
the area of the normal curve to the right
decreases from one half steadily
all the way down to zero as it become
very, very large and goes in infinity.
And therefore, at some specific
point there's a value of z,
at which the area to the right
is exactly delta over two.
The moment that happens,
we've got our confidence guarantees.
Select the value of z,
based upon this delta that is given to us.
This is called the quantile of the normal.
All right,
we are reverse engineering a probability.
Instead of saying give me a point and
then figure out the probability
associated with that area.
We are saying give me a probability and
then find which point gets
me to that probability.
Give me a z, z which has an area
of delta over two to the right.
Once you have that quantile.
At just the value of n, to try to get a
value for epsilon times square root of n,
divided by the square root of pq,
which is at least as big as that z.
Because n is an integer, I might not
be able to hit that quantile exactly.
In which case, go to the smallest
integer which gets me to the right of z.
Now the moment that happens the area
in the tail, the right tail and
the left tail,
is now assuredly no more than delta.
Here is our operating principle.
You fix an epsilon, you fix a delta.
Find the quantile
corresponding to delta over 2.
Let's use z.
Once you have that z,
select an end big enough such that
you get at least the value of z.
The moment you do that we've
got a normal approximation.
The desired probability in the limit of
large n is approximately twice the area
under the normal curve from z to infinity,
the area to the right of the normal curve.
And that, by construction,
is twice delta over 2, or delta.
This is lovely.
We should promptly take stock,
and savor this beautiful result.
Recall, what have we done?
We're given an epsilon and a delta,
an error of tolerance, and
a confidence parameter.
We want a confidence of one minus delta,
where the error is no more than epsilon.
What is our procedure?
Find a quantile z for which the area under
the curve to the right
of z is delta over 2.
Now select a sample size n big
enough such that epsilon times
the square root of n divided by the square
root of pq is at least that value z.
The moment you do that you've got a,
a probability which will not exceed delta.
Okay, immediately now.
This tells you what the size
of n is going to be.
Reverse engineer it, right.
Move the square root of pq from the
denominator to the numerator on the left.
Square quantities, move the epsilon
square away and we say that we'll require
the size of n,
the number of data points the sample size
to be at least the quantile z,
which depends on delta squared,
times p times q, which is 1 minus p,
divided by epsilon squared.
Unfortunately, the sample size requisite
seems to depend upon the unknown p.
But recall that you've already seen, when
we looked at the Chebyshev's inequality
for the binomial, that p times 1 minus p,
p times q, never exceeds one-quarter.
In fact, it achieves one quarter
exactly when p and q are one-half.
And therefore,
it will be sufficient to select for
n, the value, the quantile squared
divided by 4 epsilon squared.
And this selection, under a normal
approximation, will work for
any value of p.
Here is a crisp, beautiful and
simple estimate of the sample
size needed for a given problem.
P is unknown.
We are given an error total,
and it's epsilon.
We are given a confidence
acquired of 1 minus delta.
From here, we compute a quantile,
which is just an area under a fixed curve.
We can do this numerically.
And today an, a calculator on the internet
can give it to you instantaneously.
You now have z of delta for
the given delta, square it, divide it by 4
epsilon squared, and that gives you
an estimate of how big n has to be.
We should promptly put in some numbers
here to see how these terms of error,
confidence, and
sample size is playing out.
So the requisite statistical guarantee
is that the chance of a deviation
of epsilon or more should be small,
should be no more than delta.
Here now is a numerical calculation.
So we start with, let's say,
epsilon running from 0.1 through 0.03.
In other words, 10%, 5%, and 3%.
Look at confidence is required of,
say, 90%, 95%, 95%.
Promptly compute the quantile.
The z delta requires that the area
under the curve to the right of,
of, of z should be no
more than delta over 2.
A numerical computation shows that
when a confidence requisite is 90%,
delta as .9, the associated quartile z,
is about 1.6, 1.7.
More precisely, around 1.65
truncated to two decimal places.
And if the confidence required is
more acute, say, a 95% confidence,
we want more confidence in the answer,
then of course the z is going to go larger
because we want to a capture a smaller and
smaller portion of the area to the right.
How large?
In fact, the quanitle corresponding to
a 95% confidence is almost exactly 1.96.
Again, truncated to two decimal places.
The penultimate column shows
us the Chebyshev estimates,
most all, from 250 to about 5,500.
The last column tells us what
the estimate of the sample size are,
for a normal approximation.
Observe that there's almost
a factor of 5 and 6.
And if you look at the last row,
where we wanted an error of 3% and
a confidence of 95%, this is a sort of,
a default that people will refer to.
We see that the sample size
estimate has dropped from
around 5,500 to just over 1,000.
Now, admittedly, we waved our hands
fairly briskly here to get there.
We were appealed to a theorem
which I did not prove to you.
And my defense here is that, well,
this is going to need more machinery.
Okay.
But if we assume the result,
then we've got at least as
an approximation,
an estimate of a sample size.
How does the error of
the approximation affect this?
Is it a serious error?
And generally, the answer is no,
not very much.
It typically affects it in
the second decimal place,
which means that your sample size
estimates don't change by very much.
For example, as a rule of thumb, a sample
size of about 1,200 will take care
of all the errors in the approximation,
very nicely, thank you.
And get you an estimate which is
uniformly good for any value of p.
It will give you an error of no more
than 3% with a confidence of 95%.
I shall not try to make this any
more precise at this moment.
What I'm going to try to convey
to you here is the following.
That elementary methods dealing
with independent variables
already give us potent and
powerful tools with deep and
subtle applications in commonplace and
important problems.
And so
we shall be satisfied with a slogan.
We have seen this slogan before.
A relatively small, honest random sample.
What does honesty mean?
Well, that it is actually
independently drawn.
That there's no bias.
And small, well we've seen that for
example, if we want a guarantee of
an error of no more than 3%,
and a confidence of 95%,
something like a little more than
1,000 individuals will suffice.
A relatively small honest random
sample will give us good estimates.
Of an underlying population, and
a sample size is independent of
the size of the underlying population.
It's the same whether the under,
underlying population is tens of
thousands, or millions or billions.
And ultimately, this
why polls really work.
The normal approximation gives us
estimates, which are implementable without
an inordinate cost and
gives rise to accurate estimates.
Of course the devil is in the details and
the next time you look at a poll in
the newspaper or a report on the news, you
should cock a glance at the fine print.
Was the sample really random?
Do we have reason to
believe that it was honest?
And what was the sample size?
The normal approximation to the binomial
tells us that relatively small samples
suffice.
Something like 1,200 would work for
errors of 3% and confidence of 95%.
But we can't pare too
much meat from this bone.
If the numbers get too low,
then our errors blow up and
our confidence becomes negligible.
So for example, a poll says, reportedly
gives rise to accurate estimates and
you look at the fine print and it says,
well we polled 300 individuals.
Well that poll is worth, well,
the paper it was written on.
Discard it with [INAUDIBLE].
This is also the reason
why drug testing works.
Yes, we're still going to
need largish samples, and
it's going to be enormously
expensive to get it done right.
But the numbers are still
potentially manageable.
And so we come to this.
The normal approximation to the binomial
gives us apodictic evidence for
why polls really work,
why drug testing really works.

