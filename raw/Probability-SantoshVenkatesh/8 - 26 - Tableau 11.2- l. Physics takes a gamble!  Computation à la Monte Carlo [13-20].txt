So, we are now come to it.
The bewildering idea of
a chance-driven computation.
This idea has fueled a cottage
industry of methods,
chance-based, which percolate
through the landscape.
And we have the catch-all phrase,
tthe Monte Carlo method to
capture this ensemble of ideas.
But, we're jumping ahead of, of ourselves.
Let's come back and
summarize our key discoveries so far.
The problem?
We are given a bounded function of d
variables, let us say, on the unit cube.
And let's say the function was bounded,
an absolute value by 1.
We wish to compute its integral,
a number fixed, but
unknown at this point that
we are going to call J.
So we want to compute or estimate J.
Our process is going to involve
a random selection of a point in
the D-dimensional cube and an evaluation
of the function at that point.
Of course, this gives us a chance-driven
evaluation of the function.
No matter.
So, what have we discovered?
That the expected value of the function
evaluated at a random point
is precisely the parameter
J that we are seeking.
And that the expected squared spread of
the function value around its center,
the variance is bounded by unit.
We're immediately now led to the idea
of generating a random sample
by independent selection from
the unit cube of D-dimensions.
In other words,
I give a sequence of points.
Now notice the virtues of vector notation.
Each boldface uppercase X,
represents a collection of d variables.
But I'm going to do this experiment
n times independently so
we have repeated independent trials.
Each of these vectors is
a unique trial in its own right,
we need to identify which trial.
So let's use now superscripts because
subscripts are getting overloaded.
So our superscript 1
indicates the first trial.
A superscript 2 indicates a second trial.
And a superscript n
indicates the nth trial.
I'm going to think about
n as a modest number.
Thousands, tens of thousands,
a million, perhaps.
But not much larger and of course,
even a million evaluations of a function
is not very much for
a modern day computer.
It can do it in a trice.
So, we are going to start with
a random sample, X1 through Xn with
the same underlying distribution as
a random X chosen from the unit cube.
And what do we do next?
Naturally, we evaluate
the value of the function.
We look it up at each of these
n randomly chosen points.
And let's call the values that result Y1,
Y2, through Y sub n.
Now, what can we say about these
derived values, Y1 through Yn?
What do we know about them?
Well each Y depends upon a unique X.
And the Xs are drawn independently.
And therefore, we immediately conclude
that Y1 through Yn represents a random
sample, a sequence obtained by repeated
independent trials of some experiment.
The experiment at hand, of course, relies
upon random selection of the unit cube,
and then evaluating the value of
the function at various points.
Moreover, these values,
Y1 through Yn have a common distribution.
Why?
Because, of course,
the Xes have the same underlying
uniform distribution of the unit cube.
And therefore,
all the Ys have expectation exactly j.
And all of the Ys have
variance no more than unit.
Okay, and the moment we've got
an independent sample, like this,
we know immediately that the law of
large numbers beckons invitingly to us.
So, what is the law of large numbers
going to say in this context?
Well first,
we want to form a sum of the values Y.
Of course, these are just a sum
of the values of the function,
which is given to us at fixed but
random points 1 through n.
Okay, so, you pick X1 through Xn randomly.
You evaluate the function at those points,
and add up the values you get.
That is all it is.
And let's use our old notation again.
Let's call it S sub n,
an accumulated sum of function values.
But, expectation is added here.
And therefore the expectation of
the sum is naturally just the sum of
the individual expectations.
But each Y has got a common expectation,
J.
And therefore the expectation
of the sum is simply n times J.
What can we say about the variants?
Well, the variance of Sn is likewise
the sum of the individual variances
because we know that Y1 through
Yn are independent variables.
Each of the variances
is bounded by unit and
therefore the variance of Sn is no
more than n times unit or simply n.
And as a law of large numbers
immediately tells us that the sample
mean Sn over n is going to be concentrated
near its expectation which is exactly J.
And so
now we have a principled estimate of J,
by the underlying sample
mean from this process.
Sn over n should behave like J.
This procedure at its heart is what
we today call the Monte Carlo method.
So, where does the Monte Carlo
method arise from?
Where does the name arise from?
Well, the story is probably but it,
it goes that, von Neumann,
one of the originators of the method,
was amused by the chance-driven
context for this.
He said,
we are going to get out a fixed value,
an integral, by essentially,
gambling, by a game of chance.
And the connotations of the gambling
houses at Monte Carlo at games of chance
led to him suggesting the name
Monte Carlo for the method.
And ever since then, we call such
methods which rely upon computation
using random generations of points,
we call all of these Monte Carlo methods.
Now, this is all well and good, but
naturally one of the questions that
we would ask is, well,
we want to evaluate a J for
some mysterious purpose in physics or
somewhere else.
And you say, well,
play this mysterious game of chance.
Computer sample mean, divide by n and
that's an estimate of your
underlying unknown, J.
How good exactly is this estimate?
Let's take a prompt look at this.
So our basic question is, how well does
a sample mean estimate the parameter J,
the integral that we
are trying to estimate.
Are they going to follow
the same path as before?
First, let us specify a, an error
tolerance that we are willing to sustain.
Say we are willing to sustain a 10% error,
perhaps a 5% error,
a 3% error, maybe a 1% error.
Well, of course, we don't want to a priori
restrict what they're tolerance should be.
So we'll inevitably call
the amount of error in J that we
are willing to tolerate,
we'll call it epsilon.
And so, our basic question will be,
what is the probability that a sample mean
Sn over n deviates from its expectation,
the effects for unknown J,
an absolute value by more than epsilon?
In other words,
you are beyond error tolerance.
We would like this chance to be small.
Of course, we recognize
the structure of the analysis.
This is good for exactly the process
by which we analyzed polls,
by which we derived
the law of large numbers.
We begin by clearing the denominator.
Multiply throughout by n.
And so this expression becomes equivalent
to saying this is the probability that Sn
deviates from n times J in absolute
value by more than n times epsilon.
N times J is just the expectation of Sn.
And now, Chebyshev's inequality
beckons invitingly to us.
We know that this probability is
bounded about by the variance of
Sn divided by the square of the deviation,
the deviation here is n times epsilon.
So the square of that is n
squared epsilon squared.
We have an estimate for the variance.
Well, we haven't computed it exactly,
but we know that the variance
of Sn is bounded about by n.
And so the inequality keeps
growing in the proper direction.
The probability of
a deviation by epsilon or
more is no more than n over
n squared epsilon squared.
What have we got here.
Okay.
What we want,
remember epsilon is the tolerance
to area that we have presupposed.
So we want an area of larger of
epsilon with a small probability.
That is going to be our
confidence parameter.
So, for example, we want, let's say,
an error of no more than 3%
with a confidence of 95%,
which means that exceeding 3% should
have all the chances of no more than 5%.
If the right-hand side in
the expression does not exceed 5%,
then, I can guarantee a confidence of 95%.
Of course, I don't want to prescribe what
the confidence in your pronouncement needs
to be a priori.
It's going to be dictated by
the needs of the problem.
All right, so
more generally, we're going to presuppose
a given error tolerance epsilon.
We are allowed to make
a mistake up to epsilon.
Or, if we understand
the chance-driven game.
There's a chance that this might happen,
that we get a large
error more than epsilon.
We'll require a confidence that error
is no more than epsilon to, we say,
within 1 minus delta.
Delta as a confidence parameter.
This is also given to us ahead of time.
And now,
we've got a very simple observation.
If the right hand side,
1 over n epsilon squared,
does not exceed delta, then immediately
we can conclude that a sample
mean is going to estimate the fixed but
mysterious and unknown J.
With an error of no more than epsilon and
a confidence of at least 1 minus delta.
Rewrite our expression around the quality
and we find that the sample size
needed to estimate J
with an error of no more than epsilon and
a confidence of at least 1 minus delta.
Is no more than 1 over
epsilon squared times delta.
Let's put in some numbers here to
get a feeling for what's going on.
Suppose we want an error estimate which
gives you a fairly tight tolerance of,
say, no more than 1% error.
And you want a confidence of quite high,
say, 99%.
Epsilon is 0.01 and delta is also 0.01.
We plug it into our equation.
And we find that n needs
to be only a million.
If you sample a million point
at random inside the unit cube,
evaluate the function at
those million points.
Add them up and divide by a million.
The law of large numbers guarantees that
your sample mean estimates
that unknown integral J
with an error of no more than 1% and
a confidence of 99%.
Isn't that remarkable?
Remember, when d was 200 we started
with an apparently intractable problem.
It needed 10 to the power
of 95 computations for
a crude computation using
an elementary form of Simpson's rule.
Now we are saying forget
10 to the power 95.
I just need a million computations.
Of course, a million is nothing for
a modern computer.
You've now apparently solved
an intractable problem.
But there are some caveats.

