So, here are two desiderata.
How should we construct such a test?
So let's begin.
What do we anticipate from Sn1,
the number of elements in
the sample in the first IQ bin?
We anticipate that it should
be close to n times p1.
What about Sn2, the number of elements
in the sample in the second IQ bit?
We anticipate it should be
close to n times p2, and so on.
And so it becomes natural to posit
a test where we look at the difference,
or the distance from each Sn
value to its expected value.
Now surely,
following Korner's second maxim,
we want to take into account all the data.
So why don't we simply look
at how far Sn1 is from np1,
how far Sn2 is from np2, and
so on, and add them all up?
Would this be a good test?
What we anticipate, is that if that sum
is very large, then that would suggest
that at least one of the Sn values is
far from where we expect it to be.
Now, superficially this sounds reasonable,
but
then we realize very quickly,
that we are going to come a cropper
because the Sn values could be distributed
on either sides of the expectation.
And it is entirely possible
to get a small sum where one
element diverges far to
the left of its expected value.
In other words, we have a large negative
digression, and another value accidentally
cancels a large negative component by
a large positive digression to the right.
Oh, so we could have accidental
cancellations of large positive and
negative numbers giving rise to
a spurious feeling of normality of,
of that everything is okay,
because the sum could be small,
even though elements could be very large
positive, and very large negative.
So this is not a good test.
What could we do?
Well, we realize that we
are not interested really,
whether Sn is to the left or
to the right of its expected value.
We only want to know how far away it is.
And so a natural idea might be,
why don't we simply look at
the square of these values?
Now if you add them all up,
if that sum is large, then we know
that at least one of the Sn values
is far from where we expect.
This is beginning to sound promising.
But on introspection,
it falls foul of Korner's second maxim,
that we should make an effort to take
into account all the data fairly.
What does this mean?
Recall that the first IQ bin
has a very small probability,
0.02 or something like that.
The IQ bin in the middle has got
a relatively large probability, ten times.
Which means that in a random sample,
there are going to be very
many more elements in the middle,
then there are at extremes.
Which means that the estimates
in the bins of the extremes,
are going to be underrepresented.
They're not going to be
fairly accounted for.
To account for them fairly then,
we should re-normalize by essentially
the chances of falling into the bins,
in other words, by the varlability.
And so it might come to our mind that
the way to do this is to scale it so
that everyone is in a common scale.
And naturally, we'll scale by
something like the variability,
something proportional to the variance.
Now we've got a sum of eight
contributions to deviations
from where we expect these values to be.
The contributions capture positive
values because we are squaring
the normalized scaled by
the variability of each of the bins and
now we have eight components each
contributing in fair and just proportion.
Pause for a moment to admire the elegance
and beauty of such a simple expression
which we obtained by
simple basic principles,
the maxims that I touted earlier.
Now, yes, from a larger break
point of view it's a little messy.
But surely for a computational
point of view this is very easy.
How do we do this?
Generate a sample.
Good.
From what?
From an underlying normal law.
Good.
From the sample, categorize
the elements into which bins they fall.
Determine the values Sn1, Sn2,
through Sn8, very simple.
That's simply a matter, matter of
putting them into bins and counting.
Compare each to the pre-calculated values,
n times p1, n times p2, n times p8.
Remember, n is the sample size, it's
known to us, nominally one thousand, but
it could be anything you like.
P1 through p8 were computed by
looking at the areas under the curve.
We know these values.
Take the difference,
that's a piece of cake.
Square it, simple observation.
Divide by n times p1 through n times p.
That's easy.
Add them up.
This is a computationally
trivially simple procedure.
We can even conceive of doing it
by hand if n is not too large.
But assuredly,
on a modern day digital computer, this
can be done in a fraction and a trice.
This is called a test statistic.
Okay, the word is peculiar to statistics,
it means something obtained
by massaging the data.
A simple numerical calculation
using your sample.
And this particular sum
has been given a name,
which has become enshrined by
reasons of tradition and history.
The name we give it is
the greek letter chi, squared.
So that sum is called chi squared and
it was articulated by
Karl Pearson as a test in
settings like this in 1900.
So this kai squared meaning that sum of h,
positive quantities.
It's called the chi-squared
test statistic.
Now, this is all well and good,
at least this is a principle statistic.
The idea is that this is a big number,
whatever big means.
Then we are led to feel that
there is an aberration.
The data are not representative
of what they should be.
So this is the high-level intuition.
But all this intuition and
the philosophy will be worthless
if we cannot characterize what
the probability law of this statistic is.
Remember, chi-squared is
a chance-driven entity.
It's a number, a positive number
obtained from a random data.
What law characterizes it?
A clue is given to us by the fact that
each of the values Sn1 through Sn8
is binomially distributed and
they are courtesy.
They did more relop last year and
individually, approximately,
normally distributed.
So this looks like we are summing
the squares of normals and adding them.
There's going to be a complication.
The complication is that these values
Sn1 through Sn8 are not independent.
The reason being, that the moment
you put some elements into one bin,
you have fewer elements
to put in other bins.
Right?
So, for example, if Sn1 is n,
then everything went into the first bin
and we know that all the others are zero.
There are dependencies across these.
But none the less,
these dependencies can be handled,
the analysis is not at all easy.
But, we understand now
that whatever the law for
chi squared is, it is good to emerge
from an underlying normal law.
This was calculated by Karl Pearson, and
it turns out to have a beautiful,
elegant representation.
And I'm going to have no compunction
about showing you the density for
this law without telling
you how to get it.
After all, I haven't shown you a proof
of the central limit theorem either.
And so I would hope that certainly that
you would at least be temporarily willing
to take this on faith, that a law of
chi-squared can be characterized.
It is called a chi-squared
distribution and
it is said to have seven
degrees of freedom.
The terminology arises because you see
the sum of sn1 through sn8 is exactly
the sample size n.
In other words, any seven of them
completely determine the last one.
You only have seven degrees of freedom.
But I don't want to get into
the technical details here.
So this object, whatever it is,
arises from an underlying normality.
And it's called the chi-squared
density with a certain
number of degrees of freedom.
As I told you, the originator of
this idea was Karl Pearson, and
he was considered to perhaps be
one of the fathers of statistics.
In the very early part of the 20th
century, he, together with R.A. Fisher in
England, laid down much of the framework
of what we use in modern statistics.
I cannot resist the temptation to tell
you a little anecdote about Karl Pearson.
He was a formidable man,
sometimes an erasable man.
There's a biographical
sketch provided by H.M.
Walker in 1978 in the International
Encyclopedia of Statistics,
in which he attempts to sketch
the character of this formidable
intellectual and here's what Walker says.
Pearson was asked what was the first
thing that he could remember?
He said it was sitting in
a high chair sucking his thumb.
So presumably he was very young,
a few years old.
A person came to him and said that
he should stop sucking his thumb,
and that if he did not it
would whither and fall away.
I took out my thumb, said he,
put my two thumbs together,
and looked at them for a long time.
The thumb I suck does not look in
any way smaller than the other.
I wonder if she could be lying to me.
And there in the simple anecdote,
in one fell swoop you have one,
a rejection of constituted authority,
in the form of, presumably, his nanny.
Two, an appeal from a child
to empirical evidence.
Three, a belief in his own interpretation
of the meaning of observed data.
And four, finally,
an imputation of moral obliquity
to a person whose assessment
differed from his own.
And there we have it, Karl Pearson,
at presumably age three or four.
Now with this understanding
of a statistical test we now
have a way forward to test bird's IQ data.
The question is, are the numbers
reported for adults, children, abberate?
Well how shall we do this?
Well let us begin by constructing
a 95% confidence interval.
What does that mean?
Select an interval where the chance
of the chi-squared value falling in
that interval is .95.
Why 95%?
Well choose what you will.
Replace it by 99%,
you get a slightly different interval or
90% you get slightly bigger interval.
It does not.
It makes little matter.
Pick a number, a confidence
level that you are happy with,
let's say 95% for definiteness.
Now there are many ways
of selecting an integral
which captures 95% of the area
under the chi squared density.
A natural way to try to do this is to
sort of split the difference, as it were.
To choose an interval where two and a half
percent of the area is to the left, and
two and a half percent of
the area is to the right.
And if you did that, what we'll have done,
is essentially,
identify a left quantile, a value a,
to the left of which the area
under the curve is .25.
Together with a right
quantile B chosen such
that the area under the cab to
the right of b is also .025.
And a numerical calculation for this
particular density with seven degrees of
freedom shows you that if you want
a 95% confidence interval arranged
symmetrically like this,
then the left quantile is about 1.69.
The right quantile is about 17.01.
Now we've got a confidence interval.
If you generate a random sample and
then compute
a statistic, a function of the data, a
chi-squared, and if the chi-squared falls
outside that interval, we will be
inclined to treat it as aberrant.
Here are two rejection criteria for
potential putative data.
The first, what happens if you get data,
you compute a chi-squared value using
this very simple formulation, and
you find the chi-squared value
exceeds the number 17.01?
Well, you know the chance of this is
very small, it's two and a half percent.
We reject the data with contumely on
the grounds of an excessive irregularity.
But there is another possibility.
This possibility that I've outlined for
you is what is typically done
in a statistics package, but
the form of the density also tells us
that not only are large values for
chi-squared not to be expected, but
that very small values are also suspect.
And so, a second part of this test
could be that if chi-squared,
the value you compute from your data,
is smaller than 1.69,
we again reject the data.
But this time,
on grounds of a too suspicious regularity.
It's too good to be true.
If every one of the Sn values
lands exactly where we expect it,
that is beginning to look a little
like somebody cooked the books.
It is making the case for fraud.

