The student may find it remarkable
that this elementary analysis
of this most basic of chance experiments,
is actually of very recent vintage.
A study of coin tossing along these lines,
was published by J.B.
Keller, the American Mathematical Monthly,
as recently as 1986.
Quo vadis?
Latin for where do we go from here?
It would be as well to step back,
to get a bird's eye view, as it were,
of the larger province of natural
problems that lie within
the scope of such investigations.
Now to begin, a student might say, well,
what if we alter the conditions
of the coin tossing experiment?
The student may well be willing to believe
that if the coin were
allowed to bounce instead.
That nothing essential
changes in the analysis.
And if she has some algebraic dexterity
and a little knowledge of physics,
she might well be inclined
to try the analysis herself.
The conclusion remains the same.
Under most normal conditions, the toss of
a coin appears to have a random character.
The early uncertainties
tend to get washed away
in the process of actually tossing
the coin and capturing it.
At a larger move,
one could ask, what about other
physical dynamical systems?
Does chance arise naturally,
in such settings as well?
Let us begin again, with a historically
important example, but one which has been
made familiar by Ian Fleming's,
intrepid protagonist, James Bond.
In his fondness for
the game of roulette in gambling casinos.
Here's a, an image, an 18th century
cartoon of a roulette wheel.
And here is a modern roulette wheel.
Now, the actual mechanics of the game
are not important for our purposes.
All that matters is that one has a wheel,
which is spun.
And a ball spins round the wheel and
settles somewhere on its periphery.
And the gamblers bet on
the location the ball ends up in.
Okay.
Now, if one is a re, was a, was gambler,
if one were to go to a casino,
then one would like to have some kind
of assurance that the wheel were fair.
That the croupier could not by sleight
of hand affect where the ball would end
up, or the circumference of the wheel.
Is that really the case?
An analysis of this problem was
actually undertaken just over a century
ago in 1896.
And if you're like me, you like to get a
flavor of what the time was at that time.
This is an image of Paris in 1896.
It's due to [INAUDIBLE], it's [INAUDIBLE].
And here's another image
of Paris at that time.
Scène de rue à Paris
due to Maximilien Luce.
The year was 1896,
little more than a century ago.
It was a time of hansom cabs, top hats.
It was a time of empire.
But the mathematical problems were in
a sense then same then as they are now.
An analysis of roulette was undertaken by
Henri Poincare, who at that time was
teaching at the Sorbonne in Paris,
and here's an image of the Sorbonne.
This is actually a 17th century image
before a famous fire at the Sorbonne,
but presumably in the late 19th century
the Sorbonne had not changed in essence.
This is the University of Paris,
the Sorbonne.
And while Poincare was there,
Poincare was a,
a noted topologist a man of many parts.
And while he did not publish
in the theory of chance
he actually taught
a course in probability.
And he must have been
a remarkable lecturer,
because among the many problems he
analyzed was the game of roulette.
And his lectures were published in a book
in 1896, the Calcul des probabilités.
It's a beautiful little monograph.
If you're curious,
you can get it online for free.
And it's a wonderful historical object and
also beautifully written.
The problems are engaging and
they illustrate
a probabilistic mode of thought
in the most prosaic problems.
And this way of thought turned
out to be very, very powerful.
Now, Poincaré's ideas on roulette
could be used in other settings.
What other kinds of games of chance?
What other settings and practice do
we see where chance arises apparently
inevitability as part of the process?
But these are in essence physical systems.
For example, you know, one can think of,
are the game of billiards.
Many balls on the tabletop.
One hits the ball against other balls.
The balls scatter.
Where do the balls land up?
Up?
This is a completely
deterministic problem.
Just as a roulette wheel where,
given initial conditions,
one knows exactly in principle
where things should end up.
But of course with many balls,
even small variations in initial
conditions cause potentially dramatic
changes in where the balls end up.
Now, if one goes to a larger scale, if one
images, let's say, a room and instead of
billiard balls, one has, let's say, atoms
or molecules interacting with each other.
Bouncing off each other.
Then again one ends up with
a deterministic system,
which is well described
by a stochastic law.
In the,
at the atomic level this was famously done
by none other than
the luminary Albert Einstein.
In the year 1905 the Annus Mirabilis,
where the four papers of
Einstein changed the world.
In one of them,
perhaps the least heralded of them,
Einstein talks about how to
measure Avogadro's number.
The student might remember Avogadro's
number from a basic chemistry classes
as representing a number
of atoms in a mole of gas.
How does one measure
this mystical quantity?
Einstein proposed a methodology,
where he imagined many, many,
many atomic particles
Attracting with each other,
according to a common stochastic law,
today we call that Brownian motion.
Now based on Einstein's paper, J.B.
[INAUDIBLE] in Paris,
three years later in 1908,
conducted very precise experiments
which resulted in very accurate
measurements of our Avogadro's number.
Today it's about 6.023 times
ten to the power of 23.
Parens Labis resulted in
his getting a Noble Prize.
So, there's actually rewards
in this game as well.
Moving on.
If one lose to the game of billiards.
Imagine throwing a dart at a dartboard.
Again, the outcome should be completely
prescribed by the initial conditions.
Completely determined by
the initial parameters.
Speed, velocity, direction,
and so on and so forth.
In practice there's uncertainty in
where the dart actually ends up.
If it were not, darts would lose,
lose its appeal would it not.
And again,
we have a deterministic physical system
with apparently a random component.
Pendulums on a swing where various
parameters are uncertain, for example,
the masses,
the lengths of the cords, the speed at
which they were sent in oscillation.
The elasticity of the cords,
games of dice, blocks and
springs, and of course,
inevitably, the toss of a coin.
And if we go clockwise
around from upper left,
what I've shown you is systems
of decreasing complexity, but
all of which seem to have,
ultimately a chance-driven element to it,
though all of them are at heart
deterministic, physical, evolving systems.
Now Poincare's idea turned out to be very,
very powerful.
It was built and
expanded on by Eberhard Hopf in the 1930s.
And given the rocketed name,
the principle of arbitrary functions.
Now, what this is intended to,
to communicate is that the initial
conditions parameters,
elements that make up a physical
system have some uncertainty in them.
The uncertainty might vary
from instance to instance, but
no matter what the initial uncertainty is,
the outcome
always seems to be regulated according
to a common stochastic principle.
And therefore, since there is no
dependence on an arbitrary initial
condition, this entire body of ideas is
called a principle of arbitrary functions.
We could summarize this principle loosely,
but
the gist of it is clear in these lines.
Uncertainties in our knowledge of
many dynamical physical systems
tend to wash away, as the systems evolve.
In the long term, all these
uncertainties morph into a common
stochastic description of
how the system is evolving.
And thus, regardless of the initial
speeds, that we impart.
And the variation in individual speeds,
from individual to individual.
At the end of the day, the result of
the toss of the coin, is a fair coin.
It is unpredictable.
About half the time one gets the heads and
about half the time, one gets a tail.
This lecture was intended to show
that even commonplace physical
systems could be suitably
modeled by chance phenomena.
Of course, if one goes down to the quantum
level, then chance is inescapable.
Chance is a pattern of fabric,
of particles at a quantum level.
But even at macroscopic
deterministic physical levels,
chance appears to play
an ineluctable part of the system.
In our next lecture, we'll now move
on from the fabric of probability,
the axiomatic foundation,
to the next step,
where we ask, how does one incorporate
evidence that is obtained in
the performance of a chance experiment?
Evidence, information on the side,
about the outcomes that arise.
How can one incorporate,
in a very systematic way,
such external information into
the probabilistic framework?
This will lead us to the important
idea of conditional probability.

