So let's take stock of, of where we are.
We are interested in the probabilities SN.
That there's a success run of length
five at or before the Nth trial.
And we've just decomposed it beautifully
into a sum of probabilities
F1 through F N.
Where the f's represent the probabilities
of a first success run occurring
at a certain point.
Another question now one imagines,
well, we've reduced all,
we've moved the problem from
computing s's to computing f's.
How does one compute the f's?
We appeal once more to additivity.
Let's pause and
think about what it takes for
a success run at to
occur at the Nth trial.
Here is a key principle, a success run
will terminate or occur at trial n if and
only if,
there is a first success run occurring
at some trial j at or before n.
Let's think about this again.
If a success run occurs at trial n
then either it is a first success run,
in which case there's a first
success run at trial n, or
there is a, a prior success run.
And if there's a prior success run,
eventually there must be a first
success run, which then must
have terminated at some trial j.
So the picture is something like this.
For some j between one through n,
there must have been a success
run that terminated at that j.
We now have a renewal.
The coin tosses effectively begin anew
with trial j plus 1 as a first trial.
And then proceed on through trial n where
we're told that there is a success run.
Of course, the success run occurring
at j is the first success run.
Following that, we have a new
sequence of tosses as it were, but
in that sequence of tosses,
we could have one or more success runs.
But you are guaranteed that there will
be one success run terminating at n.
This has broken the problem
up into two beautiful pieces.
In the first piece,
we have a first success run.
Which is occurring at trial j,
by definition the probability
of that occurrence is f sub j.
Now what about a second piece?
Now where we have a renewal, the coin
tosses have effectively begun a new,
with a new count.
J plus 1 is the essentially
the first toss in the new sequence.
And the Nth toss in the new sequence is
now the n minus Jth toss in the sequence.
There are n minus j trials following j and
leading up to n.
And in those n minus j trials,
we find that a succession
of phi ones terminating in a success
run at a last of those trials.
There are n minus zero trials with
a success run at trial n minus j.
And our notation at a terminal d tells us
the probability of that
is u sub n minus j.
Notice the trials are repeated and
independent.
We've got a statistically identical
version of the experiment
which has been started
again at trial J plus one.
And now we write N minus J trials,
culminating in a success run.
And therefore the probability
is U of N minus J.
Now let's stitch both pieces together.
The key is that the pieces
correspond to disjoint trials.
And recall the basic principle of
independence, events devolving upon
non-overlapping trials must
be manifestly independent.
And therefore, the occurrence of the first
event is independent of the occurrence
of the second event and therefore
the probability of an entire sequence is
just a multiplication of
probabilities f j times u n minus j.
Notice how elegantly and
simply independence and
product measure have finessed what could
have been a very difficult calculation.
Now, what we've discovered is that if
there's a first success run at trial j,
followed by another
success run at trial n,
the probability of that is
fj times u of n minus j.
Now we simply appeal to additivity.
To stitch together
the various possibilities for
where the first success run could occur.
If it occurred in the first trial,
then you have f one times u n minus one.
If it occurred in the second trial
you have f two times u n minus two.
If it occurred in the Jth trial you
have j times u n minus j, and so on.
If it occurred in the penultimate
trial you have f n minus 1 times U 1.
And if it occurred in the last
trial then you just have F N.
Of course, all these events are disjoint,
and by additivity, once more, U N
is a sum of the probabilities of the form
F J U N minus J on the right hand side.
Oh, the expression on
the right is not friendly.
It looks complex.
It looks like you've intertwined and
mixed things together, right?
But there is going to be a light
at the end of the tunnel.
To start with,
the sums of that form are endemic.
In mathematics, in the natural sciences,
in engineering, and in applied sciences,
such sums are called convolutions and
as you proceed,
progress through your mathematical
education, if you take a course for
example in transform methods or you
take a course in let's say mechanics or
physics, you'll find such
sums cropping up repeatedly.
The idea of a convolution term
is one of those basic principles
which capture essentially how we
can stitch things together and,
in consequence they occur repeatedly in
science, and mathematics, and engineering.
They tend to be a hugely
important concept.
It's hard to overstate how
important this basic operation is.
For our purposes, all that history in the
context is not so immediately relevant,
it's something we just store away
in the back of our minds saying,
okay, I've stumbled upon a convolution.
But for our purposes, it is simply
a manifestation of additivity.
The whole is equal to
the sum of the parts.
And I've got a particular expression.
Now it looks complex, it looks messy.
But let's keep our eye firmly
on the prize, on the target.
We are interested in F N.
The probability that there's
a first success run at trial N.
And we find F N appears on the right
hand side of the equation.
Let us isolate it by moving everything
else to the other side of the equation.
And write this equivalently
in the following form.
F N is given by an expression on the right
now, starting out with u n from which
you subtract out probabilities of
the form f j times u n minus j.
Okay, it hardly seems like we've made
any progress so I just re-massaged
something and rewrote it, but it's still
a clumsy awkward looking expression.
Can I do anything with it?
Oh, but here is the key point.
Gain.
We are interested in FN.
What do we see on the righthand side?
Well, we see various values for U.
U1, U2, U3 up to UN.
We also see the values f1 through fn
minus 1, and
this leads us to our next slogan.
If you know the sequence of the u's,
u1, u2, u3, and so forth, and
you know the first n minus 1 values for
the f, f1 through fn minus 1.
Then you can compute the Nth value for
the f.
This is the beginning of
a recursive calculation which says,
"Well, maybe I can compute the f, step
by step." First compute the first one.
If I know the first one
I know the next one.
If I know the next one I know
the next one after that.
And so on down like a chain
of dominoes falling over.
So far so good.
But of course this seems that we
have fought the problem off and
made a more difficult problem.
What now can we say about the U's.
Okay, this is our next step.

