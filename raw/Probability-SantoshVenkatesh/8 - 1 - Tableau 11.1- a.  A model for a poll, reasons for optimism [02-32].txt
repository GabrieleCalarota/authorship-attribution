[MUSIC]
>> Tableau 11.
The fabulous limit laws.
Part 1: Chebyshev's enduring inequality,
and the magisterial law of large numbers.
In Tableau 10, we had seen how by
concentration of polling systems,
we found the binomial distribution
emerging coyly from the ruins.
An analysis of the structure of
the binomial distribution showed us
that there were good reasons for
optimism in the conduct of polls.
But it left begging the question
of how effective a poll really is.
To come to a right understanding
of that kind of issue,
we will require a little more digging into
the structure of these distributions,
into the probabilistic foundations.
The rewards are going to be rich.
We are not only going to be able
to characterize the quality of
polling mechanisms very precisely, but
in the process we are going
to be able to discover deep,
underlying principles which apply
to the theory of chance as a whole.
So this then is our next subject,
the subject of the fabulous limit laws.
Let us begin by revisiting polling
mechanisms to set the stage.
So, the student will recall that
we were dealing with in abstract,
a dichotomous population.
For example, two political parties,
which dominate a particular landscape.
Two creatures fighting for supremacy in,
let's say a certain part of the world.
In a production process,
the distribution of defective and
non-defective merchandise,
widgets and so on.
They see in the setting, you know,
one can imagine actually a physical
population of two kinds of objects.
Say, call them 1 and 0, or
defective and non defective,
for example, and
they exist in a certain proportion,
let's say p is a fixed but unknown
proportion of objects of type 1 now.
And let's say q,
we go to 1 minus p is the fixed but
unknown proportion of objects of type 0.
Our objective is to try to estimate p and
q, and, the idea,
of course,
is by a random sampling methodology.
If one selects randomly from
the population at a whole,
as a whole, one gets members from 1 and
0, chance driven.
And, of course,
we realize that this is exactly what
one gets when one tosses a coin.
And so, we are dealing with
a setting where we have,
as a sampling mechanism,
a Bernoulli variable.
So the basic idea here is we start
with an underlying population.
We generate a random sample by
repeated independent trials.
So here's a picture you've seen before, a
table you've seen before, which shows you
a sample of size 12, of which 3 of the 12
members were 1 and the remaining 9 were 0.
Naturally enough, this is an abstraction
of an underlying process, right?
And we quickly move on to a generic
sample of size n which corresponds
to a repeated independent sampling,
in other words, a sequence of coin tosses,
where the coin of that fixed but
unknown bias probability, p.
And now one obtains a random sample,
X1 through Xn.
We very quickly summarize the impact of
the sample by forming an accumulated sum.
Set S sub n represents
the sum of the Xs and
of course, this tells you how
many 1s there were in the sample.
And the moment we have this S sub
n in hand we now have a principle,
a guiding strategy.
We posit that the fraction of
1s in the sample, the relative
frequencies of successes in the sample,
in other words, Sn divided by n.
Is a good surrogate for the fraction of
1s expected in the general population.
In other words,
we posit that Sn over n may be a good
target estimate for
the fixed, but unknown, p.
This leads us into digging into
the in odds of the distribution of
the cumulative successes.
And that led us to discover the binomial
distribution, and the properties of
the binomial distribution give us several
reasons for optimism for this procedure.
The student will recall seeing
a figure like this before,
which shows you the mass function
associated with the relative
frequency of successes Sn over n,
plotted for different values of n.
To begin, the maximum likelihood
principle articulated by RA Fisher,
gives us grounds for
a belief that Sn over n actually is
a right choice as an estimate for p.
The student will recall that the maximum
likelihood principle embeds, says,
what value of bias best
explains the observed data, Sn?
And a, an elementary calculation involving
the binomial probabilities showed us
that in fact the largest a posteriori
probability of obtaining the observed
number of successes, as said.
Arises when we guess that
the bias is exactly the relative
frequency of successes
of the sample Sn over n.
So in other words, Sn over n is the
maximum likelihood estimate of the bias p.
A second reason for optimism rises
from the form of the mass function.
The mass function of Sn over n
attains its maximum precisely
in the immediate
neighborhood of the value p.
And so, again, this gives us
the largest probability, if you will,
of getting that kind of value.
Yet another reason for belief in
the procedure arises from the site of
centrality captured by expectation,
the probabilistic center of mass.
The expected value of Sn
over n is also exactly p.
And finally, observing the nature
of the graphs as n increased
shows us that the binomial
distribution gets increasingly
concentrated around its center.
They can put this in a more formal
context by looking at the calculation of
the variance or the expected squared
deviation of Sn from its expected value.
The variants of Sn
increases linearly with n.
It behaves like n times p times q.
Or, since the variance captures
expected square deviation from,
from the center, if we look at
the square root of the variance, or
the standard deviation, divided by N,
that should give us then,
the per unit, or
per samples spread around the center.
And that, by a simple calculation,
now becomes the square root of pq
divided by the square root of n.
This tells us that the per
unit spread around p
decreases like 1 over the square root
of n, as our sample size n increases.
In other words, we brought increasing
concentration near p, and this gives us
yet one more reason to believe that
Sn over n is a good estimate of p.
All of these give us good,
solid principle reasons for
optimism in our suggested procedure.
But they are still lacking and
where they lack is in giving us
any quantitative guarantees.
How good, really,
is the approximation Sn over n to p?
What kinds of errors
are we likely to make?
Or, a more concrete question,
for a given size of n,
for a given sample size,
what is the likely error?
Now, digging into these aspects
opens up a very rich and
fecund region for exploration.
And so, this is what is going to
be on our target for this tableau.
Before we dive into the analysis,
it will be wise to step back, and
take a look at
the assumptions of the model.
Come to a good understanding
of what we are expecting, and
what the main features are.
Let's turn to this test before we return
to an analysis of this procedure.

