So far so good.
Amusing but perhaps not of so much moment.
After all,
we have seen the similar expansions very
early on in our mathematical experience.
And this is just a binary or
a dyadic expansion along the same lines.
But here's where.
It might strike a chord in our memory.
Where have I seen such expansions
before in a different context?
Ha.
Didn't I see such expansions as
the outcomes of my idealized
chance experiment in tossing
a coin repeatedly without end?
Hm, this suggests now that there's
a connection between these
number theoretic ideas and
the representations of real numbers, and
the sample points of my idealized
coin tossing experiment.
This allows us to complete
the loop triumphantly.
Recall again the experiment is as follows.
We toss a coin repeatedly.
By the identification of a head with
a number 1, and a tail the number 0,
the sample points of the experiment
are infinite sequences of 1s and 0s.
But every such infinite sequence,
x1 x2 x3 of 1s and
0s corresponds to a real number x.
In its dyadic expansion, and
therefore to each such sequence,
I could allocate a real number, x.
This number x is a value between 0 and 1,
and in a more compact, usual notation,
we'll write this as simply,
the unit interval, 0 to 1,
enclosed in square brackets.
Now, we just restated
the problem through this
beautiful correspondence with numbers.
And we've certainly got a very compact
representation, the unit interval.
This coin tossing experiment
can be identified
with the unit interval on the real line.
Isn't that surprising?
Isn't that beautiful?
Now, the moment you've got
such a compact representation,
other things start falling into place.
What can I say about the Event A that,
that first head occurred
on the fourth toss?
Well, it's all binary strings where
the first three elements are 0,
the fourth element is a 1, and
the remaining elements are arbitrary.
Now what does it mean?
It means I'm looking at
numbers with a decimal point,
point followed by three 0s, 1.
Followed by what?
Well, at one end of the spectrum, I could
imagine followed by a sequence of tails.
An infinite sequence of tails.
In our new language,
that corresponds to the sample point,
0, 0, 0, 1 and 0s repeated at infinitum.
Okay, that gives me a bonafide x,
which is in my a.
At the other end of the spectrum,
I could start the sequence with a 0,
0, 0, 1, and
follow it with an infinity of heads.
In other words, repeating ones.
Now I notice that the binary numbers,
the x corresponding to each of these,
is at two ends of a spectrum.
All other sample points in A,
starting with 0, 0,
0, 1,
followed by some sequence of 0s and 1s.
Will lie in between these two extremes,
and
therefore I can identify A very compactly.
A is equivalent to
the set of real numbers.
X, which are such that the dyadic
expansions of x lie between two extremes.
On the left, 0.0001, recurring 0s.
On the right, 0.0001, recurring 1s.
And it's clear that any
other possibility for
the fifth toss onwards will give
me a number in-between them.
Oh this is nice, okay now I've got a,
a compact representation for what
this means, but the notation
0.00010 recurring 0.0011 recurring.
It's a little mysterious.
We should dig out and
see what does this actually mean.
So, let's start with the,
with the left-hand side.
0.0001, 0 recurring,
is just a short form for
2 to the power minus 4 or one-sixteenth.
Easy.
What about the next one?
Pause and write it down and
see if the dyadic expansion
rings a bell before you proceed.
Did you discover this?
Writing down 0.0001 with 1 recurring.
Just means in the dyadic expansion 2 to
the power minus 4 plus 2 to the power
minus 5 plus 2 to the power minus 6 and
so on, ad infinitum.
But again we discover our old friend,
the geometric series.
This sums to in the numerator,
2 to the power minus 4.
In the denominator,
1 minus 2 to the power minus 1.
In the numerator I get 1 over 16,
in the denominator I get 1 over 2.
The ratio then gives me 1 over 8.
And I've discovered really a beautiful,
magnificent
description of the event A that the first
head occurs on the fourth toss.
It corresponds to all real numbers
sandwiched in the interval from
one-sixteenth on one side to
one-eighth on the other, or
in the compact notation
that we're used to,
it is just the closed interval from
one sixteenths to one eighths.
Oh, we can't ask for
a much more beautiful and
compact representation of this
apparently abstruse problem.
Now, we could proceed from here directly,
but in a setting like this,
some geometrical attributes
makes things really vivid.
A picture, as they say,
is worth a 1,000 words.
What are we dealing with here?
We're dealing with the unit
interval on the real line.
Let's plot this.
Now in that unit interval,
that's a segment of a line,
we are looking at a sub interval.
The sub interval starts at the point
one-sixteenth on the left.
And precedes to the right
to the point one eighths.
This is a continuum of real values.
Any point in this continuum of
real values corresponds via
our dictionary of translation
via our the dyadic expansion.
To a particular sequence of heads and
tails.
And therefore corresponds to a sample
point in the underlying experiment.
We should very quickly test this out,
okay?
So what is another example of a point
somewhere between the two extremes?
Well perhaps I could say,
well I have three tails, a head, and
a head immediately after that,
followed by an infinite string of tails.
Now what does that mean?
Well that means we have the number 0.00011
followed by a recurring sequence of 0s.
Of course, this just means, 2 to the power
minus 4 plus 2 to the power minuss, and
we say, oh yeah, this is easy.
I get this.
And this gives me a point which is
exactly at the center of the interval.
This is one such sample point.
Of course, you could imagine let your,
your fancy plain numerals here.
You could imagine any sequence of 0s and
1s following the 1 in the fourth place.
And any such sequence
will give me a point.
A real number somewhere in the.
This is beautiful, this is elegant.
We really couldn't ask for more.
But we do, right?
How does one go about constructing
a probability measure?
Well the moment one has
constructed such a simple and
elegant representation for the problem.
Then the construction of the measure
becomes not trite perhaps,
but perhaps evident.
Well, let's take a look here, remember
the point, the unit interval represent
infinite strings of coin tosses,
but the coin is assumed to be fair.
And therefore all strings
are statistically identical.
There's no reason for
one to be favored over the other.
Another way of thinking
about this is this.
If you take a string of heads and tails,
and you swap the heads with tails and
tails with heads.
Then you'll get new strings
which have the same character.
From a statistical point of view.
Heads and tails are equally
likely in this experiment.
Another way of thinking about this
is we want to appeal to symmetry.
If you look at all the sequences in
the first quartile of the unit interval.
They correspond to seven
sequences of heads and tails.
If you simply alter,
replace heads by tails and tails by heads.
Replace 1s by 0s and 0s by 1s.
You end up with a corresponding quartile
at the right end of your unit interval and
therefore, this is another way of saying
the first quartile by symmetry should be
as likely as the last quartile, in fact
any quartile should have the same chance.
And all of these should have
the chance one in four.
Generalizing very quickly based on this
idea we say that the probability of
any interval, must in fact be proportional
to the length of that interval.
A quartile, an interval of length
one-fourth must have a chance one-fourth,
an interval of length one half,
wherever positioned.
Should have a chance of one half.
An interval of length one-sixteenth,
wherever positioned,
should have chance one-sixteenth.
And immediately this gives us
now a probability measure.
The probability of the particular event A,
that the first head occurred
at the fourth toss,.
Corresponds to the length of the interval
from one-sixteenth to one-eighth,
and the length is one-eighth
minus one-sixteenth, or 1 in 16.
We now have a formulation
which allows us to compute probabilities
in a principled and systematic way.
This procedure is,
satisfies positively, positivity tritely,
because length is positive.
It satisfies normalization, the unit
interval in this case has got length 1.
Probability 1.
And additivity is trite
because length is additive.
The length of two disjoint pieces
is the sum of the two lengths.
And now we have a procedure which
allows us in principle to move
beyond the setting of the discrete space,
and to move into continuous spaces.
To summarize what have we discovered here.
First,.
We have discovered, against the odds,
from a coin tossing experiment,
admittedly a conceptual coin tossing
experiment, a continuous sample space.
The outcome's conceptual
of this experiment,
take on a continuum of real values,
in this case, in the unit interval.
The basic events of interest here,
are naturally intervals.
Intervals are the fundamental
units in the continuum.
And, how do we allocate mass or
probability to intervals?
Well, we simply make it,
in this instance, proportional
to the length of the interval.

