Let's try one more problem.
And this time,
let's start with events A, B, and C and
combine some of them to
create a composite event.
What does independence of A, B, and
C say about a new event,
let's say A union B and an event C.
Again, naturally enough, we anticipate
that if A and B are independent of C,
then A union B should also
be independent of C, surely.
Pause and see if we can verify this.
Again, you would need additivity in
the guise of inclusion and exclusion.
You will need the distributive
property of intersections over unions
and you will need the definition of
independence as a rule of products.
Pause a lecture and see if we can make
headway before you restart and resume.
So surely if we want to check the truth or
falsity of whether A union B is
independent of C, we should start
immediately by looking at the intersection
of the event A union B would say.
The distributive property of intersections
over unions will allow us to write this
as the probability of
the union of two sets,
A intersection C with B intersection C,
okay?
So now we have two sets and
they are union.
Can we do something with this?
Independence at heart says something
about the probabilities of conjunction
of intersections.
On the right, I've got a union of
two objects, so, and actually I
would rather replace that expression by
an expression involving intersections.
And additivity through
the guise of inclusion and
exclusion that we saw in
tableau of five shows the way.
Remember if you have two events,
the probability of the union can be
obtained by summing the raw probabilities.
But that will give you an over count and
taking away the intersection
of probability.
And therefore,
we now have an identity of the right.
The first two terms enroll intersections.
And the third term, also is enrolled
in intersection but C appears twice.
We should do a little algebraic cleaning
since the intersection that has settled
itself is itself.
So we obtain now is simple identity.
The probability B intersection C, added
to the probability of B intersection C,
from which you take away the probability
of A intersection B, intersection C.
And now, things are right for
independence.
On the right, you have expressions
involving only intersection probabilities,
and because independence
gives you product rules for
all possible conceivable intersection
combinations, we now have a factor.
I'll take a look at this and say well,
okay, various tens are common here.
The term the probability of C
appears at each term on the right.
As mentioned of factorization takes
it out of those terms, leaving in
round brackets in front, the probability
of A plus the probability of B,
from which you take away the probability
of A, A times the probability of B.
All of these, of course,
multiplied by the probability of C.
All right, let's take a look at
the terms in round brackets.
The third expression there is
the probability of A times
the probability of B.
But if A, B and C are independent then
surely A and B are independent and
this is in fact the first
relation that must be satisfied.
The probability of A times
the probability of B is
exactly identified with
the probability of A intersection B.
This is what independence gives us for
the pair A and B.
All right, you look at it and
say well, this man is, is going crazy.
He is taking something which
is neatly factored out, and
he's now assembling it again and
making it more complex.
But, of course,
there is a method in the madness.
Let's take a good hard look at
the term in their own brackets.
Does that look familiar?
Look at the second line.
We have an expression just like that,
and introspection tells us
the thing the round brackets
represents a probability of a union.
This was additivity via inclusion and
exclusion.
The term in round brackets is exactly
the probability of A union B.
We're done.
Take a look at the left,
you have the intersection of two events,
a composite event would say.
On the right we have
a product of probabilities,
that are the composite event
with a probability of C.
We conclude inevitably,
that A union B, is independent of C.
And this argument now works
beautifully and generally.
Let's abstract the core principles
that we've seen in our two examples.
The first example said that if
we have independent events and
you complement to one or more,
you still get independent events.
The second example says in words
that if you have independent events,
and you form new events
from some subgroup,
then these new events are independent
of those that were left out.
And here now is a basis for
a beautiful compact slogan.
And this now is true, more generally
when you have independent events which
are finite in number or
even countably infinite in number.
Suppose we have independent
events are not, are not yet
defined independence for more than
three but we shall do so shortly, but
let us anticipate our setting, imagine
we have a family of independent events.
Suppose you break them up into groups,
which are disjoint, which do not overlap.
And from each group,
you construct a new event.
These new events are engendered
from non-overlapping collections
of these independent events.
Then our conclusion is that all these
new events so constructed from distinct
disjoined subsets of independent
events are also independent.
The moment you have independence,
it's preserved under such manipulations.
This turns out to be a very powerful and
general principle.
And of course it's comforting that this
is exactly what we'd anticipate in our
vague use of independence
in ordinary language.
If there are things out there which
are independent of things out there,
then results which depend upon
these phenomena are independent
of results which depend
upon these phenomena.
And a mathematical demonstration
captures that intuition beautifully.
Our next step is to abstract and
move beyond three events to
a large family of events.

