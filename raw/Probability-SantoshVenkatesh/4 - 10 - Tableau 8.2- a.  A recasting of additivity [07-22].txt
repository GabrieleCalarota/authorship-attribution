[MUSIC]
Tableau 8, Part 2.
Additivity, the theorem
of total probability.
In our previous tableau,
we built in a formal
edifice in folding inside
information into a chance process.
And this led to a formal definition of
the notion of conditional probability.
When allied with the axiom of additivity,
this gives rise to a very potent and power
principle which sometimes goes by the name
of the theorem of total probability.
Let us begin by
recapitulating the key idea
that was encapsulated in the notion
of conditional probability.
To begin, there is a target event,
H, of interest to us represented
abstractly in a Venn diagram
by that egg in your figure.
Side information is encapsulated
via the specification
of an ancillary event A of
positive probability.
The conditional probability
that H occurs given that
A has occurred is then
defined to be the ratio of
the probability of the intersection
of H with A to the probability of A.
In words, this is the proportionate
part of H that lies in A.
If we clear the denominator by multiplying
both sides by the probability of A,
then we get an equivalent formulation
which is equally evocative.
The probability of the conjunction, or
the intersection, of the two events,
H and A,
is given by a product of probabilities.
A conditional probability of H,
given A, and the probability of A.
Conditioning provides us,
in this perspective, a way of chaining,
or building up, conjunction probabilities,
systematically via
the conditioning process.
Which viewpoint is better?
Well, it depends upon the circumstance.
In the elementary settings we've seen so
far, we've begin by categorizing a chance
experiment by specifying the sample space,
the space of possible idealized outcomes,
the events of interest to us, the subsets
of interest to us in that space.
And finally, a probability measure,
which in finite settings or in countable
settings, we specify by specifying
the mass function of atomic probabilities.
Now, once a probability measured
is specify, then one can go ahead.
And using axioms of additivity,
compute probabilities of events directly.
So in such settings,
it might be easier to directly
compute a conjunction probability.
In other settings however,
the probability measure
is not specified explicitly,
but implicitly.
And in such settings,
it turns out that conditional arguments
are much easier to digest and compute,
and then we think of conditional
probabilities as an avenue towards
conjunction probabilities.
The perspective depends upon
the particular problem at hand.
Now, let us take the second of the two
equations defining conditional probability
as a starting point and see how,
by folding in additivity to the process,
a very rich and subtle principle emerges.
So, once again, we have a target event H,
side information encapsulated in
an event of positive probability A.
Now of course, the picture is evocative.
A is captured by the green shaded
region on the left of your figure.
But of course the moment you specify A,
then,
there are supple points left out of A.
This collection comprises
the complement of A, and together,
A and A complement, partition
the entire space of possibilities.
Either an outcome is an A or
it is not an A.
Since A and A complement partitioned
the entire space, A for sure I,
for even stronger reason,
they also partition H into two parts.
And we can write this down formally
using intersections as follows.
H can be decomposed into
two disjoint pieces.
That portion of H that is common to A,
H intersection A.
And that portion of H which is outside A,
that portion of H which is
common to A compliment.
The union of these two pieces
then comprises all of H.
The whole is equal to the sum
of the disjoint parts.
Now this perspective is alluring,
because now,
we have managed to decompose
H into disjoint pieces.
And additivity of probability
measure promptly rushes to the fore.
The probability of H
then can be written as
the sum of the probabilities
of the disjoint parts.
The probability of A, H intersection A,
together with the probability of H
intersection, the compliment of A.
So far so good.
But, we now have a way,
a process of representing
conjunction probabilities by chaining
with conditional probabilities.
We'll replace the first conjunction
probability by conditional probability
times the probability of A, but it can
replace the second conjunction probability
likewise, where now a complement
stands in the role of A.
And now, we've got a chained equation.
Look at it.
Absorb it.
This simple structure carries subtlety and
nuance far beyond its
apparently simple name.
The value of such
a decomposition arises if,
in an experiment, one can find a suitable
decomposition of the underlying space.
And I shall promptly illustrate this for
you, with a couple of examples.
We should begin of course,
with something simple.
And so we'll go back to one of our usual,
our all regulars.
Or go back to the game of dice.
And we'll build on this
to build intuition and
understanding of how an identity like
this can prove to be profitable.
And build upon this with a second
example of some subtlety,
some nuance, and some importance.

